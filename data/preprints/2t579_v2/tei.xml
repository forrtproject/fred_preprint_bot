<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Artificial Intelligence Circumvents Identity-Driven Biases in Source Selection</title>
				<funder ref="#_sG6Rr5m">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
				<funder>
					<orgName type="full">Google, Inc.</orgName>
				</funder>
				<funder ref="#_ydnc4FM">
					<orgName type="full">Templeton World Charity Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2025-10-14">14 October 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Laura</forename><surname>Globig</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology &amp; Center for Neural Science</orgName>
								<orgName type="institution">New York University</orgName>
								<address>
									<settlement>NY</settlement>
									<region>NY</region>
									<country>US</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rachel</forename><surname>Xu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Jigsaw (Google LLC)</orgName>
								<address>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hamza</forename><surname>Alshamy</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Center for Data Science</orgName>
								<orgName type="institution">New York University</orgName>
								<address>
									<settlement>NY</settlement>
									<region>NY, US</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Jay</forename><forename type="middle">J</forename><surname>Van Bavel</surname></persName>
							<email>jay.vanbavel@nyu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology &amp; Center for Neural Science</orgName>
								<orgName type="institution">New York University</orgName>
								<address>
									<settlement>NY</settlement>
									<region>NY</region>
									<country>US</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Norwegian School of Economics</orgName>
								<address>
									<settlement>Bergen</settlement>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Artificial Intelligence Circumvents Identity-Driven Biases in Source Selection</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-10-14">14 October 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">4AD7F27EE42AE85B8A71BDCFB1EB53C6</idno>
					<idno type="DOI">10.54224/31570)</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-10-21T13:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Artificial Intelligence</term>
					<term>Information-Seeking</term>
					<term>Source Selection</term>
					<term>Social Identity</term>
					<term>Partisan Bias</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Social identity profoundly shapes whom people choose as information sources, constraining exposure to diverse perspectives. While people are motivated to seek accurate information, they systematically avoid outgroup sources even when group membership is irrelevant to the task at hand. Here we investigate whether artificial intelligence (AI) can circumvent these identity-driven biases in source selection. In Study 1, a nationally representative sample of American adults (n = 1,054) preferred AI over human sources when seeking information about political conflicts. In Study 2 (n = 284), an incentivized political fact-checking experiment revealed that participants preferred AI sources over outgroup (d = 0.470) and even ingroup (d = 0.230) partisan sources, despite recognizing they were of equal competence. In Study 3 (n = 277), using an identity-irrelevant shape categorization task, participants only preferred AI over outgroup sources (d = 0.191), with no difference between AI and ingroup sources. Computational modeling revealed that these preferences emerge through selectively accumulated evidence against partisan advisors during deliberation, rather than differences in priors. These findings suggest that AI's perceived neutrality enables it to bypass identity-based discrimination. These results highlight the potential of AI to reduce echo chambers and broaden epistemic exposure by serving as an identity-neutral conduit for information acquisition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>In today's information-rich world <ref type="bibr" target="#b2">(Castells, 1996)</ref>, access to knowledge has never been easier, yet people's actual information environments remain surprisingly narrow <ref type="bibr" target="#b4">(Del Vicario et al., 2016;</ref><ref type="bibr" target="#b26">Sunstein, 2018)</ref>. A central, often overlooked, determinant of what people ultimately learn is who they seek information from. As errors are costly and humans are motivated to maximize their expected utility <ref type="bibr">(Mongin, 1998;</ref><ref type="bibr">Von Neumann &amp; Morgenstern, 2007)</ref>, this decision shouldin principle -be governed by the (perceived) accuracy of potential information sources <ref type="bibr" target="#b1">(Bromberg-Martin &amp; Sharot, 2020;</ref><ref type="bibr">Madsen et al., 2024;</ref><ref type="bibr" target="#b22">Sharot &amp; Sunstein, 2020)</ref>. In practice, however, source selection is often also shaped by factors beyond accuracy, such as social identity <ref type="bibr">(Van Bavel &amp; Pereira, 2018)</ref>. People actively discount outgroup sources <ref type="bibr" target="#b13">(Marks et al., 2019;</ref><ref type="bibr" target="#b33">Zhang &amp; Rand, 2023)</ref> and prefer those who are similar to them <ref type="bibr">(Mullen et al., 1992;</ref><ref type="bibr">Zou &amp; Xu, 2023)</ref>, prestigious figures <ref type="bibr">(Van Noord et al., 2023)</ref>, or members of majority groups <ref type="bibr">(Boorman et al., 2013;</ref><ref type="bibr" target="#b13">Marks et al., 2019;</ref><ref type="bibr">Otten, 2016)</ref> as information sources, even when group identity is irrelevant to the information sought <ref type="bibr" target="#b4">(Del Vicario et al., 2016;</ref><ref type="bibr" target="#b26">Sunstein, 2018)</ref>. Such outgroup derogation constrains information acquisition, narrows exposure, and contributes to polarization <ref type="bibr">(Cikara &amp; Van Bavel, 2014;</ref><ref type="bibr">Hobson &amp; Inzlicht, 2016;</ref><ref type="bibr">Molenberghs, 2022)</ref>, demanding urgent solutions. We examine whether Artificial Intelligence (AI) can overcome these partisan biases by offering a neutral source of information.</p><p>Even when people can accurately assess which sources are competent, they often fail to act on this knowledge. For instance, individuals sometimes share misinformation on social media despite being able to distinguish true from false content when asked directly <ref type="bibr" target="#b17">(Pennycook et al., 2021)</ref>. Similarly, people may continue to seek advice from familiar or identity-congruent sources even when these sources are demonstrably inaccurate <ref type="bibr" target="#b11">(Kim &amp; Kim, 2021;</ref><ref type="bibr" target="#b15">Metzger et al., 2020)</ref>. These dissociations suggest that the processes guiding accuracy detection and source selection are partially independent: people can recognize which sources are more competent yet still choose based on identity concerns, social rewards, or motivated reasoning. As a result, identity-driven biases in source selection persist even in contexts where accuracy is incentivized and evaluative judgments are intact.</p><p>Whereas information used to be exclusively provided by other humans -be it in person or indirectly through different media -this is now being supplemented by AI. AI tools have seen explosive adoption. For example, OpenAI's ChatGPT reached 100 million users within two months of its launch, making it the fastest-growing consumer application in history <ref type="bibr" target="#b0">(Blunt, 2025)</ref>. Early research indicates that people may be especially receptive to information from AI <ref type="bibr" target="#b3">(Costello, 2025;</ref><ref type="bibr" target="#b23">Stapleton et al., 2022;</ref><ref type="bibr" target="#b31">Xu et al., 2025)</ref>. As yet, however, it remains unclear how people's information-seeking preferences differ when consulting human versus AI sources, and whether AI can reduce the identity-driven biases that constrain human-to-human advice seeking. To fill this gap, we now investigate how AI interacts with existing source selection preferences, and examine the processes underlying source selection decisions.</p><p>People actively seek information to reduce uncertainty <ref type="bibr">(Hofmann et al., 2009;</ref><ref type="bibr">Schrah et al., 2006;</ref><ref type="bibr">Yaniv &amp; Kleinberger, 2000)</ref>. In doing so, they strive to improve their mental models of the world around them <ref type="bibr" target="#b22">(Sharot &amp; Sunstein, 2020)</ref>. For instance, they might read political commentary to refine their understanding of societal values, ask a psychologist for an explanation for their own emotional responses, or seek social feedback to clarify how they are perceived by others. Beyond the specific type of information sought <ref type="bibr" target="#b10">(Kelly &amp; Sharot, 2021)</ref>, the initial choice of who to consult is a crucial first step that profoundly shapes what people ultimately learn. Prior work illustrates that this decision is shaped by both accuracy goals <ref type="bibr" target="#b1">(Bromberg-Martin &amp; Sharot, 2020)</ref> and social identity goals, such as belonging, status, and moral validation <ref type="bibr">(Van Bavel &amp; Pereira, 2018)</ref>. For instance, people tend to seek information from sources that align with their partisan identity <ref type="bibr" target="#b11">(Kim &amp; Kim, 2021;</ref><ref type="bibr" target="#b12">Knobloch-Westerwick, 2012;</ref><ref type="bibr" target="#b15">Metzger et al., 2020;</ref><ref type="bibr" target="#b25">Stroud, 2010;</ref><ref type="bibr" target="#b30">Winter et al., 2016)</ref>. Democrats often prefer CNN while Republicans favor Fox News <ref type="bibr">(Hawkins &amp; Nosek, 2012;</ref><ref type="bibr">Rosentiel, 2009)</ref>.</p><p>Partisan preferences shape not only media consumption <ref type="bibr">(Cikara &amp; Van Bavel, 2014;</ref><ref type="bibr" target="#b16">Molenberghs, 2013</ref><ref type="bibr">Molenberghs, , 2022;;</ref><ref type="bibr">Morrison et al., 2012)</ref> but also beliefs about political facts <ref type="bibr">(Kahan, 2015;</ref><ref type="bibr">Rathje et al., 2023)</ref> and policy preferences <ref type="bibr">(Geerlings et al., 2017;</ref><ref type="bibr">Tribukait, 2021)</ref>. By reinforcing only congenial perspectives, these habits create curation bubbles <ref type="bibr">(Green et al., 2025)</ref> and even echo chambers <ref type="bibr" target="#b4">(Del Vicario et al., 2016;</ref><ref type="bibr" target="#b26">Sunstein, 2018)</ref> with profound epistemic consequences. Over time, selective sampling may narrow viewpoints, reinforce prejudice and polarization <ref type="bibr">(Druckman &amp; Levy, 2022)</ref>, undermine trust in democratic institutions <ref type="bibr">(Pasek et al., 2022)</ref>, and foster voter apathy <ref type="bibr">(Ahn &amp; Mutz, 2023;</ref><ref type="bibr">Crepaz, 1990;</ref><ref type="bibr">Fivaz &amp; Nadig, 2010;</ref><ref type="bibr">Moral, 2017;</ref><ref type="bibr">Phillips, 2024;</ref><ref type="bibr">Snyder III, 2011)</ref>.</p><p>Identity-driven source selection is not confined to identity-relevant contexts. Even in simple perceptual judgments, people prefer politically similar advisors over dissimilar ones <ref type="bibr" target="#b13">(Marks et al., 2019;</ref><ref type="bibr" target="#b33">Zhang &amp; Rand, 2023)</ref>. In educational contexts, students often resist seeking information from perceived outgroup sources, constraining academic development and engagement with diverse ideas <ref type="bibr">(Dion et al., 1972;</ref><ref type="bibr">Nisbett &amp; Wilson, 1977;</ref><ref type="bibr">Schuchart et al., 2021;</ref><ref type="bibr">Thorndike, 1920)</ref>. Identity thus operates at a fundamental cognitive level, alongside accuracy. Identity value biases processing across the brain <ref type="bibr">(Pereira et al., 2023)</ref>, creating a systematic tilt in whom people choose to hear from by making ingroup information feels more rewarding, and outgroup information aversive <ref type="bibr">(Hackel et al., 2017;</ref><ref type="bibr" target="#b28">Van Bavel et al., 2008)</ref>.</p><p>When encountering outgroup sources, people exhibit negative affective <ref type="bibr">(Iyengar et al., 2012;</ref><ref type="bibr" target="#b9">Iyengar &amp; Westwood, 2015)</ref>, cognitive <ref type="bibr">(Cikara et al., 2014;</ref><ref type="bibr">Mullen et al., 1992;</ref><ref type="bibr">Tajfel et al., 1971)</ref> and neural responses <ref type="bibr">(Cikara &amp; Van Bavel, 2014;</ref><ref type="bibr">Falk et al., 2012;</ref><ref type="bibr">Hobson &amp; Inzlicht, 2016;</ref><ref type="bibr" target="#b16">Molenberghs, 2013</ref><ref type="bibr">Molenberghs, , 2022;;</ref><ref type="bibr">Molenberghs &amp; Louis, 2018;</ref><ref type="bibr">Morrison et al., 2012)</ref>. This aversion reduces their willingness to sample from outgroup sources <ref type="bibr" target="#b13">(Marks et al., 2019;</ref><ref type="bibr" target="#b33">Zhang &amp; Rand, 2023)</ref>. These findings reveal a pervasive epistemic constraint: social identity systematically biases the selection of information sources, narrowing the evidence people encounter before any belief updating can even occur.</p><p>Importantly, intergroup biases may operate automatically, making them especially resistant to correction <ref type="bibr" target="#b5">(Devine, 1989;</ref><ref type="bibr" target="#b8">Greenwald &amp; Banaji, 1995;</ref><ref type="bibr" target="#b32">Yudkin et al., 2016)</ref>. Even explicit efforts to debias, such as incentivizing accuracy or urging impartiality, frequently fail to eliminate identity-driven avoidance, indicating that these group biases can persist as reflexive, ingrained responses <ref type="bibr" target="#b33">(Zhang &amp; Rand, 2023)</ref>. The social reward and threat dynamics underlying group identity biases are deeply ingrained at a neural level, meaning that conventional interventions have only limited impact on these automatic tendencies <ref type="bibr">(Krajbich, 2022;</ref><ref type="bibr">Lai et al., 2014)</ref>.</p><p>AI has begun to supplement, and in some domains replace, human sources, opening up new possibilities for source selection. Recent work suggests that large language models (LLMs) can act as new kinds of intermediaries for information-seeking. Unlike partisan human sources, LLMs distill vast corpora of human knowledge into accessible explanations, potentially reducing epistemic fragmentation and offering individuals shared evidence that informs expert judgment <ref type="bibr" target="#b3">(Costello, 2025)</ref>. To understand the implications of this new class of sources for human decision-making, we ask: Can we utilize AI to reduce identity-driven avoidance at the very first step: source selection?</p><p>We speculate that unlike humans, AI systems are not (yet) inherently embedded in social group hierarchies and may thus be perceived as more neutral during source selection <ref type="bibr" target="#b14">(Messeri &amp; Crockett, 2024)</ref>. Advances in large language models (LLMs) enable complex reasoning and human-like dialogue <ref type="bibr">(Antikatzidis et al., 2024;</ref><ref type="bibr">Argyle et al., 2023;</ref><ref type="bibr">Bail, 2024;</ref><ref type="bibr">Costello et al., 2024)</ref>. Recent studies suggest that AI usage can result in improvements to information environments: motivating news consumption <ref type="bibr">(Askari et al., 2024)</ref>, fostering more constructive political discussion <ref type="bibr">(Argyle et al., 2023)</ref>, and reducing conspiracy beliefs <ref type="bibr">(Costello et al., 2024)</ref>. Together, these findings suggest that AI could preserve sensitivity to accuracy while blunting identity-based avoidance in source selection.</p><p>We therefore test whether AI can mitigate information selection bias in both politically sensitive and politically neutral contexts. We hypothesize that (1) people will prefer AI over human sources when given the choice; and (2) that this preference will be particularly pronounced relative to humans that belong to a political outgroup. Computationally, these identity-driven biases could operate at different stages of the decision process. People might enter choices with an initial predisposition toward one type of source (a starting point bias), or they might selectively accumulate evidence in favor of some sources over others during deliberation (a drift rate bias). These dual pathways can yield the same observable behavior while reflecting distinct underlying mechanisms <ref type="bibr" target="#b18">(Ratcliff, 1978;</ref><ref type="bibr" target="#b19">Ratcliff &amp; McKoon, 2008;</ref><ref type="bibr" target="#b29">Voss et al., 2013)</ref>. Distinguishing between them is theoretically important because it reveals whether identity shapes source selection through reflexive priors, deliberative evidence processing, or both.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overview:</head><p>Across 3 studies, we investigated source selection preferences for information-seeking. In Study 1, we examined whether people prefer AI over human sources when choosing who should explain a political conflict to them in a representative sample of 1,054 Americans across multiple generations (18-65 year olds). We then ran two pre-registered behavioral experiments with politically-balanced American samples to assess whether people also preferred AI as an information source over partisan sources. In Study 2 (n = 284), we compared their information-seeking preferences for AI relative to ingroup and outgroup sources, using a highly identity-relevant political fact-checking task. In Study 3 (n = 277), we then assess whether the results from Study 2 generalize to an identity-irrelevant shape-categorization task (adapted from <ref type="bibr" target="#b13">Marks et al., 2019)</ref>. We used drift-diffusion modeling to decompose the cognitive processes underlying participants' choices. Pre-registrations, code, and data are all available on the Open Science Framework (OSF).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants prefer learning about a political conflict from AI vs humans (Study 1).</head><p>To examine whether participants prefer AI rather than human sources for politically sensitive information, we first surveyed a nationally representative sample of 1,054 adults in the United States. The sample was quota-matched by age, gender, and ethnicity to reflect the broader population. Participants completed an online survey in which they indicated whether they would prefer a human or an AI system to explain a political debate to them (see Figure <ref type="figure" target="#fig_0">1a</ref>). The majority of participants selected AI (55.6%) over a human (44.4%) as their preferred information source (see Figure <ref type="figure" target="#fig_0">1b</ref>). An exact binomial test against a null of equal preference confirmed that this effect was statistically significant (p &lt; 0.001, 95% CI [0.530, 0.590]), suggesting that AI is viewed as a more neutral or competent information source for navigating politically charged, identity-relevant contexts. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Social identity does not impair source competence perceptions during highly identity relevant tasks (Study 2).</head><p>Thus far our results suggest that participants prefer learning about political conflicts from AI over humans. Political conflicts are often partisan issues, and thus tend to be highly identity-relevant. It is thus plausible that whether or not participants prefer AI over humans, depends on the latter's partisan identity. To test this, we ran a second study, in which a new group of participants completed a political fact-checking task (see Figure <ref type="figure" target="#fig_1">2</ref>). In this task, participants were asked to choose between advisors (=information sources) of varying identity (ingroup partisan, outgroup partisan, or AI) and demonstrated accuracy (accurate vs. random). Prior to the task, they completed an initial learning phase, in which they observed each source's responses, received feedback on whether the source's response was accurate or not, and rated each source's competence. Later, in the decision-making phase, participants made their own judgments, reported their confidence, and then chose which source to consult before making a final decision. They were incentivized for accuracy and could receive a performance-related bonus payment of up to $2. This setup allowed us to test whether participants could distinguish competent from incompetent sources, and whether identity biases still guided their choices even when accuracy had tangible consequences. In Study 2 participants completed a political fact-checking task in which they had to determine whether statements were true or not. This constituted a learning phase and a test phase. (a) In the learning phase, participants learned about each source's task performance. The learning phase consisted of 6 blocks (one for each source), with 10 trials each. In each trial, a new political statement was presented, for which participants would observe the source's response and receive feedback about whether that response was correct or not. The order of the blocks was randomized. (b) After the learning phase, participants completed the test phase. This consisted of four blocks with 30 trials each. In each trial, a new political statement was presented, for which participants then had to indicate whether the statement was correct or not, and indicate how confident they were in their response. They were then given the choice to ask one of two randomly assigned sources for advice and see their chosen source's response. Afterwards they were able to update their response and indicate their revised confidence rating. Sources were counterbalanced across trials and blocks.</p><p>In a first step, we examined whether participants accurately evaluated source performance. To that end we entered task performance for each source into a 3 (source identity: ingroup, outgroup, AI) × 2 (source accuracy: accurate, random) repeated-measures ANOVA. This revealed that participants accurately estimated source competence. Participants rated accurate sources (M = 77.600, SEM = 0.776) significantly more competent than random-performing sources (M = 54.400, SEM = 0.9300; (F(1, 282) = 528.800, p &lt; 0.001, ηp² = 0.650). Importantly, their perceptions of source competence did not vary as a function of source identity (F(1.990, 561.080) = 2.240, p = 0.108, ηp² = 0.008). They did not assign higher or lower competence ratings to ingroup, outgroup, or AI sources based on their identity alone (ingroup: M = 65.600, SEM = 0.929; outgroup: M = 65.200, SEM = 0.856; AI: M = 67.200, SEM = 0.903). We also did not observe a significant interaction effect between accuracy and identity (F(1.970, 554.520) = 1.220, p = 0.295, ηp² = 0.004). Thus, social identity did not impede participants' perceptions of source competence. The results hold even when controlling for political orientation and political sectarianism (see Supplementary Tables <ref type="table" target="#tab_0">1</ref><ref type="table" target="#tab_2">2</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants prefer seeking information from AI over partisan sources (Study 2).</head><p>Since participants accurately assessed source competence, they could in principle maximize their likelihood of receiving accurate advice -and thus increase their bonus payment -by basing their choices solely on competence, irrespective of source identity. However, prior work shows that people often fail to act on competence alone, instead privileging identity-congruent sources even when these are no more accurate than alternatives <ref type="bibr" target="#b13">(Marks et al., 2019)</ref>. To test this, we assessed how frequently participants chose each source to give them advice during the test phase of the task. Across trials, each source was presented as an option an equal number of times, allowing us to compute the proportion of times each was selected.</p><p>We hypothesized that participants would prefer accurate over random sources, ingroup over outgroup sources, and AI over partisan sources. Indeed, a 3 (source identity: ingroup, outgroup, AI) × 2 (accuracy: accurate, random) repeated-measures ANOVA on proportion of times a source was selected revealed a significant main effect of accuracy (F(1, 282) = 161.484, p &lt; 0.001, ηp² = 0.360), with accurate sources (M = 0.596, SEM = 0.008) chosen more often than random ones (M = 0.404, SEM = 0.008; see Figure <ref type="figure" target="#fig_3">4</ref>). We also observed a significant main effect of source identity (F(1.713, 483.02) = 39.301, p &lt; 0.001, ηp² = 0.120), such that participants preferred seeking information from AI (M = 0.575, SEM = 0.012) over outgroup sources (M = 0.422, SEM = 0.009; t(282) = 7.913, p &lt; 0.001, d = 0.470), as well as over ingroup sources (M = 0.503, SEM = 0.009; t(282) = 3.875, p &lt; 0.001, d = 0.230). Moreover, participants preferred ingroup sources over outgroup sources (t(282) = 6.097, p &lt; 0.001, d = 0.362). There was no significant interaction between source identity and accuracy (F(1.906, 537.560) = 0.583, p =0.550, ηp² = 0.002).</p><p>Together, these findings reveal that participants balance both accuracy and identity considerations during source selection. As expected, they favored ingroup over outgroup sources, replicating prior work <ref type="bibr" target="#b13">(Marks et al., 2019)</ref>. However, participants consistently preferred AI over both ingroup and outgroup sources, even when controlling for political orientation and political sectarianism (see Supplementary Tables <ref type="table">3</ref><ref type="table">4</ref>). We found no systematic differences in belief updating across source types (see Supplementary <ref type="table">Table 5-7</ref>), indicating that AI's overcome intergroup bias at the stage of source selection. Because information-seeking is the first step in the cascade that can produce curation bubbles and echo chambers, these results suggest that AI may mitigate identity-driven avoidance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Identity biases operate through selective derogation of partisan sources during deliberation (Study 2).</head><p>Importantly, the same behavioral outcome, preferring AI sources over partisan sources, may arise from different underlying processes. First, participants may have a prior ( = starting point bias) towards AI, entering the decision predisposed to choose AI before seeing what the other option even is. Alternatively, they could also have a process bias ( = drift rate bias) towards AI, selectively discounting partisan sources during the source deliberation process, once both options are presented. This again will lead to preferential source selection of AI sources. To tease apart these possibilities and better characterize how identity and accuracy shape the decision process, responses were modeled with a drift-diffusion model <ref type="bibr" target="#b18">(Ratcliff, 1978;</ref><ref type="bibr" target="#b19">Ratcliff &amp; McKoon, 2008;</ref><ref type="bibr" target="#b29">Voss et al., 2013)</ref> with the following parameters: (1) t0-amount of non-accumulation time; (2) a-distance between decision thresholds; (3) z-starting point of the accumulation process; and (4) v-drift rate. Source identity was coded with two orthogonal contrasts (ingroup vs. AI; outgroup vs. AI), and accuracy difference between both source options was entered as a trial-wise predictor; these variables modulated the parameters depending on the model specification.</p><p>We compared 16 models in which parameters were either fixed or allowed to vary by source identity and accuracy (see Supplementary Table <ref type="table">8</ref>). The simplest model assumed all parameters were fixed, while successive models allowed variation in either z or v. We then estimated models in which two parameters varied simultaneously. Finally, the full model allowed both parameters to vary as a function of accuracy and source identity. This model space enabled us to ask whether identity concerns manifest as a starting-point bias, a process bias in drift rate, or a combination of both. The Watanabe-Aikaike Information Criterion (WAIC) was calculated for each model. The best fitting yet simplest model was one in which the drift rate (v) varied as a function of source identity and accuracy. While more complex models including starting-point bias (z) parameters showed numerically similar fit, the simpler model with only drift rate variation was preferred based on parsimony (see Supplementary Table <ref type="table">8</ref>).</p><p>We observed that accuracy differences between sources influenced the drift rate, with faster evidence accumulation rates towards more accurate sources (β = 0.299, 95% HDI [0.288, 0.311]). Participants also exhibited systematic biases in drift rate parameters as a function of source identity. Participants demonstrated large negative drift rates for both ingroup (β = -0.118, 95% HDI [-0.132,-0.104]) and outgroup sources (β = -0.241, 95% HDI [-0.255,-0.227]) relative to AI. This indicates that during deliberation, participants systematically accumulated evidence against human partisan sources and toward AI sources, with the bias being particularly pronounced against outgroup sources. See Table <ref type="table" target="#tab_0">1</ref> for complete results.  Thus far, our results reveal that people prefer learning from AI over human sources for identity-relevant topics. However, prior work suggests that identity-biases also drive information-seeking preferences in identity-irrelevant contexts <ref type="bibr" target="#b13">(Marks et al., 2019)</ref>. To determine if preferential source selection of AI extends beyond identity-relevant tasks, such as political conflicts (Study 1) or the political fact-checking task (Study 2), we next sought to determine if this preference holds for tasks that are completely unrelated to social identity. To that end, we ran a third study with a new group of participants. The task was identical to Study 2 (see Figure <ref type="figure">6</ref>), but this time instead of a political fact-checking task, we used a shape-categorization task (adapted from <ref type="bibr" target="#b13">Marks et al., 2019)</ref>, in which participants had to determine whether a shape was a "blip" or not. In reality, the category was randomly assigned. This task has previously been shown to elicit partisan bias in source selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 6. Example Trials of Identity-Irrelevant Shape Categorization Task.</head><p>In Study 3 participants completed a shape categorization task, in which they had to determine whether the shape was a "blip" or not. This constituted a learning phase and a test phase. (a) In the learning phase, participants learned about each source's task performance. The learning phase consisted of 6 blocks (one for each source), with 10 trials each. In each trial, a new shape was presented, for which participants would observe the source's response and receive feedback about whether that response was correct or not. The order of the blocks was randomized. (b) After the learning phase, participants completed the test phase. This consisted of four blocks with 30 trials each. In each trial, a new shape was presented, for which participants then had to indicate whether the shape was a "blip" or not, and indicate how confident they were in their response. They were then given the choice to ask one of two randomly assigned sources for advice and see their chosen source's response. Afterwards they were able to update their response and indicate their revised confidence rating. Sources were counterbalanced across trials and blocks.</p><p>Replicating the results from Study 2, participants rated accurate sources (M = 76.700, SEM = 0.783) significantly more competent than random-performing sources (M = 57.900, SEM = 0.993; (F(1, 271) = 362.523, p &lt; 0.001, ηp² = 0.570; see Figure <ref type="figure">7</ref>), indicating that they successfully tracked performance. By contrast, there was no main effect of source identity (F(1.953, 529.604) = 0.087, p = 0.912, ηp² = 0.0003). That is, participants did not assign higher or lower competence ratings to ingroup, outgroup, or AI sources based on their identity alone (ingroup: M = 67.200, SEM = 0.924; outgroup: M = 67.300, SEM = 0.991; AI: M = 67.600, SEM = 0.951). We also did not observe a significant interaction effect between accuracy and identity (F(1.994, 539.637) = 1.296, p = 0.276, ηp² = 0.004). Thus participants' perceptions of information source competence were primarily driven by observed performance, not by identity.</p><p>The results hold even when controlling for political orientation and political sectarianism (see Supplementary Tables <ref type="table">9</ref><ref type="table" target="#tab_0">10</ref>).</p><p>Figure <ref type="figure">7</ref>. Social Identity did not impair participants' perceptions of source competence for identity-irrelevant tasks. Participants rated accurate sources as more competent than random sources, regardless of source identity. Y axis shows perceived competence ratings on a 0-100 scale. X axis shows the accuracy level of the source during the learning phase (random = 50%, accurate = 80%). Each violin plot shows the distribution of competence ratings for outgroup (dark-orange), ingroup (light-orange), and AI (blue) sources. Individual participant responses are plotted as dots. Diamond shapes represent the mean rating for each group, and vertical error bars indicate the standard error of the mean (SEM). Violin width reflects the density of responses.***p &lt; 0.001.</p><p>Having established that participants could accurately assess source competence for both identity-relevant (Study 2) and identity-irrelevant tasks (Study 3), we next assessed whether their preferences for AI over human sources also extended to identity-irrelevant tasks. As for the identity-relevant task, a 3 (source identity: ingroup, outgroup, AI) × 2 (source accuracy: accurate, random) repeated-measures ANOVA on proportion of times a source was selected revealed a significant main effect of accuracy (F(1, 271) = 101.091, p &lt; 0.001, ηp² = 0.104), with accurate sources (M = 0.577, SEM = 0.008) chosen more often than random ones (M = 0.423, SEM = 0.008). We also once again observed a significant main effect of source identity (F(1.813, 491.413) = 6.666, p = 0.002, ηp² = 0.014), such that participants preferred seeking information from AI (M = 0.516, SEM = 0.012) over outgroup sources (M = 0.462, SEM = 0.010; t(271) = 2.728, p = 0.019, d = 0.191) and ingroup sources (M = 0.522, SEM = 0.009) over outgroup sources, respectively (t(271) = 3.998, p &lt; 0.001, d = 0.173). However, there was no difference in their information-seeking preferences between ingroup and AI sources (t(271) = 0.295, p = 0.953, d = 0.018), suggesting that perhaps with the political fact-checking task, participants were cautious of potential political biases in partisan sources. There was no significant interaction between source identity and accuracy (F(1.953, 529.210) = 1.596, p = 0.204, ηp² = 0.006). The results hold even when controlling for political orientation and political sectarianism (see Supplementary Tables <ref type="table" target="#tab_0">11</ref><ref type="table" target="#tab_2">12</ref>). We did not find significant differences in belief-updating tendencies (see Supplementary Tables <ref type="table" target="#tab_0">13</ref><ref type="table" target="#tab_0">14</ref><ref type="table" target="#tab_0">15</ref>). Taken together, these results suggest that AI may provide a novel pathway to broaden information exposure by reducing identity-driven barriers to source selection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ingroup sources achieve computational parity with AI when source identity becomes task-irrelevant (Study 3).</head><p>We then once again thought to tease apart the processes underlying our behavioral findings using DDM. As for Study 2, we observed that a model which allowed the drift rate (v) to vary as a function of source identity and accuracy provided the best model fit while accounting for parsimony (see Supplementary Table <ref type="table" target="#tab_0">16</ref>).</p><p>Participants in Study 3 also showed negative drift rates only for outgroup sources (β = -0.077, 95% HDI [-0.091, -0.063]). The ingroup penalty observed in political contexts completely disappeared in the identity-irrelevant task (β = 0.008, 95% HDI [-0.006, 0.0222]), suggesting that ingroup sources achieved computational parity with AI when political identity became irrelevant. This is consistent with our behavioral results for both studies, where participants preferred AI sources over human sources for the political fact-checking task, but only discounted outgroup sources for the identity-irrelevant shape categorization task.</p><p>Accuracy effects remained robust with participants having faster evidence accumulation rates (β = 0.220, 95% HDI [0.209, 0.232]) towards accurate sources relative to random sources. See Table <ref type="table" target="#tab_2">2</ref> for full parameter estimates.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion.</head><p>Across three studies, we find that AI can serve as a bypass for identity-driven source selection biases. When deciding whom to consult, people preferred ingroup over outgroup sources-but they consistently preferred AI over partisan human sources. This was the case despite participants accurately recognizing that ingroup, outgroup, and AI sources were equally competent. These findings suggest that AI is perceived as a comparatively neutral alternative when social identity threatens to constrain information seeking. They extend prior work showing that identity systematically biases source choice <ref type="bibr" target="#b13">(Marks et al., 2019;</ref><ref type="bibr" target="#b33">Zhang &amp; Rand, 2023</ref>) by identifying a technological pathway through which such biases can be bypassed.</p><p>Computational modeling points to differential deliberation underlying the effect. Once specific options were presented, they accumulated preference evidence in a way that penalized partisan sources -especially outgroup advisors -relative to AI (lower drift rates).</p><p>Notably, this effect was context-dependent. In politically charged, identity-relevant tasks (Study 2), people preferred AI over both ingroup and outgroup sources. In contrast, in an identity-irrelevant shape categorization task (Study 3), ingroup advisors achieved parity with AI, while only outgroup advisors continued to be disadvantaged. This asymmetry highlights different forms of bias. Outgroup discounting persisted across contexts, suggesting a more automatic and generalized form of discrimination <ref type="bibr" target="#b9">(Iyengar &amp; Westwood, 2015;</ref><ref type="bibr" target="#b16">Molenberghs, 2013)</ref>. By contrast, ingroup discounting appeared specific to political contexts, possibly reflecting people's recognition that even their own partisan allies may introduce bias when the stakes are political. Importantly, these patterns held regardless of political orientation or levels of political sectarianism, indicating that the preference for AI over partisan advisors was not confined to a particular partisan subgroup. Moreover, actual belief-updating remained stable across source identities across both contexts, indicating that context shaped selection while integration stayed comparatively constant.</p><p>The observed preference for AI over partisan human sources did not override participants' fundamental concern with accuracy. Participants reliably distinguished competent from incompetent sources and chose accurate advisors more often, regardless of whether the source was AI or human. This indicates that the preference for ingroup members as well as AI reflects strategic information seeking rather than indiscriminate bias. In addition to accuracy goals, AI served as a trusted alternative when human sources were perceived as politically biased, preserving sensitivity to competence while circumventing identity-driven avoidance.</p><p>These findings carry practical implications for the design of information environments. Algorithmic mediation represents a viable strategy for combating political polarization in digital environments, particularly when AI systems are seen as neutral arbiters rather than partisan actors. By offering a non-identity-threatening alternative to partisan advisors, AI may reduce the formation of epistemically narrow "echo chambers" <ref type="bibr" target="#b4">(Del Vicario et al., 2016;</ref><ref type="bibr" target="#b26">Sunstein, 2018)</ref>. A broader body of research reinforces this cautious optimism. It has been argued that LLMs, unlike earlier intermediaries such as media outlets or search engines, have the potential to democratize the synthesis of knowledge and blunt the epistemic fragmentation that drives polarization <ref type="bibr" target="#b3">(Costello, 2025)</ref>. Moreover, AI can do more than transmit information: it can actively mediate deliberation, producing group statements that participants prefer to human-mediated outcomes and reducing division within citizens' assemblies <ref type="bibr" target="#b27">(Tessler et al., 2024)</ref>. These findings suggest that AI may circumvent identity-driven biases not only by providing information perceived as neutral but also by structuring collective reasoning in ways that foster consensus. Taken together, these insights situate our findings within a growing recognition that AI may reshape information environments not only by bypassing identity-based avoidance at the level of source selection but also by shaping how groups negotiate disagreement.</p><p>At the same time, our results caution that AI's advantage is contingent on the public's perceptions of its neutrality <ref type="bibr" target="#b24">(Stoyanovich et al., 2020)</ref>, which comes with its own risks. Young adults, for instance, often perceive information from AI chatbots as requiring less scrutiny than human sources, which may lower evaluative standards <ref type="bibr" target="#b31">(Xu et al., 2025)</ref>. Moreover, evidence of partisan bias in LLMs <ref type="bibr" target="#b20">(Rozado, 2023)</ref> and the rise of explicitly branded systems (e.g., Grok) suggest that over time users may come to select into ideologically congenial AIs much as they do partisan media. If AI systems become strongly associated with particular institutions, political orientations, or demographic groups <ref type="bibr" target="#b14">(Messeri &amp; Crockett, 2024)</ref>, they may cease to circumvent identity biases and instead reproduce them. AI's potential benefits therefore depend on systems being explicitly designed to prioritize veracity and transparency rather than engagement <ref type="bibr" target="#b3">(Costello, 2025)</ref>. Who programs these systems-and the values embedded in them-will therefore critically shape whether AI broadens information exposure and reproduces polarization.</p><p>While this research focused specifically on political identity as a driver of source bias, the observed processes likely extend to other identity dimensions that shape information processing. Future research should investigate whether AI similarly circumvents biases based on race, gender, religious affiliation, or other social categories. Given that identity-protective cognition operates across multiple group memberships through the same ventromedial prefrontal and striatal circuitry <ref type="bibr" target="#b28">(Van Bavel et al., 2008)</ref>, AI may provide a generalizable solution for reducing various forms of intergroup bias in information consumption. Research should also examine the temporal stability of AI's bias-circumventing effects as public familiarity with AI systems increases and institutional associations become more salient.</p><p>sensitive content, participants were presented with a scenario in which either a person or AI would explain a political debate on their behalf. They were asked to indicate who they would prefer to perform this task: a human or an AI system. This scenario was designed to probe perceptions of competence and neutrality in identity-relevant information-seeking contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis.</head><p>To assess whether participants exhibited a systematic preference for AI over a human in the political debate scenario, we conducted an exact binomial test comparing the observed proportion of AI selections to a null hypothesis of equal choice probability (p = 0.5). This test was selected due to its suitability for binary categorical data and its robustness for analyzing deviations from chance-level preference.</p><p>Study 2. Participants.</p><p>Sample size was computed based on prior work <ref type="bibr" target="#b13">(Marks et al., 2019)</ref> Power calculations were performed using g*Power <ref type="bibr" target="#b6">(Faul et al., 2009)</ref> to achieve power of 0.8 (β = 0.2, α = 0.05). Two-hundred and seventy-four participants who resided in the USA, voted in the previous general election and identified as either Republican or Democrat completed the task on Prolific Academic. As pre-registered, we excluded one participant, who did not identify as Democrat or Republican. Thus, data of 283 participants were analyzed (149 Democrats, 134 Republicans, Mage = 43.465, SDage ± 12.963, range = 21-80; female = 137, male = 142, other = 3). Participants received $12 per hour for their participation, in addition to a performance-related bonus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Materials.</head><p>Fact-Checking Task.</p><p>Participants completed an incentivized political fact-check task designed to examine how individuals sought information from politically aligned (ingroup), misaligned (outgroup) human advisors (=sources), and artificial intelligence (AI) advisors. The task was implemented using JavaScript and JsPsych (v7.3.4), hosted via Firebase. It was self-paced and took approximately 30 minutes to complete. Participants first completed a set of measures assessing political orientation, political sectarianism, trust in AI, and usage frequency of AI tools. They were then introduced to six sources, defined by a 3 (source identity: ingroup, outgroup, AI) × 2 (accuracy: accurate =0.8, random=0.5) within-subject design. Each source was visually represented by a distinct avatar and clearly labeled during a brief categorization task to ensure participants could recognize their identity and role (see Figure <ref type="figure" target="#fig_2">3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning Stage</head><p>The learning stage was designed to help participants form judgments about each source's competence in a binary shape categorization task ("Is this a blip?"). It consisted of six blocks of 10 trials, with each block associated with a different source. The order of blocks and trials was randomized for each participant.</p><p>On each trial, participants viewed a novel abstract shape, followed by the source's response ("yes" or "no") and visual feedback indicating whether the source's response was correct. Source accuracy was experimentally manipulated: three sources responded correctly on 80% of trials (accurate), while the other three responded correctly on 50% of trials (chance level). After each block, participants rated the perceived competence of that source using a slider ranging from 0 (completely inaccurate) to 100 (completely accurate). Once all six sources had been observed, participants sorted them from best to worst.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decision-Making Stage</head><p>Confidence ratings were collected both before and after the source input. Participants were informed that only their final response would count toward a bonus payment of up to $2, incentivizing strategic use of source advice.</p><p>During the decision-making stage, participants completed four blocks of 30 shape categorization trials each. Each trial began with a novel shape and an initial "yes" or "no" judgment and a confidence rating from 0 (not confident at all) to 100 (extremely confident). Participants were then presented with two randomly selected source avatars and asked to choose one for advice. After viewing the selected source's response, they had the opportunity to revise their judgment and confidence rating. Source pairs varied in identity and accuracy, allowing for within-subject comparisons across all combinations of source identity and accuracy. Participants were informed that only their final decision would count toward bonus earnings, which incentivized selective and strategic use of source input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comprehension and Engagement Checks</head><p>Comprehension checks were presented before both the learning and decision-making stages.</p><p>Participants who failed were required to reread the instructions. Additional attention checks and source-identification trials ensured participants understood the roles and accuracy levels of each source.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Post-task Measures</head><p>After the main task, participants rated their own performance on a slider and completed standard demographic questions. They also completed Inclusion of Other in the Self (IOS) diagrams to indicate perceived closeness to each source and responded to open-ended questions about how they evaluated sources and made decisions. These were designed to assess if participants were aware of the study hypotheses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis. Perceived Competence.</head><p>We examined whether participants differentially perceived the competence of ingroup, outgroup, and AI advisors depending on their accuracy. Each advisor (Democrat, Republican, AI) was recoded into ingroup, outgroup, or AI based on the participant's self-reported political orientation. For each participant, competence ratings were averaged separately for each source identity (ingroup, outgroup, AI) and for each level of accuracy (accurate, random). These values were entered into a 3 (source identity: ingroup, outgroup, AI) × 2 (source accuracy: accurate, random) within-subject ANOVA. Post hoc pairwise comparisons were conducted to unpack significant effects. We additionally performed one-sample t-tests to assess whether each group's perceived competence exceeded the midpoint of the scale. All statistical tests conducted in the present article are two sided. Analysis was conducted using IBM SPSS 27 and R Studio (Version 1.3.1056). All results of interest hold when controlling for political orientation and political sectarianism (see Supplementary Materials).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Advisor Selection Analysis.</head><p>We next examined whether participants differentially preferred ingroup, outgroup, or AI advisors.</p><p>For each participant, we computed the proportion of times each source was selected (when available) as a function of agent accuracy. These proportions were entered into a 3 (source identity: ingroup, outgroup, AI) × 2 (source accuracy: accurate, random) within-subject ANOVA.</p><p>Post hoc pairwise comparisons were conducted to unpack significant main and interaction effects. To further characterize the preference data, we conducted a series of planned paired t-tests comparing proportions across sources, correcting for multiple comparisons using a Bonferroni adjustment. All results of interest hold when controlling for political orientation and political sectarianism (see Supplementary Materials).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DDM.</head><p>Our aim in modeling our task using the drift-diffusion framework was to assess how source identity and accuracy impacted the evidence accumulation process during source selection. In particular, we wanted to assess (1) whether identity concerns manifest as a starting-point bias, a process bias in drift rate, or a combination of both; and (2) whether accuracy considerations operate through similar or distinct computational processes.</p><p>We implemented and compared 16 different specifications of a DDM (see Supplementary Materials). The models included the following parameters: (1) t₀, amount of non-accumulation/non-decision time;</p><p>(2) a, distance between decision thresholds; (3) z, starting point of the accumulation process; and (4) v, drift rate, the rate of evidence accumulation. Source identity was coded with two orthogonal contrasts (ingroup vs. AI; outgroup vs. AI), with AI serving as the reference category, and accuracy difference between both source options was entered as a trial-wise predictor. These variables modulated the parameters depending on the model specification.</p><p>The model space systematically varied parameter specifications. The simplest model (Model 1) assumed all parameters were fixed, while successive models allowed variation in either z or v as a function of accuracy alone (Models 2-4) or source identity alone (Models 5-7). We then estimated models in which both accuracy and identity influenced single parameters (Models 8-12), followed by models where both parameters varied simultaneously (Models 13-16). This comprehensive model space enabled us to isolate the specific computational processes through which identity and accuracy biases emerge.</p><p>We used the HSSM software toolbox <ref type="bibr">(Fengler et al., in prep)</ref> to estimate the parameters of our drift-diffusion models. The HSSM package employs Bayesian parameter estimation, using the No-U-Turn Sampler (NUTS) to draw samples from the posterior distributions of the parameters.</p><p>In fitting the models, we used HSSM's default weakly informative priors for all parameters. Models were fit jointly to choices and reaction times (RTs). We drew 10,000 posterior samples per chain after tuning and used 5,000 tuning (burn-in) iterations, running 4 independent chains in total. To assess convergence, we inspected the Gelman-Rubin R^R^ statistic from ArviZ summaries. In each case, R^ was close to one (&lt;1.1), suggesting adequate convergence. Model fits were compared using the Watanabe-Akaike Information Criterion (WAIC).We assessed whether differences in WAIC between models were meaningful by comparing the difference in expected log predictive density (elpd) to its standard error, with differences exceeding 4 standard errors considered statistically significant. When models showed equivalent fit (differences &lt; 4 SE), we selected the most parsimonious model with fewer effective parameters.</p><p>Study 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants.</head><p>Sample size was computed based on prior work <ref type="bibr" target="#b13">(Marks et al., 2019)</ref>. Power calculations were performed using g*Power <ref type="bibr" target="#b6">(Faul et al., 2009)</ref> to achieve power of 0.8 (β = 0.2, α = 0.05). Two-hundred and seventy-seven participants who resided in the USA, voted in the previous general election and identified as either Republican or Democrat completed the task on Prolific Academic. As pre-registered, we excluded five participants, who did not identify as Democrat or Republican. Thus, data of 272 participants were analyzed (133 Democrats, 139 Republicans, Mage = 39.246, SDage ± 12.881, range = 18-75; female = 140, male = 130, other = 2).</p><p>Participants received $12 per hour for their participation, in addition to a performance-related bonus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Materials.</head><p>Shape Categorization Task. Participants completed an incentivized shape categorization task.</p><p>The task was identical to the task in Study 2, except that this time participants had to decide whether a shape was a "blip" or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis.</head><p>Analysis was identical to that in Study 2.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Participants preferred AI over a human to explain a political debate. (a) Participants were asked to indicate whether they trusted AI or another person more to explain a political conflict. (b) A significantly greater number of participants sought information about a political conflict from AI (blue) compared to a person (orange). Bars reflect raw choice frequencies. *** p &lt; 0.001.</figDesc><graphic coords="6,73.50,73.50,468.00,195.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Example Trials of Identity-Relevant Political Fact-Checking Task.In Study 2 participants completed a political fact-checking task in which they had to determine whether statements were true or not. This constituted a learning phase and a test phase. (a) In the learning phase, participants learned about each source's task performance. The learning phase consisted of 6 blocks (one for each source), with 10 trials each. In each trial, a new political statement was presented, for which participants would observe the source's response and receive feedback about whether that response was correct or not. The order of the blocks was randomized. (b) After the learning phase, participants completed the test phase. This consisted of four blocks with 30 trials each. In each trial, a new political statement was presented, for which participants then had to indicate whether the statement was correct or not, and indicate how confident they were in their response. They were then given the choice to ask one of two randomly assigned sources for advice and see their chosen source's response. Afterwards they were able to update their response and indicate their revised confidence rating. Sources were counterbalanced across trials and blocks.</figDesc><graphic coords="7,73.50,73.50,513.24,145.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Social Identity did not impair participants' perceptions of source competence. Participants rated accurate sources as more competent than random sources, regardless of source identity. Y axis shows perceived competence ratings on a 0-100 scale. X axis shows the accuracy level of the source during the learning phase (random = 50%, accurate = 80%). Each violin plot shows the distribution of competence ratings for outgroup (dark-orange), ingroup (light-orange), and AI (blue) sources. Individual participant responses are plotted as dots. Diamond shapes represent the mean rating for each group, and vertical error bars indicate the standard error of the mean (SEM). Violin width reflects the density of responses.***p &lt; 0.001.</figDesc><graphic coords="8,129.75,-0.80,785.57,442.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Participants preferred seeking advice from accurate and from politically aligned sources. A main effect of accuracy emerged such that accurate sources were chosen more often than random sources. Participants also preferred AI over both ingroup and outgroup sources, as well as ingroup over outgroup sources. Y axis shows the proportion of times each source was selected during the decision-making stage. X axis shows source accuracy (random = 50%, accurate = 80%). Each violin plot shows the distribution of selection proportions for ingroup (light orange), outgroup (dark orange), and AI (blue) source. Individual participant responses are plotted as dots. Diamond shapes represent the mean proportion chosen for each group, and vertical error bars indicate the standard error of the mean (SEM). Violin width reflects the density of responses. Significance brackets indicate pairwise comparisons. ***p &lt; 0.001.</figDesc><graphic coords="9,130.13,261.94,351.75,260.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Source selection preferences emerge through systematic devaluation during deliberation. Drift-diffusion model parameter estimates reveal systematic biases in how participants processed information from different sources. Participants accumulated evidence more slowly from both human sources (ingroup = yellow, outgroup = orange) compared to AI (=blue). AI serves as the reference category (β = 0) for both parameters. Error bars represent 95% HDIs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Participants preferred seeking advice from accurate and from politically aligned sources. A main effect of accuracy emerged such that accurate sources were chosen more often than random sources. Participants also preferred AI over outgroup sources, and ingroup over outgroup sources. No difference was observed between ingroup and AI sources. Y axis shows the proportion of times each source was selected during the decision-making stage. X axis shows source accuracy (random = 50%, accurate = 80%). Each violin plot shows the distribution of selection proportions for ingroup (light orange), outgroup (dark orange), and AI (blue) source. Individual participant responses are plotted as jittered dots. Diamond shapes represent the mean proportion chosen for each group, and vertical error bars indicate the standard error of the mean (SEM). Violin width reflects the density of responses. Significance brackets indicate pairwise comparisons. t = p &lt; 0.10, **p &lt; 0.01, **p &lt; 0.001.</figDesc><graphic coords="14,133.50,73.50,345.00,262.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Participants accumulate evidence more slowly from outgroup sources compared to AI. There was no difference between ingroup and AI. AI is the reference category (β = 0) for both parameters. Error bars represent 95% HDIs.</figDesc><graphic coords="15,153.00,73.50,306.00,202.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="10,156.38,501.62,299.25,197.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="12,132.00,440.32,348.00,261.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 . Parameter estimates of evidence accumulation process (Study 2).</head><label>1</label><figDesc></figDesc><table><row><cell>Estimates</cell><cell>Mean [95% HDI]</cell></row><row><cell>Decision threshold (a)</cell><cell>1.244 [1.237,1.251]</cell></row><row><cell>Non-decision time (t0)</cell><cell>0.188 [0.182,0.194]</cell></row><row><cell>Starting point (z)</cell><cell>0.526 [0.522,0.530]</cell></row><row><cell>v Intercept</cell><cell>0.002 [-0.009,0.013]</cell></row><row><cell>v Ingroup vs AI</cell><cell>-0.188 [-0.132,-0.104]</cell></row><row><cell>v Outgroup vs AI</cell><cell>-0.241 [-0.255,-0.227]</cell></row><row><cell>v Accuracy Difference</cell><cell>0.299 [0.288,0.311]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Context-dependent Source Selection Biases: Only Outgroup-Derogation in Source Selection persists when the task is identity-irrelevant (Study 3).</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 . Parameter estimates of evidence accumulation process (Study 3). Estimates Mean [95% HDI]</head><label>2</label><figDesc></figDesc><table><row><cell>Decision threshold (a)</cell><cell>1.284 [1.278,1.290]</cell></row><row><cell>Non-decision time (t(0))</cell><cell>0.000 [0.000,0.001]</cell></row><row><cell>v Intercept</cell><cell>-0.010 [-0.021,0.001]</cell></row><row><cell>v Ingroup vs AI</cell><cell>0.008 [-0.006,0.022]</cell></row><row><cell>v Outgroup vs AI</cell><cell>-0.077 [-0.091, -0.063]</cell></row><row><cell>v Accuracy Difference</cell><cell>0.220 [0.209,0.232]</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div><p>Funding Sources: This project was funded with support from <rs type="funder">Google, Inc.</rs> awarded to L.K.G., and J.J.V.B. In addition, J.J.V.B. were supported by a grant from the <rs type="funder">National Science Foundation</rs> (#<rs type="grantNumber">2334148</rs>) and the <rs type="funder">Templeton World Charity Foundation</rs> (JVB, doi.org/10.<rs type="grantNumber">54224/31570</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_sG6Rr5m">
					<idno type="grant-number">2334148</idno>
				</org>
				<org type="funding" xml:id="_ydnc4FM">
					<idno type="grant-number">54224/31570</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion.</head><p>Together, these findings demonstrate that AI can circumvent the deeply ingrained identity biases that constrain human-to-human source selection. Across three studies, participants consistently preferred AI over outgroup advisors, even when they recognized all sources as equally competent. Computational modeling revealed that this preference reflects a deliberative devaluation of partisan input, particularly from outgroups. These results highlight both the promise and the boundary conditions of AI as an epistemic tool. On the one hand, AI's perceived neutrality enables it to broaden exposure to diverse information, reducing the formation of epistemically narrow echo chambers. On the other hand, this advantage depends on maintaining perceptions of impartiality and embedding systems within accountability structures that prioritize accuracy and transparency. By uncovering the cognitive mechanisms through which AI reshapes source selection, our work provides both theoretical insight into identity-driven biases and practical guidance for designing AI systems that promote healthier information environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods.</head><p>We report how we determined our sample size, all data exclusions, all manipulations and all measures in the experiment. The research methods were approved by the New York University Ethics Committee (IRB-FY2024-8271, IRB-FY2025-9303). Qualtrics survey files, anonymized data and analysis code is available on our Open Science Framework (OSF) page.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study 1. Participants.</head><p>One-thousand and fifty-four participants residing in the United States were recruited using Prolific Academic. The sample was quota-matched to be nationally representative of the U.S. population by age, gender, and ethnicity. No participants were excluded from analysis. The final sample included 532 women, 504 men, and 18 individuals who self-identified as another gender. Participants ranged in age from 18 to 85 years (M = 45.773, SD = 15.613). Based on self-reported political orientation, 601 identified as liberal, 286 as conservative, and 167 as politically neutral. For all experiments presented in this article, ethical approval was provided by the New York University Research Ethics Committee and all participants gave informed consent. All experiments were performed in accordance with the principles expressed in the Declaration of Helsinki. All samples were politically balanced for Democrats and Republicans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Materials.</head><p>Participants completed a 20-minute online survey. Here, we report the measure relevant to the current study. Additional measures were included as part of related projects <ref type="bibr" target="#b7">(Globig et al., 2024)</ref>; full survey materials and code are available on OSF. To assess trust in AI for politically</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename></persName>
		</author>
		<ptr target="https://www.wsj.com/us-news/law/google-search-antitrust-enforcement-limits" />
		<title level="m">Google Ruling Shows Antitrust Tools Struggle to Keep Up With Tech Markets. The Wall Street Journal</title>
		<imprint>
			<date type="published" when="2025-09-04">2025, September 4</date>
		</imprint>
	</monogr>
	<note>alphabet-c b1e5b6e</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The value of beliefs</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Bromberg-Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sharot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="561" to="565" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Castells</surname></persName>
		</author>
		<title level="m">The information age: Economy, society and culture (3 volumes)</title>
		<meeting><address><addrLine>Oxford</addrLine></address></meeting>
		<imprint>
			<publisher>Blackwell</publisher>
			<date type="published" when="1996">1996. 1997. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large language models as disrupters of misinformation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Costello</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41591-025-03821-5</idno>
		<ptr target="https://doi.org/10.1038/s41591-025-03821-5" />
	</analytic>
	<monogr>
		<title level="j">Nature Medicine</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">2092</biblScope>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The spreading of misinformation online</title>
		<author>
			<persName><forename type="first">M</forename><surname>Del Vicario</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bessi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zollo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Scala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Caldarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Quattrociocchi</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1517441113</idno>
		<ptr target="https://doi.org/10.1073/pnas.1517441113" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="554" to="559" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Stereotypes and prejudice: Their automatic and controlled components</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Devine</surname></persName>
		</author>
		<idno type="DOI">10.1037/0022-3514.56.1.5</idno>
		<ptr target="https://doi.org/10.1037/0022-3514.56.1.5" />
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="18" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Statistical power analyses using G* Power 3.1: Tests for correlation and regression analyses</title>
		<author>
			<persName><forename type="first">F</forename><surname>Faul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Erdfelder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Buchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-G</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1149" to="1160" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Perceived (Mis) alignment in Generative Artificial Intelligence Varies Across Cultures</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Globig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rathje</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Van Bavel</surname></persName>
		</author>
		<ptr target="https://osf.io/suqa2/download" />
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Implicit social cognition: Attitudes, self-esteem, and stereotypes</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Greenwald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Banaji</surname></persName>
		</author>
		<idno type="DOI">10.1037/0033-295X.102.1.4</idno>
		<ptr target="https://doi.org/10.1037/0033-295X.102.1.4" />
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="27" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fear and Loathing across Party Lines: New Evidence on Group Polarization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Iyengar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Westwood</surname></persName>
		</author>
		<idno type="DOI">10.1111/ajps.12152</idno>
		<ptr target="https://doi.org/10.1111/ajps.12152" />
	</analytic>
	<monogr>
		<title level="j">American Journal of Political Science</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="690" to="707" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Individual differences in information-seeking</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sharot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">7062</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Temporal Selective Exposure: How Partisans Choose When to Follow Politics</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11109-021-09690-1</idno>
		<ptr target="https://doi.org/10.1007/s11109-021-09690-1" />
	</analytic>
	<monogr>
		<title level="j">Political Behavior</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1663" to="1683" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Selective Exposure and Reinforcement of Attitudes and Partisanship Before a Presidential Election</title>
		<author>
			<persName><forename type="first">S</forename><surname>Knobloch-Westerwick</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1460-2466.2012.01651.x</idno>
		<ptr target="https://doi.org/10.1111/j.1460-2466.2012.01651.x" />
	</analytic>
	<monogr>
		<title level="j">Journal of Communication</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="628" to="642" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Epistemic spillovers: Learning others&apos; political views reduces the ability to assess and use their expertise in nonpolitical domains</title>
		<author>
			<persName><forename type="first">J</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Copland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Sunstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sharot</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cognition.2018.10.003</idno>
		<ptr target="https://doi.org/10.1016/j.cognition.2018.10.003" />
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">188</biblScope>
			<biblScope unit="page" from="74" to="84" />
			<date type="published" when="2018">2019. April 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Artificial intelligence and illusions of understanding in scientific research</title>
		<author>
			<persName><forename type="first">L</forename><surname>Messeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Crockett</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41586-024-07146-0</idno>
		<ptr target="https://doi.org/10.1038/s41586-024-07146-0" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">627</biblScope>
			<biblScope unit="issue">8002</biblScope>
			<biblScope unit="page" from="49" to="58" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cognitive Dissonance or Credibility? A Comparison of Two Theoretical Explanations for Selective Exposure to Partisan News</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Metzger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Hartsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Flanagin</surname></persName>
		</author>
		<idno type="DOI">10.1177/0093650215613136</idno>
		<ptr target="https://doi.org/10.1177/0093650215613136" />
	</analytic>
	<monogr>
		<title level="j">Communication Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="28" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The neuroscience of in-group bias</title>
		<author>
			<persName><forename type="first">P</forename><surname>Molenberghs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroscience &amp; Biobehavioral Reviews</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1530" to="1536" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Shifting attention to accuracy can reduce misinformation online</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pennycook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mosleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Arechar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eckles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Rand</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41586-021-03344-2</idno>
		<ptr target="https://doi.org/10.1038/s41586-021-03344-2" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">592</biblScope>
			<biblScope unit="issue">7855</biblScope>
			<biblScope unit="page" from="590" to="595" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A theory of memory retrieval</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ratcliff</surname></persName>
		</author>
		<idno type="DOI">10.1037/0033-295X.85.2.59</idno>
		<ptr target="https://doi.org/10.1037/0033-295X.85.2.59" />
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="59" to="108" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Drift Diffusion Decision Model:Theory and data</title>
		<author>
			<persName><forename type="first">Roger</forename><surname>Ratcliff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gail</forename><surname>Mckoon</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.biotechadv.2011.08.021.Secreted</idno>
		<ptr target="https://doi.org/10.1016/j.biotechadv.2011.08.021.Secreted" />
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="873" to="922" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The political biases of chatgpt</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rozado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Sciences</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">148</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Inference from Iterative Simulation Using Multiple Sequences</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical Science</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="457" to="472" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">How people decide what they want to know</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sharot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Sunstein</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41562-019-0793-1</idno>
		<ptr target="https://doi.org/10.1038/s41562-019-0793-1" />
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="14" to="19" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Chatbot-delivered acceptance and commitment therapy with adolescents: A pilot randomized controlled trial</title>
		<author>
			<persName><forename type="first">A</forename><surname>Stapleton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lavelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mchugh</surname></persName>
		</author>
		<idno type="DOI">10.31234/osf.io/j2kpt</idno>
		<ptr target="https://doi.org/10.31234/osf.io/j2kpt" />
	</analytic>
	<monogr>
		<title level="j">OSF</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The imperative of interpretable machines</title>
		<author>
			<persName><forename type="first">J</forename><surname>Stoyanovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Van Bavel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">V</forename><surname>West</surname></persName>
		</author>
		<idno type="DOI">10.1038/s42256-020-0171-8</idno>
		<ptr target="https://doi.org/10.1038/s42256-020-0171-8" />
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="197" to="199" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Polarization and Partisan Selective Exposure</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Stroud</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1460-2466.2010.01497.x</idno>
		<ptr target="https://doi.org/10.1111/j.1460-2466.2010.01497.x" />
	</analytic>
	<monogr>
		<title level="j">Journal of Communication</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="556" to="576" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Sunstein</surname></persName>
		</author>
		<idno type="DOI">10.1515/9781400890521</idno>
		<ptr target="https://doi.org/10.1515/9781400890521" />
		<title level="m">#Republic: Divided Democracy in the Age of Social Media</title>
		<imprint>
			<publisher>Princeton University Press</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">AI can help humans find common ground in democratic deliberation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Tessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jarrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sheahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Chadwick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Koster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Campbell-Gillingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Parkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Summerfield</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.adq2852</idno>
		<ptr target="https://doi.org/10.1126/science.adq2852" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">386</biblScope>
			<biblScope unit="issue">6719</biblScope>
			<biblScope unit="page">2852</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The Neural Substrates of In-Group Bias: A Functional Magnetic Resonance Imaging Investigation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Van Bavel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Cunningham</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1467-9280.2008.02214.x</idno>
		<ptr target="https://doi.org/10.1111/j.1467-9280.2008.02214.x" />
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1131" to="1139" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Diffusion models in experimental psychology: A practical introduction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nagler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lerche</surname></persName>
		</author>
		<idno type="DOI">10.1027/1618-3169/a000218</idno>
		<ptr target="https://doi.org/10.1027/1618-3169/a000218" />
	</analytic>
	<monogr>
		<title level="j">Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="385" to="402" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Selective Use of News Cues: A Multiple-Motive Perspective on Information Selection in Social Media Environments</title>
		<author>
			<persName><forename type="first">S</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Metzger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Flanagin</surname></persName>
		</author>
		<idno type="DOI">10.1111/jcom.12241</idno>
		<ptr target="https://doi.org/10.1111/jcom.12241" />
	</analytic>
	<monogr>
		<title level="j">Journal of Communication</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="669" to="693" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Information Modes&quot;: A Framework for Trust and Information Seeking</title>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="DOI">10.54501/jots.v3i1.245</idno>
		<ptr target="https://doi.org/10.54501/jots.v3i1.245" />
	</analytic>
	<monogr>
		<title level="j">Journal of Online Trust and Safety</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Reflexive intergroup bias in third-party punishment</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Yudkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rothmund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Twardawski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Van Bavel</surname></persName>
		</author>
		<idno type="DOI">10.1037/xge0000190</idno>
		<ptr target="https://doi.org/10.1037/xge0000190" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1448" to="1459" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sincere or motivated? Partisan bias in advice-taking</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Rand</surname></persName>
		</author>
		<idno type="DOI">10.1017/jdm.2023.28</idno>
		<ptr target="https://doi.org/10.1017/jdm.2023.28" />
	</analytic>
	<monogr>
		<title level="j">Judgment and Decision Making</title>
		<imprint>
			<biblScope unit="page" from="18" to="e29" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
