<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Split-Screen Distraction: The Role of Extraneous Visual Demands in Learning from Video</title>
				<funder ref="#_G3qW5Bq">
					<orgName type="full">Gorilla x PSA</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Brendan</forename><forename type="middle">A</forename><surname>Schuetze</surname></persName>
							<email>brendan.schuetze@utah.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Educational Psychology</orgName>
								<orgName type="institution">University of Utah</orgName>
								<address>
									<settlement>Salt Lake City</settlement>
									<region>UT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Educational Psychology</orgName>
								<orgName type="institution">University of Utah</orgName>
								<address>
									<addrLine>1721 Campus Center Drive RM 3220</addrLine>
									<settlement>Salt Lake City</settlement>
									<region>UT</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Split-Screen Distraction: The Role of Extraneous Visual Demands in Learning from Video</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F68CDB04E5B40E7AE41E30DD95A0296C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-10-21T14:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Sludge Content</term>
					<term>Split-Screen Videos</term>
					<term>Split Attention</term>
					<term>Multimedia</term>
					<term>Multitasking</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A genre of online videos known as "sludge content" has recently surged in popularity.</p><p>These videos typically present two clips simultaneously, with one primary and one muted secondary video, creating an intentionally overstimulating viewing experience. Given the reliance on overstimulation and the inherent multitasking demands, these videos raise questions relevant to theories of multimedia learning and cognitive load. Inspired by this content format, we conducted a series of within-person studies to test whether simultaneous split-screen videos lead to changes in comprehension and memory. Across two pre-registered within-person studies (NStudy 1 = 75, NStudy 2 = 100), we examined whether simultaneous split-screen presentations impaired comprehension or memory. Contrary to predictions, we found no strong evidence that simultaneous video presentation affected memory. We found limited self-reported differences in interest, with higher interest reported for non-split-screen videos in Study 2, but no differences in attention difficulty or cognitive load. These findings suggest that viewers may adapt to splitscreen visual input more efficiently than commonly assumed, raising new questions about attention and learning in contemporary multimedia environments.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Significance Statement</head><p>Social media platforms such as TikTok, Instagram, and YouTube have popularized a type of split-screen video known as "sludge content." Sludge content consists of two videos stitched together to play simultaneously. Typically, a primary educational or narrative video plays alongside a muted secondary video that is engaging but unrelated (e.g., footage of videogames or dominos falling over). Many viewers and commentators worry that this split-screen video format may harm attention and comprehension as a result of multitasking. Others have argued that the added stimulation could help sustain focus and benefit memory. Across two controlled experiments, we tested whether the presence of a secondary video influenced learning from educational material. We found no evidence that sludge content reduced comprehension. Neither did the sludge content enhance memory for the main video. Rather performance was approximately the same between the normal single video and split-screen "sludge" presentations. Furthermore, the sludge content did not consistently alter the viewers' self-reported attention, interest, or perceived mental load. These results challenge the assumption that modern multitasking media necessarily harm learning and highlight the importance of empirically examining the influence of attention-grabbing design features on cognition in contemporary digital environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Split-Screen Distraction: The Role of Extraneous Visual Demands in Learning from Video</head><p>Split screen videos are becoming a common element of popular social media platforms, such as TikTok, YouTube, and Instagram, which are heavily used by young adults <ref type="bibr" target="#b3">(Bestvater, 2024)</ref>. The particular style of split-screen videos common on these platforms juxtaposes a primary video, usually with someone speaking about a personal life experience or explaining something (e.g., makeup tutorials), while a muted repetitive secondary video plays simultaneously below. As noted by <ref type="bibr" target="#b7">Fares (2025)</ref>, this secondary video often uses gameplay footage from popular videogames (e.g., Subway Surfers, Minecraft) or other relaxing scenes (e.g., dominos falling, balls rolling down a ramp). Although the reasons for the algorithmicallysupported popularity of sludge content is somewhat murky, it is typically believed that these videos are posted on social media platforms, because they increase viewer retention <ref type="bibr" target="#b32">(Weaver, 2023)</ref>.</p><p>This particular genre of split-screen video has received the pejorative, yet widespread designation of "TikTok sludge" or "sludge content." The existence and popularity of these videos has received varied, yet often intense reactions from elements of the public. In a recent Scientific American article, titled "Sludge Videos Are Taking Over TikTok-And People's Mind" puts forth a skeptical view of the content, suggesting that multitasking may hurt watchers' attention spans in the long run-however, the article notes that research is needed on these popular social media trends, and that very little existed as of its publication in early 2024 <ref type="bibr" target="#b14">(Mattson, 2024)</ref>. Indeed, concerns about the negative effects of sludge content on attention is a common theme in most popular discussions of this trend (e.g., D <ref type="bibr">'Anastasio, 2023;</ref><ref type="bibr" target="#b12">Lingenfelter, 2023)</ref>. But this view is not universal. For example, well-known science communicator, Michael Stevens, has argued that split-screen video watching mirrors the types of activities that humans have long engaged in prior to social media, saying "we get panicky about attention spans … before TikTok, before the Internet -people would like talk to each other <ref type="bibr">[and]</ref> watch the birds at the same time … people would divide their attention. They would talk on the phone, and also watch the cars passing by on their street. That's sludge content pre-Internet." <ref type="bibr" target="#b29">(Smosh Alike, 2023)</ref>. Others have put forth an even more positive view of sludge content, arguing that the secondary videos might help to maintain learners' calm, improve their attention, and ultimately benefit their memory for the content they are consuming <ref type="bibr" target="#b7">(Fares, 2025)</ref>. These conflicting views of sludge content and the associated concerns about the impact of this content on viewer's learning and memory calls for theoretically-motivated, applied research into its effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Splitscreen Videos, Attention, and Multimedia Learning Theory</head><p>Though popular opinions on the effects of sludge content on human cognition are far from consensus, psychological theory can also help anticipate the effects of split-screen video consumption on human memory and attention. Fundamentally, split-screen videos are a form of multimedia, usually combining sound and (at least) two independent sets of moving images, but sometimes more. As such, multimedia learning theory <ref type="bibr" target="#b16">(Mayer, 2002</ref><ref type="bibr" target="#b17">(Mayer, , 2024) )</ref> and perceptual load theory <ref type="bibr" target="#b11">(Lavie et al., 2014)</ref>, can be used to make informed theoretical-backed predictions concerning the effects of this trend.</p><p>Overload and Underload Hypotheses. As summarized by <ref type="bibr" target="#b30">Sümer et al. (2025)</ref>, there are two general hypotheses concerning the effects of consuming multiple forms of media simultaneously: the overload and underload hypotheses. The overload hypothesis states that having multiple videos playing at once will lead to decreased mnemonic encoding as a result of overloading the viewers' attentions. Conversely, the underload hypothesis puts forth the notion that additional stimulation in the form of the secondary video might lead to increased focus, as the viewer might not seek other even more counterproductive forms of stimulation (e.g., mindwandering, checking one's phone).</p><p>As exemplified by <ref type="bibr" target="#b14">Mattson (2024)</ref>, the overload hypothesis is relatively intuitive. Indeed, generally multitasking impairs performance <ref type="bibr" target="#b15">(May &amp; Elder, 2018)</ref> and this phenomenon does not seem to get better with practice, as frequent media multitaskers are actually more susceptible to distraction <ref type="bibr" target="#b21">(Ophir et al., 2009)</ref>. However, findings in cognitive science surrounding selective attention and perceptual load theory suggest that the results of the split-screen video task might not be as negative as often assumed. Research on selective attention-the ability to focus one's attention on a specific target stimulus even in the midst of other distractor stimuli-has shown that people are able to focus on relevant material and block out the influence of distracting stimuli <ref type="bibr" target="#b6">(Egeth, 1967;</ref><ref type="bibr" target="#b9">Johnston &amp; Dark, 1986;</ref><ref type="bibr" target="#b13">Makov et al., 2023)</ref>, sometimes to a surprising extent <ref type="bibr" target="#b25">(Simons, 2000;</ref><ref type="bibr">e.g., Simons &amp; Chabris, 1999)</ref>.</p><p>Perceptual load theory suggests processing will be impaired if the perceptual demands of the task are too high. Furthermore, the processing of distractor stimuli depends on the perceptual and cognitive demand of the task at hand <ref type="bibr" target="#b20">(Murphy et al., 2016)</ref>. In classic studies of perceptual load, participants are commonly asked to complete a focal task, while ignoring peripheral distractors, such as letters or shapes appearing elsewhere on the screen (e.g., <ref type="bibr" target="#b10">Lavie, 1995)</ref>. In these experiments, when task demand is high and the distractor stimuli is irrelevant to the focal task, then attentional selectivity will tend to occur relatively early, meaning very little attention will be paid to the distractor (in this case the secondary video). When cognitive and perceptual load of the primary task is low, such as perhaps in the case of listening to a speaker describe mundane events, more attention might be given to the distractor stimulus. In other words, perhaps attentional resources can adapt to the demands of the task, and we should not expect large effects of a distractor should this adaptive capability not be exceeded.</p><p>There is also empirical evidence for the underload hypothesis. Here the evidence comes from research on task vigilance and mind-wandering <ref type="bibr" target="#b30">(Sümer et al., 2025)</ref>, with the primary finding that vigilance can decrease as the time spent on the same task increases <ref type="bibr" target="#b24">(See et al., 1995)</ref>. This theory applied to the sludge content being that in the absence of the sludge viewers would become habituated to the primary video and thus their attention would wander. When the secondary distractor video is added, this might prevent the viewer's attention from wandering and thus actually increase performance <ref type="bibr" target="#b7">(Fares, 2025)</ref>. This hypothesis might also be interpreted in light of research finding that brief mental breaks through momentary task switching can increase task vigilance <ref type="bibr" target="#b0">(Ariga &amp; Lleras, 2011)</ref>. In this way, we could view the ability to momentarily watch the secondary distractor video as a means of offering the viewer a brief break through which slips in task vigilance could be remediated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Split-Screen Videos and the Split-Attention</head><p>Effect. These attentional dynamics also intersect with work from multimedia learning theory, particularly the split-attention effect <ref type="bibr" target="#b18">(Mayer &amp; Moreno, 1998;</ref><ref type="bibr" target="#b23">Schroeder &amp; Cenkci, 2018)</ref>, which states that multimedia learning environments should not require learners to integrate information from multiple sources concurrently. For example, <ref type="bibr" target="#b1">Ayres and Sweller (2005)</ref> describe the difficulties learners will have if they need to integrate between the text of a geometry problem and the problem solution's solution if both sources of information are spatially separated. It is thought that this separation increases extraneous cognitive load, and thus potentially overwhelms the learners' working memory. This source of cognitive load can often be circumvented in video environments by replacing on-screen text with narration <ref type="bibr" target="#b18">(Mayer &amp; Moreno, 1998)</ref>. Indeed, multimedia environments should be designed such that information is conveyed in an integrated fashion.</p><p>Split-screen videos present an interesting case, because they seem at first glance to controvert multimedia principles; the genre leans into extreme stimulation in a way that seems like it would assuredly lead to split attention and thus decreased ability to encode the to-belearned information. However, upon closer examination the format does seem to follow some of the multimedia principles, even if by trial-and-error. Most saliently, sludge content typically contains a single audio source, as one video is typically muted. Furthermore, the primary video is usually visually uninformative, consisting of a narrator speaking to the camera. In this sense, even though there are two videos playing, there is only one important audio channel and one important (or at least stimulating) visual channel.</p><p>It should also be acknowledged that one of the main tenets of the split-attention effect is that learning is impaired due to the need to integrate across spatially or temporally separate material <ref type="bibr" target="#b2">(Ayres &amp; Sweller, 2021)</ref>, however, in the sludge content, there is usually no need to integrate information from the secondary video with that of the first. Ostensibly, the secondary video is included simply to provide an additional source of visual stimulation, with low cognitive demands <ref type="bibr" target="#b32">(Weaver, 2023)</ref>. Another interesting aspect of the sludge genre is that the secondary distractor video is usually non-realistic footage from videogames, perhaps reflecting the fact that under some conditions realistic imagery has been found to impair retention independent of splitattention effects <ref type="bibr" target="#b27">(Skulmowski, 2023)</ref>.</p><p>Although there is little research on social media-style split-screen videos from the perspective of human memory, <ref type="bibr" target="#b7">Fares (2025)</ref> has recently reported results of a withinparticipants' (n = 24) study of sludge content on comprehension of a personal narratives paired with gameplay footage from the videogame Minecraft. This study overlaid audio narratives and subtitles over a single type of distractor (Minecraft gameplay), comparing it to audio only and subtitles only conditions. As such, it was not testing the true split-screen video content, typical of the sludge content discussed in the present study, as there was only ever one video playing. The main effect of the distractor video was small (η²p = 0.002) and non-significant. The use of a single type of distractor also opens up the question of whether the (null) effects were limited to the stimulus used <ref type="bibr" target="#b33">(Yarkoni, 2022)</ref>.</p><p>Interestingly, the null results of <ref type="bibr" target="#b7">Fares (2025)</ref> are not uncommon in the general research domain of simultaneous multimedia stimulation. This result mirrors other research which failed to find a significant impact of subtitles on learning from video <ref type="bibr" target="#b4">(Cojean &amp; Martin, 2021)</ref>. This also mirrors findings by <ref type="bibr" target="#b30">Sümer et al. (2025)</ref> who found that listening to background music while studying did not significantly affect task performance or attention.</p><p>Given the lack of well-powered studies on sludge content, the popular interest in this phenomenon, the disparate predictions it elicits with regard to multimedia comprehension and learning, it seems as though a controlled study of this contemporary form of multimedia could provide nuanced evidence to the discussion of so-called "sludge content."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Present Study</head><p>In the present series of within-participant studies, we test the effects of split-screen videos on comprehension and memory for short (7-15 minute) video lectures. In Study 1, we use TED talks as the primary stimuli of interest. Though these videos have imagery associated with them, it tends to be minimalistic and on-screen for relatively long periods of time. In Study 2, we use more visually taxing animated content aimed at conveying scientific information. Here, important information is conveyed both via the animation and the accompanying voiceover. In SPLIT-SCREEN VIDEOS AND MEMORY 10 Study 2, we also include secondary distractor videos that present their own unrelated text to further manipulate the potential effects of visual load.</p><p>We also expand the potential implications of this study by (a) capturing self-reported measures of engagement and mental demand, and (b) checking for moderation by age of participant, as short-form video content is typically associated with young adult viewers, ages 18-34 <ref type="bibr" target="#b3">(Bestvater, 2024)</ref>. Together, these studies allow us to examine whether the impact of splitscreen distractions depends on the characteristics of the viewer or underlying visual demands of the lecture material, thereby providing insight into the conditions under which such media may hinder or leave unaffected comprehension and memory. Theoretically these studies contribute to the development of multimedia learning theory, while practically helping parents, educators, and social media consumers better understand the effects of the split-screen content that is being increasingly consumed across video-based social media.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study 1: Video Lectures with Slides</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data and Materials Availability</head><p>Both of the present studies were pre-registered. Pre-registrations, open data, and analysis files can be found on the Open Science Framework, https://osf.io/adbkc/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Participants. Power analysis indicated that a sample size of 75 participants would be sufficient to detect a small effect d = 0.20 with upwards of 88% power. Thus, we pre-registered our target N = 75. As specified in our preregistration, our sample came from two sources, an educational psychology subject pool at a large university in the American Mountain West and Prolific, an online participant pool where experiments can be completed in exchange for compensation. Prolific participants were recruited from English-speaking countries (Australia, Ireland, United States, and United Kingdom). We note that because this experiment was conducted within-subjects, the usage of two participant pools would not contribute unmeasured confounds to the study relevant to estimating the main effect of the split-screen videos, however, it could lead to confounds related to the age analyses, which we resolve in Study 2.</p><p>Participants (N = 75) were recruited from two sources: the university subject pool (n = 42) and Prolific (n = 33). Ages ranged from 18 to 65 years (MAge = 31.1, MedianAge = 23, SD = 14.5). Of those who provided demographic information, 44 identified as female, 27 as male, and 3 as non-binary. Participants identified primarily as white (55%), followed by Black (7%), Hispanic or Latinx (6%), and Asian (5%).</p><p>Stimuli. Two videos from the TED lecture series were selected as the primary stimuli of interest. One video presented by Rajesh Rao <ref type="bibr" target="#b22">(Rao, 2011)</ref> discussed computational linguistics and the other video by Kevin Slavin <ref type="bibr" target="#b28">(Slavin, 2012)</ref> centered around the impact of algorithms on the world. These videos had been previously used in video learning studies by <ref type="bibr" target="#b19">Mueller and Oppenheimer (2014)</ref>. As common in TED lectures, the videos depict a single lecturer presenting a polished lecture on their topic of expertise between 15 and 20 minutes in length. The videos toggle between shots of the lecturer, supplemented by closeups on the presentation slides. These videos were selected as self-contained stand-ins for the type of content that might be presented in a college class.</p><p>Two distractor videos were used, but each participant only saw one of these distractors.</p><p>One distractor featured gameplay from the videogame Subway Surfers, depicting a repetitive gameplay loop of a character jumping from railcar to railcar while avoiding obstacles and collecting coins. The other distractor video showed footage from the videogame Minecraft. This video featured a character navigating a long obstacle course sequence composed of floating blocks and different biomes. These distractor videos were selected, because Minecraft and Subway Surfers are commonly-cited as prototypical examples of the type of videos used in TikTok sludge content (e.g., D <ref type="bibr">'Anastasio, 2023;</ref><ref type="bibr" target="#b14">Mattson, 2024)</ref>. The order of the videos, the presence of the distractor video, and the chosen distractor video, were all randomly selected for each participant.</p><p>Procedure. Figure <ref type="figure">1</ref> summarizes the procedure used in Studies 1 and 2. The primary split-screen versus traditional video manipulation was performed within-subjects. Each participant watched two videos in a random order. One of these videos was randomly assigned to have a distractor video playing simultaneously during the entire runtime (upon ending distractor videos looped to the beginning). When the distractors were shown, participants were told to focus on learning the content of the video lecture, which always appeared on the left-hand side of the screen. In the traditional non-distractor condition, the video lecture was shown in the middle of the screen.</p><p>After watching each video, the participant answered two engagement questions assessing interest and attentional difficulty. Following each learning phase, participants completed a short distractor task in order to disrupt rehearsal and prevent active maintenance of the learned material in working memory. These distractor activities were composed of 25 simple one-andtwo digit math problems (e.g., "What is 7 -4?") randomly interleaved with 25 reaction time questions. During the math questions, the participant was given a textbox to indicate the correct answer (e.g., by typing 3 and hitting enter). During the reaction time questions, participants saw four squares arranged horizontally. One of these squares was always assigned a slightly darker color than the rest. The participant was required to answer using the number keys 1, 2, 3, or, 4 to indicate from left-to-right which square was filled with a different color. On average these 50 SPLIT-SCREEN VIDEOS AND MEMORY 13 distractor trials were completed in approximately a minute and a half after the first video (MTime = 96 s, Mediantime = 94 s, SD = 20 s). Participants finished these 50 trials slightly faster after the second video (MTime = 88 s, Mediantime = 85 s, SD = 18 s).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1</head><p>General Procedure Timeline for Studies <ref type="table">1</ref> and<ref type="table">2</ref> Note. The order of the split-screen versus traditional conditions, the content of the primary (lecture) video, and the content of the secondary (distractor) video were all randomly counterbalanced between participants. The memory test for each video consisted of 10 multiple choice questions.</p><p>After finishing the 50 total distractor trials, the participants were presented with a multiple choice and true/false comprehension test. Each video was associated with 10 or 11 comprehension questions. These questions were developed by the authors and designed to test the participants' memory for facts stated directly in the respective video lecture. For example, one test question for <ref type="bibr" target="#b28">Slavin's (2012)</ref> talk asked "What do Epagogix's algorithms claim to be able to do?" With the correct answer being: "Predict box office success." Full test banks can be found on the Open Science Framework.</p><p>After completing the multiple choice test that matched the video they had just seen, they watched the second video. The second video was always in the opposite condition to the first video (i.e., randomly counter-balanced within subjects). After the second video, the same blocks of trials were completed: engagement questions, distractor task, and comprehension test for the second video. After completing the second comprehension test, the participants filled out a short survey. The final survey captured demographic information and asked whether there were any difficulties with following task instructions or other technical issues that arose. All participants gave informed consent, and ethical approval was given for the present series of studies by the authors' Institutional Review Board (#IRB_00181555).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The following analyses were pre-registered. All analyses were performed at the itemlevel in R using the lme4 package for multilevel modeling. All modes included random intercepts for participant and question ID.</p><p>Test Performance. Overall test performance was relatively evenly distributed between 0 and 100%, M = 0.60, Median = 0.63, SD = 0.23. Mean performance in the split-screen condition (M = 0.58, Median = 0.60, SD = 0.23) was slightly lower than the traditional condition (M = 0.61, Median = 0.64, SD = 0.24, d = -0.13). A mixed-effects logistic regression was used to test whether accuracy differed by video type (traditional vs. split-screen). The effect of adding the split-screen video was not statistically significant, log OR = 0.17, SE = 0.12, z = 1.35, p = .18, indicating that accuracy did not differ across conditions. The corresponding average marginal effect, indicating an average difference of 3% (AME = 0.03, 95% CI = [-1%, 7%], p = .18), was small and non-significant.</p><p>A follow-up model tested whether the effect of video type was moderated by participant age. The model revealed a significant age-by-video type interaction, log OR = 0.02, SE = 0.01, z = 2.15, p = .032. Inspection of the coefficients suggested that performance on traditional videos tended to increase with age, whereas performance for split-screen videos decreased with age.</p><p>Neither the main effect of age (log OR = -0.01, SE = 0.01, z = -0.62, p = .53) nor the main effect of video type (log OR = -0.58, SE = 0.37, z = -1.59, p = .11) was significant. In line with recommendations by von Hippel and Schuetze (2025), we attempted to replicate this interaction in Study 2, however, this replication attempt was not successful.</p><p>Self-Report Measures of Engagement. Self-report data were analyzed using mixedeffects linear models with random intercepts for participant and video. Participants reported slightly greater difficulty maintaining attention during the split-screen videos than during the traditional videos, but this effect was not statistically significant, b = -0.29, t(73) = -1.68, p = .10.</p><p>Likewise, self-reported interest did not differ significantly by condition, b = 0.24, t(74) = 1.56, p = .12, though traditional videos were rated as directionally more interesting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study 2: Increased Visual Load of Primary and Secondary Videos</head><p>Because Study 1 yielded no significant effects of the sludge content on comprehension, in Study 2 we increased visual load to test whether more complex stimuli would elicit stronger split-attention costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants.</head><p>In line with our pre-registered recruitment target, participants (N = 100) were recruited from the online participant platform Prolific. Participants were recruited from English-speaking countries (Australia, Ireland, the United States, and the United Kingdom). On average, participants were middle-aged (MAge = 42.1; MedianAge = 40.0, SD = 13.66). Most participants were women (n = 54). The majority of participants were white (n = 68), followed by Black (n = 19), Asian (n = 6), Hispanic and or Latinx (n = 3), Native Hawaiian and/or Pacific Islander (n = 1).</p><p>Stimuli. The primary difference between Studies 1 and 2 was related to stimuli selection.</p><p>In Study 2, four primary video lectures were used. Unlike Study 1, these video lectures were not filmed in a TED-talk style format, but rather made extensive use of animated sequences and much more minimal (or sometimes no) depiction of the narrator. Like the TED talks, these videos exhibit high production value. The new stimuli were also selected to be more recently created, having been produced between 2014 and 2025. Videos were gathered from popular science communication YouTube channels, with high production values (CrashCourse, Kurtzgesagt, and StudyHall). Video topics were drawn from STEM and social science topics (e.g., the function of bacteriophages, and visual perception). These videos are typically designed to supplement high school and college courses; for example, the StudyHall video concerned Piaget's stages of development, and was produced in partnership with Arizona State University.</p><p>In Study 2, we increased the pool of possible distractor videos from two to eight. Four were relatively typical distractor videos for sludge content, consisting of gameplay footage, repetitive sequences of marbles falling, or depictions of animated fruit dancing (mirroring Study 1). The other four videos were lyric videos, a subgenre of music videos with animated depictions of the lyrics to a song. These lyric videos were included in our distractor bank in order to test whether distractor videos with more textual content would create a perceptual overload leading to impaired performance. Each participant only saw one of these eight randomly sampled secondary distractor videos alongside one of the primary instructional videos.</p><p>Procedure. The procedure mirrored that of Study 1, except for two changes. First, the stimuli and associated comprehension test questions were changed as described in the Stimuli section above. Second, an additional post-video measure of cognitive load was added, specifically the NASA task load index (NASA TLX) -mental demand question <ref type="bibr" target="#b8">(Hart &amp; Staveland, 1988)</ref>. All other features of the procedure stayed the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>As with Study 1, the following analyses were pre-registered and performed using multilevel logistic and linear models in R using the package lme4.</p><p>Test Performance. Overall test performance was slightly higher than that of Study 1, M = 0.69, Median = 0.70, SD = 0.21. Opposite of Study 1, mean performance in the split-screen condition (M = 0.70, Median = 0.75, SD = 0.22) was slightly higher than the traditional condition (M = 0.68, Median = 0.67, SD = 0.21, d = 0.09). A mixed-effects logistic regression was conducted to examine the effect of the within-subjects video type manipulation (traditional vs. split-screen distractor) on participants' accuracy across questions, with random intercepts for participant and question. We found no significant effect of video type manipulation, log OR = -0.06, SE = 0.11, z = -0.54, p = .59. The average marginal effect of video type was negligible (AME = -0.01, 95% CI = [-5%, 3%], p = .59), indicating that the average proportion correct differed by only one percent between conditions. Distractor Type. An exploratory follow-up model further sub-divided the conditions, analysing whether the traditional distractor (gameplay) videos differed from the text-heavy (lyric) videos in terms of comprehension test performance. These analyses between the three dummy-coded conditions (traditional, split-screen gameplay, and split-screen lyrics) likewise showed no significant differences, as indicated by a likelihood ratio test, Δχ²(2) = 3.23, p = .20.</p><p>Participants viewing the lyric videos condition showed descriptively higher accuracy compared to the videos without the distractors (AME = 0.06, p = .08).</p><p>Moderation by Age. To test whether age moderated effects of video type, we fitted a model including the interaction between centered age and video type. Neither the main effect of age (log OR = -0.01, SE = .01, z = -0.80, p = .43) nor the interaction (log OR &lt; 0.01, SE &lt; .01, z = 0.15, p = .88) was significant, suggesting that age was not associated with comprehension accuracy. The predicted probabilities from this model are shown in Figure <ref type="figure" target="#fig_0">2</ref>. In other words, we did not replicate the significant interaction from Study 1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Reported Measures</head><p>Attention Difficulty. The mean attention difficulty in the split-screen condition was 2.51, while the mean in the split-screen condition was 2.32 out of 5. Self-reported attention difficulty did not differ significantly between video types, b = -0.19, t(99) = -1.30, p = .20.</p><p>Interest Level. Participants reported significantly higher interest for the traditional videos (M = 4.02) than for the videos presented split-screen (M = 3.79), b = 0.24, t(98) = 2.18, p = .03. This difference remained significant even when accounting for a potential interaction with age, b = 0.29, t(88) = 2.54, p = .01, however, no significant main effect of age nor interaction between age and condition was found (ps &gt; 0.05). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Perceived</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>The rise of split-screen "TikTok sludge" or "slop" videos has led to questions in the popular media concerning the effects of these forms of multimedia on learning and comprehension. Some commentators have argued that these videos will lead to reduced memory performance due to perceptual overload, while others have argued that these videos will have no effect (Smosh Alike, 2023) or actually improve focus by keeping the viewer stimulated <ref type="bibr" target="#b7">(Fares, 2025)</ref>. Indeed, these split-screen videos present an interesting task to study from the lens of multimedia learning theory given their intense demands on the viewer's attention.</p><p>Yet in the present study, we did not find evidence that these videos affect learning in a statistically significant way. In Study 1, the average marginal effect of the distractor video on the comprehension test was negative 3% (95% CI = [-1%, 7%]), while in Study 2 the average marginal effect was positive 1% (95% CI = [-5%, 3%]). While the present analyses cannot rule out an effect of split-screen videos, these confidence intervals are not compatible with a large effect of these videos (i.e., &gt; +/-10% on comprehension tests in the short-term), as long as multimedia presentation follows the general form tested in the present study. Critical here is likely the feature of sludge content where the secondary video is muted and contains no relevant information to be later tested. Another potential important feature is that the secondary video does not usually contain images of people or faces, which have been found to disrupt selective attention <ref type="bibr" target="#b20">(Murphy et al., 2016)</ref>.</p><p>To summarize, our findings suggest that the presence of a secondary, visually engaging but task-irrelevant video did not meaningfully impair or enhance comprehension, attention, or perceived cognitive load. We did, however, find weak idiosyncratic effects that would not survive corrections for multiple comparisons in select follow-up analyses across Studies 1 and 2. Specifically, in Study 1, we found a small interaction between video type and age, suggesting that older participants learned less from the sludge videos. However, this did not replicate in Study 2. Similarly, in Study 2 we found small negative effects of the split-screen videos on interest, however, this effect was not present in Study 1. Thus, we did not find strong or replicable effects of split-screen videos on any of our behavioral or self-report outcomes of interest (see von Hippel &amp; Schuetze, 2025).</p><p>Despite their apparent violation of multimedia design principles, split-screen "sludge" videos may not fundamentally disrupt learning when the primary information source is clear and auditory attention is allowed to focus on a single source. These results are in-line with recent findings about background music during learning <ref type="bibr" target="#b30">(Sümer et al., 2025)</ref>. Furthermore, the findings highlight the resilience of cognitive processing under complex visual stimuli. More directly, the present studies suggest that concerns about the immediate cognitive costs of "sludge" content, as expressed by many commentators (e.g., D <ref type="bibr">'Anastasio, 2023;</ref><ref type="bibr" target="#b14">Mattson, 2024)</ref>, on memory may be overstated. Future research should examine whether these null effects persist under more demanding tasks, longer exposure or longer retention intervals, or when the secondary video competes for auditory as well as visual attention.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2.</figDesc><graphic coords="18,93.20,347.98,425.54,240.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Task Load. Cognitive load as operationalized by the NASA-TLX did not significantly differ by video type, b = 0.13, t(97) = 1.26, p = .21. A weak interaction with ageif interpreted -would suggest a potential trend toward lower task load with increasing age for traditional videos, b = -0.01, p = .09.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="13,72.00,209.99,468.00,159.00" type="bitmap" /></figure>
		</body>
		<back>

			<div type="funding">
<div><p>A grant from the <rs type="funder">Gorilla x PSA</rs> (<rs type="programName">Psychological Science Accelerator) program</rs> to the author supported the participant recruitment for Study 2. Pre-registrations, open data, and analysis files can be found on the <rs type="institution">Open Science Framework</rs>, https://osf.io/adbkc/.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_G3qW5Bq">
					<orgName type="program" subtype="full">Psychological Science Accelerator) program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Brief and rare mental &quot;breaks&quot; keep you focused: Deactivation and reactivation of task goals preempt vigilance decrements</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ariga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lleras</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cognition.2010.12.007</idno>
		<ptr target="https://doi.org/10.1016/j.cognition.2010.12.007" />
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="439" to="443" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The split-attention principle in multimedia learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ayres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sweller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Cambridge handbook of multimedia learning</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Mayer</surname></persName>
		</editor>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="135" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The split-attention principle in multimedia learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ayres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sweller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Cambridge handbook of multimedia learning</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Mayer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Fiorella</surname></persName>
		</editor>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="199" to="211" />
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Bestvater</surname></persName>
		</author>
		<ptr target="https://www.pewresearch.org/internet/2024/02/22/how-u-s-adults-use-tiktok/" />
		<title level="m">How U.S. adults use TikTok. Pew Research Center</title>
		<imprint>
			<date type="published" when="2024-02-22">2024. February 22</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Reducing the split-attention effect of subtitles during video learning: Might the use of occasional keywords be an effective solution?</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cojean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Martin</surname></persName>
		</author>
		<idno type="DOI">10.3917/anpsy1.214.0417</idno>
		<ptr target="https://doi.org/10.3917/anpsy1.214.0417" />
	</analytic>
	<monogr>
		<title level="m">L&apos;Année Psychologique</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page" from="417" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">TikTok&apos;s Subway Surfers videos signal demise of our attention span</title>
		<author>
			<persName><forename type="first">C</forename><surname>D'anastasio</surname></persName>
		</author>
		<ptr target="https://www.bloomberg.com/news/newsletters/2023-02-03/tiktok-subway-surfers-family-guy-videos-are-terrible-for-attention-span" />
		<imprint>
			<date type="published" when="2023-02-03">2023, February 3</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Selective attention</title>
		<author>
			<persName><forename type="first">H</forename><surname>Egeth</surname></persName>
		</author>
		<idno type="DOI">10.1037/h0024088</idno>
		<ptr target="https://doi.org/10.1037/h0024088" />
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="57" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The use of sludge content for effective recall of information</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Fares</surname></persName>
		</author>
		<ptr target="https://scholarworks.lib.csusb.edu/etd/2318" />
	</analytic>
	<monogr>
		<title level="m">Electronic Theses, Projects, and Dissertations</title>
		<imprint>
			<date type="published" when="2025">2025. 2318</date>
		</imprint>
		<respStmt>
			<orgName>California State University, San Bernardino</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Development of NASA-TLX (Task Load Index): Results of Empirical and Theoretical Research</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Staveland</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0166-4115(08)62386-9</idno>
		<ptr target="https://doi.org/10.1016/S0166-4115(08)62386-9" />
	</analytic>
	<monogr>
		<title level="m">Advances in Psychology</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1988">1988</date>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="139" to="183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Selective attention</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J</forename><surname>Dark</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev.ps.37.020186.000355</idno>
		<ptr target="https://doi.org/10.1146/annurev.ps.37.020186.000355" />
	</analytic>
	<monogr>
		<title level="j">Annual Review of Psychology</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="43" to="75" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Perceptual load as a necessary condition for selective attention</title>
		<author>
			<persName><forename type="first">N</forename><surname>Lavie</surname></persName>
		</author>
		<idno type="DOI">10.1037//0096-1523.21.3.451</idno>
		<ptr target="https://doi.org/10.1037//0096-1523.21.3.451" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human perception and performance</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="451" to="468" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Blinded by the load: Attention, awareness and the role of perceptual load</title>
		<author>
			<persName><forename type="first">N</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Konstantinou</surname></persName>
		</author>
		<idno type="DOI">10.1098/rstb.2013.0205</idno>
		<ptr target="https://doi.org/10.1098/rstb.2013.0205" />
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society B: Biological Sciences</title>
		<imprint>
			<biblScope unit="volume">369</biblScope>
			<date type="published" when="1641">2014. 1641. 20130205</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Lingenfelter</surname></persName>
		</author>
		<ptr target="https://www.34st.com/article/2023/03/sludge-content-tik-tok-subway-surfers/" />
		<title level="m">The absurdity of Tiktok Sludge Content. 34th Street Magazine</title>
		<imprint>
			<date type="published" when="2023-03-16">2023. March 16</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unattended, distracting or irrelevant&quot;: Theoretical implications of terminological choices in auditory selective attention research</title>
		<author>
			<persName><forename type="first">S</forename><surname>Makov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Har-Shai Yahav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zion Golumbic</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cognition.2022.105313</idno>
		<ptr target="https://doi.org/10.1016/j.cognition.2022.105313" />
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">231</biblScope>
			<biblScope unit="page">105313</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sludge videos are taking over TikTok -and people&apos;s mind</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mattson</surname></persName>
		</author>
		<ptr target="https://www.scientificamerican.com/article/sludge-videos-are-taking-over-tiktok-and-peoples-mind1/" />
	</analytic>
	<monogr>
		<title level="j">Scientific American</title>
		<imprint>
			<date type="published" when="2024-01-10">2024. January 10</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient, helpful, or distracting? A literature review of media multitasking in relation to academic performance</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Elder</surname></persName>
		</author>
		<idno type="DOI">10.1186/s41239-018-0096-z</idno>
		<ptr target="https://doi.org/10.1186/s41239-018-0096-z" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Educational Technology in Higher Education</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multimedia learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Mayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Psychology of learning and motivation</title>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="85" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The past, present, and future of the cognitive theory of multimedia learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Mayer</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10648-023-09842-1</idno>
		<ptr target="https://doi.org/10.1007/s10648-023-09842-1" />
	</analytic>
	<monogr>
		<title level="j">Educational Psychology Review</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A split-attention effect in multimedia learning: Evidence for dual processing systems in working memory</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Moreno</surname></persName>
		</author>
		<idno type="DOI">10.1037/0022-0663.90.2.312</idno>
		<ptr target="https://doi.org/10.1037/0022-0663.90.2.312" />
	</analytic>
	<monogr>
		<title level="j">Journal of Educational Psychology</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="312" to="320" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The pen is mightier than the keyboard: Advantages of longhand over laptop note taking</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Oppenheimer</surname></persName>
		</author>
		<idno type="DOI">10.1177/0956797614524581</idno>
		<ptr target="https://doi.org/10.1177/0956797614524581" />
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1159" to="1168" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Twenty years of load theory-Where are we now, and where should we go next?</title>
		<author>
			<persName><forename type="first">G</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Groeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Greene</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13423-015-0982-5</idno>
		<ptr target="https://doi.org/10.3758/s13423-015-0982-5" />
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1316" to="1340" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cognitive control in media multitaskers</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ophir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Wagner</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.0903620106</idno>
		<ptr target="https://www.pnas.org/doi/full/10.1073/pnas.0903620106" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="15583" to="15587" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A Rosetta Stone for a lost language</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rao</surname></persName>
		</author>
		<ptr target="https://www.ted.com/talks/rajesh_rao_a_rosetta_stone_for_a_lost_language" />
	</analytic>
	<monogr>
		<title level="j">TED Conferences</title>
		<imprint>
			<date type="published" when="2011-03">2011. March</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Spatial contiguity and spatial split-attention effects in multimedia learning environments: A meta-analysis</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Cenkci</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10648-018-9435-9</idno>
		<ptr target="https://doi.org/10.1007/s10648-018-9435-9" />
	</analytic>
	<monogr>
		<title level="j">Educational Psychology Review</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="679" to="701" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Meta-analysis of the sensitivity decrement in vigilance</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Howe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Warm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">N</forename><surname>Dember</surname></persName>
		</author>
		<idno type="DOI">10.1037/0033-2909.117.2.230</idno>
		<ptr target="https://doi.org/10.1037/0033-2909.117.2.230" />
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="230" to="249" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attentional capture and inattentional blindness</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Simons</surname></persName>
		</author>
		<idno type="DOI">10.1016/S1364-6613(00)01455-8</idno>
		<ptr target="https://doi.org/10.1016/S1364-6613(00)01455-8" />
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="147" to="155" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gorillas in our midst: sustained inattentional blindness for dynamic events</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Simons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Chabris</surname></persName>
		</author>
		<idno type="DOI">10.1068/p281059</idno>
		<ptr target="https://doi.org/10.1068/p281059" />
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1059" to="1074" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Realistic details impact learners independently of split-attention effects</title>
		<author>
			<persName><forename type="first">A</forename><surname>Skulmowski</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10339-022-01123-z</idno>
		<ptr target="https://doi.org/10.1007/s10339-022-01123-z" />
	</analytic>
	<monogr>
		<title level="j">Cognitive Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="187" to="198" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">How algorithms shape our world</title>
		<author>
			<persName><forename type="first">K</forename><surname>Slavin</surname></persName>
		</author>
		<ptr target="https://www.youtube.com/watch?v=ENWVRcMGDoU" />
		<imprint>
			<date type="published" when="2012-11-25">2012, November 25</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">I spent a day with VSAUCE</title>
		<author>
			<persName><forename type="first">Smosh</forename><surname>Alike</surname></persName>
		</author>
		<ptr target="https://www.youtube.com/watch?v=wf6x4FHTMKo" />
		<imprint>
			<date type="published" when="2009">2023. June 9</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">With a little help from my playlists: The impact of background music on sustained attention performance</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sümer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Serfas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Büttner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">B</forename><surname>Büttner</surname></persName>
		</author>
		<idno type="DOI">10.1037/xhp0001374</idno>
		<ptr target="https://doi.org/10.1037/xhp0001374" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance. Advance online publication</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">How not to fool ourselves about the heterogeneity of treatment effects</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Von Hippel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Schuetze</surname></persName>
		</author>
		<idno type="DOI">10.1177/25152459241304347</idno>
		<ptr target="https://doi.org/10.1177/25152459241304347" />
	</analytic>
	<monogr>
		<title level="j">Advances in Methods and Practices in Psychological Science</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Weaver</surname></persName>
		</author>
		<ptr target="https://www.cbc.ca/news/entertainment/sludge-content-1.6716185" />
		<title level="m">Sludge content is consuming TikTok. Why aren&apos;t we talking about it? CBC News</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The generalizability crisis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yarkoni</surname></persName>
		</author>
		<idno type="DOI">10.1017/S0140525X20001685</idno>
		<ptr target="https://doi.org/10.1017/S0140525X20001685" />
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="page" from="45" to="e46" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
