<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Using Large Language Models to Estimate Belief Strength in Reasoning</title>
				<funder ref="#_vSWbWzH #_24Wg4Mz">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
				<funder ref="#_P5PfDBw">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Jérémie</forename><surname>Beucler</surname></persName>
							<email>jeremie.beucler@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">LaPsyDÉ</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Université Paris-Cité</orgName>
								<address>
									<postCode>F-75005</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zoe</forename><surname>Purcell</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">LaPsyDÉ</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Université Paris-Cité</orgName>
								<address>
									<postCode>F-75005</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lucie</forename><surname>Charles</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Biological and Behavioural Sciences</orgName>
								<orgName type="institution">University of London</orgName>
								<address>
									<settlement>London</settlement>
									<region>Queen Mary</region>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wim</forename><surname>De Neys</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">LaPsyDÉ</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Université Paris-Cité</orgName>
								<address>
									<postCode>F-75005</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">LaPsyDÉ</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Université Paris-Cité</orgName>
								<address>
									<addrLine>46, rue Saint-Jacques</addrLine>
									<postCode>F-75005</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Using Large Language Models to Estimate Belief Strength in Reasoning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">16EEF700D12D3682DA5F8C50B5EEE1EF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-10-21T13:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>base-rate neglect</term>
					<term>belief strength</term>
					<term>heuristic</term>
					<term>large language models</term>
					<term>open-access database</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Accurately quantifying belief strength in heuristics-and-biases tasks is crucial yet methodologically challenging. In this paper, we introduce an automated method leveraging large language models (LLMs) to systematically measure and manipulate belief strength. We specifically tested this method in the widely used "lawyer-engineer" base-rate neglect task, in which stereotypical descriptions (e.g., someone enjoying mathematical puzzles) conflict with normative base-rate information (e.g., engineers represent a very small percentage of the sample).</p><p>Using this approach, we created an open-access database containing over 100,000 unique items systematically varying in stereotype-driven belief strength. Validation studies demonstrate that our LLM-derived belief strength measure correlates strongly with human typicality ratings and robustly predicts human choices in a base-rate neglect task. Additionally, our method revealed substantial and previously unnoticed variability in stereotype-driven belief strength in popular base-rate items from existing research, underlining the need to control for this in future studies.</p><p>We further highlight methodological improvements achievable by refining the LLM prompt, as well as ways to enhance cross-cultural validity. The database presented here serves as a powerful resource for researchers, facilitating rigorous, replicable, and theoretically precise experimental designs, as well as enabling advancements in cognitive and computational modeling of reasoning.</p><p>To support its use, we provide the R package baserater, which allows researchers to access the database to apply or adapt the method to their own research.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Using Large Language Models to Estimate Belief Strength in Reasoning</head><p>Consider the following problem adapted from the classic "lawyer-engineer" problem <ref type="bibr" target="#b15">(Kahneman &amp; Tversky, 1973)</ref>:</p><p>There is an event with 1,000 people, of which 996 are lawyers and 4 are engineers. Jack is a randomly chosen participant who attended the event. He is a 45-year-old man, who is married and has four children. He is generally conservative, careful, and ambitious. He shows no interest in political and social issues and spends most of his free time on his many hobbies which include home carpentry, sailing, and mathematical puzzles. Which is more likely: that Jack is an engineer or a lawyer?</p><p>Despite the overwhelming base-rate probability that only 4 out of 1,000 attendees are engineers, most people incorrectly choose the "engineer" option-reacting to stereotypical cues, such as an affinity for mathematical puzzles, rather than the base-rate information. This inclination to overlook base-rate information is known as base-rate neglect (for a review, see <ref type="bibr">Pennycook et al., 2022)</ref>, a well-documented cognitive bias with significant implications for decision-making in various domains, including medicine (e.g., <ref type="bibr" target="#b2">Bergus et al., 1995)</ref> and justice <ref type="bibr">(Thompson &amp; Schumann, 2017)</ref>.</p><p>Base-rate neglect and similar cognitive biases are well accounted for by dual-process theories, which describe human thinking as an interplay between fast, effortless intuitive ("System 1") processing and slower, more effortful, deliberative ("System 2") processing (e.g., <ref type="bibr" target="#b11">Evans &amp; Stanovich, 2013;</ref><ref type="bibr" target="#b14">Kahneman, 2011)</ref>. According to these theories, cognitive biases arise primarily because individuals frequently rely on System 1 processing, using heuristic cues-mental shortcuts-that facilitate quick but often inaccurate decisions.</p><p>Although dual-process theories have significantly advanced our understanding of human cognition, precisely how heuristic and logical-probabilistic information (e.g., base-rates) interact within these systems remains unclear. One critical barrier to resolving this issue has been methodological: researchers lack precise, systematic methods to measure and manipulate belief strength-the degree to which heuristic cues influence reasoning (also sometimes referred to as heuristic strength in the literature). A belief corresponds to prior knowledge about the world. In the reasoning literature, the role of belief has been most clearly articulated in the context of belief bias in syllogistic reasoning, where such prior knowledge interferes with judgments of logical validity <ref type="bibr" target="#b10">(Evans et al., 1983)</ref>. For instance, when asked to evaluate the logical validity of the following syllogism: "No addictive things are inexpensive. Some cigarettes are inexpensive.</p><p>Therefore, some cigarettes are not addictive," participants tend to rate this valid argument as invalid because it contradicts their prior belief that all cigarettes are addictive. In the context of the "lawyer-engineer" problem, these beliefs take the form of stereotype-driven expectations, for example the belief that someone who enjoys mathematical puzzles is likely to be an engineer.</p><p>Here, belief strength is best understood as the strength of the stereotype, that is, the associative strength between a descriptive trait and a social category.</p><p>Typically, the information triggering the heuristic is verbal, as it needs to activate prepotent, automatic responses such as stereotypes in the base-rate neglect task. Consequently, measuring belief strength is costly and requires repetitive individual ratings from human participants to be correctly estimated. This has constrained researchers' ability to accurately measure and manipulate belief strength in reasoning tasks. Addressing this critical methodological gap is the primary aim of the current paper.</p><p>At their core, all heuristics-and-biases problems, such as the "lawyer-engineer" problem above, involve a conflict between (a) information that cues an intuitive, heuristic response (e.g., stereotype information suggesting Jack is an engineer) and (b) information that should be considered according to normative principles of logic and probability (e.g., base-rate information indicating Jack is far more likely to be a lawyer).</p><p>To date, researchers have primarily manipulated two aspects of such problems. First, researchers have varied the alignment or conflict between heuristic and logical information by reversing base-rate probabilities to create "no-conflict" scenarios (e.g., changing the example to 996 engineers and 4 lawyers; De <ref type="bibr" target="#b7">Neys et al., 2011;</ref><ref type="bibr" target="#b8">De Neys &amp; Glumicic, 2008;</ref><ref type="bibr">Stupple &amp; Ball, 2008;</ref><ref type="bibr">Stupple et al., 2011)</ref>. Second, they have systematically varied the strength of logical information itself, for instance, using extremely skewed base-rates (e.g., 995 lawyers/5 engineers) versus moderately skewed base-rates (e.g., 700 lawyers/300 engineers; <ref type="bibr" target="#b23">Pennycook et al., 2015)</ref>.</p><p>To our knowledge, however, no comparable systematic manipulations or precise measurements of belief strength have yet been developed.</p><p>On a purely methodological level, precise measurement of belief strength would enable researchers to control for variability across items. Indeed, current research typically assumes uniformly strong belief strength across stimuli, but unaccounted variability could significantly impact item reliability. For example, consider the widely used rapid-response base-rate neglect items <ref type="bibr" target="#b22">(Pennycook et al., 2014)</ref>, which use single adjectives to cue stereotypes instead of a lengthy individuating description (e.g., "There are 995 secretaries and 5 drummers. Person 'L' is loud. Is person 'L' more likely to be a secretary or a drummer?"). Each item assumes a consistent, high stereotype-driven belief strength. However, it is questionable whether each adjective points equally strongly toward one group compared to the other, across the different base-rate items.</p><p>While variations in belief strength across items might not be important for experiments with large item sets that examine robust effects, they could undermine the validity of studies relying on small item sets or exploring subtle interactions (e.g., <ref type="bibr" target="#b1">Bago &amp; De Neys, 2020)</ref>. This issue is particularly relevant in studies that divide items into subsets, such as training or debiasing studies using prepost designs, where an imbalance in belief strength between subsets of items could bias the results if not properly counterbalanced. On a more theoretical level, accurately measuring and manipulating belief strength is essential for understanding cognitive processes underlying heuristics-and-biases tasks. Currently, relying primarily on binary manipulations (e.g., conflict versus no-conflict) restricts researchers' ability to precisely examine how participants weigh heuristic versus logical information. Such limited stimulus variation also constrains computational modeling in reasoning research, as multiple competing models or functions could fit participants' data equally well, as illustrated in Figure <ref type="figure" target="#fig_0">1</ref>. Parametric manipulations of heuristic information would thus offer greater sensitivity, enabling researchers to better distinguish between competing reasoning models <ref type="bibr" target="#b6">(De Neys, 2023)</ref>. This would align reasoning research with practices in other cognitive science fields-such as perception or reinforcement learning-where researchers routinely use precise stimulus manipulations to dissect underlying cognitive or computational processes.</p><p>To address these methodological and theoretical limitations, recent advances in natural language processing (NLP)-particularly large language models (LLMs)-offer a promising new approach. LLMs have successfully enabled the automation of verbal rating tasks with very high reliability (e.g., <ref type="bibr" target="#b9">DiStefano et al., 2024;</ref><ref type="bibr" target="#b18">Le Mens et al., 2023;</ref><ref type="bibr">Ornstein et al., 2023)</ref>. In this paper, we introduce a novel automated method using LLMs to systematically quantify belief strength in base-rate neglect items. We implement this approach using two high-performance LLMs: GPT-4 (OpenAI, 2023) and LlaMa 3.3-70B-Instruct <ref type="bibr">(Grattafiori et al., 2024)</ref>.</p><p>We validate this automated measure against human judgments by examining both explicit participant ratings and, importantly, actual choice patterns using these items. We further demonstrate the utility of our approach by applying it to widely used base-rate neglect stimuli from previous research <ref type="bibr" target="#b23">(Pennycook et al., 2015)</ref>, revealing substantial, previously unnoticed variability in belief strength. Finally, we provide an extensive, open-access database of over 100,000 unique base-rate neglect items, following the rapid-response format from <ref type="bibr" target="#b22">Pennycook et al. (2014)</ref>. This comprehensive resource facilitates more precise, rigorous, and replicable future research. To support such use, we also provide the R package baserater <ref type="bibr" target="#b3">(Beucler, 2025)</ref>, which allows researchers to access the database and apply or adapt the method to their own experimental needs easily.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 1: Typicality Ratings Validation</head><p>Typicality ratings reflect the extent to which specific traits are perceived as representative of specific groups (e.g., "Nurses are typically kind"). In the present framework, typicality operationalizes belief strength by capturing how strongly a descriptive trait is associated with a given group. Experiment 1 aimed to validate our automated method for estimating typicality ratings using LLMs. We generated these ratings systematically with LLMs and compared them to human-generated ratings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Base-Rate Task</head><p>We focus on the rapid-response base-rate task <ref type="bibr" target="#b22">(Pennycook et al., 2014)</ref>, widely used in reasoning research for its standardized and concise format. In each trial, a short vignette presents the composition of the sample (e.g., "This study contains nurses and politicians"), a description of a person with a neutral name and an adjective cueing a stereotype associated with one of the groups in the sample (e.g., "Person 'L' is kind"), and base-rate information (e.g., "There are 5 nurses and 995 politicians"). The task is to indicate which group the person most likely belongs to (e.g., "Is Person 'L' more likely to be a nurse or a politician?").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Estimation of Typicality Ratings Using LLMs</head><p>Stereotypes as Likelihood Estimates. In our base-rate neglect example, the information triggering the heuristic response-which we aim to quantify-is the stereotype embedded in the person's description. In this context, we treat stereotype strength as a specific type of belief strength-namely, a belief about category membership informed by stereotypical cues. Formally, a stereotype (e.g., "Nurses are kind") can be expressed as a conditional probability, or likelihood, of observing a specific trait (e.g., kind) given that one belongs to a specific group (e.g., nurse): p(trait|group). For instance, in a base-rate item such as: "There are 995 politicians and 5 nurses.</p><p>Person 'L' is kind. Is person 'L' more likely to be a politician or a nurse?", both (a) the likelihood and (b) the base-rate information must be integrated to accurately estimate the probability.</p><p>According to Bayes' theorem, the posterior probability that person 'L' is a nurse given that she is kind is expressed as follows:</p><formula xml:id="formula_0">𝑝(𝐻|𝐷) = !(#)!(%|#) !(#)!(%|#)'!(¬#)!(%|¬#) (1)</formula><p>where p(H) and p(¬H) are the base-rate probabilities that person 'L' is a nurse or a politician, respectively, and p(D|H) and p(D|¬H) are the likelihoods of observing the trait "kind" given that person 'L' is a nurse and a politician, respectively. To quantify the strength of the stereotype in each base-rate neglect item, we thus need to quantify this likelihood information for each groupadjective pair (e.g., for nurse-kind and politician-kind). In this context, an item with a high stereotype-driven belief strength will have a large disparity in likelihood between groups -such as an item where a trait is much more strongly associated with one category than the other.</p><p>In natural language, this likelihood is often captured by statements of "typicality." For instance, saying "Nurses are typically kind" expresses that the probability a person is kind, given that they are a nurse, is high. Because LLMs are trained on extensive human-generated datasets, they inherently capture broad societal biases and perceptions, making them well-suited for estimating stereotype-based likelihoods. Prior research by Le <ref type="bibr" target="#b18">Mens et al. (2023)</ref> demonstrated that LLMs can generate "typicality" ratings-quantifying how representative certain attributes are within specific categories-achieving near-perfect correlation (r &gt; 0.9) with aggregated human judgments in domains such as literary genres and political affiliations.</p><p>Prompt Design. Building on this approach, we use LLMs to estimate the typicality of a given trait for a specific group. For each group-adjective pair (e.g., drummer-loud), we ask the LLM to rate how well the adjective (ADJECTIVE) describes the prototypical member of a group (GROUP). This approach directly follows <ref type="bibr" target="#b23">Pennycook et al. (2015)</ref>, who created base-rate items by asking participants to select traits they felt best represented prototypical group members. To help the LLM quantify this relationship clearly, we provided additional context through three illustrative examples (i.e., few-shot learning). Each LLM prompt consisted of two distinct sections: a context-setting instruction (system prompt) followed by detailed instructions and examples (user prompt). An example prompt is provided below (words in capital letters represent variables that change with each adjective or group):</p><p>System prompt "You are expert at accurately reproducing the stereotypical associations humans make, in order to annotate data for experiments. Your focus is to capture common societal perceptions and stereotypes, rather than factual attributes of the groups, even when they are negative or unfounded."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>User prompt</head><p>"Rate how well the adjective "ADJECTIVE reflects the prototypical member of the group "GROUP" on a scale from 0 ("Not at all") to 100 ("Extremely").</p><p>To clarify, consider the following examples:</p><p>1. 'Rate how well the adjective "FUNNY" reflects the prototypical member of the group "CLOWN" on a scale from 0 (Not at all) to 100 (Extremely).' In this example, you would likely give a high rating because the adjective 'FUNNY' closely aligns with the typical characteristics of a 'CLOWN.' 2. 'Rate how well the adjective "FEARFUL" reflects the prototypical member of the group "FIREFIGHTER" on a scale from 0 (Not at all) to 100 (Extremely).' In this example, you would likely give a low rating because the adjective 'FEARFUL' diverges significantly from the typical characteristics of a 'FIREFIGHTER.'</p><p>3. 'Rate how well the adjective "PATIENT" reflects the prototypical member of the group "ENGINEER" on a scale from 0 (Not at all) to 100 (Extremely).' In this example, you would likely give a moderate rating falling around the middle of the scale, because the adjective "PATIENT" neither closely aligns nor diverges significantly from the typical characteristics of an "ENGINEER."</p><p>Your response should be a single score between 0 and 100, with no additional text, letters, or symbols included."</p><p>LLMs Specification and Parameters. We used two high-performance LLMs: GPT-4</p><p>(version gpt-4-0613, queried in late April 2024 via OpenAI's API) and the LlaMa 3.3-70B-Instruct model (accessed via Hugging Face's hosted inference API). GPT-4 was chosen for its state-of-theart performance, while LlaMa 3.3, a leading open-weight and publicly accessible model, was included to enhance transparency and reproducibility.</p><p>To account for variability in the model's responses, we follow the general approach described by Le <ref type="bibr" target="#b18">Mens et al. (2023)</ref> and generate 20 independent typicality scores for each groupadjective combination. To control variability in the model's responses, we set two sampling parameters to their default values of 1: temperature, which controls randomness in token selection (higher temperature increases randomness, lower temperature produces more deterministic responses), and "Top-P", which means the model considers the entire posterior probability distribution of candidate tokens (lower values would limit sampling to the most probable tokens, whereas a value of 1 includes all tokens). In rare cases when the model fails to return a numerical response, we resubmit the prompt until obtaining a total of 20 valid ratings. The final likelihood estimate p(adjective|group) is computed as the average of these 20 typicality scores divided by 100 to yield a probability ranging between 0 and 1. Note that individual scores were not recorded following aggregation since they showed very little variability. Supplementary Material 1 includes density plots and summary statistics describing the distribution of typicality ratings produced by each model across all group-adjective combinations.</p><p>Typicality Matrix. We selected our list of groups and adjectives from the base-rate items in <ref type="bibr" target="#b23">Pennycook et al. (2015)</ref>. The groups consisted of various professions chosen associated with common stereotypes (e.g., surgeon, artist, clown), while the adjectives reflected personality traits perceived as stereotypical (e.g., nerdy, arrogant, kind). Since our goal was to combine every group pairwise, we removed generic groups (poor people, rich people, girls, men, women) that could lead to ambiguous base-rate items creating class inclusion issues (e.g., "Is it more likely that the person is a man or a doctor?"). Additionally, we expanded the existing material by adding 14 additional groups (e.g., psychologist, soldier, fashion designer) and 41 new adjectives (e.g., naive, altruistic, shy), resulting in a total of 58 groups and 66 adjectives (see Supplementary Material 2 for the full list).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Typicality Ratings Validation Experiment</head><p>In Experiment 1, we collect individual human typicality ratings using the same procedure that we use with the LLMs (e.g., "Rate how well the adjective 'nerdy' reflects the prototypical member of the group 'computer scientist'"). Our goal was to examine the correlation between human-generated ratings and those generated by the LLM.</p><p>Participants. Participants were recruited through the Prolific platform (www.prolific.com) and compensated £1.00 for 10 minutes of participation. Since stereotypes are likely culture-specific, we recruited only native English-speaking North-American participants from the U.S. or Canada, consistent with the original study by <ref type="bibr" target="#b23">Pennycook et al. (2015)</ref>, which tested Canadian participants. A total of 50 participants were recruited (26 females; M age = 37.4, SD = 12.3), of whom 32% reported high school, 44% a bachelor's degree, 22% a master's degree, and 2% a PhD as their highest level of education.</p><p>Procedure. At the start of the experiment, participants reviewed the three examples provided to the LLM (see prompt above). Participants then completed a rating task for a subset of 100 groupadjective combinations sampled from the full LLM-generated typicality matrix. The subset was chosen to uniformly represent the entire range of typicality scores present in the full database (Experiment 1 GPT-4 typicality range: 11.3-95.8; full LLM database GPT-4 typicality range: 7.4-99.8). For each combination, participants rated how well an adjective described the prototypical member of a specified group (e.g., "Rate how well the adjective 'IDEALISTIC' reflects the prototypical member of the group 'WRITER'"). These combinations were presented in a random order. Participants provided ratings using a visual analog scale ranging from 0 ("Not at all") to 100</p><p>("Extremely"), with the selected rating displayed numerically above the scale. As shown in Figure <ref type="figure" target="#fig_1">2A</ref>, typicality ratings from GPT-4 closely matched participants' average ratings in Experiment 1 (r = 0.88, p &lt; .001), with LlaMa 3.3 showing slightly lower but still strong correlation (r = 0.82, p &lt; .001). Note that averaging the typicality ratings produced by GPT-4 and LlaMa 3.3 produced only a very modest improvement (r = .89), indicating that combining both models offers limited additional predictive accuracy over GPT-4 alone, which may be explained by the high correlation between the two LLMs' ratings (see Supplementary Material 3). LLM ratings can thus closely approximate the average human typicality rating in the context of stereotypes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>To better compare LLM performance to human raters, we use the Equivalent Number of Observations (ENO) score, following Le <ref type="bibr" target="#b18">Mens et al. (2023)</ref>. The ENO score quantifies how many human ratings are required to achieve predictive accuracy equivalent to that of a given model.</p><p>Predictive accuracy, in this context, is defined as the average Pearson correlation between participants' typicality ratings and the ratings generated by the model.</p><p>To compute the ENO score, we performed a bootstrap analysis. Specifically, we correlated the ratings from one random participant (the holdout) with the average ratings from varying numbers of other randomly selected participants. We repeated this process 1,000 times for each sample size (ranging from 1 to 49 participants in our case). By comparing the average correlations from these bootstrap samples to the model's predictive accuracy, we determined the ENO scorethe number of human ratings required to match the model's performance. A higher ENO score indicates greater predictive accuracy, as it corresponds to less noise and more consistent judgments, similar to how increasing sample size reduces variance and improves reliability.</p><p>The results are shown in Figure <ref type="figure" target="#fig_1">2B</ref>. GPT-4 outperformed LlaMa 3.3, with an average correlation of r = 0.63 with a single human rating-equivalent to the performance of four to five aggregated human raters. In comparison, LlaMa 3.3 reached an average correlation of r = 0.58, matching the performance of two to three aggregated human raters. Importantly, increasing the number of human raters only yields marginal gains, as it eventually plateaus at approximately r = 0.70.</p><p>Finally, to assess the robustness of our approach, we explored alternative methods for eliciting typicality ratings from the LLMs by varying model settings and using alternative prompts.</p><p>Overall, the correlation with human typicality ratings was minimally affected by changes in settings or prompts (LlaMa 3.3: r = 0.78-0.82; GPT-4: r = 0.85-0.89). The detailed results of these analyses are reported in Supplementary Material 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 2: Measuring Stereotype Strength in Base-Rate Neglect Items</head><p>In Experiment 1, we validated our approach by demonstrating that LLM-generated typicality ratings closely align with explicit human judgments. Although this correlation provides strong evidence for the validity of our automated method, a critical next step is to examine whether these ratings can predict actual behavioral choices. To address this, we compute a measure of stereotype strength-based on the relative typicality between two groups-and examine its predictive validity in a base-rate neglect task using our newly created item database in Experiment 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stereotype Strength Measure</head><p>A typical rapid-response base-rate neglect item from <ref type="bibr" target="#b22">Pennycook et al. (2014)</ref> includes two groups (e.g., nurses and politicians), one descriptive adjective (e.g., "kind"), and two corresponding typicality ratings-one rating for how typical the adjective is of each group.</p><p>Because typicality ratings were provided on a 0-100 scale, we converted them to likelihoods by dividing each rating by 100. Likelihoods of zero were offset (1e-6) to allow calculations of the log ratio. This included 0.55% of LlaMa 3.3 ratings and 0% of GPT-4 ratings. To ensure this postprocessing did not affect our conclusions, we examined the correlations between human and LlaMa ratings before and after excluding raw zero ratings; this exclusion did not impact the correlation (see Supplementary Material 5). To quantify how strongly a given adjective favors one group over the other, we computed stereotype strength as the logarithm of the ratio between these two group-adjective likelihoods:</p><formula xml:id="formula_1">𝑙𝑜𝑔 !()|* ! ) !()|* " )<label>(2)</label></formula><p>where p(A|G) is the likelihood derived from the typicality rating for adjective A given group G.</p><p>This log ratio provides a symmetrical measure that quantifies the strength of association between the trait and each group. A log ratio of zero indicates equal likelihood of the trait in both groups, meaning the adjective is uninformative with respect to the two groups. Positive values suggest a stronger association with the first group, while negative values indicate a stronger association with the second group.</p><p>For instance, consider the two following items from <ref type="bibr" target="#b23">Pennycook et al. (2015)</ref>: computer programmers-construction workers-nerdy and consultants-aerobics instructors-helpful. We might imagine that the likelihood of being perceived as nerdy, given that one is a computer programmer, is high, p(nerdy|computer programmer) = 0.8, while it is much lower for a construction worker, p(nerdy|construction worker) = 0.1. This yields a log ratio of log(0.8/0.1) = 2.08, which strongly favors the computer programmers group over the construction workers group.</p><p>Conversely, we might imagine that both p(helpful|teacher) and p(helpful|doctor) are high, say 0.8, which results in a log ratio of log(0.8/0.8) = 0. This means that, in this case, the item will fail to elicit a strong heuristic response.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Construction of Base-Rate Item Database</head><p>Figure <ref type="figure">3</ref>. Overview of the base-rate item database pipeline. The typicality of each adjective is first computed for each group using an LLM (step 1) to create the typicality matrix. Each typicality score can be interpreted as the conditional probability of having a specific trait given that one belongs to a particular group (e.g., the probability of being kind given that one is a computer scientist). Next, for each pairwise combination of groups with one adjective (e.g., clown-nursefunny), the stereotype strength is determined by calculating the log ratio of the two typicality scores/conditional probabilities, to create the final base-rate item database (step 2).</p><p>To create our base-rate item database, we first computed the typicality rating of each adjective for each group using the LLM method described above, separately for GPT-4 and LlaMa 3.3. This resulted in a 66-adjective × 58-group typicality matrix. We then created individual baserate items, each consisting of two groups and one adjective (e.g., clown-nurse-funny). We generated every possible pairwise combination of groups (1,653 combinations) and computed the stereotype strength for each of the 66 adjectives, yielding a total of 109,098 base-rate items. The full pipeline is described in Figure <ref type="figure">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Base-Rate Neglect Validation Experiment</head><p>In Experiment 2, we use our newly created item database in the rapid-response base-rate neglect task <ref type="bibr" target="#b22">(Pennycook et al., 2014)</ref>, where we systematically vary stereotype strength to see whether it accurately predicts participants' choices. Note that we rely on GPT-4 measures, as they proved to be more correlated with human judgments than those derived from LlaMa 3.3 (see Experiment 1 results).</p><p>Participants. We recruited 151 native English-speaking participants from the U.S. or Canada (74 females; M age = 39, SD = 11.9) via Prolific, who were compensated £3.00 for 30 minutes. 33% reported high school, 48% a bachelor's degree, 15% a master's degree, and 4% a PhD as their highest education level.</p><p>Procedure. Participants solved 240 base-rate neglect problems, divided into 4 blocks of 60 trials. Each trial began with a fixation cross presented during 500 ms, followed by the sample composition (e.g., "This study contains computer programmers and hippies."), a brief description of an individual with a neutral name and adjective (e.g., "Person 'G' is nerdy."), and the base-rate information (e.g., "There are 50 computer programmers and 950 hippies."). Participants indicated the most likely group membership by pressing 'C' or 'N' to select the left or right response, respectively. Confidence ratings were collected after each response.</p><p>Items were dynamically and randomly sampled for each participant from the base-rate neglect database to ensure uniform coverage of stereotype strength. To achieve this, we divided the full set of items into predefined bins based on the difference between the two typicality scores (e.g., 0-14, 14-28, …, up to 84-the highest observed difference in the dataset). These bins allowed us to sample items across the full range of stereotype strength values in a controlled manner. Participants thus saw different, unique base-rate items, matched in stereotype strength according to these stereotype strength difference bins-a procedure made possible by the large number of items available in our database. In parallel, we used ten different base-rate ratios ranging from 50/950 to 950/50 in steps of 100, so that some problems involved balanced base rates while others were more extreme. Each base-rate ratio was paired with an equal number of items from each stereotype-strength bin to maintain balanced coverage across the full range of both dimensions.</p><p>While item selection was based on typicality score differences, our analyses rely on the absolute log ratio of category likelihoods, which provides a more theoretically grounded measure of stereotype strength (see above). Since our primary interest is the relationship between stereotype strength and participant choices, we collapsed our analyses across base-rate values. Note that when stereotype strength is high but base-rates are weak for a given response option, choosing the stereotype-consistent option is not always the normatively correct response under Bayes' rule.</p><p>However, our focus here is not on normative accuracy, but on how strongly stereotype strength predicts human choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>LLMs' typicality ratings closely match human judgments, but a more critical test is whether our stereotype-driven belief strength measure (i.e., the log ratio of typicality ratings) predicts actual human decisions in a base-rate neglect experiment (Experiment 2). In all analyses, response choice was coded as 1 when participants selected the option on the left and 0 otherwise. Stereotype  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis of Existing Base-Rate Items</head><p>The previous analysis showed that our stereotype-based belief strength measure effectively predicts participants' choices in a controlled base-rate neglect task, highlighting its validity.</p><p>However, an important question is how strongly the items from prior research-designed to evoke stereotypical responses-are characterized by our metric. Do they score uniformly high on our stereotype strength measure?</p><p>In this section, we leverage our method to quantify the stereotype strength of our selection of base-rate neglect items previously used by <ref type="bibr" target="#b23">Pennycook et al. (2015)</ref>. Since we excluded items involving overly generic groups (e.g., "poor people" or "girls"; see Method section above), our analysis focuses on 88 of the original 132 items. We then compare these to the items in our newly created base-rate item database. items). Note that the log ratios in the database are always positive by construction, since we ensured that the item with the highest typicality ratings was always in the numerator of the ratio.</p><p>As shown in Figure <ref type="figure" target="#fig_4">5A</ref>, none of the existing base-rate items elicit a stereotype in the opposite direction. In other words, no item has a negative log ratio value, where the group favored by the adjective differs from the one assumed by the item. Nonetheless, the stereotype strength of these items varies widely, spanning from 0.02 to 2.17 in absolute log ratio values (M = 0.97, SD = 0.54).</p><p>Supplementary Material 6 shows the stereotype strength distributions based on ratings from LlaMa 3.3.</p><p>To illustrate the potential behavioral implications of this variability, we used our generalized logistic mixed-effects model fitted on the data from Experiment 2 (i.e., the base-rate neglect experiment). The model allows us to predict response choice as a function of the GPT-4 stereotype strength in the existing items with a very good fit (see Figure <ref type="figure" target="#fig_3">4</ref>). For the weakest item in the existing dataset (consultant -aerobics instructor -helpful), the log ratio is 0.02, predicting a 51% probability of selecting "consultant". This suggests that participants would be at chance level when choosing based on the stereotype (corresponding to the midpoint of the sigmoid curve in Figure <ref type="figure" target="#fig_3">4</ref>). Conversely, for the strongest item (high school coach -librarian -loud), the log ratio is 2.17, predicting that "high school coach" would be selected 94% of the time. Overall, for the average log ratio value in existing items (0.97), the model predicts that the participant would choose this group 77% of the time. This wide variation in stereotype strength among base-rate items could thus be problematic for experimental validity, as weaker stereotypes may inconsistently trigger heuristic responses, undermining reliability and replicability of results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overview of the Base-Rate Item Database</head><p>Figure <ref type="figure" target="#fig_4">5B</ref> shows the full distribution of stereotype strengths within the database, while Supplementary Material 7 summarizes key statistics separately for the LlaMa 3.3 and GPT-4 ratings. As expected, most adjectives do not favor either of the two groups, resulting in a stereotype strength distribution heavily skewed to the right. However, given the substantial size of the database (109,098 items), it still contains many high-strength stereotype items. For instance, while Figure <ref type="figure" target="#fig_4">5B</ref> may give the impression that very few items exceed a (high) GPT-4 stereotype strength of 1.5, the database actually contains 2,446 such items, providing a rich dataset of high stereotype items. For illustration, Figure <ref type="figure">6</ref> shows an example of all 1,653 possible items that can be created based on one of our 66 adjectives ("arrogant"). In addition, Supplementary Material 8 provides examples of these items along with their associated typicality ratings and stereotype strengths from both GPT-4 and LlaMa 3.3.</p><p>Figure <ref type="figure">6</ref>. Illustration of all possible items for the adjective "Arrogant." The upper part of the matrix shows stereotype strength, computed as the log-ratio of GPT-4 typicality ratings (column / row). Higher values (darker red) indicate that the column group is rated as more "arrogant" than the row group (e.g., "politician" over "kindergarten teacher" in the top-left cell). The lower part of the matrix shows predicted human choice probabilities from the mixed-effects logistic model fit to the base-rate neglect task (Experiment 2), collapsed across base-rate values. Higher values (darker green) reflect a greater likelihood of choosing the row group over the column group based on the stereotype (e.g., "politician" over "kindergarten teacher" in the bottom-right cell). The bar plots on the margins show the raw GPT-4 typicality ratings across each group, where taller and redder bars indicate higher typicality scores for that group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>In this paper, we introduce an automated method to measure and quantify belief strength in heuristics-and-biases research using LLMs focusing on the popular base-rate neglect task. Using this approach, we created a comprehensive, open-access database containing over 100,000 unique base-rate neglect items. Importantly, the database spans a wide range of stereotype-driven belief strengths, allowing researchers to systematically vary the strength of the corresponding heuristic response in a precise manner.</p><p>We validated this method by comparing it with human judgments, using both participant typicality ratings (i.e., how typical a description is of a specific group; Experiment 1) and their actual choices on the newly generated base-rate items (Experiment 2). Our results show that our automated measure of belief strength, generated with LLMs, is highly correlated with human typicality judgments. More importantly, the LLM-derived belief strength measure strongly predicts human choices in a base-rate neglect task, so that the stronger the belief strength induced by a stereotype, the more likely participants are to choose the option that aligns with the stereotype.</p><p>Overall, GPT-4 outperformed the publicly available LlaMa 3.3 model, as GPT-4's ratings showed higher correlations with human typicality judgments. Additionally, averaging ratings from both models did not yield substantial improvements in performance. While we report results from LlaMa 3.3 to support research transparency and reproducibility, we recommend using GPT-4 ratings for applications where performance is critical.</p><p>Our LLM-derived measure is thus a highly cost-effective proxy for the average belief strength across a population. Measuring belief strength using human raters involves averaging judgments from multiple participants. However, the question of how many ratings are needed to produce accurate estimates of belief strength is unclear. Comparing our LMM and human rater approaches, we found that increasing the number of raters beyond a certain threshold yields diminishing returns (see Figure <ref type="figure" target="#fig_1">2B</ref>). Indeed, we observed that interindividual differences in stereotype perception limited the precision achievable by any aggregate measure, human or automated. A more precise measure would involve obtaining individual ratings directly from each participant performing the task, with the need to add an additional session following the main experiment dedicated to estimating those associations.</p><p>However, human-based methods are often impractical due to substantial constraints related to budget, time, and resources, especially when evaluating large numbers of stimuli. Although imperfect, our automated approach effectively addresses these practical challenges by striking an efficient balance between precision and scalability, providing reliable estimates of average belief strength at the group level. Consequently, this approach allows researchers to quickly generate extensive datasets with standardized measures, which would otherwise be difficult to conduct using traditional human-based methods alone. In our case, we generated 3,828 typicality ratings to build the 109,098-item database. Each rating required a single 418-token API call (398 input tokens and 20 output tokens to generate and average 20 individual LLM ratings). The total cost for this process was approximately $54.00 using GPT-4 and $1.42 using LlaMa 3.3. Using the more recent OpenAI GPT-4o model would have cost only about $4.60 for the same task. By comparison, collecting equivalent human data on Prolific, which required five individual ratings per item to achieve similar performance to GPT-4, would have cost about $320 in total, assuming a pay rate of $7.72 per hour per participant and a 30% platform fee. Overall, the LLM-based approach is roughly six times less costly with GPT-4, 70 times less costly with GPT-4o, and 225 times less costly with LlaMa 3.3 than human ratings.</p><p>Finally, we applied this approach to widely used base-rate neglect items from <ref type="bibr" target="#b23">Pennycook et al. (2015)</ref>, which were all designed to elicit a strong heuristic response by presenting descriptions that strongly favored one group over the other. Our analysis revealed substantial variation in belief strength driven by stereotype content across these items. Specifically, some items exhibited high stereotype-based belief strength, while others were comparatively less effective in eliciting a strong belief. This variability has important consequences for reasoning research. In the context of a base-rate neglect experiment, for instance, this suggests that existing items may not equally trigger heuristic responses, potentially affecting the reliability and replicability of findings across studies.</p><p>By systematically quantifying belief strength, our approach provides a way to better control for this variability. This enables one to refine item selection or to dynamically adjust belief strength to match experimental needs. To facilitate its broader use, we also provide the R package baserater <ref type="bibr" target="#b3">(Beucler, 2025)</ref>, which allows researchers to access the database and evaluate new base-rate items (e.g., in a different language or with longer stereotypical descriptions). Users can choose a preferred large language model, adjust generation parameters, and customize the prompt to explore whether different models, settings, and prompt formulation better predict the human typicality ratings from Experiment 1. The package can also be used to create a new base-rate item database from scratch, for instance in a different linguistic and cultural context. Access to the LLMs requires an API token from the open-source platform Hugging Face (https://huggingface.co). All functionalities are documented in the package, which includes a tutorial to guide users through typical use cases.</p><p>In this paper, we focused on short base-rate items that are widely used in reasoning research for precise measurement of reaction times or neuroimaging due to their standardized format.</p><p>However, our method could also be used to quantify belief strength in more complex cases-not only simple stereotypes but also multifaceted or abstract beliefs involving multiple attributes or contextual cues. For instance, it can be applied to items using richer stereotypical descriptions, similar to the classic lawyer-engineer problem. These scenarios may offer greater ecological validity, as individuals must evaluate multiple pieces of information rather than a single cue. Note, however, that future work should establish the generalizability of our method to longer and more complex descriptions.</p><p>Similarly, our approach could also be adapted to other heuristics-and-biases tasks. One natural candidate is the conjunction fallacy demonstrated in the well-known "Linda problem" (Tversky &amp; <ref type="bibr">Kahneman, 1983)</ref>. In this task, a stereotypical description triggers a heuristic response conflicting with a fundamental probability rule-that the probability of two events occurring together cannot exceed the probability of either event alone. Using our automated approach, we could systematically manipulate and quantify the belief strength conveyed by stereotypical descriptions in such problems. Additionally, syllogistic reasoning tasks, featuring a conflict between logical validity and the believability of the conclusion, offer another possible application.</p><p>Here too, one could systematically vary the belief strength of the conclusion to explore how heuristic processes interact with logical reasoning. Our method thus provides a generalizable framework for investigating situations in which heuristic responses triggered by verbal descriptions conflict with formal logical principles.</p><p>Our research has some limitations. First, although we used a prompt directly inspired by previous research on LLMs' typicality ratings (Le <ref type="bibr" target="#b18">Mens et al., 2023)</ref> as well as base-rate neglect item construction <ref type="bibr" target="#b23">(Pennycook et al., 2015)</ref>, a more data-driven approach could enhance the predictive power of our belief strength measure. Notably, automatic prompt-engineering techniques (e.g., <ref type="bibr" target="#b0">Abraham et al., 2025)</ref>, which use LLMs to iteratively craft high-quality prompts, could further improve the prediction accuracy of our typicality measure, given that prompt selection can significantly affect performance outcomes (Weber &amp; Reichardt, 2023). However, note that variations in our prompt and LLM settings resulted in only small differences in performance (see Supplementary Material 4).</p><p>Our current base-rate database has only been validated with participants from the U.S. and Canada. Although some stereotypes-such as "clowns are funny"-may appear broadly generalizable, others could be more culture-specific and thus fail to trigger heuristic responses in different populations. It will therefore be important for future work to consider potential intercultural differences. To ensure that belief strength measures remain applicable across cultures, we recommend refining prompts to better align with the target cultural context (e.g., <ref type="bibr" target="#b16">Kovač et al., 2023)</ref>, using fine-tuned LLMs trained specifically on culturally distinct datasets (e.g., <ref type="bibr" target="#b5">Chan et al., 2024)</ref>, or employing culturally adapted LLMs trained on augmented survey data <ref type="bibr" target="#b19">(Li et al., 2024)</ref>.</p><p>By harnessing automated methods and large-scale data generation, our approach provides researchers more control and precision in quantifying belief strength. Note. "Old" groups and adjectives refer to those originally used in <ref type="bibr" target="#b23">Pennycook et al. (2015)</ref>.</p><p>"Your focus is to capture common societal perceptions and stereotypes prevalent within U.S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>culture."</head><p>The results are reported in Table <ref type="table">S3</ref>. Overall, the variation was small, and our current approach was near the best-performing settings, suggesting that our prompt and parameter choices are robust.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Example Items from the Database</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table S5</head><p>Example items from the base-rate database. Note. Stereotype strength corresponds to the log ratio of the ratings as per Equation (2).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Illustration of different hypothetical response functions (linear, sigmoid, and step-like)</figDesc><graphic coords="6,72.00,166.79,468.00,334.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. LLMs ratings are highly predictive of human typicality ratings. a) Relationship between</figDesc><graphic coords="14,72.00,111.60,468.00,234.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>strength was computed as the log ratio of typicality values in favor of the left option: log p(adjective | group left) / p(adjective | group right). Figure4illustrates the proportion of choices as a function of stereotype strength, aggregated across all base-rate conditions. For visualization purposes, we binned the signed log ratio-calculated in favor of the response option presented on the left-into 10 intervals of width 0.5, ranging from -2.5 to 2.5. The figure shows that participants' choices are well predicted by our stereotype strength measure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Proportion of choices as a function of the GPT-4 stereotype-driven belief strength</figDesc><graphic coords="22,72.00,72.00,468.00,334.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Stereotype strength distributions based on the GPT-4 ratings. a) Stereotype strength</figDesc><graphic coords="24,72.00,72.00,468.00,281.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="18,61.35,111.60,468.00,343.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="38,72.00,151.20,468.00,280.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="46,72.00,107.60,468.00,280.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Groups and Adjectives Used in the Database Table S2</head><label></label><figDesc>This methodological advancement not only addresses critical limitations in current research, such as unnoticed variability in belief strength across items, but also substantially improves the ability to distinguish among competing cognitive models of heuristic reasoning. Our open-access database containing Psychological Review, 90(4), 293. https://doi.org/10.1037/0033-295X.90.4.293 Weber, M., &amp; Reichardt, M. (2023). Evaluation is all you need. Prompting Generative Large Language Models for Annotation Tasks in the Social Sciences. A Primer using Open Models.List of groups and adjectives used in the database.</figDesc><table><row><cell cols="7">Pennycook, G., Newton, C., &amp; Thompson, V. A. (2022). Base-rate neglect. In Cognitive Group New Group Adjective New Adjective</cell></row><row><cell cols="2">illusions (pp. 44-60). Routledge. gardener</cell><cell>Old</cell><cell></cell><cell>empathetic</cell><cell></cell><cell>New</cell></row><row><cell>aerobics instructor</cell><cell></cell><cell>Old</cell><cell></cell><cell>stubborn</cell><cell></cell><cell>New</cell></row><row><cell cols="7">https://www.taylorfrancis.com/chapters/edit/10.4324/9781003154730-5/base-rate-neglect-sixteen year old Old trustworthy New</cell></row><row><cell>politician</cell><cell></cell><cell>Old</cell><cell></cell><cell>meticulous</cell><cell></cell><cell>New</cell></row><row><cell cols="7">over 100,000 systematically rated items, validated against human judgments, represents a powerful gordon-pennycook-christie-newton-valerie-thompson kindergarten teacher Old inventive New</cell></row><row><cell cols="7">Group drummer resource that can significantly enhance the rigor, replicability, and theoretical clarity of future New Group Adjective New Adjective Old charismatic New Stupple, E. J. N., &amp; Ball, L. J. (2008). Belief-logic conflict resolution in syllogistic chef Old reserved New</cell></row><row><cell cols="7">farmer bartender heuristics-and-biases research. reasoning: Inspection-time evidence for a parallel-process model. Thinking &amp; Reasoning, 14(2), Old intelligent Old Old altruistic New computer programmer Old arrogant Old pilot Old original New</cell></row><row><cell cols="4">flight attendant social worker 168-181. https://doi.org/10.1080/13546780701739782 Old Old high school coach Old veterinarian Old</cell><cell>nerdy impulsive kind zealous</cell><cell></cell><cell>Old New Old New</cell></row><row><cell>dentist journalist</cell><cell></cell><cell>Old Old</cell><cell></cell><cell>loud rational</cell><cell></cell><cell>Old New</cell></row><row><cell>lawyer police officer</cell><cell></cell><cell>Old New</cell><cell></cell><cell>careful idealistic</cell><cell></cell><cell>Old New</cell></row><row><cell>engineer electrician</cell><cell></cell><cell>Old New</cell><cell></cell><cell>argumentative conservative</cell><cell></cell><cell>Old New</cell></row><row><cell>real estate agent fitness trainer</cell><cell></cell><cell>Old New</cell><cell></cell><cell>persuasive solitary</cell><cell></cell><cell>Old New</cell></row><row><cell>accountant psychologist</cell><cell></cell><cell>Old New</cell><cell></cell><cell>immature passionate</cell><cell></cell><cell>Old New</cell></row><row><cell>surgeon actor</cell><cell></cell><cell>Old New</cell><cell></cell><cell>active adventurous</cell><cell></cell><cell>Old New</cell></row><row><cell>architect historian</cell><cell></cell><cell>Old New</cell><cell></cell><cell>funny cautious</cell><cell></cell><cell>Old New</cell></row><row><cell>librarian DJ</cell><cell></cell><cell>Old New</cell><cell></cell><cell>disorganized extravagant</cell><cell></cell><cell>Old New</cell></row><row><cell>lab technician diplomat</cell><cell></cell><cell>Old New</cell><cell></cell><cell>dishonest jovial</cell><cell></cell><cell>Old New</cell></row><row><cell>artist environmental activist</cell><cell></cell><cell>Old New</cell><cell></cell><cell>gentle cooperative</cell><cell></cell><cell>Old New</cell></row><row><cell>scientific consultant music producer proof</cell><cell>in</cell><cell>criminal Old New</cell><cell>trials</cell><cell>(pp. sensitive attractive</cell><cell>371-391).</cell><cell>Routledge. Old New</cell></row><row><cell>scientist fashion designer</cell><cell></cell><cell>Old New</cell><cell></cell><cell>creative muscular</cell><cell></cell><cell>Old New</cell></row><row><cell>nanny photographer</cell><cell></cell><cell>Old New</cell><cell></cell><cell>helpful shy</cell><cell></cell><cell>Old New</cell></row><row><cell>boxer soldier</cell><cell></cell><cell>Old New</cell><cell></cell><cell>strong social</cell><cell></cell><cell>Old New</cell></row><row><cell>paramedic fashion model</cell><cell></cell><cell>Old New</cell><cell></cell><cell>brave warm</cell><cell></cell><cell>Old New</cell></row><row><cell>businessman</cell><cell></cell><cell>Old</cell><cell></cell><cell>bossy moody</cell><cell></cell><cell>Old New</cell></row><row><cell>secretary</cell><cell></cell><cell>Old</cell><cell cols="2">unconventional lazy</cell><cell></cell><cell>Old New</cell></row><row><cell>executive manager</cell><cell></cell><cell>Old</cell><cell></cell><cell>quiet hardworking</cell><cell></cell><cell>Old New</cell></row><row><cell>assistant</cell><cell></cell><cell>Old</cell><cell></cell><cell>organized imaginative</cell><cell></cell><cell>Old New</cell></row><row><cell>nurse</cell><cell></cell><cell>Old</cell><cell cols="2">reliable narrow-minded</cell><cell></cell><cell>Old New</cell></row><row><cell>writer</cell><cell></cell><cell>Old</cell><cell></cell><cell>ambitious boring</cell><cell></cell><cell>Old New</cell></row><row><cell>telemarketer</cell><cell></cell><cell>Old</cell><cell></cell><cell>charming selfish</cell><cell></cell><cell>New New</cell></row><row><cell>clown</cell><cell></cell><cell>Old</cell><cell></cell><cell>confident narcissistic</cell><cell></cell><cell>New New</cell></row><row><cell>fireman</cell><cell></cell><cell>Old</cell><cell></cell><cell>efficient</cell><cell></cell><cell>New</cell></row><row><cell>pianist</cell><cell></cell><cell>Old</cell><cell></cell><cell>friendly</cell><cell></cell><cell>New</cell></row><row><cell>doctor</cell><cell></cell><cell>Old</cell><cell></cell><cell>generous</cell><cell></cell><cell>New</cell></row><row><cell>hippy</cell><cell></cell><cell>Old</cell><cell></cell><cell>naive</cell><cell></cell><cell>New</cell></row><row><cell>construction worker</cell><cell></cell><cell>Old</cell><cell></cell><cell>witty</cell><cell></cell><cell>New</cell></row></table><note><p><p><p>Stupple, E. J., Ball, L. J.,</p>Evans, J. S. B., &amp; Kamal-Smith, E. (2011)</p>. When logic and belief collide: Individual differences in reasoning times support a selective processing model. Journal of Cognitive Psychology, 23(8), 931-941. https://doi.org/10.1080/20445911.2011.589381 Thompson, W. C., &amp; Schumann, E. L. (2017). Interpretation of statistical evidence in criminal trials: The prosecutor's fallacy and the defense attorney's fallacy. In Expert evidence and https://www.taylorfrancis.com/chapters/edit/10.4324/9781315094205-15/interpretationstatistical-evidence-criminal-trials-william-thompson-edward-schumann Tversky, A., &amp; Kahneman, D. (1983). Extensional versus intuitive reasoning: The conjunction fallacy in probability judgment. arXiv. https://doi.org/10.48550/arXiv.2401.00284 2.</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We thank <rs type="person">Gordon Pennycook</rs> for sharing pretest data on typicality ratings with us, as well as <rs type="person">Laura Charbit</rs> and <rs type="person">Nicolas Beauvais</rs> for their comments on earlier versions of the manuscript.</p></div>
<div><head>Open Practices Statement</head><p>All data, materials, and analysis scripts are publicly available at: https://doi.org/10.17605/OSF.IO/JCEYD. The baserater package can be downloaded from: https://github.com/Jeremie-Beucler/baserater. None of the reported studies were preregistered.</p></div>
			</div>
			<div type="funding">
<div><head>Funding</head><p>This research was supported by the <rs type="funder">Agence Nationale de la Recherche</rs> (<rs type="projectName">ANR</rs>; <rs type="grantNumber">ANR-23-AERC-0006</rs> to <rs type="projectName">ZP</rs>, <rs type="grantNumber">ANR-23-CE28-0004-01</rs> to WDN) and the <rs type="funder">Economic and Social Research Council (UKRI, ESRC</rs>; <rs type="grantNumber">ES/V00378X/1</rs> to LC).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_vSWbWzH">
					<idno type="grant-number">ANR-23-AERC-0006</idno>
					<orgName type="project" subtype="full">ANR</orgName>
				</org>
				<org type="funded-project" xml:id="_24Wg4Mz">
					<idno type="grant-number">ANR-23-CE28-0004-01</idno>
					<orgName type="project" subtype="full">ZP</orgName>
				</org>
				<org type="funding" xml:id="_P5PfDBw">
					<idno type="grant-number">ES/V00378X/1</idno>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Availability of Data, Materials and Code</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>All</head><p>data, materials and analysis scripts are available at: https://doi.org/10.17605/OSF.IO/JCEYD. The baserater package can be downloaded from: https://github.com/Jeremie-Beucler/baserater.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>The online supplementary material is available via the Open Science Framework at: https://doi.org/10.17605/OSF.IO/JCEYD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declarations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflicts of Interest/Competing Interests</head><p>The authors have no competing interests to declare that are relevant to the content of this article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics Approval</head><p>This study was performed in line with the principles of the Declaration of Helsinki. The study was approved by the CER U-Paris ethics committee (Université de Paris).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consent to Participate</head><p>All participants provided informed consent before taking part in the study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consent for Publication</head><p>Not applicable.  Note. SD = standard deviation; Min = minimum; Max = maximum. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Typicality Ratings Across</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Correlation Between LLMs' Typicality Ratings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Validation of Prompt and LLM Settings</head><p>Table <ref type="table">S3</ref> Correlations between model predictions and human typicality ratings across prompt and settings variations in Experiment 1. To assess the robustness of our prompt and settings, we tested four variations on the 100 group-adjective pairs from Experiment 1 to compare it with the approach we implemented throughout the paper ("Current Approach"):</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>-Deterministic: same prompt as the original one, but with temperature set to 0 to force a single deterministic output without averaging multiple responses;</p><p>-Zero-shot learning: removed the three examples from the user prompt;</p><p>-Frequency format: changed the user prompt to ask for a frequency estimation instead of a typicality rating, e.g., "Imagine a group of 100 GROUP MEMBERS. How many of them would you expect to be ADJECTIVE?";</p><p>-Cultural context: specified that the stereotypes should be those prevalent in U.S. culture (since our participants were from the U.S. or Canada), by slightly changing the system prompt:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Excluding Raw Values of 0 for LlaMa 3.3 Ratings in Experiment 1</head><p>One possible explanation for the lower performance of LlaMA 3.3 compared to GPT-4 is that LlaMA 3.3 occasionally generates extreme typicality ratings of 0, which does not occur for GPT-4. To test this, we recomputed the correlation between human typicality ratings and LlaMA 3.3 in Experiment 1 after excluding all raw ratings of 0 before averaging. This exclusion did not affect the correlation (r = 0.82, p &lt; .001). These results suggest that the lower performance of LlaMA 3.3 is unlikely to be driven by the presence of zero ratings, but rather reflects genuinely weaker model performance compared to GPT-4. strength distribution in the new base-rate item database (n = 109,098 items). Note that the log ratios in the database are always positive by construction, since we ensured that the item with the highest typicality ratings was always in the numerator of the ratio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Summary Statistics of the Database</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table S4</head><p>Summary statistics of the stereotype strength measure across all groups-adjective combinations for   Note. N represents the number of observations. SD = standard deviation; Q1 = first quartile; Q3 = third quartile; Min = minimum; Max = maximum.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Arnal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Marie</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2407.10645</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2407.10645" />
		<title level="m">Prompt Selection Matters: Enhancing Text Annotations for Social Sciences with Large Language Models</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Advancing the specification of dual process models of higher cognition: A critical test of the hybrid model view</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>De Neys</surname></persName>
		</author>
		<idno type="DOI">10.1080/13546783.2018.1552194</idno>
		<ptr target="https://doi.org/10.1080/13546783.2018.1552194" />
	</analytic>
	<monogr>
		<title level="j">Thinking &amp; Reasoning</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Clinical reasoning about new symptoms despite preexisting disease: Sources of error and order effects</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Bergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Chapman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gjerde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Elstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Family Medicine</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="314" to="320" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">baserater: An R package using large language models to estimate belief strength in reasoning [Computer software]</title>
		<author>
			<persName><forename type="first">J</forename><surname>Beucler</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.15449192</idno>
		<ptr target="https://doi.org/10.5281/zenodo.15449192" />
	</analytic>
	<monogr>
		<title level="j">Zenodo</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Data, materials, and analysis scripts for Using Large Language Models to Estimate Belief Strength in Reasoning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Beucler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Purcell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>De Neys</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
	<note>OSF repository</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L R</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Silvestri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>O'donnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Palla</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2312.02401</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2312.02401" />
		<title level="m">Enhancing Content Moderation with Culturally-Aware Models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>Open Science Framework</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Advancing theorizing about fast-and-slow thinking</title>
		<author>
			<persName><forename type="first">W</forename><surname>De Neys</surname></persName>
		</author>
		<idno type="DOI">10.1017/S0140525X2200142X</idno>
		<ptr target="https://doi.org/10.1017/S0140525X2200142X" />
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="page" from="46" to="e111" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Biased but in doubt: Conflict and decision confidence</title>
		<author>
			<persName><forename type="first">W</forename><surname>De Neys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cromheeke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Osman</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0015954</idno>
		<ptr target="https://doi.org/10.1371/journal.pone.0015954" />
	</analytic>
	<monogr>
		<title level="j">PloS One</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">15954</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Conflict monitoring in dual process theories of thinking</title>
		<author>
			<persName><forename type="first">W</forename><surname>De Neys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Glumicic</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cognition.2007.06.002</idno>
		<ptr target="https://doi.org/10.1016/j.cognition.2007.06.002" />
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1248" to="1299" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automatic Scoring of Metaphor Creativity with Large Language Models</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Distefano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Beaty</surname></persName>
		</author>
		<idno type="DOI">10.1080/10400419.2024.2326343</idno>
		<ptr target="https://doi.org/10.1080/10400419.2024.2326343" />
	</analytic>
	<monogr>
		<title level="j">Creativity Research Journal</title>
		<imprint>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the conflict between logic and belief in syllogistic reasoning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S B</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Barston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pollard</surname></persName>
		</author>
		<idno type="DOI">10.3758/BF03196976</idno>
		<ptr target="https://doi.org/10.3758/BF03196976" />
	</analytic>
	<monogr>
		<title level="j">Memory &amp; Cognition</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="295" to="306" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dual-process theories of higher cognition: Advancing the debate</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S B</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Stanovich</surname></persName>
		</author>
		<idno type="DOI">10.1177/1745691612460685</idno>
		<ptr target="https://doi.org/10.1177/1745691612460685" />
	</analytic>
	<monogr>
		<title level="j">Perspectives on Psychological Science</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="223" to="241" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Grattafiori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jauhri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kadian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Al-Dahle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Letman</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schelten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vaughan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hartshorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sravankumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Korenev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hinsvark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2407.21783</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2407.21783" />
		<title level="m">The LlaMa 3 Herd of Models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Kahneman</surname></persName>
		</author>
		<title level="m">Thinking, fast and slow</title>
		<imprint>
			<publisher>Macmillan</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On the psychology of prediction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kahneman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tversky</surname></persName>
		</author>
		<idno type="DOI">10.1037/h0034747</idno>
		<ptr target="https://doi.org/10.1037/h0034747" />
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="237" to="251" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">G</forename><surname>Kovač</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sawayama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Portelas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Colas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Dominey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-Y</forename><surname>Oudeyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<idno type="DOI">10.48550/arXiv.2307.07870</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2307.07870" />
		<title level="m">Large Language Models as Superpositions of Cultural Perspectives</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Uncovering the semantics of concepts using GPT-4</title>
		<author>
			<persName><forename type="first">Le</forename><surname>Mens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kovács</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Pros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename></persName>
		</author>
		<idno type="DOI">10.1073/pnas.2309350120</idno>
		<ptr target="https://doi.org/10.1073/pnas.2309350120" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">49</biblScope>
			<biblScope unit="page">2309350120</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Culturellm: Incorporating cultural differences into large language models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sitaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2402.10946</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2402.10946" />
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="84799" to="84838" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Achiam</forename><surname>Openai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Akkaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Aleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Altenschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Anadkat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Avila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Balcom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Baltescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Belgum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2303.08774</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2303.08774" />
		<title level="m">GPT-4 Technical Report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">How to train your stochastic parrot: Large language models for political texts</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Ornstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">N</forename><surname>Blasingame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Truscott</surname></persName>
		</author>
		<idno type="DOI">10.1017/psrm.2024.64</idno>
		<ptr target="https://doi.org/10.1017/psrm.2024.64" />
	</analytic>
	<monogr>
		<title level="j">Political Science Research and Methods</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="264" to="281" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cognitive style and religiosity: The role of conflict detection</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pennycook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Cheyne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Koehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Fugelsang</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13421-013-0340-7</idno>
		<ptr target="https://doi.org/10.3758/s13421-013-0340-7" />
	</analytic>
	<monogr>
		<title level="j">Memory &amp; Cognition</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">What makes us think? A threestage dual-process model of analytic engagement</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pennycook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Fugelsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Koehler</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cogpsych.2015.05.001</idno>
		<ptr target="https://doi.org/10.1016/j.cogpsych.2015.05.001" />
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="34" to="72" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
