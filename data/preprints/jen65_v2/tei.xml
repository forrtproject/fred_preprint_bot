<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Estimating Correlations Across Tasks In Experimental Psychology</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shanglin</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Irvine</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Jeffrey</forename><forename type="middle">N</forename><surname>Rouder</surname></persName>
							<email>jrouder@uci.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Irvine</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Cognitive Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>92697</postCode>
									<settlement>Irvine</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Estimating Correlations Across Tasks In Experimental Psychology</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">047D925226599096E13A9DD23B127BE5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-10-21T13:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Individual differences</term>
					<term>correlations</term>
					<term>hierarchical models</term>
					<term>prior selection</term>
					<term>Bayesian analysis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Understanding how people covary in performance across experimental tasks is central to individual-difference psychology. The classic Pearson correlation has two strengths: (1) it is invariant to the scale of measurement, and (2) it is invariant to including additional variables in the analysis. However, it is susceptible to attenuation from measurement noise.</p><p>Bayesian hierarchical models address this issue by modeling measurement error directly.</p><p>Resulting estimates, however, depend on prior specifications and are not invariant to scale or variable inclusion. We compare three common priors-Inverse Wishart (IW), Scaled Inverse Wishart (SIW), and LKJ-to assess robustness to prior assumptions in hierarchical settings. Our main tools are visualizing the priors and evaluating their effects on posterior estimates through simulation. When prior settings match ground truth, all priors recover true correlations accurately in low-dimensional settings. When prior variance is misspecified, the IW shows strong bias: low-variance priors inflate correlations, and high-variance priors deflate them. The SIW shows the same pattern but less severely, while the LKJ remains largely unaffected by scale misspecification. When more variables are added, the IW is most stable, whereas the SIW and LKJ show slight shrinkage toward lower correlations. The main drawback of the LKJ is computational speed-models with it can take orders of magnitude longer than those using IW or SIW. Overall, the LKJ provides the most accurate estimates, while the SIW offers a practical compromise for large-scale models where computational speed is crucial.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Estimating Correlations Across Tasks In Experimental Psychology</head><p>It is common in experimental psychology to study how people covary across tasks.</p><p>An example is assessing whether people who are highly susceptible to Stroop interference are also susceptible to Flanker interference. Researchers who study individual differences hope that the pattern of observed correlations may provide insight into the nature and dimension of processing. Here are two well-known examples where this hope was realized:</p><p>First, in personality psychology, the pattern of correlations across various personality questionnaire items forms the evidence for the Big 5 theory of personality <ref type="bibr" target="#b19">(McCrae &amp; Costa Jr, 1997)</ref>. Second, in cognitive psychology, the pattern of correlations across executive-function tasks forms the evidence for various theories of executive function including the Inhibition-Shifting-Updating theory of <ref type="bibr" target="#b20">Miyake et al. (2000)</ref>. Although each of these theories has been criticized, the general method of studying covariation across tasks remains timely and topical.</p><p>Estimating correlations at first glance seems straightforward: Point estimates come from Pearson's sample correlation formula <ref type="bibr" target="#b23">(Pearson, 1900)</ref>; confidence intervals (CIs) come from Fisher's z-transform method <ref type="bibr" target="#b5">(Fisher, 1921)</ref>. Each of these computations has the following beneficial property: The analyst can change the location and scale of the variables without changing the estimation of correlation. For example, suppose we are studying the association between heart rate and ambient temperature. One analyst chooses to measure temperature in Celsius while another in Fahrenheit even though the measure vary in location (where the zero point is) and scale (how big a degree is). Fortunately, the point estimate and associated CIs do not depend on this choice-the same values are obtained under linear transform of either variable. This property may be termed location and scale invariance. Even more impressively, each of the computations display invariance to inclusion of other variables. For example, suppose one researcher considered height and weight in isolation and another considered height, weight, shoe size and waist circumference simultaneously. It is not obvious that the correlation coefficient between weight and height would be the same for both researchers. In the latter case, the resulting set of correlation in the correlation matrix must be positive semidefinite implying a common constraint. Yet, it may be shown that the correlation for the latter depends only on height and weight, and the point estimate and CIs are the same in both cases. Given these two advantages, as well as the ease of computation, estimation of correlations seems a solved problem.</p><p>A critical complication comes when variables are measured with substantial measurement error, as they almost always are in psychology. Let's take the following case where each of many individuals participates in two psychological tasks. Let Y i1 and Y i2 be the scores for the ith individual, i = 1, . . . , I respectively for Task 1 and Task 2. To incorporate measurement error, let</p><formula xml:id="formula_0">Y ij | θ ij ∼ N(θ ij , τ 2 j ), j = 1, 2,</formula><p>where θ i1 and θ i2 are true scores for the ith individual for respectively Task 1 and Task 2.</p><p>The terms τ 1 and τ 2 are the error variance for the two tasks. Figure <ref type="figure" target="#fig_5">1A</ref> shows a scatter plot of hypothetical individual true scores θ i1 and θ i2 for I = 200 individuals. These are correlated; the true population value is .7 and the goal is to recover this value. The sample correlation for these 200 individuals is .71, which is quite close to the true population value. Figure <ref type="figure" target="#fig_5">1B</ref> shows the scatter plot of observed scores Y i1 and Y i2 , and again, we wish to recover the true value of .7. These observed scores are perturbed by error in random directions and to random degrees. The net result is that the correlation is attenuated <ref type="bibr">(Spearman, 1904)</ref>. The sample correlation on observed scores is .41 in value, which is quite far from the true population value of .7.</p><p>More formal development is instructive in addressing the problem. First, a bivariate normal is placed on true scores:</p><formula xml:id="formula_1">    θ i1 θ i2     ∼ N 2         µ 1 µ 2     ,     σ 2 1 ρσ 1 σ 2 ρσ 1 σ 2 σ 2 2         .</formula><p>The key quantity here is population correlation ρ, and it is this quantity we hope to recover.</p><p>Of course, we do not observe true scores, but the observations. The distribution on Y ij is:</p><formula xml:id="formula_2">    Y i1 Y i2     ∼ N 2         µ 1 µ 2     ,     σ 2 1 + τ 2 1 ρσ 1 σ 2 ρσ 1 σ 2 σ 2 2 + τ 2 2         .</formula><p>From this distribution, it is now clear that the sample correlation from observed scores, denoted ρ measures:</p><formula xml:id="formula_3">E(ρ) ≈   σ 1 σ 2 σ 2 1 + τ 2 1 σ 2 2 + τ 2 2   ρ.</formula><p>The coefficient here describes attenuation as the denominator necessarily larger than the numerator. And without any further information about measurement noise, we are unsure by how much.</p><p>Our application is to experimental psychology, and tasks tend to be comprised of many repeated trials. The usual course of analysis is averaging over replicates to form individual-by-task scores which are then correlated. There is noise in these trials, which means the averages also have noise, although certainly less so. This trial noise effect on the averages results in attenuation. Moreover, in this averaging approach, the only way to reduce attenuation is to increase the number of trials per individual. It cannot be reduced by adding more individuals as adding individuals simply adds more perturbed points to the scatter plot.</p><p>An attractive alternative to averaging for disattenuating correlations in analysis is through hierarchical models <ref type="bibr">(Haines et al., 2025;</ref><ref type="bibr" target="#b18">Matzke et al., 2017;</ref><ref type="bibr" target="#b26">Rouder &amp; Haaf, 2019)</ref>. In a hierarchical model, multiple sources of noise are specified and all the data, not just the averages, are used. For the above example, trial noise as well as covariability of people in tasks are included. Hierarchical models have become popular in experimental psychology because they lead to more accurate estimation and inference across a wealth of paradigms <ref type="bibr" target="#b28">(Rouder &amp; Lu, 2005;</ref><ref type="bibr" target="#b31">Rouder &amp; Province, 2019)</ref>. In this case, the separation of trial noise from variability across tasks in suitable hierarchical models leads to disattenuated estimates of correlation <ref type="bibr" target="#b25">(Rouder, Chavez de la Peña, Mehrvarz, &amp; Vandekerckhove, 2023;</ref><ref type="bibr" target="#b30">Rouder &amp; Mehrvarz, 2024)</ref>. As proof of concept, Figure <ref type="figure" target="#fig_5">1C</ref> shows the posterior distribution of ρ when all the data are analyzed in a subsequently presented hierarchical model. As can be seen, they are centered close to the true value of .7 rather than the attenuated value of .41.</p><p>Estimation of correlation in hierarchical models is not without difficulties. The measurement invariances in sample correlations-invariance to location and scale and invariance to inclusion of additional variables-may not be attainable or even desirable in hierarchical settings.</p><p>Analysis of hierarchical models is convenient in the Bayesian framework <ref type="bibr" target="#b6">(Gelman, Carlin, Stern, &amp; Rubin, 2004)</ref>. This paper addresses how to estimate a correlation matrix in both hierarchical and conventional normal models. A key issue is the specification of the prior. How does the choice of prior affect the posterior distribution of correlation coefficients? We frame our assessment in terms of measurement invariances highlighted above. Invariance to location occurs fairly naturally for reasons discussed subsequently and will not play a role. Invariance to scale and invariance to inclusion of additional variables is more difficult with the methods we present. For example, in all the prior specifications discussed here, the analyst must set a scale or expectation of the degree of variability beforehand. Robustness to scale specification occurs when the posterior of correlation coefficients are relatively unaffected by reasonable variation in this scale setting. Likewise, robustness to inclusion occurs when the posterior of correlations coefficients is relatively unaffected by the inclusion or exclusion of a handful of additional variables.</p><p>We study the following prior classes for correlations: The Inverse Wishart prior <ref type="bibr" target="#b22">(O'Hagan &amp; Forster, 2004)</ref>, which offers advantages of conjugacy, computational convenience; the Scaled Inverse Wishart prior <ref type="bibr" target="#b13">(Huang &amp; Wand, 2013)</ref>, which is a continuous mixture of Inverse Wishart components and provide more flexibility in modeling variances; the LKJ prior <ref type="bibr" target="#b15">(Lewandowski, Kurowicka, &amp; Joe, 2009)</ref>, which is less informative than either Wishart-based alternatives (Tokuda, Goodrich, Mechelen, <ref type="bibr">Gelman, &amp; Tuerlinckx, 2025)</ref>. This paper explores the conditions under which each of these choices is useful for estimating correlations. Our main approach to comparing these models is through simulation with known truths. In each simulation run, posterior distributions of population-level correlations are compared to these truths. This simulation approach is precedented for covariance matrices, and examples include <ref type="bibr">Rouder et al. (2007), Schuurman, Grasman, and</ref><ref type="bibr">Hamaker (2016)</ref>; <ref type="bibr" target="#b16">Liu, Zhang, and Grimm (2016)</ref>. To our knowledge, however, we are the first to do so for assessing correlations across tasks in experimental designs. Moreover, although the Inverse Wishart prior has been studied extensively, there is far less work with the Scaled Inverse Wishart and LKJ priors, perhaps because they are relatively new and only incorporated into Jags and Stan in the last decade.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Manifest and Hierarchical Models</head><p>We use the term manifest model for the conventional case where researchers place a multivariate model on observables, and manifest models are appropriate when there is an ignorable degree of measurement error. Example of variables which we may treat as free of measurement error are body metrics such as height, weight, and waist circumference. This case is easy in the conventional setup; one simply uses sample correlations along with Fisher confidence intervals. For the Bayesian case, the manifest model is given as follows:</p><p>Let Y ij denote the score for the ith individual, i = 1, . . . , I on the jth task, j = 1, . . . , J, and let Y i = (Y i1 , . . . , Y iJ ) ′ be a column vector of scores for the ith individual:</p><p>Manifest Model :</p><formula xml:id="formula_4">Y i ∼ N J (µ, Σ),</formula><p>where µ = (µ 1 , . . . , µ J ) ′ is a column vector of means per task and Σ is a variance matrix:</p><formula xml:id="formula_5">Σ =             σ 2 1 ρ 12 σ 1 σ 2 . . . ρ 1J σ 1 σ J ρ 12 σ 1 σ 2 σ 2 2 . . . ρ 2J σ 2 σ J . . . . . . . . . . . . ρ 1J σ 1 σ J ρ 2J σ 2 σ J . . . σ 2 J            </formula><p>.</p><p>The variance matrix can be expressed in terms of a correlation matrix as follows:</p><formula xml:id="formula_6">Σ =             σ 1 0 . . . 0 0 σ 2 . . . 0 . . . . . . . . . . . . 0 0 . . . σ J             ×             1 ρ 12 . . . ρ 1J ρ 12 1 . . . ρ 2J . . . . . . . . . . . . ρ 1J ρ 2J . . . 1             ×             σ 1 0 . . . 0 0 σ 2 . . . 0 . . . . . . . . . . . . 0 0 . . . σ J            </formula><p>, which may be written compactly as:</p><formula xml:id="formula_7">Σ = D(σ)ρD(σ),<label>(1)</label></formula><p>where D(σ) is a diagonal matrix with σ = (σ 1 , . . . , σ J ) on the diagonal and ρ is the correlation matrix. Priors on µ may be set broadly making the model robust to changes in location.</p><p>A hierarchical model is appropriate when there are several repeated observations for each individual in each task. Each individual performs L ij trials on each task, and the score on each trial is denoted Y ijℓ , where, as before i indexes individual, i = 1, . . . , I, j indexes task, j = 1, . . . , J, and ℓ indexes replicate, ℓ = 1, . . . , L ij . The simplest model is Hierarchical Model:</p><formula xml:id="formula_8">Y ijℓ | θ ij ∼ N(θ ij , τ 2 j ), θ i ∼ N(µ, Σ).</formula><p>The </p><formula xml:id="formula_9">θ i = (θ i1 , .</formula><p>. . , θ iJ ) ′ is a column vector of true scores, and the correlation among tasks, the target of study, is contained within Σ via Equation (1).</p><p>In Bayesian analysis, priors are needed on parameters. In the manifest model, priors are needed for mean µ and variance Σ. In the hierarchical model, priors are needed for these two as well as for trial variances τ 2 j . In practice, prior on µ and the collection of trial variances are not problematic as diffuse priors work well. The critical component is the variance matrix Σ, especially in the hierarchical model. The goal then is to pick a good prior on Σ so that the collection of correlations may be faithfully recovered without undue influence.</p><p>In the next section, we review some choices of prior for Σ. Before doing so, we note that in most multivariate studies of performance, researchers estimate correlations en route to further analysis such as factor modeling or structural-equation modeling. So, why study correlations themselves? <ref type="bibr" target="#b1">Bollen (1989)</ref> notes that factor models and structural-equation models impose constraints on correlation matrices. In comparison, we are studying unconstrained priors-priors that do not restrict the pattern of covariance other than that the matrix is a proper covariance matrix. Having good unconstrained priors allows researchers to explore possible patterns without unwarranted assumptions. These models then serve as a general model against which constrained models, that is specific factor model or structural-equation models, may be compared.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Priors on Covariance and Correlation</head><p>In this section, we provide three common choices of prior and explore how the constraints within them could affect invariance to scale and invariance to inclusion. We visualize the covariances using insights from <ref type="bibr">Tokuda et al. (2025)</ref>.</p><p>Inverse Wishart Prior. The Inverse Wishart distribution (IW) is a popular choice for a prior on covariance matrix Σ because it is conjugate for normally-distributed observations. When a covariance matrix follows an Inverse Wishart, we write:</p><formula xml:id="formula_10">Σ ∼ Inverse Wishart(S, v),</formula><p>where S and v are prior settings that must be chosen beforehand. Setting v is the degrees-of-freedom of the distribution. It may be set to v = J + 1 as a default, where J is the size of covariance matrix (the number of tasks or measures). The more critical setting is that of the scale matrix S. This matrix may be diagonal, and the diagonal entries, set beforehand, are the expected value of variance. The question then is how these settings affect the posterior of correlation.</p><p>Figure <ref type="figure" target="#fig_6">2</ref> shows a series of visualizations of the prior for the default v = J + 1 and S = I, the identity matrix. Figure <ref type="figure" target="#fig_6">2A</ref> shows the marginal prior on the pairwise correlations, that is, the off-diagonal elements in ρ. These are uniformly distributed on the interval [-1, 1], and this distribution holds regardless of the number of tasks J. Such a prior is highly desirable as no ranges of correlations are unduly favored. The top row addresses the invariance to scale. It shows the relationship between the prior on scale, or standard deviation ( Σ jj ) and correlation. Figure <ref type="figure" target="#fig_6">2B</ref> is a contour plot of prior joint distribution of a correlation coefficient and the standard deviation ( Σ jj ). Here we see an issue-there is a notable lack of independence. The two histograms on the top row (Figure <ref type="figure" target="#fig_6">2C</ref>-D) highlight the dependencies. These plots are conditional prior densities on ρ. Figure <ref type="figure" target="#fig_6">2C</ref> is conditional on small standard deviations and the overweighting of small-magnitude correlations is seen. Figure <ref type="figure" target="#fig_6">2D</ref> is conditional on large standard deviations and the overweighting of large-magnitude correlations is seen. What do these dependencies mean?</p><p>They imply that the prior is dependent on the specification of scale, and moreover, the analyst needs a reasonable sense of the variation. If the analyst sets the scale S too small, the data are concordant with relatively high variabilities and posteriors may be biased toward large-magnitude correlations. Conversely, if the analyst sets the scale S too large, the data are concordant with relatively low variabilities and posteriors may be biased toward low-magnitude correlations. The advantage of the Inverse Wishart prior is computational speed. The prior is conjugate with respect to the normal, hence conditional posterior distributions are also Inverse Wishart with updated parameters. Inverse Wishart prior is convenient to sample from, and MCMC chains run quickly and efficiently (as will be seen). The usual critique is that the scale specification is too informative and excludes small possible variance values <ref type="bibr" target="#b7">(Gelman &amp; Hill, 2007;</ref><ref type="bibr" target="#b13">Huang &amp; Wand, 2013)</ref>. We are less concerned with this aspect here-our focus is squarely on the effect of the estimation of correlation.</p><p>Scaled Inverse Wishart Prior. The Scaled Inverse Wishart distribution (SIW), from <ref type="bibr" target="#b13">Huang and Wand (2013)</ref>, is relatively new prior for covariance. It is a bit misnamed for it is not a scaled version of the Inverse Wishart (which already has a scale setting).</p><p>Instead, the distribution is formed by taking a continuous weighted mixture of Inverse</p><p>Wishart distributions across all scales. The hope then is that this more diffuse form lessens any burden of specifying a single scale a priori. When a covariance matrix follows a Scaled</p><p>Inverse Wishart, we write,</p><formula xml:id="formula_11">Σ ∼ SIW(v, s),</formula><p>where v is a degrees-of-freedom parameter and s is a vector of scales on standard deviations, Σ</p><p>1/2 jj . The Scaled Inverse Wishart is related to the Inverse Wishart as follows. If</p><formula xml:id="formula_12">Σ ∼ SIW(v, s), then, Σ | α 1 , . . . , α J ∼ Inverse Wishart v + J -1, 2v D 1 α 1 , . . . , 1 α J , α j ind ∼ Inverse-Gamma 1 2 , 1 s 2 j ,</formula><p>where D() denotes a diagonal matrix. The degrees-of-freedom parameter v plays a different role in this parameter than in the Inverse Wishart. In the Inverse Wishart, v references the number of variables, and v = J + 1 is a useful default corresponding to a uniform prior on all correlations. In the Scaled Inverse Wishart, the default value is v = 2 (see <ref type="bibr" target="#b13">Huang and Wand, 2013)</ref>. With this setting, the marginal priors on correlation coefficients are distributed uniformly on [-1,1], and this distribution holds for all J (see Figure <ref type="figure" target="#fig_7">3A</ref>). The main question revolves around the role of s, which are scale values on the standard. Figure <ref type="figure" target="#fig_7">3B</ref> shows the joint distribution of variability and correlation for the Scaled Inverse Wishart prior for comparable settings to the Inverse Wishart. Here, there remains some dependencies, though to a lesser degree than with the Inverse Wishart. Figure <ref type="figure" target="#fig_7">3C-D</ref> show the conditional prior distributions, and the presence of the dependence though to a lesser degree may be seen. Figure <ref type="figure" target="#fig_7">3E</ref>-G show, respectively, the joint and conditional distributions of correlation coefficients. The behavior here is the same as the Inverse Wishart-the prior tends to make correlation coefficients similar in magnitude and hence violate at least to some degree invariance to inclusion.</p><p>LKJ Prior. The LKJ prior <ref type="bibr" target="#b15">(Lewandowski et al., 2009)</ref> is known as a least-informative prior. This prior makes use of Equation ( <ref type="formula" target="#formula_7">1</ref>)-rather than placing priors directly on Σ, priors are placed on σ, the vector of standard deviations, and ρ, the correlation matrix. These specifications are separate and decouples variance and correlation.</p><formula xml:id="formula_13">σ j ∼ Half-t(2, s j ), j = 1, . . . , J, ρ ∼ LKJ(v),</formula><p>The distribution labeled "Half-t" is the positive half of a scaled t distribution with 2 degrees of freedom and scale s j . The collection s = (s 1 , . . . , s J ) serves as scale settings that must be set before analysis, and plays a similar role as that in the Scaled Inverse Wishart.</p><p>The LKJ prior is a distribution specifically for correlation matrices, and the distribution depends on shape parameter v. The default setting is v = 1, and the marginal distribution on the correlation coefficients is shown in Figure <ref type="figure" target="#fig_8">4A</ref>. Unlike the Inverse Wishart and Scaled Inverse Wishart, this distribution depends on the number of variables or tasks, J. When the distribution is bivariate, J = 2, the prior distribution on correlation is uniformly distributed on [-1,1]. However, as more tasks are added, the prior emphasizes smaller-magnitude correlations, which is a violation of invariance to inclusion. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Prior Specification For The Manifest Model</head><p>The discussed priors are broad defaults that are used repeatedly throughout the literature. Our expectation was that in the manifest case with little trial noise, they would each be excellent choices. To explore the effects of prior specification on the measurement of correlation, we used the anthropometric data from the U.S. Army <ref type="bibr" target="#b0">(Army, 2014)</ref>. In the data set, over 6000 soldiers provided about 100 different body measures. We examined the 4082 identified male soldiers, and correlated the height with weight for a random sample of I = 200 of them. To set the scale, we used the standard deviation of the full sample of 4082 as reference. For example, the standard deviation of height was 6.86 cm, and we used this number as a baseline. To assess robustness to misspecification, we took this value and manipulated it from 1/10th baseline to 10 times baseline, or in this case, from 0.69 cm to 68.6 cm. This is a huge range of variation for an empirical scientist. We estimated either the height or weight alone as a bivariate problem, or included another 8 anthropometric measurements as a 10-variable problem.</p><p>Table <ref type="table">1</ref> shows the effect of prior for both the bivariate case ("Exclude" for exclude the other variables) or the 10-variate case ("Include" for include the other variables) for the The above application shows that all three prior classes do well in the manifest case.</p><p>There is sufficient prior mass in all cases that data with as few as 200 cases are more than sufficient to dominate the prior. It matters little here which of the three priors is used.</p><p>Stated alternatively, there is little reason to use Bayesian analysis at all for the manifest case; the sample correlation along with Fisher's z-transform confidence intervals provides the same summary.</p><p>The manifest case is only appropriate when there is little measurement noise.</p><p>Tolerably low measurement noise, such as in weight measures, is the exception and not the rule in experimental psychology. In fact, in all cases we know of, there is a high degree of measurement noise. And in all these cases, the Pearson correlation coefficient is inadvisable because of a large degree of attenuation. Hence, the critical question is how these priors perform when the data have measurement noise and this measurement noise is modeled in a hierarchical context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Simulation Study 1: Two Tasks. Calibrated Scale Settings</head><p>Simulations in this paper follow a common form. First, to generate data, true individual parameters (θ i ) are sampled from a multivariate normal parent distribution. For two tasks each individual has two true values with one for each task, and there is a single true population correlation parameter ρ. The value of ρ is set as ground truth and the estimate of which serves as the target of analysis. Here are the settings for the parent distribution on θ i :</p><formula xml:id="formula_14">θ i ∼ N 2         .5 .5     ,     .1 2 ρ × .1 2 ρ × .1 2 .1 2         ,</formula><p>In all the simulations reported here, there were 200 synthetic individuals, that is,</p><formula xml:id="formula_15">I = 200.</formula><p>Trial level data were sampled from true individual values θ i as Y ijℓ ∼ N(θ ij , τ 2 j ).</p><p>To speed simulations and draw sharp contrasts between estimation approaches, we set a low number of trials, L = 20, throughout. True correlations were set to ρ = .3, ρ = .5, and ρ = .7 to cover a variety of cases; trial variance τ 2 j was set either to a low-noise value of .2 2 or to a high noise value of .5 2 , and these values held for all tasks.</p><p>In each simulation, trial-level data were generated from the above steps and analyzed with hierarchical models that varied only in the prior specification on Σ. One specification was the Inverse Wishart prior with shape set to the default setting of v = 3 and scale s 2 is set to .1 2 , which matches the true between-individual variation. Another was the Scaled Inverse Wishart prior with default shape, v = 2, and scale s = .1, which matches true between-individual standard deviation. The final specification is the LKJ with prior shape the default of v = 1 and prior scale s = .1. The Inverse Wishart and Scaled Inverse Wishart prior specifications do not have an explicit parameter ρ, but it is straightforward to compute a value of ρ on each iteration of the MCMC chain using Equation (1). The LKJ has lower triangle matrix outputs on each iteration; the cross product of this matrix yields the posterior correlation matrix.</p><p>A simulation consisted of 100 replicate runs. New data were generated on each replicate, and the same data were submitted to analysis by each model. The Inverse Wishart model and the Scaled Inverse Wishart model were implemented in JAGS <ref type="bibr" target="#b4">(Denwood, 2016;</ref><ref type="bibr" target="#b24">Plummer, 2003)</ref> using a Gibbs sampling algorithm. For each model, 1000 burn-in iterations were followed by 3000 iterations retained for posterior computation.</p><p>Because JAGS does not support the LKJ prior, the LKJ model was implemented in RStan <ref type="bibr">(Carpenter et al., 2017)</ref> using the NUTS algorithm. Again, there were 1000 burn-in One way of characterizing the results is to compute the RMSE for each estimate across the 100 runs in the simulation. Table <ref type="table">2</ref> shows these RMSE values and the pattern is clear. For low noise and true correlation of .3, the hierarchical model with any of the three prior is about as accurate as knowing the individuals' true values. There is loss in the high noise case, but the performance is about equivalent.</p><formula xml:id="formula_16">iterations</formula><p>One of the most useful benefits of Bayesian hierarchical analysis is that posterior distributions themselves provide estimates on how well parameters are estimated. The relevant quantity is the posterior credible interval <ref type="bibr" target="#b14">(Kruschke, 2014)</ref>, which is somewhat analogous to a confidence interval (cf., <ref type="bibr" target="#b21">Morey, Hoekstra, Rouder, Lee, &amp; Wagenmakers, 2016)</ref>. The credible intervals for the first 50 runs in the high-noise case is shown in Figure <ref type="figure" target="#fig_10">6</ref>. Consider the upper left-hand plot which is for a true population correlation of ρ = .3.</p><p>There are 50 horizontal lines for the 50 runs. In this case, each line is a posterior credible interval for the correlation coefficient for the run, and the dot is the posterior mean. For ρ = 0.3 and ρ = 0.7, for all four priors the vast majority (96%) of credible intervals contain the true value. For ρ = 0.5, this proportion was slightly lower (88%) but still reasonably high. Overall, the credible intervals appear to provide satisfactory coverage. The simulations show encouraging results with all three priors.</p><p>One of the primary advantages of the Inverse Wishart and Scaled Inverse Wishart priors is their computational convenience. The Inverse Wishart is conjugate with the normal distribution, which makes sampling relatively inexpensive. This computational efficiency enables the faster execution: In Simulation 1, LKJ prior's sampling efficiency was approximately ten times lower than Inverse Wishart's and Scaled Inverse Wishart prior's (see Figure <ref type="figure" target="#fig_11">7</ref>). This multiple in efficiency only grows with the size of the covariance matrix.</p><p>Although this computational-time gap is substantial in relative terms, it should not be overstated. For a single dataset, analyses with any of these priors typically complete within a few minutes, and the difference is mainly relevant in simulation studies. More importantly, the LKJ prior offers critical advantages in robustness which motivates our recommendation to consider it for analyzing real-world data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Simulation Study 2: Four Tasks with Calibrated Priors</head><p>The results in Simulation Study 1 show the benefits of hierarchical modeling and the relative equivalence of the three priors. The study, however, covered just two tasks and a single correlation coefficient. We performed Simulation Study 2 with 4 tasks. The four-task simulations follow almost all the same settings in Simulation Study 1 with the exception of the ground truth correlation matrices. The true correlations for the four-task versions are</p><p>shown in Figure <ref type="figure" target="#fig_12">8A</ref>. The four-task true correlation matrix followed a two-factor pattern with set values of ρ 12 = 0.3, ρ 13 = 0.5, ρ 14 = 0.7.</p><p>We ran the same simulation study with these new correlations matrices, and assessed how well correlations may be recovered with the averaging method and three priors. The results for the four-task setup are shown in Figure <ref type="figure" target="#fig_13">9</ref> and Table <ref type="table">2</ref>. These results are presented in the same format as those for Simulation 1, and the results are quite similar.</p><p>As before, correlations from trial averaging have noticeable attenuation; there is much improvement with the hierarchical models, and performance is similar with four priors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Simulation Study 3: Robustness to scale settings</head><p>Simulation Study 1 and 2 are best-case scenarios in that the prior setting on variability matched the variance used in data generation. We cannot be so lucky in application because we do not know the true scale. As discussed previously, the Inverse</p><p>Wishart exhibits dependency between correlation and variability meaning that poor settings of scale may bias the posterior of the correlation coefficients. And the Scaled Inverse Wishart has the same dependency though not to as large a degree. We saw only a minor effect of these posterior settings for the manifest model as the data for I = 200 observations were sufficient in number to dominate the prior. Restated, all three priors demonstrate a robustness to scale setting in that context. Yet, hierarchical models are more complex and more heavily parameterized. Consequently, the influence of the prior is often much greater.</p><p>How does the setting of the scale of variance, s 2 affect posterior estimation of correlation in a hierarchical model with measurement noise? This question is addressed by</p><p>Simulation Study 3 where s (or s 2 ) is manipulated across a few levels. To match the data-generation process in the previous simulations, we set s = .1 (or s 2 = .01). Here, we take values that are either double or half this value, as well as values that are 5 times or 1/5th this value, that is s = .02, .05, .1, .2, .5. We reran Simulation Study 2 with four tasks and with intermediate trial noise of τ 2 j = .4 2 . As can be seen, there is much dependence on s. This behavior is not desirable. The lower row shows the same for true correlations of .7, and the trend here is even more obvious. The second column shows plots for Scaled Inverse Wishart prior, it is less sensitive to prior settings compared with Inverse Wishart, however, when s is 5 times smaller than the true value, it has a tendency to over-estimate the correlation.</p><p>The third and fourth columns show similar plots for the LKJ prior. The third column is for changes in s, and the behavior here may be directly compared to that for the Inverse Wishart prior. Here, we see the desired robustness of posterior correlation to prior variance scale settings. Table <ref type="table">3</ref> shows the RMSE values for these comparable prior settings-they favor the LKJ when the setting is far from the data-generating value. Because the LKJ fared favorably, we also explored the robustness to settings of v, the prior setting on correlations. The values of v were manipulated through v = .25, .5, 1, 2, 4, and the results for true low correlation values exhibit the same robustness. The only scenario where there was an effect of prior settings is for higher true correlations as shown in the lower panel on the right.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Simulation Study 4: Robustness to Inclusion</head><p>None of the three priors are invariant to inclusion. For the Inverse Wishart and Scaled</p><p>Inverse Wishart priors there is increased prior density for correlation coefficients that are similar in magnitude (see Figures <ref type="figure" target="#fig_6">2E-G</ref> and<ref type="figure" target="#fig_7">3E-G</ref>); for the LKJ prior there is decreasing density for extreme magnitude correlations as the number of variables is increased (see Figure <ref type="figure" target="#fig_8">4A</ref>). Do these dependencies affect posterior estimates in reasonably-sized data?</p><p>To explore the issue, we constructed ground truths with correlations shown in Figure <ref type="figure" target="#fig_12">8B</ref>. The key correlation is that between Tasks 1 and 2, which was set to the high value of .8. In one case, the data from Tasks 1 and 2 were submitted to a bivariate analysis much as in Simulation 1. In a second case, data form all 8 tasks were submitted to an 8-dimensional multivariate normal model, and the focus was on the estimate in the correlation between Task 1 and 2 with the six other tasks included and simultaneously analyzed. The trial noise was moderate atτ = .4 and prior scales on variability were set to .1ˆ2 (Inverse Wishart prior) or .1 (Scaled Inverse Wishart and LKJ priors) as in Simulation 1.</p><p>Figure <ref type="figure" target="#fig_15">11</ref> shows the results as scatter plots. First, focus on the small black dots on the diagonal. These are the correlation between true individual values for Tasks 1 and Task 2, and these are the same whether 2 tasks or 8 tasks are analyzed as sample correlation is invariant to inclusion. There are 100 of these points corresponding to the 100 simulation runs. The colored larger points are posterior mean estimates for the correlation between Task 1 and Task 2. The x-axis is for the two tasks alone in a bivariate analysis; the y-axis is the same correlation when the other six tasks are included. For the Inverse Wishart, the estimates are well-centered whether the two tasks were considered in isolation or in the 8-task context. For the SIW and LKJ, the prior dependency is more clear. There is a tendency toward lower posterior means overall, and this effect is accentuated in the 8 task case as expected. As an aside, we had expected similar behavior for the Inverse Wishart and Scaled Inverse Wishart, so we reran the Inverse Wishart simulation with different seeds to check. We obtained the same patterns. We are not sure why the Inverse Wishart fared so well but are reasonably confident this finding is not a statistical fluke.</p><p>RMSE values for Simulation 4 are reported in Table <ref type="table">4</ref>. One critical comparison is between the the estimates of .8 true-valued correlation coefficient for the two-task vs. eight-task context. The Inverse Wishart prior is slightly better in the 8-task case than the 2-task case, and this result replicated on an additional simulation with different seeds.</p><p>The RMSE is slightly worse in 8-task case than the 2-task case for the SIW and LKJ, and</p><p>given the systematic pattern of underestimation in Figure <ref type="figure" target="#fig_15">11</ref>, this result is more interpretable. Table <ref type="table">4</ref> shows that while the estimate of the high correlation value between Task and Task 2 was accurate for the Inverse Wishart, the estimate of the low correlations between the other tasks was less accurate. This accuracy reflects an inflation of the low correlation values for this prior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Simulation Study 5: Robustness To Distribution of Variability for LKJ Prior</head><p>In the Inverse Wishart and Scaled Inverse Wishart priors, the analyst places a prior on covariance which encompasses both the variance and correlation. The resulting marginal prior on variance is inverse gamma for the Inverse Wishart prior, and the marginal marginal prior on standard deviation is a half-t for the Scaled Inverse Wishart.</p><p>The LKJ prior is qualitatively different in that separate, priors are placed on variability and correlation. The analyst is free to choose any form for the prior on standard deviation they wish. We chose a half-t distribution. Does this choice matter?</p><p>One reason to suspect this choice does not matter is that there is prior independence between the variability and correlation. Hence, any dependency comes from the likelihood itself. Nonetheless, to explore the possibility we ran a brief simulation with seven different distributions on standard deviation in the LKJ prior. The distributions are shown in Figure <ref type="figure" target="#fig_16">12A</ref>, and the center of these distribution were at a value around s = .1.<ref type="foot" target="#foot_0">1</ref> Although the priors differ in shape and tail behavior, they all share a common reference point-the center-allowing a reasonable comparison of their influence on the posterior correlation distribution.</p><p>The simulation setup was the four task setup in Simulation 2 with the correlations shown in Figure <ref type="figure" target="#fig_12">8A</ref> which has correlation values of .3, .5, and .7. The upper right panel shows the distribution of posterior means as a smoothed density for the 100 replicates for each prior. The true value here is .3. As can be seen, the choice of prior form matters little perhaps with the exception of the lognormal distribution. The The lower plot shows the case for the true value of .7, and the same result holds. Overall, it seems to matter little which broad distribution is used for the standard deviation in the LKJ prior so long as it has broad coverage and tails at least as fat as an exponential.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Simulation 6: Contrasts</head><p>The preceding simulations are designed to provide insight into the performance of the three priors and to be computationally convenient. For these purposes, we used simplified models that do not account for experimental conditions and contrasts. The generalization of the hierarchical model setup to is straightforward. Consider for simplicity tasks comprised of two contrasting conditions. Examples include Stroop or flanker tasks (congruent vs. incongruent), priming tasks (primed vs. not primed), or task-switching (repeated vs. switched). The key in all these tasks is that the measure of interest is a contrast, say the contrast between incongruent and congruent or between repeated and switched conditions. The following model centers this contrast. An observation is denoted Y ijkℓ where i denotes the individual, j denotes the task, k = 1, 2 denotes the condition, and ℓ denotes the replicate trial. The data model is</p><formula xml:id="formula_17">Y i jkℓ ∼ N(α ij + x k θ ij , τ 2 j ),</formula><p>where α ij is the overall mean or intercept for the ith person in the jth task, x k = -.5, .5 is the contrast code for the kth condition, θ ij is the contrast or slope for the ith person in the jth task, and τ 2 j is the variability across replicate trials in the jth task. The target of interest is θ ij , the contrast, and the previous individual latent multivariate normal model, θ i ∼ N J (µ, Σ) is appropriate. And as before, priors are needed on Σ.</p><p>The main difficulty with the Inverse Wishart prior is that the prior choice of scale on variability affects the posterior distribution of correlations (see Figure <ref type="figure" target="#fig_4">10</ref>). This problem is less salient for contrast parameters because researchers often have appropriate a priori information. Here, we argue that researchers often know at least ot a rough approximation the variability of contrast parameters across individuals. The argument starts with on observation about the direction of individual effects: <ref type="bibr" target="#b27">Rouder and Haaf (2021)</ref> argue that most experimental manipulations yield individual true contrasts that are only in one direction. Take, for example, the Stroop effect where color naming is quicker on average to congruent than incongruent stimuli. The argument is that each individual has a true Stroop effect in the same direction, and that nobody truly names the color of incongruent items faster than congruent ones. When all individuals have true effects in the same direction, <ref type="bibr" target="#b10">Haaf and Rouder (2017)</ref> call it a everybody does situation. This everyone does situation places substantial limits on the scale of variability. Suppose a contrast has a mean of 60 ms, which is typical of Stroop, flanker, and priming effects. The variability in this effect could not be too large else a sizable proportion of individuals would have true effects in the opposing direction. For example, if the mean effect was 60 ms, the standard deviation may not be larger than 30 ms as any more would imply more than 2.5% of the population have the opposite effect. Hence, just by knowing the mean effect, we have good a priori information about the variability across individuals. The question is whether posterior distributions of correlations vary appreciably with this knowledge in the Inverse Wishart setup.</p><p>To answer this question, we ran a simulated Stroop experiment with two conditions in each of two tasks-say a color Stroop task and a numerical Stroop task <ref type="bibr" target="#b17">(MacLeod, 1991)</ref>. The conditions of the simulations were designed to be typical of several Stroop data sets we have examined (see <ref type="bibr" target="#b9">Haaf, Hoffstadt, &amp; Lesche, 2024)</ref>. As before there were 200 synthetic individuals. Each ran 150 trials in each task and each condition. Individuals varied in their true overall speed: α ij iid ∼ Normal(600, 100 2 ). They also varied in their true</p><p>Stroop effect as follows: These Stroop effects were drawn from a bivariate normal with a mean vector of 60 ms in each task. The standard deviation in each was 25 ms, and the correlation across the two tasks was .5. Trial noise, τ j was to 175 ms.</p><p>To test how the prior choice of scale on variability affects the posterior distribution of correlations, we reran the analysis with several choices of s 2 . Our guiding principle was that s 2 should reflect a range of reasonable prior opinion but no more. Here, for a 60 ms mean, we though value of (40 ms) 2 was very large as it implies 7% of individual truly have a reverse Stroop effect where they truly respond quicker to incongruent than congruent items. Likewise, a value of (15 ms) 2 , the mean being 4-times the standard deviation, strikes us as very small. Hence, the question is how varying values of s 2 in this range affects posterior correlation estimates. The resulting posterior correlation distributions are plotted as box plot in Figure <ref type="figure" target="#fig_17">13</ref>, and the effect of prior scale across this reasonable range is modest at best. In conclusion, the variability issue is less salient for contrast parameters especially when researchers have a good idea what the mean effect may be.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>Hierarchical models provide a superior approach to estimating correlations over sample correlations computed from sample means. These models treat trial noise and variation across individuals separately. As a result, correlation estimates are disattenuated and uncertainty reflects both variation across individuals and trials. Sample correlations from sample means, in contrast, have neither of these sanguine properties-they are dramatically attenuated and the associated confidence intervals are too small as they do not reflect trial variability. We recommend that hierarchical models be always used in estimating correlations from experimental data where trials are nested within individuals. Analyses without these models run a substantial risk of misinterpretation.</p><p>The question then is what prior specification is best for estimating correlation coefficients? In this paper, we have compared the Inverse Wishart, Scaled Inverse Wishart, and LKJ prior specifications in manifest and hierarchical contexts. The recommendation is that the LKJ prior is better for general use with empirical data sets. The reason for this recommendation is that the LKJ prior is less sensitive to prior settings than the others. Our recommendation reflects a visualization of the priors as well as simulations of robustness.</p><p>A comparison of the visualization of the priors (Figures <ref type="figure" target="#fig_6">2</ref><ref type="figure" target="#fig_7">3</ref><ref type="figure" target="#fig_8">4</ref>) shows that the Inverse Wishart and Scaled Inverse Wishart priors have at least some sensitivity to prior scale setting. Furthermore, each is sensitive to the inclusion of additional variables inasmuch as correlation coefficients are shrunk to be more similar in magnitude. The LKJ prior on correlation is robust to variance scale settings, but the marginal prior on correlation becomes more peaked at small magnitude values as more variables are included in the analysis.</p><p>The performance of the priors in simulation reflect in part biases evident in the visualizations of the priors. The dependence in the Inverse Wishart of correlation on prior scale manifests itself in simulation-we draw readers' attention to the difficult results for the Inverse Wishart models in Figure <ref type="figure" target="#fig_4">10</ref>. Although the LKJ prior is a safer choice, it is important not to overstate the costs. The scale of variance, the critical prior setting, is not particularly mysterious or unknown. For many applications, especially those with contrasts, the range of reasonable prior scale may indeed be small.</p><p>The choice of the LKJ prior is perhaps the safest. There are two considerations analysts should be aware of in making this choice: The first is that the LKJ marginal prior is a function of the number of variables included in an analysis. This makes the choice of how many variables to include salient. Including more variables lowers in magnitude the posterior estimate of each though the degree of this biasing effect is not overwhelming. The second is that the LKJ prior is computationally slow and does not scale well to higher dimensional models. For example, for eight variables, the Inverse Wishart and Scaled</p><p>Inverse Wishart models ran at about 40 samples per second; the LKJ prior ran at 2 sample per second. That factor of 20 may be the difference between 1 day and 20 days of run time.</p><p>Moreover, this factor increases with increasing numbers of variables, so it is entirely possible to wait weeks and months for an LKJ prior analysis. If the computational demands of the LKJ are difficult to meet, ee recommend the Scaled Inverse Wishart prior over the Inverse Wishart as a default alternative. The Scaled Inverse Wishart prior is far less sensitive to prior scale while providing for just as rapid computations.</p><p>Lastly, we remind readers that simulation results are just that. We have simulated a small sliver of possible cases that reflect our best judgment of the needs of the community.           Posterior means are also stable in the LKJ prior with respect to variation in the prior shape parameter. These stabilities make the LKJ prior an attractive choice when there is little guiding prior knowledge.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure</head><label></label><figDesc>Figure 2E addresses invariance to inclusion. It shows the relationship between two correlation coefficients, or how the inclusion of one variable affects another. The contour plot reveals a lack of independence. Correlation coefficients tend to be similar in magnitudeto each other, and there is less prior mass where one is large in magnitude and the other is small in magnitude. This relationship is visualized more concretely in the conditional prior distributions in Figure2F-G. Analysts that include additional variables with large-magnitude correlations may bias posteriors toward larger correlations; those that include additional variables with small-magnitude correlation may bias posteriors in that direction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figures</head><label></label><figDesc>Figures 4B-D show the relationship between variability and correlation, and there is no relationship by construction. This independence is visualized in the conditional histograms (Figure 4C-D). Likewise, there is no relationship between different pairwise correlations (Figure 4C-D). The LKJ violates invariance to inclusion by downweighting smaller values with more variables; the Inverse Wishart and Scaled inverse Wishart priors do so by making correlation values more similar to each other in magnitude.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>range of scales. The entries show the point estimate and CIs of the correlation between height and weight. The Pearson sample correlation, as shown, does not vary for include and exclude cases. Overall, results from priors are fairly similar and match the sample-correlation results well. There are two relatively minor deviations. First, posterior estimates from the Inverse Wishart prior are a tad too low when the standard deviation scale is 10 times from baseline, and this underestimation reflects the prior dependencies show in Figure 2B-D. Second, the posterior estimates from the LKJ are a tad too low when additional variables are included. This underestimation reflects the dependence on the marginal prior on correlation with increasing numbers of variables as shown in Figure 4A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>followed by 3,000 retained iterations. Mixing was satisfactory in all cases as assessed by visual inspection of the chains, inspection of autocorrelation functions, and computation of effective sample sizes(ESS, Gong &amp; Flegal, 2016). In fact, ESS values in this report always exceeded 2000 effective samples from the 3000 retained samples indicating good mixing throughout. We evaluate each model's efficiency as ESS per second.The results of the simulation are shown in Figure5. Consider the five left-most box plots colored in red, purple, green, yellow, and blue. The true correlation here is ρ = .3, as indicated above and as denoted by the dashed line. For these five box plots, there is a low degree of trial noise as indicated by τ j = .2. The red box plot is the distribution of sample correlations among the 200 individual true values across the 100 runs. The variation serves as what is expected from finite samples of 200 people if there were infinite numbers of trials. We use this as a best-case outcome as individual true-values are known. The remaining four boxplots are based on the trial-level data. The purple box plot is the distribution of correlations from the averaging method where averages are tabulated across L = 20 trials. As expected, this distribution is biased slightly low. This bias is the attenuation of correlation from measurement noise. The bias is not that severe because there is not a high degree of trial noise. The green, yellow, blue box plots are distributions of the posterior mean of the correlation parameter across the runs for the three different priors. These are similar to the correlations among true values indicating that the hierarchical model with any of the three priors does an excellent job of accounting for trial noise. The results for the high-trial-noise case follows in the next four bars with the subtitle τ j = .5. The following trends hold: (i) there is much attenuation for averaging (purple box plot); (ii) correlation recovery is somewhat more variable for the hierarchical models; and (iii) the performance does not vary appreciably across different priors. The remaining panels show the results with higher true correlations. The results are largely the same with the exception that attenuation from averaging becomes more pronounced as the true correlation increases. Overall, there is little to distinguish the performance of Inverse Wishart, Scaled Inverse Wishart, and LKJ priors in these simulations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 10</head><label>10</label><figDesc>Figure 10 shows sets of smoothed densities. The densities are not posterior densities of the correlation coefficient. They are the distribution of the posterior mean across the 100 replicates. Consider the dotted line in the leftmost upper panel. This panel is for the Inverse Wishart for true correlations of .3. That dotted line is the sample correlation among true individual values, and it is not perturbed by trial noise. It corresponds to the red box plots in Figures6 and 9, and it serves as a best case. The solid lines correspond to different settings of s for the Inverse Wishart. As can be seen, there is much dependence on s. This</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1 . The effects of trial variability on correlation estimates. Synthetic individuals were sampled from a bivariate distribution with a true population correlation of .7. A. Scatter plot among individual true values for 200 individuals. the correlation is .73 in value which is close to the true value. B. True values from individuals are not known and must be estimated from trial data by averaging. If the trials are too few or the trials are excessively noise, then the averaged individuals' scores themselves are randomly perturbed from true values. This perturbation systematically attenuates the observed correlation among the tasks. C. Hierarchical models provide separate estimates of trial noise and covariance across individuals in tasks. By separately modeling different sources of variation, the posterior estimates of correlation are disattenuated; moreover, uncertainty in the posterior reflects limitations from the finite samples of participants and trials.</figDesc><graphic coords="38,72.01,75.19,467.99,156.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2 . Visualization of the Inverse Wishart prior distribution. A. Marignal prior on correlation (pairwise correlation coefficients). B. Joint prior on standard deviation and correlation. C-D Priors on correlation conditioned on low and high variability, respectively. E. Joint prior on two correlation coefficients. F-G. Prior on one correlation coefficient conditional on low and high values of another, respectively.</figDesc><graphic coords="39,72.00,76.60,468.01,234.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3 . Visualization of the Scaled Inverse Wishart prior distribution. A. Marignal prior on correlation (pairwise correlation coefficients). B. Joint prior on standard deviation and correlation. C-D. Priors on correlation conditioned on low and high variability, respectively. E. Joint prior on two correlation coefficients. F-G. Prior on one correlation coefficient conditional on low and high values of another, respectively.</figDesc><graphic coords="39,72.00,405.58,468.01,234.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4 . Visualization of the LKJ distribution. A. Marignal prior on correlation (pairwise correlation coefficients). B. Joint prior on standard deviation and correlation. C-D Priors on correlation conditioned on low and high variability, respectively. E. Joint prior on two correlation coefficients. F-G. Prior on one correlation coefficient conditional on low and high values of another, respectively.</figDesc><graphic coords="40,72.00,75.73,468.01,234.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 .</head><label>5</label><figDesc>Figure5. The recovery of correlation for two tasks. There is noticeable attenuation from averaging across trials. There are only slight difference among the four prior specifications.</figDesc><graphic coords="40,72.00,402.98,468.01,280.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6 . Means and 95 credible intervals for recovered correlation from four model across the first 50 runs. Intervals that fail to cover the true correlation are colored in red. These occur at an expected proportion for all prior specifications.</figDesc><graphic coords="41,72.00,91.73,468.01,561.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7 . Effective sample size per second for four models in the 2-task and 4-task cases.</figDesc><graphic coords="42,72.00,106.23,468.00,234.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8 . Groud truth correlations. A. Simulation 2 with 4 tasks. B. Simulation 4 with 8 tasks.</figDesc><graphic coords="42,72.00,436.68,468.01,216.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 9 .</head><label>9</label><figDesc>Figure9. The recovery of correlation for four tasks. There is substantial attenuation from averaging across trials. There are moderate deviations from true values for all prior specifications across the 100 runs, except that under high trial noise and low true correlation, the Inverse Wishart prior tended to over-cover the correlation.</figDesc><graphic coords="43,72.00,224.91,468.01,280.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 10 .</head><label>10</label><figDesc>Figure10. Posterior correlation estimates for four tasks under three models with varying prior settings. Each density curve represents the posterior correlation distributions across 100 runs per condition. The dotted curve shows the individual truth, while the vertical dashed line denotes the true correlation. Posterior means of correlations are highly sensitive to prior scale settings for the Inverse Wishart prior, and less sensitive to prior scale settings for the Scaled Inverse Whishart prior. In contrast, posterior means are much more stable to this prior variation with the LKJ prior. Posterior means are also stable in the LKJ prior with respect to variation in the prior shape parameter. These stabilities make the LKJ prior an attractive choice when there is little guiding prior knowledge.</figDesc><graphic coords="44,72.00,162.05,468.01,334.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11 . Robustness to inclusion of additional variables. The scatter plots show the relationship of posterior correlation estimates whether in isolation (x-axis) or in the context of 8 tasks (y-axis). There is some downward influence form the prior with inclusion for the Scaled Inverse Wishart prior and LKJ priors.</figDesc><graphic coords="45,72.00,104.37,468.01,151.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12 . Different scale prior distributions' impact on correlation estimates in LKJ model. A. Seven selected scale prior distributions. All of them were setted to center around the true scale value of 0.1. B. The posterior correlation estimates for low and C. high correlation. All scale prior distributions resulted in similar posterior correlation estimates.</figDesc><graphic coords="45,72.00,392.25,468.01,234.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 13 .</head><label>13</label><figDesc>Figure 13 . Posterior distributions of the correlation coefficient as a function of prior variance setting. Boxplots do not vary appreciable showing the stability of these posteriors across a reasonable range of settings. The true population correlation is 0.5; the true correlation among these 200 participant is slightly less and is the dashed vertical line.</figDesc><graphic coords="46,72.00,219.07,468.00,292.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>New Methods in Cognitive Psychology, 32-66. Schuurman, N. K., Grasman, R. P. P. P., &amp; Hamaker, E. L. (2016). A Comparison of Inverse-Wishart Prior Specifications for Covariance Matrices in Multilevel</figDesc><table><row><cell>Table 2 Table 3</cell><cell>Table 4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">RMSE with Calibrated Priors for Individual Variation RMSE without Calibrated Priors for Individual Variation RMSE in 8-tasks inclusion case.</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Case</cell><cell cols="3">τ j = 0.2 s True Correlation IW</cell><cell cols="2">SIW</cell><cell>LKJ</cell><cell>τ j = 0.5</cell></row><row><cell cols="9">Autoregressive Models. Multivariate Behavioral Research, 51 (2-3), 185-206. Case True Correlation IW SIW LKJ IW SIW True Correlation Prior 0.02 0.05 0.10 0.20 0.50 2 Tasks</cell><cell>LKJ</cell></row><row><cell cols="4">doi:10.1080/00273171.2015.1065398</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9">Spearman, C. (1904). 'General intelligence,' objectively determined and measured. The 2 Tasks 0.3 0.8 0.064 0.073 0.08</cell></row><row><cell cols="9">American Journal of Psychology, 15 (2), 201-293. doi:10.2307/1412107 0.3 0.043 0.043 0.042 0.168 0.153 IW 0.186 0.147 0.119 0.094 0.115 8 Tasks</cell><cell>0.154</cell></row><row><cell cols="9">Tokuda, T., Goodrich, B., Mechelen, I. V., Gelman, A., &amp; Tuerlinckx, F. (2025). 0.5 0.042 0.041 0.041 0.162 0.164 SIW 0.152 0.121 0.108 0.105 0.105 0.2 0.157 0.137 0.102</cell><cell>0.17</cell></row><row><cell cols="9">Visualizing distributions of covariance matrices. Journal of Data Science, Statistics, 0.7 0.034 0.036 0.036 0.079 0.103 0.105 LKJ 0.117 0.115 0.116 0.115 0.116 0.8 0.056 0.093 0.097</cell></row><row><cell cols="6">and Visualisation, 5 (7). doi:10.52933/jdssv.v5i7.132 4 Tasks 0.5</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.3</cell><cell>IW</cell><cell>0.047 0.186</cell><cell>0.044 0.138</cell><cell>0.048 0.105</cell><cell cols="2">0.192 0.103</cell><cell>0.148 0.206</cell><cell>0.151</cell></row><row><cell></cell><cell>0.5</cell><cell>SIW</cell><cell>0.045 0.139</cell><cell>0.047 0.108</cell><cell>0.047 0.103</cell><cell cols="2">0.119 0.105</cell><cell>0.116 0.105</cell><cell>0.121</cell></row><row><cell></cell><cell>0.7</cell><cell>LKJ</cell><cell>0.037 0.104</cell><cell>0.048 0.101</cell><cell>0.044 0.101</cell><cell cols="2">0.081 0.101</cell><cell>0.121 0.1</cell><cell>0.107</cell></row><row><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>IW</cell><cell>0.153</cell><cell>0.109</cell><cell>0.087</cell><cell></cell><cell>0.144</cell><cell>0.304</cell></row><row><cell></cell><cell></cell><cell>SIW</cell><cell>0.113</cell><cell>0.093</cell><cell>0.109</cell><cell></cell><cell>0.117</cell><cell>0.121</cell></row><row><cell></cell><cell></cell><cell>LKJ</cell><cell>0.09</cell><cell>0.091</cell><cell>0.089</cell><cell></cell><cell>0.092</cell><cell>0.094</cell></row></table><note><p><p>Perhaps the thorniest remaining issues involve when to include variables in analyses. It is our sense that there is more work to be done in understanding the statistical consequences of such choices.</p>Science: A Tutorial.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Note that some distributions do not have means.</p></note>
		</body>
		<back>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Data Availability. No human data are collected in conjunction with this manuscript.</p><p>Code Availability. JAGS and Stan code for the models may be found at https://osf.io/qe9ts.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declarations</head><p>Funding: JNR was supported by ONR N00014-23-1-2792.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict of</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Anthropometric Survey of U.S. Army Personnel: Methods and Summary Statistics (No. ADA611869). Army Natick Soldier Research and Engineering Center</title>
		<author>
			<persName><surname>Army</surname></persName>
		</author>
		<ptr target="https://apps.dtic.mil/sti/citations/ADA611869" />
		<imprint>
			<date type="published" when="2012">2014. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Structural equations with latent variables</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Bollen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">B</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Goodrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bettencourt</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stan: A probabilistic programming language</title>
		<author>
			<persName><forename type="first">A</forename><surname>Riddell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="page">76</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Runjags: An R package providing interface utilities, model templates, parallel computing methods and additional distributions for MCMC models in JAGS</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Denwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the &quot;Probable Error&quot; of a Coefficient of Correlation Deduced from a Small Sample</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Metron</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3" to="32" />
			<date type="published" when="1921">1921</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Carlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
		<title level="m">Bayesian data analysis</title>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>Chapman and Hall</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>nd edition</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Data analysis using regression and multilevel/hierarchical models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
	<note>Retrieved from https: //books.google.com/books?hl=en&amp;lr=&amp;id=lV3DIdV0F9AC&amp;oi=fnd&amp;pg=PR17&amp;dq= gelman+hill+2007&amp;ots=6nhGI8NyQ4&amp;sig=c4k2j5JxOlu2VUHZEsFOV04wwY0</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A Practical Sequential Stopping Rule for High-Dimensional Markov Chain Monte Carlo</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Flegal</surname></persName>
		</author>
		<idno type="DOI">10.1080/10618600.2015.1044092</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="684" to="700" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Attentional Control Data Collection: A Resource for Efficient Data Reuse</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Haaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hoffstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lesche</surname></persName>
		</author>
		<idno type="DOI">10.31234/osf.io/4evy6</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Developing Constraint in Bayesian Mixed Models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Haaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Rouder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Methods</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="779" to="798" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">N</forename><surname>Haines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Kvam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Beauchaine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Pitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A tutorial on using generative models to advance psychological science: Lessons from the reliability paradox</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Turner</surname></persName>
		</author>
		<idno type="DOI">10.1037/met0000674</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Methods</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Simple Marginally Noninformative Prior Distributions for Covariance Matrices</title>
		<author>
			<persName><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Wand</surname></persName>
		</author>
		<idno type="DOI">10.1214/13-BA815</idno>
	</analytic>
	<monogr>
		<title level="j">Bayesian Analysis</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="439" to="452" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Doing Bayesian data analysis, 2nd edition: A tutorial with R, JAGS, and Stan</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Kruschke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Academic Press</publisher>
			<pubPlace>Waltham, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generating random correlation matrices based on vines and extended onion method</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lewandowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kurowicka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Joe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Multivariate Analysis</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1989" to="2001" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Comparison of Inverse Wishart and Separation-Strategy Priors for Bayesian Estimation of Covariance Parameter Matrix in Growth Curve Analysis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Grimm</surname></persName>
		</author>
		<idno type="DOI">10.1080/10705511.2015.1057285</idno>
	</analytic>
	<monogr>
		<title level="j">Structural Equation Modeling: A Multidisciplinary Journal</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="354" to="367" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Half a Century of Research on the Stroop Effect: An Integrative Review</title>
		<author>
			<persName><forename type="first">C</forename><surname>Macleod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="163" to="203" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bayesian inference for correlations in the presence of measurement error and estimation uncertainty</title>
		<author>
			<persName><forename type="first">D</forename><surname>Matzke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Selker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">D</forename><surname>Weeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Scheibehenne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E.-J</forename><surname>Wagenmakers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Collabra: Psychology</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Personality trait structure as a human universal</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Mccrae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Costa</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Psychologist</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">509</biblScope>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The unity and diversity of executive functions and their contributions to complex &quot;frontal lobe&quot; tasks: A latent variable analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Miyake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Emerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Witzki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Howerter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Wager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="100" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The fallacy of placing confidence in confidence intervals</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Morey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hoekstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Rouder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E.-J</forename><surname>Wagenmakers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="103" to="123" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Kendall&apos;s advanced theory of statistics</title>
		<author>
			<persName><forename type="first">A</forename><surname>O'hagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Forster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bayesian inference</title>
		<meeting><address><addrLine>Arnold</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can reasonable be supposed to have arisen from random sampling</title>
		<author>
			<persName><forename type="first">K</forename><surname>Pearson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Magazine</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="157" to="175" />
			<date type="published" when="1900">1900</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">JAGS: A Program for Analysis of Bayesian Graphical Models Using Gibbs Sampling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Plummer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Workshop on Distributed Statistical Computing</title>
		<meeting>the 3rd International Workshop on Distributed Statistical Computing</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">On Cronbach&apos;s merger: Why experiments may not be suitable for measuring individual differences</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Rouder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Chavez De La Peña</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mehrvarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vandekerckhove</surname></persName>
		</author>
		<idno type="DOI">10.31234/osf.io/8ktn6</idno>
		<imprint>
			<date type="published" when="2023-12-21">2023, December 21</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A psychometrics of individual differences in experimental tasks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Rouder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Haaf</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13423-018-1558-y</idno>
		<ptr target="https://doi.org/10.3758/s13423-018-1558-y" />
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin and Review</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="452" to="467" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Are There Reliable Qualitative Individual Difference in Cognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Rouder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Haaf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cognition</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An Introduction to Bayesian Hierarchical Models with an Application in the Theory of Signal Detection</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Rouder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin and Review</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="573" to="604" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Rouder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Speckman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Morey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Naveh-Benjamin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Detection Models with Random Participant and Item Effects</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="621" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hierarchical-Model Insights for Planning and Interpreting Individual-Difference Studies of Cognitive Abilities</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Rouder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mehrvarz</surname></persName>
		</author>
		<idno type="DOI">10.1177/09637214231220923</idno>
	</analytic>
	<monogr>
		<title level="j">Current Directions in Psychological Science</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="128" to="135" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Rouder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Province</surname></persName>
		</author>
		<title level="m">Bayesian Hierarchical Models in Psychological</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
