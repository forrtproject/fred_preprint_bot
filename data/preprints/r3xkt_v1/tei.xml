<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Evaluation of the Pedagogical Soundness and Usability of AI-Generated Lesson Plans Across Different Models and Prompt Frameworks in High-School Physics</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main">An Evaluation of the Pedagogical Soundness and Usability of AI-Generated Lesson Plans Across Different Models and Prompt Frameworks in High-School Physics</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0A5FA2B5A1875FFC9F43139C057F1DA7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-10-19T16:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>5</term>
					<term>Gemini 2.5 Flash</term>
					<term>DeepSeek V3.2</term>
					<term>and Grok 4-using three structured prompt frameworks: TAG (Task</term>
					<term>Audience</term>
					<term>Goal)</term>
					<term>RACE (Role</term>
					<term>Audience</term>
					<term>Context</term>
					<term>Execution)</term>
					<term>and COSTAR (Context</term>
					<term>Objective</term>
					<term>Style</term>
					<term>Tone</term>
					<term>Audience</term>
					<term>Response Format) Large Language Models</term>
					<term>Generative AI</term>
					<term>Lesson Planning</term>
					<term>Pedagogical Evaluation</term>
					<term>Prompt Engineering</term>
					<term>Physics Education</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This study evaluates the pedagogical soundness and usability of AI-generated lesson plans across five leading large language models-ChatGPT (GPT-5), Claude Sonnet 4.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Since the release of ChatGPT and other generative AI tools, educators have increasingly explored their potential applications in the education field. Artificial Intelligence has opened up new possibilities for teaching and learning, both in K-12 and higher education <ref type="bibr" target="#b1">(Dornburg &amp; Davin, 2025;</ref><ref type="bibr" target="#b3">Fan et al., 2024)</ref>.</p><p>Among the many uses for AI in education, one possibility includes using AI to streamline lesson planning. This is particularly attractive as it allows educators to save time and generate new ideas for their lessons. By inputting a simple prompt, AI tools can generate fully formatted lesson plans within seconds.</p><p>However, while AI-generated lesson plans are extremely convenient, concerns have arisen about their pedagogical quality <ref type="bibr" target="#b7">(Karaman &amp; G√∂ksu, 2024;</ref><ref type="bibr" target="#b9">Powell &amp; Courchesne, 2024)</ref>. These concerns primarily pertain to their accuracy, their adaptability to student needs, and overall classroom feasibility. While previous studies have praised AI's ability to produce logically organized and human-like lesson plans efficiently, many have emphasized that human guidance is essential to ensure effectiveness. Moreover, most prior evaluations and studies of AI lesson planning have focused on single AI models <ref type="bibr" target="#b0">(Baytak, 2024)</ref>, leaving a gap in cross-model evaluations of AI-generated lesson plans.</p><p>Additionally, a small but growing amount of research has explored how the structure and detail of prompts can influence the quality of AI outputs. Prompt engineering frameworks such as TAG (Task, Audience, Goal), RACE (Role, Audience, Context, Execution), and COSTAR (Context, Objective, Style, Tone, Audience, Response Format) have been developed and tested to optimize the quality of AI outputs. These frameworks have been applied and investigated in various domains; however, little to no research has examined how these prompt frameworks specifically influence the quality of AI-generated lesson plans. Understanding this relationship is essential for educators to effectively harness AI tools to generate lesson plans.</p><p>To address these research gaps, this study presents an evaluation of lesson plans generated by five leading AI chatbot models-ChatGPT (GPT-5), Claude Sonnet 4.5, Gemini 2.5 Flash, DeepSeek V4.2, and Grok 4-for a single high-school physics topic. Each chatbot was asked to generate lesson plans for the topic of The Electromagnetic Spectrum using three different prompt frameworks: TAG, RACE, and COSTAR. This research design allows an assessment not only of how different AI models differ in pedagogical performance but also of how different prompt frameworks influence the output quality of AI-generated teaching plans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Objective</head><p>The main objective of this study is to evaluate the accuracy, quality, and feasibility of lesson plans generated by the five leading free-access AI chatbots. This investigates how suitable it is to utilize Artificial Intelligence to create lesson plans for the classroom. The objectives of the study can be categorized into 2 specific objectives: SO1. Evaluate the pedagogical quality of AI-generated lesson plans produced by five leading generative AI models (ChatGPT, Claude, Gemini, Grok, and DeepSeek). SO2. Investigate the impact of prompt framework design by analyzing how TAG, RACE, and COSTAR prompt frameworks influence the instructional quality, structure, and creativity of AI-generated lesson plans.</p><p>By pursuing these objectives, this study aims to provide evidence-based insights into how both AI chatbot choice and prompt framework influence the quality of AI-generated lesson plans, offering guidance for educators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Literature Review</head><p>The use of Large Language Models (LLMs) for lesson planning has gained some significant attention and traction among educators. Following this trend, education researchers have begun to systematically evaluate the quality, usability, and effectiveness of AI-generated lesson plans. This meta-analysis will synthesize current research that directly assesses AI-generated lesson plans across multiple dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pedagogical Quality and Student Outcomes</head><p>Some controlled studies have found that AI-generated lesson plans can match or even exceed the effectiveness of traditionally developed plans when measured by student achievement through test scores. <ref type="bibr" target="#b7">Karaman and G√∂ksu (2024)</ref> conducted a pretest-posttest quasi-experimental study involving 39 third-grade students, with 24 students in an experimental group and 15 students in a control group <ref type="bibr" target="#b7">(Karaman &amp; G√∂ksu, 2024)</ref>. The experimental group was taught using lesson plans generated entirely by ChatGPT, while the control group received lessons based on teacher-developed plans. The students took a multiple-choice test of 25 questions at the start and end of the experiment. Over the course of a five-week mathematics unit, both groups demonstrated significant gains, with very large effect sizes (Cohen's d ‚âà 1.25-1.27). The ChatGPT group improved from a mean pre-test score of 38.4 to a post-test score of 61.2 (t(23) = 6.47, p &lt; .001, d = 1.268), while the teacher-plan group improved from 34.0 to 55.3 (t(14) = 5.54, p &lt; .001, d = 1.250). Importantly, there was no statistically significant difference between the groups' final scores (t = 0.851, p = 0.400), indicating that AI-generated plans were at least as effective as teacher-created plans in promoting learning outcomes.</p><p>In a similar study, <ref type="bibr" target="#b3">Fan et al. (2024)</ref> developed a tool called LessonPlanner, which embeds Gagn√©'s instructional design principles into an AI-driven lesson planning interface <ref type="bibr" target="#b3">(Fan et al., 2024)</ref>. By including 12 teachers as evaluators, these expert evaluators scored LessonPlanner plans higher in areas such as instructional clarity, alignment with learning objectives, integration of theory and practice, and feasibility. In addition to improved quality, teachers reported reduced planning time and cognitive load.</p><p>Together, these findings suggest that AI-generated lesson plans, especially when supported by structured frameworks or design scaffolds, can be pedagogically robust and instructionally effective. However, these benefits depend heavily on user input, prompt structure, and post-generation editing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Structural Consistency and Prompt Variability</head><p>Research on Artificial Intelligence over the past two years has also examined the field of prompt engineering.</p><p>As for prompt variability in AI-generated lesson plans, Dornburg and Davin from Cambridge University conducted a detailed evaluation of ChatGPT-generated foreign-language lesson plans under increasingly specific prompts <ref type="bibr" target="#b1">(Dornburg &amp; Davin, 2025)</ref>. Five prompts were designed to include increasing amounts of context and structure. The first few prompts sometimes omitted essential components like formative assessments, context, or learning objective alignment. However, in the latter prompts, when rubrics or stepwise instructions were embedded in prompts, outputs were significantly more complete and coherent. This shows that AI models do not reliably infer pedagogical priorities unless explicitly guided, making the human role in creating and framing prompts crucial towards generating an effective teaching plan.</p><p>In the study, Dornburg and Davin also observed that AI-generated plans often reflected superficial coherence but lacked deeper instructional logic. That is, sequencing activities didn't gradually increase in cognitive complexity. This supports the notion that AI lesson planning largely requires human guidance and scaffolding to achieve pedagogically meaningful and effective teaching plans <ref type="bibr" target="#b2">(Dornburg &amp; Davin, 2024</ref><ref type="bibr" target="#b5">, 2025)</ref>. While LLMs do have the ability to generate grammatically coherent and basic plans, they fail to implement more deeply productive scaffolding. This shows that AI models are not able to infer pedagogical priorities unless being guided explicitly, hence making the design of prompts crucial.</p><p>Taken together, Dornburg and Davin's research consolidates the notion that prompt specificity, clarity, and embedded evaluation criteria directly affect the quality, consistency, and pedagogical fidelity of AI-generated lesson plans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Instructional Characteristics and Alignment with Standards</head><p>While AI-generated lesson plans are shown to be able to achieve a basic level of structural coherence, studies have noted the fact that these lesson plans contain limitations in curricular alignment, differentiation, and connections to real-life implications and adaptation to local contexts. <ref type="bibr" target="#b0">Baytak (2024)</ref> analyzed a content analysis of 18 lesson plans generated by ChatGPT and Google Gemini across the 7th-grade mathematics, science, literature, and social studies classes <ref type="bibr" target="#b0">(Baytak, 2024)</ref>. The AI outputs were observed to generally produce clear objectives, logically sequenced activities, and basic assessment tasks, thus allowing them to be sufficient to be used in lessons. However, it was noted that the learning objectives from the generated lesson plans were narrower than the learning objectives of the New Jersey Student Learning Standards, hence showing that they rarely included links to national, state, or international curricular standards. Further, it was pointed out that the generated lesson plans by ChatGPT and Gemini did not meaningfully integrate technology tools or strategies, but rather included simple materials such as "whiteboard or projector", "markers or pens", and "handouts or articles". <ref type="bibr" target="#b6">Kahraman and Kƒ±yƒ±cƒ± (2025)</ref> evaluated the effectiveness of artificial intelligence in generating middle school science lesson plans against Turkey's national science curriculum <ref type="bibr" target="#b6">(Kahraman and Kƒ±yƒ±cƒ±, 2025)</ref>. While the objectives of the class and real-world context were addressed, the generated plans lacked formative assessment loops, reflection, and differentiation strategies for students with varying needs and abilities. Similarly, <ref type="bibr" target="#b8">Lee and Zhai (2024)</ref> found that pre-service teachers' AI-assisted lesson plans were partially aligned with curricular standards and required adaptation to fit the local classroom needs <ref type="bibr" target="#b8">(Lee &amp; Zhai, 2024)</ref>. <ref type="bibr" target="#b9">Powell and Courchesne (2024)</ref> investigated the opportunities and risks involved in utilizing ChatGPT to generate first-grade science lesson plans. They found an additional risk that the lesson plans included factual errors and fabricated resources <ref type="bibr" target="#b9">(Powell &amp; Courchesne, 2024)</ref>.</p><p>Overall, studies indicate that AI-generated lesson plans excel at coherence but consistently fall short in deeper pedagogical aspects <ref type="bibr" target="#b0">(Baytak, 2024;</ref><ref type="bibr" target="#b6">Kahraman &amp; Kƒ±yƒ±cƒ±, 2025;</ref><ref type="bibr" target="#b8">Lee &amp; Zhai, 2024;</ref><ref type="bibr" target="#b9">Powell &amp; Courchesne, 2024)</ref>. These include alignment with curriculum, differentiation, formative assessment, and context-sensitive adaptations. These findings suggest that AI tools are most effective when used in partnership with experienced educators who can validate and refine outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Summary of Findings</head><p>Across studies, several common patterns emerge:</p><p>1. Comparable effectiveness: AI-generated lesson plans are capable of producing learning outcomes comparable to human-designed plans when combined with structured prompts and scaffolds 2. Importance of prompting: Embedding instructional frameworks, rubrics, or checklists consistently enhances output quality and reduces variability 3. Curricula alignment gaps: AI plans often lack explicit references to standards, differentiation strategies, technology integration, and feedback 4. Human intervention: Across all studies, teacher intervention has been stated to be necessary to ensure content accuracy, pedagogical soundness, and suitability for diverse learners</p><p>In conclusion, research shows that AI lesson planning shows substantial promise. However, its effectiveness is contingent upon structured prompt design, human guidance, and meticulous curricular adaptation.</p><p>The literature analysis identifies a clear gap in the research of AI lesson planning: cross-model comparative analyses examining how AI system choice and prompt framework influence the quality of lesson plans. The present study addresses this need by evaluating five leading LLMs (ChatGPT, Claude, Gemini, DeepSeek, Grok) using three prompt frameworks (TAG, RACE, and COSTAR). Hence, this provides the first systematic multi-dimensional assessment of AI-generated lesson plans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methodology</head><p>The methodology implements a single, fully automated evaluation to analyze the pedagogical quality of AI-generated lesson plans produced by different AI models under different prompt frameworks. The automated evaluation applies four predefined metrics to each lesson plan: readability and linguistic complexity, factual accuracy and hallucination detection, standards alignment, and the cognitive level of learning objectives. The aim is to determine how effectively each AI model can generate structured, pedagogically sound lesson plans that align with established teaching and learning principles.</p><p>Five of the most popular AI models were selected: ChatGPT (GPT-5), Claude Sonnet 4.5, Gemini 2.5 Flash, DeepSeek V3.2, and Grok 4. Each model was prompted to generate lesson plans for the subject physics using three distinct prompt frameworks: TAG (Task-Audience-Goal), RACE (Role-Action-Context-Execute), and COSTAR (Context-Objective-Style-Tone-Audience-Response Format). This factorial design (5 models √ó 3 prompts) results in a total of fifteen AI-generated lesson plans. For comparability, all lesson plans were generated under standardized conditions in June 2025.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Lesson Topic</head><p>The physics topic selected for the prompt was The Electromagnetic Spectrum. This topic was chosen as it represents a core high-school physics topic that integrates conceptual understanding, mathematics, and real-world application. The prompts instructed each AI model to produce a complete 60-minute lesson plan for a mixed-ability classroom of Grade 9 to 12 students. The generated lesson plans were expected to include clear learning objectives, differentiation strategies, assessment methods, and a closing reflection activity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Prompt Frameworks and Input Design</head><p>Three widely used prompt frameworks were applied to examine how structured inputs influence the quality of AI-generated lesson plans. To ensure impartiality, the amount of detail, contextual guidance, and instructional requirements was kept identical across all prompts. Each version of the prompt contained the same information about lesson duration (60 minutes), class type (mixed-ability students in Grades 9-12), required lesson components, and topic. The only variable manipulated was the organizational structure of the framework itself. This design isolates the effect of framework structure on the output quality of AI-generated lesson plans rather than differences in prompt specificity.</p><p>The prompts can be seen below:</p><p>TAG (Task-Audience-Goal):</p><p>‚Ä¢ Task: Generate a complete 60-minute lesson plan on The Electromagnetic Spectrum.</p><p>‚Ä¢ Audience: Mixed-ability high-school students (Grades 9-12).</p><p>‚Ä¢ Goal: Ensure learning objectives are clear, activities are engaging and age-appropriate, and the plan includes differentiation strategies, hands-on demonstrations, formative assessment, and a closing reflection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RACE (Role-Action-Context-Execute):</head><p>‚Ä¢ Role: You are a high-school physics teacher preparing a 60-minute lesson.</p><p>‚Ä¢ Action: Design a detailed lesson plan on The Electromagnetic Spectrum.</p><p>‚Ä¢ Context: The class consists of mixed-ability students in Grades 9-12. Lessons should include clear learning objectives, engaging and age-appropriate activities, differentiation strategies, hands-on demonstrations, formative assessment, and a closing reflection.</p><p>‚Ä¢ Execute: Present the plan in a structured format suitable for classroom use, including objectives, materials, procedures, and assessment methods.</p><p>COSTAR (Context-Objective-Style-Tone-Audience-Response Format):</p><p>‚Ä¢ Context: You are a high-school physics teacher teaching a mixed-ability high-school physics class (Grades 9-12) learning about The Electromagnetic Spectrum. ‚Ä¢ Objective: Create a 60-minute lesson plan where learning objectives are clear, activities are engaging and age-appropriate, and the lesson includes differentiation strategies, hands-on demonstrations, formative assessment, and a closing reflection ‚Ä¢ Style: Structured, practical, and easy for a teacher to implement.</p><p>‚Ä¢ Tone: Professional, supportive, and student-centered.</p><p>‚Ä¢ Audience: Grades 9-12 students with varying levels of prior science knowledge.</p><p>‚Ä¢ Response Format: Provide a complete lesson plan including objectives, materials, step-by-step activities, assessment methods, and a short teacher reflection section.</p><p>By maintaining uniform content density and expectations across all the prompts, this setup ensures that any observed differences in generated teaching plans stem solely from differences in prompt framework structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Data Collection</head><p>Each AI model generated one lesson plan per prompt, resulting in a total of fifteen lesson plans. All outputs of lesson plans were anonymized and stored in text format (.txt) for computational analysis.</p><p>After export, all files were anonymized and normalized to support automated processing. No content edits were made.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Automated Evaluation Pipeline</head><p>A fully automated evaluation pipeline was implemented to quantify the pedagogical and linguistic characteristics of each lesson plan. A total of four computational metrics were applied: 1. Readability and Linguistic Complexity 2. Factual Accuracy and Hallucination Detection 3. Standards and Curriculum Alignment 4. Cognitive Level of Learning Objectives All computational analyses were executed in Python 3.10. Established natural-language and data-processing libraries, including pandas, nltk, scikit-learn, and sentence-transformers, were used to compute quantitative data across all model-framework combinations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Readability and Linguistic Complexity</head><p>The first metric is the readability and linguistic complexity metric. This metric evaluates the accessibility and linguistic appropriateness of each lesson plan. Readability is a critical determinant of whether AI-generated lesson plans can be effectively understood and implemented by teachers and students. In education materials, text that is too complex or overly simplified can hamper comprehension or reduce conceptual depth. To quantify these linguistic characteristics, several readability and linguistic complexity measures were applied.</p><p>The Flesch-Kincaid Grade Level (FKGL) index estimates the U.S. school grade level required to comprehend a given text <ref type="bibr" target="#b4">(Flesch, 1948)</ref>. It is calculated by: Scores between 60 and 70 generally correspond to plain English that is comprehensible to high-school students, while scores below 30 indicate highly academic texts <ref type="bibr" target="#b4">(Flesch, 1948)</ref>.</p><formula xml:id="formula_0">ùêπùêæùê∫ùêø = 0.</formula><p>The Gunning Fog Index (FOG) provides an additional measure of readability. It incorporates the percentage of complex words (words with three or more syllables) into analyses. The calculation is expressed as:</p><formula xml:id="formula_1">ùêπùëÇùê∫ = 0. 4 ( ùë°ùëúùë°ùëéùëô ùë§ùëúùëüùëëùë† ùë°ùëúùë°ùëéùëô ùë†ùëíùëõùë°ùëíùëõùëêùëíùë† ) + 100( ùëêùëúùëöùëùùëôùëíùë• ùë§ùëúùëüùëëùë† ùë°ùëúùë°ùëéùëô ùë§ùëúùëüùëëùë† ) ‚é° ‚é£ ‚é§ ‚é¶</formula><p>Gunning Fog Index values near 10 to 12 correspond to text suitable for grade 9-12 audiences, while values above 16 indicate writing similar to an academic paper.</p><p>Furthermore, to capture the lexical diversity of the lesson plans, the Type-Token Ratio (TTR) is computed. The TTR is defined as the number of unique words divided by the total number of words in a text. Higher TTR values indicate a more complex and richer vocabulary use.</p><p>Finally, basic measures of syntactic cohesion were calculated through values such as average sentence length. The average sentence length was calculated by obtaining the mean number of words per sentence.</p><p>Longer average sentence lengths tend to indicate greater syntactic complexity and more difficult readability.</p><p>Each readability and linguistic index was calculated following all lesson plans being converted to text files (.txt). These linguistic metrics were then aggregated across all teaching plans for lessons and frameworks to create a composite linguistic profile for each model and framework. This serves as an indicator of how effectively AI-generated lesson plans balance clarity and sophistication.</p><p>Within educational contexts, readability directly impacts pedagogical usability. Hence, the linguistic profile will also provide insight into the feasibility of such lesson plans for the target audience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Factual Accuracy and Hallucination Detection</head><p>The second metric is the factual accuracy and hallucination detection metric. It examines the factual reliability of the AI-generated lesson plans. As physics instruction and lessons depend on accurate communication of mathematical and conceptual relationships, any minor error can distort scientific understanding. A rule-based verification pipeline is implemented to identify and quantify factual inconsistencies or hallucinated content across all teaching plans.</p><p>Omissions in the lesson plans of key information and conceptual relationships were treated as errors. A plan scored an omission error whenever a targeted relationship or constant was not present.</p><p>In the first stage of the verification process, the pipeline extracted all numerical constants, equations, and domain-specific entities from the text. Regular expressions were used to detect canonical expressions about The Electromagnetic Spectrum, such as , , and . Constants, including the ùëê = Œªùëì ùê∏ = ‚Ñéùëì ùê∏ = ‚Ñéùëê Œª speed of light, Planck's constant, and frequency or wavelength units, were also extracted to validate.</p><p>In the second stage, these extracted entities were cross-validated against the values from verified databases. Deviations greater than ten percent from the literature values were categorized as minor errors, while categorical inversions or conceptual errors were classified as major errors. Omissions of any target item were counted as minor errors.</p><p>Additionally, the pipeline identified fabricated information from the generated lesson plans, including made-up experiments or nonexistent concepts. Each plan's cumulative factual reliability was quantified using a developed Hallucination Index (HI). The HI is defined as:</p><formula xml:id="formula_2">ùêªùêº = (2 √ó ùëöùëéùëóùëúùëü ùëíùëüùëüùëúùëüùë†) + (1 √ó ùëöùëñùëõùëúùëü ùëíùëüùëüùëúùëüùë†)</formula><p>Lower HI values indicate higher factual accuracy. This numerical index provides a consistent measure of factual soundness. Within the educational domain, this precision is especially crucial, as factual errors lead to misconceptions that hinder conceptual learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Standards and Curriculum Alignment</head><p>The third metric is the standards and curriculum alignment metric. This metric assessed the degree of alignment of each AI-generated lesson plan with established educational standards. In particular, the Next Generation Science Standards (NGSS) for high-school physics was chosen on the topic of The Electromagnetic Spectrum, which was the HS-PS4 cluster.</p><p>As none of the prompts explicitly instructed the AI models to follow a specific curriculum, this analysis captures how spontaneously the models generated learning objectives that resemble those of recognized physics standards.</p><p>To measure alignment, each lesson plan was first scanned to extract sentences that expressed learning objectives. These objectives were then converted into semantic embeddings using the Sentence-BERT language model with the sentence-transformers library. The same process was applied to the five official standards for the NGSS HS-PS4 cluster.</p><p>Next, the study calculated the cosine similarity between both texts. Cosine similarity measures how similar two texts are in terms of meaning. The formula is given by:</p><formula xml:id="formula_3">ùëÜ = ùê¥‚Ä¢ùêµ ùê¥ | | | | ùêµ | | | |</formula><p>A and B in this case are vector representations of the two learning objective texts. The cosine similarity calculation gives a score on a scale from 0 to 1. A score of 0 indicates no alignment, while a score of 1 means that both texts are identical in meaning.</p><p>Scores below 0.20 generally indicate weak alignment, while scores between 0.20 and 0.35 suggest moderate alignment. Further, scores above 0.35 reflect strong conceptual alignment. This method provides a clear way to determine how closely AI-generated lesson plans reflect the curriculum of formal physics education.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.4">Cognitive Level of Learning Objectives</head><p>The fourth metric is the cognitive level of the learning objectives metric. This metric assessed the cognitive rigor of the generated lesson plans by classifying the learning objectives of each lesson plan according to Bloom's Revised Taxonomy <ref type="bibr">(Anderson &amp; Krathwohl, 2001</ref>).</p><p>Bloom's Taxonomy is a widely used framework that classifies learning objectives by cognitive complexity. The six levels of cognitive complexity range from recall to creative problem solving. The taxonomy is widely used in instructional design to evaluate the intellectual complexity and demand of educational materials.</p><p>Each learning objective from the lesson plans was first parsed to extract the principal verb using dependency parsing. The extracted verb was then matched against a curated vocabulary list of over 250 pedagogical verbs, each classified according to Bloom's six levels of thinking: Remember (1), Understand (2), Apply (3), Analyze (4), Evaluate (5), and Create (6).</p><p>Each verb extracted from the teaching plan was then assigned a numerical score corresponding to its taxonomy level. The mean score was calculated across all objectives in the lesson plan to compute a Cognitive Demand Index (CDI). This index provided a continuous measure of cognitive rigor, ranging from 1 to 6.</p><p>A lower CDI (1-2) represents a focus merely on factual knowledge and comprehension tasks. A moderate CDI (3-4) suggests that the plan moves beyond factual knowledge and encourages analytical and procedural thinking. Higher CDI values (5-6) signify engagement with evaluative judgement and inquiry-based learning, representations of advanced scientific reasoning. This classification of a Cognitive Demand Index allowed the study to identify whether AI lesson plans primarily produced surface-level learning outcomes or successfully incorporated higher-order thinking that aligned with more contemporary pedagogical goals in STEM education.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.5">Summary</head><p>Together, these four metrics form an analytical framework for evaluating the pedagogical soundness of AI-generated lesson plans. This methodological approach enables objective, quantitative, and replicable assessment that allows for direct comparison across models and prompt frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Statistical Aggregation</head><p>All automated metrics were first computed for each plan and then aggregated to the model and frameworks. For the model-level aggregation, each model's three outputs generated under TAG, RACE, and COSTAR were averaged to obtain a single composite score per metric. For the framework-level aggregation, scores were averaged across all five models. All reported values, therefore, represent mean performance metrics. This provides a stable basis for cross-model and cross-framework comparison in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>All fifteen lesson plans were generated across the five AI models and underwent computational analysis (See Appendix A).</p><p>This section presents quantitative findings from the automated evaluation across readability and linguistic complexity, factual accuracy and hallucination detection, standards and curriculum alignment, and the cognitive demand of learning objectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Automated Evaluation Results</head><p>Below are all the evaluated results for all metrics across frameworks and models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Framework</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Readability and Linguistic Complexity</head><p>According to the computational analysis presented in Section 5.1, the linguistic accessibility shows substantial variation by AI model and moderate variation by prompt framework.</p><p>Across all lesson plans, the Flesch-Kincaid Grade Level ranged from 7.63 to 21.01 (Claude, COSTAR), while Flesch Reading Ease (FRE) ranged from 6.91 to 58.44 across the same two lesson plans. These results suggest that, depending on model choice, AI systems can generate lesson plans ranging from high-school level readability to university-level academic readability and complexity.</p><p>When metrics are averaged by model, DeepSeek produced the most accessible lesson plan, with a mean FKGL of approximately 8.64 and a mean FRE of 54.65. Both of these values align well with the comprehension level of mixed-ability high-school students in grades 9-12. In contrast, Claude and Grok generated text that was considerably more complex, with mean FKGL values of 19.89 and 13.32. This is characterized by longer average sentence lengths and a higher proportion of polysyllabic words within the teaching plans. Their outputs often included subordinate clauses, increasing readability difficulty.</p><p>Gemini and ChatGPT occupied an intermediate position in terms of readability within the models, having mean FKGL values of 11.03 and 10.86, respectively. These values maintain a moderate readability suitable for students in grades 9-12.</p><p>Prompt framework effects on readability were present but insignificant compared to model effects. On average, COSTAR yielded the lowest mean FKGL (12.43). This is followed closely by RACE (12.76) and TAG (13.05).</p><p>These findings indicate that the AI model used is the primary determinant of readability, while framework structure plays a minor role in influencing readability. Models such as DeepSeek produce text that requires minimal to no editing for mixed-ability classrooms of students in grades 9-12, whereas Claude's dense academic register may be more suitable for teacher reference materials rather than direct teaching for student use.</p><p>Teachers who are aiming for more readable and balanced lesson plans should prioritize selecting a linguistically appropriate and accessible AI model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Factual Accuracy and Hallucination Detection</head><p>Factual Accuracy and Hallucination Detection, defined by the Hallucination Index created varied to a similar degree between models and frameworks.</p><p>At the model level, Grok demonstrated the strongest factual reliability with a mean Hallucination Index of approximately 2.00. Its lesson plans consistently included the wave equation ( ) and clear ùëê = Œªùëì differentiation between the seven spectrum bands, but sometimes omitted the numerical constant for light speed and Planck's constant. ChatGPT and Gemini followed closely with Hallucination indices of 2.50 and 2.33. DeepSeek yielded the highest Hallucination Index value of 2.83, reflecting more frequent omissions and limited quantitative references. Overall, none of the lesson plans showcased any factual inaccuracies; however, several plans contained omissions of expected items, which is reflected in non-zero Hallucination Index scores.</p><p>Prompt framework effects on factual accuracy were more pronounced than on readability. The RACE prompt framework had the lowest mean Hallucination Index (2.0), followed by TAG (2.3) and COSTAR 2.9). This pattern suggests that either the procedural scaffolding of the RACE framework, particularly the "Role" and "Execute" components, encourages the inclusion of equations, constants, and ordered listings by framing the model's output as a professional teacher's deliverable, or the addition of "Style" and "Tone" of the COSTAR framework encouraged too much of a focus on overall response manner rather than specific content.</p><p>These results reveal that factual reliability is sensitive to both model choice and prompt framework. While Grok consistently produced the most accurate content, the RACE framework also offered a potential procedural advantage across models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Standards Alignment</head><p>Although not explicitly instructed to do so, from the initial observation of the teaching plans, all three teaching plans generated by Claude Sonnet 4.5 automatically aligned lesson content with the Next Generation Science Standards (NGSS) of HS-PS4-1, HS-PS4-3, and HS-PS4-5 related to wave properties and electromagnetic radiation. This was explicitly stated in the teaching plans generated by Claude. Two teaching plans generated by Grok and one teaching plan generated by ChatGPT also followed the NGSS standards and explicitly indicated alignment.</p><p>At the model level, Claude displayed the highest average NGSS similarity with a similarity of 0.110, indicating that its outputs most closely approximated the structure and phrasing of curriculum-based objectives under the HS-PS4 cluster. Other models clustered near this value with small variances, with Grok also having a high similarity of 0.092 and ChatGPT having a similarity value of 0.063. This suggests overall moderate alignment, considering no specific instruction based on curriculum alignment was given.</p><p>Framework effects followed a similar hierarchy to that found in the factual analysis. RACE produced the strongest alignment with an NGSS mean of 0.082, followed by TAG with an NGSS mean of 0.081 and COSTAR with an NGSS mean of 0.070.</p><p>When looking at specific standards, alignment was strongest for HS-PS4-1-which addresses the relationships among frequency, wavelength, and speed-since nearly all plans included references to the mathematical relationship of . Alignment with HS-PS4-3, HS-PS4-4, and HS-PS4-5, which ùëê = Œªùëì separately focus on models of electromagnetic radiation, effects of different frequencies of electromagnetic radiation when absorbed by matter, and the application of waves in technological devices, was moderately low compared to alignment with HS-PS4-1. This could be explained as the latter three standards are not the main high-school physics topics within the topic of The Electromagnetic Spectrum.</p><p>The findings indicate that, in the absence of explicit standards in the prompt, models naturally approximate the foundational concepts of HS-PS4-1 but struggle with higher-level applications that involve cross-disciplinary reasoning. This can be observed most significantly in AI models Claude and Grok. Frameworks such as RACE improve incidental alignment by ensuring that objectives are expressed with procedural completeness. Ultimately, to achieve stronger curricular correspondence, teachers must incorporate explicit NGSS references and guidance so that the generated lesson plan follows the curriculum standards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Cognitive Demand of Objectives</head><p>Bloom's taxonomy analysis revealed that most objectives in the AI-generated lesson plans were concentrated at the lower cognitive tiers. Across all models, verbs associated with Remember (e.g., identify, list, describe) and Understand (e.g., explain, compare, discuss, interpret) accounted for the majority of occurrences. These two verbs represent the two lowest levels of cognitive demand in Bloom's taxonomy.</p><p>ChatGPT, Claude, and DeepSeek exhibited slightly higher proportions of Understand-level verbs (0.310-0.401), with overall higher values for the Cognitive Demand Index, at 2. <ref type="bibr">294, 2.262, and 2.193</ref>. This suggests a stronger emphasis on conceptual comprehension. Gemini and Grok both leaned more heavily toward Remember-level verbs (0.479-0.600), often focusing on recall-oriented objectives. The Apply level across teaching plans appeared sporadically through verbs such as use, calculate, and solve, whereas higher-order verbs related to Analyze, Evaluate, and Create were extremely rare.</p><p>The predominance of lower-order cognitive verbs suggests that large language models tend to emulate basic textbook-style objectives that emphasize comprehension and recall. This pattern reflects the fact that the models' distribution of educational training data is dominated by instructional materials such as textbooks, worksheets, and curriculum documents that primarily use lower-order verbs emphasizing recall and comprehension. To achieve inquiry-driven or more creative outcomes, teachers need to explicitly request a mix of Bloom-level verbs and require corresponding tasks and activities to be suitable for higher cognitive levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Cross-Metric Synthesis</head><p>Integrating and summarizing all metrics reveals three dominant trends. Firstly, model choice largely governs the readability of the teaching plan. It is shown that DeepSeek is the most accessible in terms of linguistic complexity, while the Claude and Gemini models produced teaching plans with denser academic language that is slightly beyond the reading level of grade 9-12 students.</p><p>Secondly, framework structure and model choice both affect factual accuracy to approximately the same degree. The RACE prompt framework achieved the lowest mean Hallucination Index of 2.00 and the highest NGSS curriculum alignment of 0.092. Among models, Grok was also highly reliable, averaging a 2.00 on the Hallucination Index.</p><p>Thirdly, cognitive demand stayed clustered at lower Bloom Taxonomy tiers consistently across all generated lesson plans, with all five AI models having CDI values indicating cognitive demand on the levels of Remember and Understand.</p><p>Together, these findings suggest the most effective classroom configuration pairs a readability-friendly model such as DeepSeek with the prompt framework RACE, supplemented by explicit instructions towards cognitive level, curriculum references, class content, and context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>The results demonstrate that model type and prompt framework influence different dimensions of pedagogical quality of AI-generated lesson plans.</p><p>Readability and linguistic complexity were primarily a function of the chosen AI model. DeepSeek consistently produced plans that aligned with secondary-level reading targets, while Claude and, to a lesser extent, Grok tended toward denser academic language. Such variation in linguistic accessibility carries practical implications for classroom use, as lesson plans written at the appropriate readability level minimize the teacher's need for subsequent simplification or rephrasing.</p><p>Prompt framework exerted a stronger effect on factual completeness and curricular alignment than on readability. The prompt framework RACE reliably reduced the Hallucination Index with a mean value of 2.00 and produced the highest incidental similarity to NGSS language at the framework level with a mean similarity value of 0.082. This suggests that the procedural structure of the RACE framework, which includes both the "Role" and "Execute" sections, encourages models to generate more comprehensive and technically grounded lesson content by framing the model's output as a professional teacher's deliverable. Comparatively, the prompt framework COSTAR yielded slightly lower readability grade levels than the prompt frameworks RACE or TAG on average, but this advantage did not translate into factual accuracy or curriculum alignment. This pattern shows that stylistic guidance alone is insufficient for scientific completeness. Instead, prompt frameworks that foreground and clarify roles, actions, and execution checklists are more likely to elicit the clear physics anchors required for The Electromagnetic Spectrum.</p><p>Additionally, model reliability exhibited a narrower spread than readability but still revealed insightful differences. The AI model Grok achieved the lowest hallucination score overall, with a mean Hallucination index of 2.00, whereas DeepSeek, despite its readability advantage, omitted targeted items slightly more often, having the highest hallucination score. The absence of a consistent correlation between linguistic accessibility and reliability is notable.</p><p>The Bloom Taxonomy analysis confirmed a default lean toward lower cognitive tiers in objective statements from AI-generated lesson plans. Across AI models and prompt frameworks, Remember and Understand verbs dominated mostly, with Apply verbs appearing intermittently and Analyze, Evaluate, and Create rarely present. The average Cognitive Demand Index values between all models ranged from 1.578 to 2.294. This distribution mirrors the conventional pattern found in educational textbooks, which tend to emphasize factual recognition and conceptual explanation. Since learning objectives shape both assessment design and classroom activities, the consistent absence of higher-order verbs may constrain instructional depth, leading to lessons that prioritize recall and basic understanding over analysis or creation. Hence, with AI-generated lesson plans, educators must explicitly specify higher-order cognitive verbs to ensure that objectives extend beyond recall and comprehension.</p><p>The cross-metric synthesis points to a practical method and configuration for generating AI lesson plans that are both linguistically accessible and scientifically accurate and complete. A readability-optimized AI model, like DeepSeek, paired with the prompt framework RACE and an explicit "execute" checklist produces outputs that require less manual remediation. The checklist should include specific lesson content, explicit references to curriculum material, and inclusion of at least one Analyze-level and one Create-level objective with aligned assessments.</p><p>Several limitations qualify the interpretation of these findings. The dataset covers a single topic within high-school physics, so external validity to other topics and disciplines is not guaranteed. The sample size of fifteen plans limits the precision of dispersion estimates at both the model and framework levels. The alignment metric measures semantic similarity to NGSS curriculum descriptors rather than formal standards mapping. Finally, output quality can drift as models are updated over time, which means the relative performance observed in this study may evolve when repeated. Despite these limitations, the convergence of results across independent metrics strengthens the trends observed. Readability differences are large and consistent enough to shape conclusions, framework effects on factual completeness are stable across models, and the low baseline for higher-order objectives is significantly evident in both the verb distributions and the cognitive demand indices. These convergences provide an actionable foundation for educators and instructional designers seeking to operationalize generative AI in planning workflows.</p><p>Future research can extend this work along several axes. Expanding to multiple physics topics and to other subjects would test the robustness of the model. Incorporating classroom-based outcomes such as student learning gains, time-on-task, and teacher editing time would connect automated metrics to instructional impact. Finally, incorporating analyses of accessibility features-such as multilingual adaptability, accommodations for learners with disabilities, and the inclusion of culturally responsive examples would expand the notion of usability beyond readability and factual accuracy, extending this research into a broader pedagogical domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>This study conducted a multi-model, multi-framework evaluation of AI-generated lesson plans for The Electromagnetic Spectrum in high-school physics using four automated metrics. The evidence indicates that model selection primarily determines readability. The prompt framework structure primarily determines factual completeness and incidental curricular resemblance. Across all conditions, objectives concentrated at the lower tiers of Bloom's taxonomy.</p><p>The practical implication is clear. Educators seeking immediately usable plans should combine a readability-friendly model with a prompt framework such as RACE and supply a detailed execution checklist that specifies essential physics anchors, curriculum material, and at least one Analyze and one Create objective tied to aligned activities and assessments. This configuration minimizes post-editing, improves scientific completeness, and pushes objectives beyond recall and comprehension into higher cognitive levels.</p><p>The study's scope is limited to one topic and a modest number of plans, and its alignment and hallucination metrics are proxies rather than exhaustive audits. Even with these constraints, the patterns are clear enough to guide near-term practice and to inform future experimental designs. By pairing the right model with the right framework and by making non-negotiable pedagogical requirements explicit, AI-assisted lesson planning can produce outputs that are both classroom-ready and pedagogically sound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Declaration of generative AI and AI-assisted technologies in the manuscript preparation process</head><p>During the preparation of this work the author used ChatGPT in order to refine Python code snippets for readability indices, verb extraction, and similarity analysis and used ChatGPT, DeepSeek, Grok, Gemini, and Claude to generate lesson plans for the aim of the study. After using this tool/service, the author reviewed and edited the content as needed and take(s) full responsibility for the content of the published article. <ref type="bibr">Anderson, L. W., &amp; Krathwohl, D. R. (2001)</ref>. A taxonomy for learning, teaching, and assessing: A revision of Bloom's taxonomy of educational objectives. Longman.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">References</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Readability and Hallucination Metrics Averaged by Framework</figDesc><table><row><cell></cell><cell>TTR</cell><cell>FRE</cell><cell>FKGL</cell><cell>FOG</cell><cell>HI</cell></row><row><cell>COSTAR</cell><cell>0.43</cell><cell>38.06</cell><cell>12.43</cell><cell>15.53</cell><cell>2.90</cell></row><row><cell>RACE</cell><cell>0.48</cell><cell>34.08</cell><cell>12.76</cell><cell>15.89</cell><cell>2.20</cell></row><row><cell>TAG</cell><cell>0.45</cell><cell>35.37</cell><cell>13.05</cell><cell>16.06</cell><cell>2.30</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Cognitive Metrics Averaged by Framework</figDesc><table><row><cell>Model</cell><cell>TTR</cell><cell>FRE</cell><cell>FKGL</cell><cell>FOG</cell><cell>HI</cell></row><row><cell>ChatGPT</cell><cell>0.46</cell><cell>41.73</cell><cell>10.86</cell><cell>14.04</cell><cell>2.50</cell></row><row><cell>Claude</cell><cell>0.47</cell><cell>9.89</cell><cell>19.89</cell><cell>23.29</cell><cell>2.67</cell></row><row><cell>DeepSeek</cell><cell>0.43</cell><cell>54.65</cell><cell>8.64</cell><cell>11.47</cell><cell>2.83</cell></row><row><cell>Gemini</cell><cell>0.45</cell><cell>43.44</cell><cell>11.03</cell><cell>14.20</cell><cell>2.33</cell></row><row><cell>Grok</cell><cell>0.47</cell><cell>29.47</cell><cell>13.32</cell><cell>16.14</cell><cell>2.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Readability and Hallucination Metrics Averaged by Model</figDesc><table><row><cell>Model</cell><cell>HS-PS4-1</cell><cell>HS-PS4-3</cell><cell>HS-PS4-4</cell><cell>HS-PS4-5</cell><cell>NGSS Mean</cell></row><row><cell>ChatGPT</cell><cell>0.130</cell><cell>0.042</cell><cell>0.043</cell><cell>0.038</cell><cell>0.063</cell></row><row><cell>Claude</cell><cell>0.190</cell><cell>0.084</cell><cell>0.107</cell><cell>0.058</cell><cell>0.110</cell></row><row><cell>DeepSeek</cell><cell>0.115</cell><cell>0.044</cell><cell>0.049</cell><cell>0.039</cell><cell>0.061</cell></row><row><cell>Gemini</cell><cell>0.142</cell><cell>0.033</cell><cell>0.043</cell><cell>0.027</cell><cell>0.061</cell></row><row><cell>Grok</cell><cell>0.183</cell><cell>0.091</cell><cell>0.052</cell><cell>0.043</cell><cell>0.092</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Curriculum Alignment Metrics Averaged by Model</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Cognitive Metrics Averaged by Model</figDesc><table /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bloom -Remember</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bloom -Understand</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bloom -Apply</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bloom -Analyze</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bloom</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The Content Analysis of the Lesson Plans Created by ChatGPT and Google Gemini</title>
		<author>
			<persName><forename type="first">A</forename><surname>Baytak</surname></persName>
		</author>
		<idno type="DOI">10.46303/ressat.2024.19</idno>
		<ptr target="https://doi.org/10.46303/ressat.2024.19" />
	</analytic>
	<monogr>
		<title level="j">Research in Social Sciences and Technology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="329" to="350" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">ChatGPT in foreign language lesson plan creation: Trends, variability, and historical biases</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dornburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Davin</surname></persName>
		</author>
		<idno type="DOI">10.1017/S0958344024000272</idno>
	</analytic>
	<monogr>
		<title level="j">ReCALL</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="332" to="347" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">To what extent is ChatGPT useful for language teacher lesson plan creation?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dornburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Davin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.09974</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.01102</idno>
		<title level="m">LessonPlanner: Assisting Novice Teachers to Prepare Pedagogy-Driven Lesson Plans with Large Language Models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A new readability yardstick</title>
		<author>
			<persName><forename type="first">R</forename><surname>Flesch</surname></persName>
		</author>
		<idno type="DOI">10.1037/h0057532</idno>
		<ptr target="https://doi.org/10.1037/h0057532" />
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Psychology</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="221" to="233" />
			<date type="published" when="1948">1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><surname>Hs-Ps</surname></persName>
		</author>
		<ptr target="https://www.nextgenscience.org/dci-arrangement/hs-ps4-waves-and-their-applications-technologies-information-transfer" />
		<title level="m">Technologies for Information Transfer | Next Generation Science Standards</title>
		<imprint>
			<date type="published" when="2025-10-11">October 11, 2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Evaluating the Efficacy of AI-Generated Inquiry-Based Lesson Plans in Science</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kahraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kƒ±yƒ±cƒ±</surname></persName>
		</author>
		<idno type="DOI">10.19126/suje.1463067</idno>
		<ptr target="https://doi.org/10.19126/suje.1463067" />
	</analytic>
	<monogr>
		<title level="j">Sakarya University Journal of Education</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="40" to="53" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Are Lesson Plans Created by ChatGPT More Effective? An Experimental Study</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">ƒ∞</forename><surname>G√∂ksu</surname></persName>
		</author>
		<idno type="DOI">10.46328/ijte.607</idno>
		<ptr target="https://doi.org/10.46328/ijte.607" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Technology in Education</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="107" to="127" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.01674</idno>
		<title level="m">Using ChatGPT for Science Learning: A Study on Pre-service Teachers&apos; Lesson Planning</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Opportunities and risks involved in using ChatGPT to create first grade science lesson plans</title>
		<author>
			<persName><forename type="first">W</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Courchesne</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0305337</idno>
		<ptr target="https://doi.org/10.1371/journal.pone.0305337" />
	</analytic>
	<monogr>
		<title level="j">PloS One</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="305337" to="e305337" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Test-Retest and Inter-Analyst Reliability of the Automated Readability Index, Flesch Reading Ease Score, and the Fog Count</title>
		<author>
			<persName><forename type="first">G</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Kincaid</surname></persName>
		</author>
		<idno type="DOI">10.1080/10862967509547131</idno>
		<ptr target="https://doi.org/10.1080/10862967509547131" />
	</analytic>
	<monogr>
		<title level="j">Journal of Reading Behavior</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="149" to="154" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName><surname>Appendix</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
