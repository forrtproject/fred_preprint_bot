<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">New Evidence and Design Considerations for Repeated Measure Experiments in Survey Research</title>
				<funder>
					<orgName type="full">Time-sharing Experiments for the Social Sciences</orgName>
					<orgName type="abbreviated">TESS</orgName>
				</funder>
				<funder ref="#_rgxvGdz #_ZSucUkr">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_PQ3bXa8">
					<orgName type="full">Bass Connections at Duke University</orgName>
				</funder>
				<funder>
					<orgName type="full">Rapoport Family Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2025-10-15">October 15, 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Diana</forename><surname>Jordan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Trent</forename><surname>Ollerenshaw</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Andrew</forename><surname>Trexler</surname></persName>
							<email>atrexler@wisc.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Houston</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Wisconsin-Madison</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">New Evidence and Design Considerations for Repeated Measure Experiments in Survey Research</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-10-15">October 15, 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">03274185B214416F18143FDA936DA995</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-10-21T14:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We re-examine recent influential claims that repeated measure experimental designs do not introduce bias and offer large precision gains in survey research (Clifford, Sheagley, and Piston 2021). We test these claims by experimentally varying the design of six classic political science experiments across three distinct large samples of U.S. adults (total N = 13, 163). In contrast to the original study, we observe consistent attenuation of treatment effects in repeated measure designs. However, this average design effect is small enough, and the precision gains large enough, that we largely affirm the recommendation to employ repeated measure designs in many practical research applications. We additionally extend the literature on repeated measure designs by exploring how several design considerations affect the bias-precision trade-off, such as the use of within-subject versus between-groups designs, the relative separation of repeated measures within single surveys, and differences in respondent characteristics across sample types.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Political scientists increasingly leverage randomized experiments to estimate causal effects in human subjects research, particularly through surveys. A common experimental design, the between-groups "post-only" design, randomly assigns participants to treatment conditions and measures the outcome variable(s) only post-treatment. The average treatment effect (ATE) is estimated by comparing the outcome means across groups. However, this predominant design suffers from low precision when estimating treatment effects and may therefore miss small or heterogeneous treatment effects <ref type="bibr" target="#b42">(Mutz 2011)</ref>, while also risking substantial overestimates of effects <ref type="bibr" target="#b19">(Gelman and Carlin 2014;</ref><ref type="bibr" target="#b38">Loken and Gelman 2017)</ref>. Given the increasing evidence that low precision contributes to low replicability rates in social science research <ref type="bibr" target="#b1">(Arel-Bundock et al. 2022;</ref><ref type="bibr" target="#b19">Gelman and Carlin 2014)</ref>, improving experimental design is essential for advancing research in political science and related disciplines.</p><p>A common alternative experimental design is the "repeated measure" design, which measures outcomes both pre-and post-treatment. By measuring respondents' pre-treatment outcome levels, repeated measure designs can substantially increase precision in ATE estimates.</p><p>Yet researchers have often been reluctant to implement repeated measure designs, especially within the same survey, due to concerns that pre-treatment measurement of outcomes may inadvertently bias ATE estimates by priming respondents to the treatment, inducing pressure to provide consistent responses or creating demand incentives. Lacking clear evidence about the degree of bias introduced versus precision gained, researchers have historically opted against repeated measure designs.</p><p>However, a recent influential study by <ref type="bibr">Clifford, Sheagley, and Piston (2021, referred to as CSP hereafter)</ref> in the American Political Science Review experimentally manipulates design type, providing evidence that repeated measure designs enhance precision without biasing ATE estimates. CSP conclude that traditional concerns about repeated measure designs can be largely dismissed, recommending "that researchers use pre-post and within-subject designs whenever possible" <ref type="bibr">(Clifford, Sheagley, and</ref><ref type="bibr">Piston 2021, 1062)</ref>. These recommendations have gained traction in the social sciences: in the first four years since publication, 1 CSP was cited by 118 peer-reviewed studies, of which 83 cite CSP specifically to justify using repeated measure designs.</p><p>The rapid adoption of repeated measure designs speaks to the importance of CSP's findings. Yet CSP's conclusions rest on just six experiments-a valuable but ultimately limited basis for such a broad shift in survey experimental practice, and thus one that merits large-scale replication. Further, researchers lack information on key design considerations that could impact the utility of repeated measure designs in some settings. For example, it remains unclear whether these designs are suitable for short surveys. CSP placed their pre-and post-treatment measures far apart, reflecting the intuition that placing them close together might increase bias by making the repetition more apparent. In this and other respects, best practices for implementing repeated measure designs remain underdeveloped.</p><p>In a large-scale replication and extension, we substantially expand the available evidence on repeated measure designs and address three key knowledge gaps. First, we assess the suitability of repeated measure designs for between-groups versus within-subject experiments.</p><p>Second, we analyze how the proximity between repeated measures alters design effects, offering insights on the suitability of repeated measures designs when pre-and post-treatment measures are placed close together. Third, we conduct experiments on both probabilityand non-probability-based samples with diverse respondent pools to assess how respondent characteristics like professionalization and attentiveness affect the bias-precision trade-off.</p><p>We experimentally manipulate the design of six published political science experiments, including three within-subject experiments and three between-groups experiments to allow for comparison across experiment types. We randomly vary the proximity of repeated measures in these experiments to evaluate how this design consideration affects bias and precision. We field all six experiments in omnibus surveys on three distinct online samples of U.S. adults (N j = 18 studies, N i = 13, 163 respondents, N ij = 78, 978 total observations).</p><p>These include a sample from the probability-based AmeriSpeak panel maintained by NORC (n i = 4, 033) and two non-probability samples (Lucid n i = 4, 869, Prolific n i = 4, 261). These large samples provide excellent statistical power to detect small design effects and assess moderators.</p><p>Contrary to CSP's original findings, we observe a small but consistent attenuation of treatment effects in repeated measures designs relative to post-only designs. Despite this design effect, our findings largely affirm CSP's case for repeated measure designs, as the substantial precision gains often outweigh the weak attenuation in treatment effects to produce (in expectation) more accurate ATE estimates in many practical applications. Further, we provide robust evidence that repeated measure designs are suitable for both within-subject and between-groups experiments, across probability and non-probability samples with varying levels of respondent professionalization and attention, and in surveys where repeated measures must necessarily appear in close proximity. That said, we also find some evidence that asking attitude-recall questions or fielding multiple repeated measure experiments in one survey may exacerbate later design effects. In sum, while we identify some circumstances where well-powered post-only designs may be preferable, our findings reinforce the field's nascent shift toward repeated measures designs and the enhanced precision they offer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Repeated Measure Designs in the Social Sciences</head><p>Survey experiments are widely used for social inquiry, with the "post-only," betweengroups design being the most common in political science <ref type="bibr" target="#b10">(Clifford, Sheagley, and Piston 2021)</ref>. In this design, participants are randomly assigned and exposed to treatment or control stimuli, then outcomes are measured post-treatment and compared across conditions, with differences between the treatment groups' outcomes interpreted as the average treatment effect (ATE). Under a set of relatively weak assumptions-successful randomization, the stable unit treatment value assumption (SUTVA), no differential attrition-the post-only design provides unbiased estimates of the ATE.</p><p>A major downside of post-only designs is that the treatment effect is often imprecisely estimated. Treatment interventions in the social sciences typically explain only a small fraction of the variation in the outcome variable. Post-only designs therefore often have large residual errors, reducing statistical power-a critical consideration for experimental design <ref type="bibr" target="#b46">(Rainey 2025)</ref>. Statistical power (β) is the probability that a test rejects its null hypothesis in favor of a specified alternative hypothesis if it is true, a common goal of experiments.</p><p>Power for a two-tailed test of a treatment effect (τ ) can be expressed as</p><formula xml:id="formula_0">β = 1 -Φ cdf [Φ -1 std (1 - α 2 ) • SE τ ; µ = τ, σ = SE τ ] [1]</formula><p>where Φ cdf is the cumulative density function of a normal distribution, Φ -1 std is the inverse of the standard normal distribution, α is the chosen significance threshold of the test (for example, 0.05), and SE τ is the standard error of the treatment effect τ , whose sampling distribution is assumed to be a normally distributed variable with a mean of µ = τ and standard deviation of σ = SE τ . With an ordinary least squares (OLS) estimator, a standard approach for hypothesis testing in survey experiments, the standard error of the ATE (SE τ ) is estimated as</p><formula xml:id="formula_1">SE τ = σ N i=1 (D i -D) 2 [2]</formula><p>where D i , an indicator for assignment to treatment, and the root mean squared error σ is an asymptotic function of the residual errors ûi and the sample size N :</p><formula xml:id="formula_2">σ ≈ N i=1 ûi 2 N [3]</formula><p>The large residuals common to post-only designs thus reduce statistical power by producing large standard errors around the treatment effect, expanding the confidence interval around τ and reducing the probability that this interval excludes the null value of the parameter of interest (e.g., τ = 0), thus reducing the likelihood the null hypothesis can be rejected.</p><p>Post-only designs therefore require large samples to reliably detect and precisely estimate treatment effects <ref type="bibr" target="#b44">(Peters 2017)</ref>.</p><p>Imprecision has myriad negative effects on scientific knowledge production. Imprecise studies risk failing to detect small treatment effects and variations in effects <ref type="bibr" target="#b42">(Mutz 2011</ref>) and may overestimate effect sizes <ref type="bibr" target="#b19">(Gelman and Carlin 2014;</ref><ref type="bibr" target="#b38">Loken and Gelman 2017)</ref>. Structural incentives to publish "positive" findings meeting conventional significance thresholds can lead to published experiments with noisy data and brittle evidence propping up theories <ref type="bibr" target="#b20">(Gerber, Green, and Nickerson 2001;</ref><ref type="bibr" target="#b33">Kühberger, Fritz, and Scherndl 2014)</ref>. Statistical imprecision and underpowered experiments are increasingly recognized as major contributors to low replicability rates in social science research <ref type="bibr" target="#b1">(Arel-Bundock et al. 2022;</ref><ref type="bibr" target="#b19">Gelman and Carlin 2014)</ref>. Increasing precision in survey experiments is vital to enhancing the credibility of empirical social science.</p><p>Repeated measure designs offer improvements over post-only designs in terms of precision and power. Researchers have long recognized that the standard errors of estimated treatment effects can be reduced by adjusting for pre-treatment covariates, as this reduces the residual errors ûi by accounting for some additional variation in the outcome variable. This accordingly reduces SE τ by approximately (1 -ρ 2 ) <ref type="bibr" target="#b14">(Cox and McCullagh 1982;</ref><ref type="bibr" target="#b5">Bloom 1995)</ref>, where ρ is the correlation between the outcome variable Ŷi and the pre-treatment covariate Xi -meaning that the stronger the correlation, the greater the reduction in SE τ .</p><p>Statistical power thus improves to:</p><formula xml:id="formula_3">β ≈ 1 -Φ cdf [(Φ -1 std (1 - α 2 ) • (1 -ρ 2 ) • SE τ ; µ = τ, σ = SE τ ] [4]</formula><p>The logic of repeated measure designs is that the pre-treatment covariate most likely to strongly correlate with the post-treatment outcome is an identical pre-treatment outcome measure. This design therefore measures outcomes both before and after exposure to treatment. By adjusting for respondents' pre-treatment outcome levels, this approach greatly enhances the precision of treatment effect estimates, substantially reducing the sample size required to achieve conventional levels of statistical power. Repeated measures designs come in two main types: between-groups, where respondents are randomized to either treatment or control stimuli, and within-subject, where all respondents receive both stimuli <ref type="bibr" target="#b37">(List 2025;</ref><ref type="bibr" target="#b10">Clifford, Sheagley, and Piston 2021)</ref>.</p><p>Despite these advantages, researchers often worry that repeated measure designs may bias the ATE estimate. Repeated measure designs require an additional assumption beyond those in post-only designs: that pre-treatment measurement does not itself influence posttreatment outcomes differentially across treatment arms. Three concerns cast doubt on that assumption: priming, where pre-treatment measurement may lead respondents to focus on specific considerations (e.g., <ref type="bibr" target="#b30">Klar, Leeper, and Robison 2020)</ref>; consistency pressures, where respondents may feel pressure to provide post-treatment responses that align with their pre-treatment responses (e.g., <ref type="bibr" target="#b9">Cialdini, Trost, and Newsom 1995;</ref><ref type="bibr" target="#b52">Tourangeau and Rasinski 1988)</ref>; and demand effects, where respondents may adjust their post-treatment responses based on their perception of the study's purpose (e.g., Charness, Gneezy, and Kuhn 2012;</p><p>Zizzo 2010; but see <ref type="bibr" target="#b41">Mummolo and Peterson 2019)</ref>.</p><p>Conventional wisdom thus suggests a trade-off between bias and precision when considering post-only or repeated measure designs. In practice, political science survey experiments have typically prioritized minimizing bias over addressing imprecision, defaulting to postonly designs <ref type="bibr" target="#b10">(Clifford, Sheagley, and Piston 2021)</ref>. To our knowledge, however, CSP is the only study to date that empirically tests the bias-precision trade-off for repeated measure designs. Their internal meta-analysis of six experiments found no significant differences in estimated ATEs between the two designs, but found that repeated measures designs substantially improve precision, allowing researchers to achieve more power with fewer participants.</p><p>For instance, a 1,000 respondent two-arm post-only experiment has roughly 80 percent power to detect a treatment effect of 0.20 standard deviations, but a repeated measure design can achieve the same power with about 200 to 600 respondents, depending on the strength of the correlation between pre-and post-treatment measures. Given these precision gains and minimal evidence of bias, CSP argue that there is no meaningful bias-precision trade-off and recommend that researchers employ repeated measure designs as the default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contribution and Hypotheses</head><p>As of April 2025, CSP (2021) has been cited in 118 peer-reviewed studies, of which 83 are original studies citing CSP to justify a repeated measure design (see Appendix CSP provides a valuable, overdue examination of the bias-precision trade-off in repeated measure designs. However, the empirical literature on this design remains limited. CSP's analysis is only well-powered to rule out large design effects-on the order of altering the ATE by 40 percent or more <ref type="bibr">(Huber and Graham FC)</ref>. Moderate design effects, which could be substantively meaningful in many contexts, require larger samples to detect.</p><p>Additionally, key questions about best practices for repeated measure designs remain unanswered. First, 41 percent of studies citing CSP used within-subject repeated measure designs rather than between-groups designs (see Appendix Table <ref type="table" target="#tab_0">A</ref>.5.1), yet just one of CSP's experiments employed a within-subject design-a rather limited basis for a large shift in research practice. Because within-subject designs assign all participants to treatment (either before or after the control), they may alter the scope of consistency, demand, or priming effects relative to between-groups designs-meaning that design effects may differ in magnitude across the two approaches.</p><p>Our study substantially expands the evidence on design effects under within-subject designs by replicating three within-subject experiments in each of three samples, totaling nine studies with a meta-analytic sample over 43 times larger than the single study analyzed by CSP. Simultaneously, we replicate three of CSP's between-groups experiments on the same samples to further expand the evidence base for between-groups repeated measure designs. This allows us to rigorously test the preregistered 1 hypothesis that:</p><p>H1: Repeated-measure experimental designs do not bias estimated ATEs in either (a)</p><p>between-group experiments or (b) within-subject experiments.</p><p>Second, we are not aware of any study to date that assesses how the proximity between repeated measures affects bias and precision. An intuitive hypothesis is that increasing the distance (i.e., adding more survey content) between repeated measures could reduce bias by mitigating priming, obscuring researcher intent, and enabling respondents to "forget" their pre-treatment responses, reducing pressure (or ability) to respond consistently. Indeed, given this intuition, many repeated measure experiments employ multi-wave panel surveys, allowing days or weeks of separation between measures. In single surveys, researchers (including CSP) commonly place their pre-and post-treatment questions at opposite ends of the survey. However, experimenters frequently work with very short surveys (or short modules in omnibus surveys), facing resource or logistical constraints that may require placing repeated measures close together. Further, close proximity may even be advantageous if it reduces random noise, strengthening the correlation between the repeated measures and increasing precision. We manipulate proximity between repeated measures in our surveys to test the following preregistered hypothesis:</p><p>H2: Repeated-measure experimental designs increase bias in estimated ATEs when measures are repeated measures are presented close together.</p><p>Third, CSP's six experiments used two student and four online non-probability samples.</p><p>While their findings are important given the reliance on such convenience samples in experimental research <ref type="bibr" target="#b28">(Jerit and Barabas 2023;</ref><ref type="bibr" target="#b32">Krupnikov and Levine 2014)</ref>, it is unclear if the null design effect CSP observe is due to sampling design. Student samples differ from older adults on a variety of attitudinal and behavioral dimensions <ref type="bibr" target="#b48">(Sears 1986)</ref>. Probability-based sampling designs recruit respondents that are not only more representative of the target 1 Anonymized preregistration materials are available here.</p><p>population, but also less professionalized and more attentive than opt-in non-probability panelists <ref type="bibr" target="#b29">(Kennedy et al. 2016;</ref><ref type="bibr" target="#b39">MacInnis et al. 2018)</ref>.</p><p>Differences in respondent characteristics may affect the relative strength of priming, consistency, or demand effects in repeated measure designs. For example, one of CSP's experiments (N = 965 students) revealed that many respondents whose responses changed pre-post also self-reported that their attitudes stayed the same. This may result from the unobtrusiveness of repeated measures-affirming their utility-but respondent inattentiveness may also play a role. Attentive respondents may be more likely to recognize repeated questions and alter their post-treatment responses accordingly. Similarly, professionalized respondents may be accustomed to experiments and react differently to repeated measures than less professionalized respondents. We explore these possibilities by fielding identical experiments on both probability and non-probability samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data and Methods</head><p>We replicate six previously published survey experiments, summarized in ) that asks about support for spending on "welfare" or "assistance to the poor." We expect support to be higher when spending is described as assistance to the poor relative to welfare. In Study 5, we replicate a classic study on affirmative action from <ref type="bibr" target="#b57">Wilson et al. (2008)</ref> which asks about support for affirmative action for women or for racial minorities. We expect support to be higher when the policy is aimed at women relative to racial minorities. In Study 6, we replicate a study from de Benedictis-Kessner and Hankinson (2019) on support for opening a new methadone clinic to address opioid addiction, in which the proposed clinic would be nearby (a quarter mile away) or further away (two miles away) from where the respondent lives. We expect that support will be higher in the latter condition. We thus define treatment and control (somewhat arbitrarily) such that the relevant ATE in each study is expected to be positive, to facilitate comparison across all six experiments.</p><p>These six studies were selected for their relative brevity (no more than three questions each), allowing us field more studies in a single survey, and because each found moderate to large treatment effects in the original studies, improving our ability to estimate design effects. <ref type="foot" target="#foot_1">3</ref> We purposively selected replication studies to cover diverse topics and treatments (e.g., informational treatments, party cues, framing effects) to provide breadth across areas of substantive inquiry <ref type="bibr" target="#b12">(Clifford, Leeper, and Rainey 2024;</ref><ref type="bibr" target="#b11">Clifford and Rainey 2025)</ref>. Four of our studies also appear in CSP's original paper<ref type="foot" target="#foot_2">4</ref> and we supplement these with two  <ref type="bibr">Hankinson (2019, denoted Study 6)</ref> to increase the number of within-subject studies. We thus have three between-groups and three within-subject repeated measure designs for comparison against otherwise equivalent post-only designs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Design</head><p>We fielded all six studies on three omnibus surveys and manipulated the experimental designs in a preregistered multi-stage randomization procedure (detailed in Appendix B.3).</p><p>All respondents in each sample (combined N i = 13, 163 respondents) completed all six experiments (combined N ij = 78, 978 observations). Our randomization procedure independently assigned design conditions (post-only or repeated measures), treatment conditions (treatment or control stimuli), and the order of the experimental content for each respondent.</p><p>Of note, we hypothesized that design effects might be more pronounced when repeated measures are close together because respondents may be more likely to remember answering the same or similar questions, strengthening any priming, consistency, or demand effects. To test for this potential heterogeneity, our procedure has three noteworthy design features.</p><p>First, respondents were randomly assigned to four repeated measure designs and two post-only designs. The relatively larger share of repeated measure designs increases power for testing differences in design effects at various "distances" between repeated measures (defined as the number 5 of survey items separating the repeated measures). Second, we randomly assigned approximately half of respondents to complete two repeated measure experiments in very close succession, with just 0-3 units of separation between measures. This increases statistical power where we suspected we might find non-linear changes in design effectsi.e., when repeated measures are very close together-while still allowing for comparisons across distances by independently randomizing the content that could appear in between any given pair of repeated measures. Third, we included six wholly unrelated questions about the National Football League (NFL), randomized alongside the main content, to extend the right tail of the "distance" distribution and provide additional distractor items that could appear between repeated measures. 6 The realized distribution of distances is shown in Figure <ref type="figure" target="#fig_0">1</ref> (see Appendix B.3 for details and illustrative examples). 7</p><p>Our randomization procedure thus provides us with unbiased estimates of design effects while maximizing statistical power where we expected (a priori) that it would matter mostthat is, where repeated measures appear very close together in the survey. By comparing point estimates for the ATE under the post-only versus repeated measure design, we can identify design effects (i.e., bias) introduced from repeated measure designs (addressing H1).</p><p>We can also identify precision gained from repeated measure designs by comparing standard</p><p>A slight caveat: we follow the Time-sharing Experiments for the Social Sciences (TESS) guidelines for evaluating the "length" of survey items, such that our four longest items each count as two units of distance. Specifically, two items each for the GMO and opioid experiments include longer paragraphs that precede the outcome question, and each therefore counts as two units (paragraph and question). Appendix B.4 provides the exact value of "TESS units" assigned to each item. In our surveys, each repeated measure experiment was separated by between 0 and 20 TESS units of distance. This variable is highly correlated with the amount of time elapsed (measured in milliseconds) between repeated measures. Excluding outlier observations in which the elapsed time between measures is more than thrice the median completion time for the entire survey, these correlations range from 0.68 to 0.74 across samples. For this purpose, NORC bundled our AmeriSpeak survey with six questions about the NFL fielded by an uninvolved researcher. We maintained these NFL items in the non-probability samples. See B.4 for details.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> shows the pooled distances; the distributions are very similar for each sample and experiment. errors for the post-only and repeated measure designs (using bootstrapped regressions with equivalent sample sizes to account for the 2:1 oversampling of repeated measure designs).</p><p>And by oversampling scenarios in which repeated measures appear in close proximity, we can test whether this proximity moderates design effects (addressing H2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sampling Approach</head><p>We fielded our experiments on three samples with concurrent omnibus surveys from June 27 th through July 15 th , 2024. Building on CSP's original studies, which drew samples from undergraduate pools or opt-in online panels, we obtained one sample from a probabilitybased online panel (NORC's AmeriSpeak panel) in addition to two non-probability samples recruited via quota sampling on Prolific and Lucid. These vendors are often used for po-litical science research and offer substantial diversity in terms of respondent professionalization, respondent attentiveness, and sample representativeness on observables <ref type="bibr" target="#b51">(Stagnaro et al. 2024)</ref>. Table <ref type="table" target="#tab_3">2</ref> summarizes key information for each sample; for further information, see Appendix B. </p><formula xml:id="formula_4">(N i = 4,029) Probability 6.1 min 2 1 2 Prolific (N i = 4,261)</formula><p>Non-probability 7.2 min 30 2</p><p>3 Lucid</p><formula xml:id="formula_5">(N i = 4,869)</formula><p>Non-probability 7.3 min 15 4</p><p>Two key respondent characteristics vary across our three samples. The first is respondent professionalization, which refers to survey respondents' familiarity with and frequency of survey-taking. Most Americans take few surveys regularly; however, a small minority of Americans take many surveys frequently for income or entertainment <ref type="bibr" target="#b24">(Hillygus, Jackson, and Young 2014)</ref>. These professionalized respondents constitute an out-sized share of non-probability panels like Prolific and Lucid because high-propensity respondents can voluntarily opt into such panels and take surveys on demand. In contrast, members of probability-based panels like AmeriSpeak can only join if randomly sampled, and organizations that manage such panels invite panelists to take surveys relatively infrequently.</p><p>Indeed, our AmeriSpeak respondents are much less professionalized than our Prolific and Lucid respondents in terms of the number of recent surveys taken and unique survey panel memberships (Table <ref type="table" target="#tab_3">2</ref>).</p><p>Professionalization may cause respondents to react differently to repeated measures, though in what direction remains uncertain. On the one hand, professionalized respondents may be inured to peculiarities of survey experiments like repeated measures, dampening design effects. Alternatively, professionalized respondents may be more likely to recognize that questions before and after experimental stimuli are testing for opinion change, heightening demand effects or consistency pressures. How respondent professionalization influences design effects is theoretically unclear and empirically untested.</p><p>The second relevant dimension is response quality, defined here as respondent attention and effort. A perennial issue in survey research is that respondents do not always pay close attention or put much effort into their responses, introducing statistical noise and possibly bias <ref type="bibr" target="#b3">(Berinsky et al. 2021)</ref>. Response quality issues are acute in self-administered surveys where there is no interviewer to induce attention and effort (e.g., <ref type="bibr" target="#b6">Cannell, Miller, and Oksenberg 1981;</ref><ref type="bibr" target="#b7">Chang and Krosnick 2009;</ref><ref type="bibr" target="#b35">Lerner and Tetlock 1999)</ref>. Because online surveys typically provide monetary incentives, some participants engage in extreme satisficing or speeding to maximize hourly earnings <ref type="bibr" target="#b23">(Hillygus and LaChapelle 2022)</ref>, and may use generative AI and other automated tools to do so <ref type="bibr">(Veselovsky et al. 2023;</ref><ref type="bibr">Veselovsky, Ribeiro, and West 2023)</ref>. In repeated measure designs, less attentive and effortful respondents may still be subject to issues like priming, consistency, and demand effects, but their disengagement might reduce the likelihood or strength of these biases.</p><p>To address response quality, some vendors engage in extensive panel management, such as requiring panelists to pass quality filters (e.g., consistency checks, attention checks), while other vendors leave quality control to researchers. Consequently, non-probability samples can vary considerably in respondent attention and effort; some recent evidence suggests that Lucid performs relatively poorly and Prolific performs relatively well on these metrics <ref type="bibr" target="#b51">(Stagnaro et al. 2024</ref>). On our Prolific and Lucid surveys, we included six preregistered quality checks (see Appendix B) and drop respondents that failed at least two from our main analyses. 8 Prolific respondents failed 0.115 checks on average; this falls to 0.081 in the analysis sample after we exclude 38 respondents who failed at least two (as preregistered).</p><p>Lucid respondents failed an average of 0.684 checks, which falls to 0.279 after we exclude 681 who failed at least two. Lucid respondents are thus less attentive and effortful than Prolific respondents on average, variation that we exploit to test whether these characteristics affect the performance of repeated measure designs.</p><p>In summary, our study expands and advances the evidentiary basis for repeated measure designs. We replicate four studies from CSP and two additional within-subject experiments from the political science literature in each of three large samples to test whether repeated measure designs introduce design effects (i.e., attenuation or exaggeration of the ATE),</p><p>totaling 18 studies with a combined N ij = 78, 978. This represents a nearly tenfold increase over CSP's pooled samples. Our large samples not only provide power to detect small design effects, but also enable us to test for potential heterogeneity in design effects across several critical design considerations: experiment type (between-groups or within-subject), proximity of repeated measures, and vendor sampling designs and consequent respondent characteristics. Our study thus provides both well-powered tests and novel insights into how various design considerations affect the utility of repeated measure experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We first summarize the results of each experiment under each design and report the estimated design effect. Next, we report our overall findings on the design effect of repeated measures through a series of internal meta-analyses of the 18 experiments. We then test for potential heterogeneity in design effects along several key dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary of Experimental Results</head><p>For each experiment, we report the observed ATE for both post-only and repeated measures designs. To facilitate comparison across experiments, we rescale all outcome variables to range from 0 (most opposed) and 1 (most supportive). For the between-groups exper-iments (Studies 1-3) we compare the difference in ATEs by estimating separate ordinary least squares (OLS) regressions for each design. These regressions model the post-treatment outcome as a function of a binary treatment indicator,<ref type="foot" target="#foot_5">9</ref> with the pre-treatment outcome included as a covariate in the repeated measures design. <ref type="foot" target="#foot_6">10</ref> We then combine these regressions via seemingly unrelated regression estimation, which allows us to conduct a linear combination test for equivalence of ATEs across the two designs.</p><p>For the within-subject experiments (Studies 4-6), we compare the difference in ATEs using random effects models. These models regress the dependent variable on an indicator for treatment interacted with an indicator for repeated measure assignment. This approach explicitly acknowledges the nested nature of the data by clustering standard errors at the respondent level (as some respondents contribute two observations), capturing individuallevel variation in the outcome (the "random" effects) that is unrelated to the explanatory variables. The coefficient on the interaction term estimates the difference in ATEs between the designs.</p><p>As preregistered, we follow prior authors' inclusion of specific pre-treatment covariates (e.g., partisanship, ideology) in the model for each experiment, as noted below. We report the results of each experiment separately for each of the three samples (AmeriSpeak, Prolific, and Lucid). A summary of the results is provided in Table <ref type="table" target="#tab_4">3</ref>, which we briefly detail below.<ref type="foot" target="#foot_7">11</ref> </p><p>Study 1: Foreign Aid</p><p>In this between-groups experiment, we regress support for foreign aid spending on a treatment indicator for receiving information that foreign aid spending is about 1% of the federal budget. Following CSP, we include partisanship and ideology as covariates. All three samples replicate CSP's finding (and that of Gilens 2001) that the informational treatment increases support for foreign aid in both the post-only and repeated measure designs. As with CSP's study, we find that the repeated measure design attenuates this treatment effect in the Prolific sample (p = 0.002). The design effects are negative but not significant in the AmeriSpeak (p = 0.127) and Lucid (p = 0.630) samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study 2: Prescription Drug Imports</head><p>In this between-groups experiment, we regress support for prescription drug imports on a treatment indicator for receiving a signal about typical party positions, interacted with an indicator for Democratic versus Republican partisanship (excluding pure independents).</p><p>The interaction coefficient provides a measure of polarization in attitudes between parties.</p><p>All three samples replicate CSP's finding that party cues increase attitudinal polarization in both the post-only and repeated measure designs. We find an attenuation of this treatment effect in the repeated measure design in the AmeriSpeak sample (p = 0.017); the estimated design effects are negative but not significant in the Prolific (p = 0.450) and Lucid (p = 0.301) samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study 3: GMOs</head><p>In this between-groups experiment, we regress support for GMOs on an indicator for receiving a pro-GMO frame (versus an anti-GMO frame). Following CSP, we include partisanship and ideology as covariates. All three samples replicate CSP's finding that the positive frame increases support for GMOs in both the post-only and repeated measure designs. We find an attenuation of this treatment effect in the repeated measure design in the AmeriSpeak sample (p = 0.049). The design effects are negative but not significant in the Prolific (p = 0.273) and Lucid (p = 0.210) samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study 4: Anti-poverty</head><p>In this within-subject experiment, we regress support for anti-poverty spending on an indicator for whether these efforts are described as "assistance to the poor" (1) versus "welfare" (0), interacted with an indicator for the two-question repeated measures condition (1)</p><p>versus the single-question post-only condition (0). Following CSP, we include partisanship and ideology as covariates. All three samples replicate CSP's finding (and that of Smith 1987) that support for anti-poverty spending is greater when characterized as "assistance to the poor," in both the post-only and repeated measure designs. We find attenuated treatment effects in the repeated measure designs the AmeriSpeak (p = 0.025), the Prolific sample (p = 0.001), and (at the 0.10 level) the Lucid sample (p = 0.071).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study 5: Affirmative Action</head><p>In this within-subject experiment, we regress support for affirmative action on an indicator for whether the policies apply to women (1) or racial minorities (0), interacted with an indicator for assignment to the repeated measures (1) versus post-only condition (0).</p><p>All three samples replicate the finding of <ref type="bibr" target="#b57">Wilson et al. (2008)</ref> that support is greater for affirmative action for women relative to racial minorities, in both the post-only and repeated measure designs. We find an attenuated treatment effect at the 0.10 level (consistent with the original study's claim that asking both items induces consistency; see <ref type="bibr" target="#b57">Wilson et al. 2008)</ref> in the Prolific sample (p = 0.071). The estimated design effects are negative but not significant in the AmeriSpeak (p = 0.453) and Lucid (p = 0.483) samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study 6: Opioid Clinic</head><p>In this within-subject experiment, we regress support for opening a new methadone clinic on an indicator for whether the proposed clinic is located a quarter mile away (1) versus two miles away (0), interacted with an indicator for assignment to the repeated measures</p><p>(1) versus post-only condition (0). All three samples replicate the finding of de Benedictis-Kessner and Hankinson (2019) that support is greater when the proposed clinic is located further away, in both the post-only and repeated measure designs. In contrast to the other five studies, we find a significant positive design effect (exaggerating the treatment effect) from the repeated measure design in the Prolific sample (p = 0.004), but no significant design effects in the AmeriSpeak (negative estimate, p = 0.481) and Lucid (positive estimate, p = 0.821) samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Repeated Measure Designs Cause (Slight) Attenuation of Treatment Effect Estimates</head><p>Across six experiments replicated thrice each in large samples, nearly every estimated design effect is negative. These design effects are often large, as shown in the final column of Table <ref type="table" target="#tab_4">3</ref>. We observe a median 20.1 percent reduction in the ATE from repeated measure designs relative to post-only designs across the 18 experiments. This consistent pattern suggests repeated measure designs attenuate treatment effects, but that the design effects are too small to reliably detect in individual experiments. We therefore conduct This typical attenuation effect is most consistent for between-groups experiments in our data. While we do not find find a statistically significant design effect for either type of experiment in any one sample,<ref type="foot" target="#foot_9">13</ref> the between-groups estimate across samples is statistically significant (estimate -0.210, p = 0.003). The within-subject estimate across samples is smaller and not statistically significant (estimate -0.149, p = 0.153). This is due to a clear outlier in the Prolific sample, in which we observe a large positive design effect in the opioid clinic experiment. A meta-analysis of the within-subject experiments across samples but excluding this outlier is quite similar to the between-groups experiments (design effect estimate -0.227, p = 0.003).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Repeated Measure Designs Increase Statistical Power</head><p>Although we find evidence of treatment effect attenuation in repeated measure designs, these designs may still be preferable due to large precision gains. Since our experimental design assigns respondents to complete twice as many repeated measure experiments as postonly experiments, directly comparing standard errors would artificially privilege the precision of the repeated measure design, due simply to differential assignment. To address this, we re-estimate each ATE via a bootstrapping procedure that uses samples of identical size for both designs. Specifically, for each experiment in each sample, we estimate the respective models for the post-only and repeated measure designs 1,000 times, each time substituting a randomly drawn sample (with replacement) equal to the maximum number of unique observations in the post-only setting for that experiment in that sample. From these 1,000 estimated models, we then calculate pooled standard errors using Rubin's rule. In effect, this procedure estimates the relative precision across experimental designs for samples of equal size. Table <ref type="table" target="#tab_5">4</ref> shows the bootstrapped ATE and standard errors under each design, as well as the percentage change in standard error and root mean squared error (RMSE) that the repeated measure design provides.</p><p>As Table <ref type="table" target="#tab_5">4</ref> shows, we find that repeated measure designs provide large gains to precision.</p><p>We observe a median 49.4 percent reduction in standard error across all 18 experiments, with a minimum reduction of 31.1 percent. Similarly, we observe a median 41.0 percent reduction in the RMSE, with a minimum reduction of 28.6 percent. The consistently large reductions in standard errors confirm that repeated measure designs offer significant improvement in statistical precision. As we document in the Discussion, these major precision gains can often outweigh the disadvantage of slight attenuation to provide improved accuracy in many research settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Minimal Moderation by Distance Between Repeated Measures</head><p>Researchers regularly place pre-and post-treatment measures at opposite ends of a survey-or even on separate waves in panel surveys-to minimize the probability that respondents will recall being previously asked the same question and alter their post-treatment response. Given our finding that repeated measure designs slightly attenuate treatment effects, a reasonable concern is that a short survey or module might induce greater bias in the estimated ATE due to the proximity of the repeated measures. Our experimental design randomly varies the distance between pre-and post-treatment measures, allowing us to test how distance impacts design effects. We estimate a series of ATEs at each discrete distance between pre-and post-treatment measures (in counts of survey items, ranging from 0 to 19)</p><p>for each experiment in each sample and standardize these ATEs relative to the post-only ATE observed for each study (see the third column of Table <ref type="table" target="#tab_4">3</ref>). <ref type="foot" target="#foot_10">14</ref> We then regress the standardized ATEs on the distance variable. Figure <ref type="figure" target="#fig_2">3</ref> shows the standardized ATEs and associated 95 percent confidence intervals for each experiment at each degree of separation.  We find that the effect of distance between repeated measures is detectable but substantively small, as the regression line in Figure <ref type="figure" target="#fig_2">3</ref> suggests. Each additional item separating the pre-and post-treatment measures is estimated to attenuate the repeated measure ATE by -0.002 (p = 0.010) on average, or by about 1.4 percent of the mean ATE in our data.</p><p>Including fixed effects for the sample and experiment gives a similar but more precise result: the estimated attenuation in the expected ATE is -0.001 (p = 0.009) on average, or about 1.2 percent of the mean ATE in our data. <ref type="foot" target="#foot_12">16</ref> The slight influence of distance on the overall design effect suggests that repeated measure designs are about as well suited to close placement as to separating the measures by several minutes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Some Moderation by Repeated Exposures to Repeated Measure Designs</head><p>An important feature of our design that distinguishes our approach from that of <ref type="bibr" target="#b10">Clifford, Sheagley, and Piston (2021)</ref> is the inclusion of four repeated measure experiments in a single survey administered to each respondent. We also include several questions that ask respondents to recall their previous attitude on the pre-treatment measure and assess how much their opinion had changed-an unusual question that may in itself exacerbate consistency or demand pressures on subsequent repeated measure experiments. These features of our implementation may alter the design effect as respondents progress through the survey, in a way that unrelated distractor content would not.</p><p>We conduct an exploratory analysis to test whether design effects differ between respondents' first to fourth exposures to repeated measure designs. We estimate a meta-analytic design effect for (only) the first repeated measure experiment each participant encounters in their individual survey experience (pooling across experiments and samples, k = 18), and then estimate separate meta-analytic design effects for the second, third, and fourth repeated measure experiment encountered. These results are shown in Table <ref type="table" target="#tab_6">5</ref>. We find that treatment effect attenuation increases as the respondent encounters more repeated measure experiments in the survey, from a meta-analytic mean of -0.131 (p = 0.018) in the first exposure to -0.266 (p &lt; 0.001) in the fourth exposure. The difference between these two design effect estimates is statistically significant, as shown in a fixed-effect model specification reported in Appendix Notably, much of this increase appears to be a consequence of the attitude-recall questions in our surveys. In Appendix Table <ref type="table" target="#tab_0">A</ref>.1.3, we estimate separate meta-analytic design effects for repeated measure experiments later in the survey flow (that is, the second, third, or fourth repeated measure experiment) but prior to any attitude-recall questions, versus those that appeared following an attitude-recall question. Later repeated measure experiments that appeared before any attitude-recall questions exhibit a similar design effect size (estimate -0.145, p = 0.017) to the earliest repeated measure experiment for each respondent (estimate -0.131, p = 0.018). In contrast, repeated measure experiments with post-treatment measurement after at least one attitude-recall question (i.e., on an earlier experiment) show a larger design effect (estimate -0.249, p &lt; 0.001). We thus identify a small but meaningful design effect of singular repeated measure experiments, but fielding multiple repeated measure experiments in a single survey or (especially) using attitude-recall questions may exacerbate the design effect on later repeated measure experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>No Moderation by Respondent Professionalization</head><p>Non-probability samples are commonly used for experiments because of their convenience and relatively low costs <ref type="bibr" target="#b28">(Jerit and Barabas 2023)</ref>. Many online non-probability panelists take surveys frequently for income or enjoyment, making them professionalized and prone to satisficing <ref type="bibr" target="#b24">(Hillygus, Jackson, and Young 2014;</ref><ref type="bibr" target="#b23">Hillygus and LaChapelle 2022)</ref>. In contrast, NORC's probability-based AmeriSpeak panel restricts participation frequency to maintain respondent quality and avoid excessive professionalization, meaning that respondent attention may be higher in this sample.</p><p>Differences between probability and non-probability panels could affect the design effect of repeated measure designs. Greater respondent attention could increase recall of a pre-treatment measure or response, potentially elevating priming, consistency, or demand pressures on post-treatment responses. Respondent satisficing and speeding could reduce recall of a pre-treatment measure (and possibly exposure to treatment; see Hillygus, Jackson, and Young 2014) to potentially suppress these effects. Higher professionalization may inure respondents to repeated measures, improving their effectiveness. Conversely, professionalization may enable respondents to connect repeated questions to experimental stimuli, exacerbating consistency pressures or altering demand effects.</p><p>While we find no significant sample-level differences in design effects (as Figure <ref type="figure" target="#fig_1">2</ref> shows), our measures of respondent professionalization allow us to conduct exploratory analyses of how within-sample variation in respondent characteristics impacts the design effect of repeated measures. At the end of each survey, we asked respondents how many other online surveys they had completed in the past 30 days, as well as how many online survey companies they had completed surveys for in the past 30 days (active panel memberships). As expected, our Prolific and Lucid respondents are much more professionalized than the AmeriSpeak panelists: the median AmeriSpeak respondent reported completing just 2 surveys for 1 panel in the past 30 days, whereas the median Prolific and Lucid respondent reported completing 40 surveys for 2 panels and 17 surveys for 4 panels, respectively.<ref type="foot" target="#foot_13">17</ref> Within each sample, we then split respondents at the median on each dimension of professionalization, re-analyze each experiment using the subsample for each group, and the meta-analyze the estimated design effects (reported in Appendix A.2). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Respondent Attention and Perceived Attitude Change</head><p>Another way to assess the impact of respondent attention in repeated measures designs is to analyze how well respondents can recall their previous (pre-treatment) attitude after exposure to treatment. In their pre-post study on GMOs, CSP asked whether respondents' support for GMOs had changed since earlier in the survey-that is, since the pre-treatment measurement. CSP found that 40.5 percent of respondents provided different answers on the two measures and that, of these, 58.8 percent (mis)-reported that their attitudes had remained stable. CSP concluded that respondents may struggle to provide consistent responses in repeated measures experiments, even if feeling pressure to do so, simply because many cannot recall their earlier responses. This, CSP argue, reduces the risk of design effects in repeated measure experiments.<ref type="foot" target="#foot_14">18</ref> </p><p>For our three between-groups experiments, we followed the post-treatment measure with a similar recall question for respondents assigned to the repeated measure condition (total n ij = 26, 333 across all samples, offering an analysis sample 27 times larger than CSP's previous single study). 19 Specifically, we asked whether the respondent's preferences about the relevant issue had changed since being asked it earlier in the survey; respondents could indicate whether their support had decreased, increased, or stayed about the same. 20 We distinguish respondents into the three groups based on observed pre-post change (less supportive, no change, or more supportive) and likewise group them by self-reported perceived change (less supportive, about the same, or more supportive).</p><p>We report the rates of perceived versus observed change in Appendix A.3. Like CSP, we find that few respondents who provided different responses between pre-and post-treatment measures were able to correctly identify that change (39.1 percent of observations). However, in an exploratory analysis described in Appendix A.3, we find no evidence that respondents' ability to accurately perceive (or accurately self-report) their direction of change has any impact on the magnitude of the design effect. This analysis suggests that prior attitude recall may not be the most critical factor in producing the slight average attenuation bias we find in repeated measure designs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>Our study provides critical new evidence on the merits of repeated measure designs for experimental research. Like CSP's landmark studies, we find that repeated measure designs consistently offer enormous improvements in statistical precision over traditional post-only designs, observing a 49.4 median reduction in the ATE's standard errors across 18 studies. Unlike CSP, however, we find a small but consistent design effect in the repeated measure setting, observing a median 20.1 percent attenuation of the ATE relative to the post-only design. Figure <ref type="figure" target="#fig_3">4</ref> summarizes the balance of our evidence on this fundamental 19 Because the within-subject experiments ask about plausibly different quantities (or at least quantities perceived to be different) in the two measurements, attitude change is not conceptually well-grounded in that setting. 20 See Appendix B.4 for exact language. trade-off. Based on these findings, we provide practical recommendations for researchers as they consider whether and how to implement repeated measure designs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Repeated Measure Designs versus Post-only Designs</head><p>Our experiments provide evidence that repeated measure designs consistently attenuate treatment effects, which may lead some readers to conclude that post-only designs should be preferred to repeated measure designs. However, many survey experiments primarily aim to identify whether a given treatment shifts the outcome in a hypothesized direction (versus a null effect). For this purpose, statistical power is an essential considerationand repeated measure designs clearly dominate in this regard, as Figure <ref type="figure" target="#fig_3">4</ref> shows. Power is determined by the ratio of the treatment effect and its standard error (Rainey 2025); thus, although repeated measure designs slightly attenuate the treatment effect (the numerator), the resulting slight loss of power is often outweighed by power gained from shrinking the standard error (the denominator). In many applied settings, this power trade-off favors repeated measure designs, which are better suited to reliably detect true (albeit attenuated) treatment effects.</p><p>Should researchers who seek to estimate a treatment effect's precise magnitude (not simply its presence or direction) prefer post-only designs, given our findings of attenuated ATEs? Even here, we contend that repeated measure designs are often superior. Although post-only designs are unbiased in expectation, their imprecision in finite samples will cause estimates to vary widely around the true ATE. Our results suggest that repeated measure designs are so much more precise that even their slightly attenuated ATE estimates will fall closer to the true ATE in many circumstances.</p><p>To illustrate these tradeoffs, we simulate 1,000 two-arm (treatment and control) experi- Figure <ref type="figure" target="#fig_5">5</ref> shows that repeated measure designs always offer superior power for hypothesis testing, with the largest gains when the true ATE or the sample size is smaller; the magnitude of the design effect has a comparatively small impact on power. For smaller samples and smaller true effect sizes, Figure <ref type="figure" target="#fig_6">6</ref>  Post-only designs may therefore still be preferable for researchers with access to a large sample and reason to expect (a priori) a strong treatment effect or strong design effects. Although a repeated measure design will usually offer an improvement in statistical power, this benefit declines as sample and true effect size increase (see Figure <ref type="figure" target="#fig_5">5</ref>). In these circumstances, the expected attenuation of the ATE can be large enough relative to the precision gains to make a repeated measure design less accurate in expectation than a post-only design.</p><p>A related concern pertains to experiments on socially sensitive topics. <ref type="foot" target="#foot_17">23</ref> Here, pretreatment outcome measurement may substantially heighten social desirability biases that could induce respondents to falsify post-treatment responses (either towards consistency or towards responsiveness to the treatment, depending on the experiment), potentially putting the estimated ATE in greater jeopardy. <ref type="foot" target="#foot_18">24</ref> Repeatedly probing respondents about a very sensitive topic may also cause them emotional distress and increase attrition, raising ethical and practical concerns with repeated measure designs. While some of our experiments could be considered sensitive (e.g., on affirmative action and opioid clinics), none are on topics as sensitive as (say) illegal drug use or participation in violence, which are often examined in list experiments (e.g., <ref type="bibr" target="#b2">Aronow et al. 2015;</ref><ref type="bibr" target="#b18">García-Sánchez and Queirolo 2021;</ref><ref type="bibr" target="#b47">Redlawsk, Tolbert, and Franko 2010;</ref><ref type="bibr" target="#b55">Walsh and Braithwaite 2008)</ref>. Thus, we lack robust evidence on the risks of repeated measures for experiments on very sensitive topics, and researchers should proceed cautiously when using them in such contexts.</p><p>As social science moves towards larger samples to address concerns about under-powered research (e.g., <ref type="bibr" target="#b1">Arel-Bundock et al. 2022</ref>), a post-only design is also a reasonable choice for high-powered experiments when the aim is to minimize absolute error, rather than to minimize the detectable effect. Researchers might particularly prefer a large-N post-only design to accurately estimate the magnitude of a treatment effect as a quantity of interest for downstream analyses. A meaningfully attenuated estimate of (e.g.) an online learning intervention could, for example, risk introducing substantial bias in downstream cost-benefit analyses for evaluating widespread policy implementation.</p><p>That said, post-only designs present considerable opportunity costs. Given arbitrarily large but nevertheless finite resources, experimenters could use repeated measure designs to increase the number of treatment arms or field multiple distinct experiments with smaller samples for roughly the same costs (and statistical power) as a single, larger post-only experiment. 25 This approach enables researchers to extend the range of interventions tested and improve cost effectiveness, expanding the scope of empirical inquiry.</p><p>In sum, though we note some general circumstances in which experimenters should still consider a post-only design, we largely concur with CSP that repeated measure designs are a better default than post-only designs. Applied social science often prioritizes identifying directional effects over estimating precise magnitudes, and like other disciplines political science continues to struggle with under-powered research <ref type="bibr" target="#b1">(Arel-Bundock et al. 2022</ref>). We 25 An alternate approach would be to use a mixed design strategy by randomly assigning participants to different designs for same experiment, as we do here. In theory, this would allow the experimenter to estimate the design effect for their specific setting and intervention. In practice, however, we view this as a risky approach. As we document, design effects are identifiable but small across a range of settings and interventions, meaning that excellent power for both design types is required to precisely estimate them. Absent sufficiently large samples, a mixed design strategy is thus likely to yield a very noisy estimate of the design effect, which may lead researchers to falsely conclude that the design effect is zero (or very large) in their particular context.</p><p>encourage the use of power calculations at the design stage to compare post-only and repeated measure designs under different assumptions about treatment effect size and precision (for helpful guidance on these comparisons, see <ref type="bibr" target="#b46">Rainey 2025)</ref>. Nonetheless, because treatment effects in the behavioral sciences tend to be small <ref type="bibr" target="#b0">(Amsalem and Zoizner 2020;</ref><ref type="bibr" target="#b17">Funder and Ozer 2019;</ref><ref type="bibr" target="#b21">Gignac and Szodorai 2016;</ref><ref type="bibr" target="#b26">Hummel and Maedche 2019;</ref><ref type="bibr" target="#b56">Walter et al. 2020)</ref> and are rarely known to the experimenter a priori, repeated measure designs are generally the conservative choice. Absent a strong, justified expectation of a large treatment effect and access to a large sample, researchers are likely better served by repeated measure designs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Between-groups versus Within-subject Experiments</head><p>One of this study's aims was to expand the evidence base on within-subject repeated measure designs. CSP's useful initial evidence comes from one experiment on anti-poverty spending (N = 900 students). We analyze three within-subject experiments (on anti-poverty, affirmative action, and opioid policy) across three samples for a total n ij = 39, 489, providing robust evidence on the utility of this type of repeated measure design. While we find that within-subject experiments are susceptible to some slight attenuation bias, we find that this bias is (if anything) smaller than for between-groups repeated measure experiments, and the precision gains are perhaps greater. In our bootstrapped analyses of equivalent sample sizes between designs (see Table <ref type="table" target="#tab_5">4</ref>), we observe a median 17.3 percent attenuation of the ATE for the within-subject experiments versus 25.1 percent for the between-groups experiments;</p><p>we also observe a 57.9 percent median reduction in the standard error versus a 41.0 percent reduction. We recommend repeated measure designs for both types of experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Probability versus Non-probability Samples</head><p>Fielding all of our experiments on three samples simultaneously allows us to assess the suitability of repeated measure designs for diverse sampling designs and respondent characteristics. We observe large differences in respondent characteristics between the three samples, such as higher professionalization in the non-probability samples and cross-sample variation in respondents' ability to recall their pre-treatment responses (see Appendix A.3).</p><p>Nevertheless, we find no consistent differences in design effects across samples (see Figure <ref type="figure" target="#fig_1">2</ref>).</p><p>We further find no evidence that respondent professionalization alters design effects within each sample (see Appendix A.2). These results thus support repeated measure designs for both probability and non-probability general population samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Brief Survey Modules</head><p>In experimental survey research, repeated measures are commonly placed as far apart as possible to enable respondents to "forget" their pre-treatment responses, thus minimizing the risk of bias to the ATE. In our experiments, we randomly varied the proximity of pre-and post-treatment measures and find that distance between repeated measures alters the design effect only slightly, such that the attenuation bias increases marginally when the measures are placed further apart, as shown in Figure <ref type="figure" target="#fig_2">3</ref>. Substantively, placing pre-and post-treatment measurements very close together appears to have similar results as placing them several minutes apart.<ref type="foot" target="#foot_19">26</ref> While our evidence offers no compelling reason to avoid distractor content between repeated measures, our findings offer reassurance that researchers can use repeated measure designs even when constrained to very limited survey space that precludes providing much separation.<ref type="foot" target="#foot_20">27</ref> </p><p>That said, our exploratory analyses regarding differences in the design effect from iterative exposure to multiple repeated measure designs in a single survey (see Appendix A.1) offers an important caution about repeated measure designs in omnibus surveys. We find that the attenuation in treatment effects from a repeated measure design increases slightly as respondents encounter more repeated measure experiments in our surveys, and that this increase is particularly pronounced for experiments fielded after an attitude-recall question.</p><p>When combining multiple studies in a single survey-as is common practice today through collaborative data collection efforts like TESS or the Cooperative Election Studies-repeated measure experiments placed in later modules may risk more severe attenuation of treatment effects, especially if attitude-recall questions are used in earlier modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concluding Remarks</head><p>Considering the sum of our evidence, we offer three final remarks. First, we note that our study says little about the relative prevalence of priming, consistency, or demand effects.</p><p>While one or more of these conventional concerns may contribute to the slight attenuation in repeated measure designs, there is likely heterogeneity in the relative strength of each across individuals, and some may even be operating in opposing directions to produce net attenuation on average. We encourage future research to better disentangle this knot.</p><p>Second, our 18 studies cover a lot of ground but necessarily leave much unexplored. For example, experiments on highly sensitive topics subject to strong social desirability bias may suffer from larger design effects than we identify here. Our omnibus surveys are relatively short and exclusively use the self-administered web survey mode. Repeated measure designs in other survey experimental contexts such as face-to-face or phone interviews, where the interviewers' presence may alter respondent reactions to repeated measurements <ref type="bibr" target="#b34">(Lavrakas, Kelly, and McClain 2019)</ref>, may face additional challenges that we cannot examine hereparticularly for sensitive topics. Nevertheless, because experimental interventions and selfadministered web surveys like ours are quite common in contemporary experimental research <ref type="bibr" target="#b10">(Clifford, Sheagley, and Piston 2021;</ref><ref type="bibr" target="#b28">Jerit and Barabas 2023)</ref>, we believe that our evidence provides useful insights for many experimental research contexts.</p><p>Finally, we return to the broad shift in design practice that has followed CSP's evidencebacked suggestion that "the default should shift away from the post-only design and toward repeated measure designs" <ref type="bibr">(Clifford, Sheagley, and</ref><ref type="bibr">Piston 2021, 1063)</ref>. Through our largescale replications and extensions, our contribution should be viewed as a qualified endorse-ment of this new standard for experimental design. There remain some circumstances in which research aims can reasonably justify a traditional post-only design, but in our view these cases are not the modal enterprise in the discipline today. Our accumulated evidence suggests that the burden of justifying an experimental design should weigh more heavily on the use of post-only over repeated measure designs, rather than the historical reverse.</p><p>A Supplemental Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Internal Meta-analyses</head><p>In Table <ref type="table" target="#tab_0">A</ref>.1.1, we report the tabular results of the main internal meta-analyses of design effects that are shown in Figure <ref type="figure" target="#fig_1">2</ref> of the main text. Design effect estimates are expressed as the proportional change in the post-only design ATE.</p><p>Note that our experimental design combines multiple experiments into a single omnibus survey, a common practice in contemporary social science. Because the order of these experiments is randomized, we can assess whether the design effect of repeated measure designs (relative to the post-only design) changes as respondents are iteratively exposed to more repeated measure design experiments (from a first to fourth encounter within a single survey). Table <ref type="table" target="#tab_6">5</ref> in the main text reports internal meta-analyses using all 18 experiments, subset by respondents' first through fourth repeated measure design encountered in the survey. Table <ref type="table" target="#tab_0">A</ref>.1.2 reports a combined fixed-effects meta-analysis that estimates differences in the design effect (relative to the post-only design) between the first and subsequent repeated measure design experiments. Note: Table <ref type="table" target="#tab_13">displays</ref> the results of an internal meta-analysis (k = 18) of the repeated measure design effect with a fixed effect for the order of repeated measure design experiment encountered. The first repeated measure design experiment encountered is held as the reference category.  A.3 Perceived Attitude Change  Observed Pre-Post Change Self-reported Perceived Change  ples and experimental context, with a contingency table of perceived versus observed change for each between-groups experiment in each sample. We find some differences in overall accuracy by sample:</p><p>Lucid respondents accurately perceived their level of change in 60.1 percent of observations, whereas the overall accuracy rate is 68.1 percent in the AmeriSpeak sample and 78.1 percent in the Prolific sample. In part, this appears to be because Prolific respondents were more stable in their attitudes; in every experiment, a higher percentage of Prolific respondents were both stable in their observed pre-post responses and self-reported no change in attitude than for either of the other two samples. These results align with recent evidence that Prolific respondents tend to be more attentive than Lucid respondents, but may react differently to some treatment <ref type="bibr" target="#b51">(Stagnaro et al. 2024)</ref>.</p><p>We also observe some slight heterogeneity among the three between-groups experiments. The (crosssample) accuracy rate is highest for the foreign aid experiment at 70.8 percent and slightly lower in the drug imports experiment at 68.9 percent. The lowest accuracy rate is in the GMO framing experiment 65.5 percent, which is perhaps to be expected given that all respondents in that experiment received either a positive or negative framing (no pure control) and were thus more likely to change their responses post-treatment. Indeed, we see that only 24.2 and 24.8 percent of respondents in the foreign aid and drug imports experiments (respectively) actually moved, whereas 41.1 percent of respondents did so in the GMO experiment. Does the accuracy of respondents' self-perceptions of their attitude change (or lack thereof) relate to the design effect of repeated measure experiments? We conduct an exploratory analysis by separating the between-groups repeated measure observations into two subsamples, based on the first such repeated measure each respondent encountered in the survey flow: those who accurately perceived their level of attitude change in that experiment (that is, the three cross-diagonal cells from the bottom left to top right in Figure A.3.1) versus those who did not (all other cells). <ref type="foot" target="#foot_21">28</ref> We then re-estimate the design effect (as proportional change in the post-only design ATE in each sample) with each subsample for each subsequent between-groups repeated measure experiment that each respondent encountered, and finally meta-analyze these estimated design effects for accurate versus inaccurate respondents. We condition on the first between-groups repeated measure experiment to analyze subsequent repeated  measure experiments (exclusively) to ensure that the conditioning variable is independent of treatment assignment in the analyzed experiments. 29 The results are reported in Table <ref type="table" target="#tab_0">A</ref>.3.1.</p><p>We find little evidence of a relationship between self-reported recall accuracy and the design effect,</p><p>29 Though this restriction substantially reduces the sample size (and power) of these analyses, using accuracy within each experiment itself would confound any potential effect of accuracy with a mechanical effect from treatment assignment. The recall questions that allow us to distinguish accurate versus inaccurate perceptions of change are post-treatment. Treatment itself predicts inaccuracy (33.9 percent among treated observations, 29.3 percent among control observations) because treated respondents are more likely to provide a different response relative to their pre-treatment observation, whereas most control respondents can accurately satisfice by self-reporting no change. Propensity to accurately report one's own level of change in each respective condition may vary by unobserved respondent characteristics; through this mechanism, analyzing the design effect conditional on accuracy directly within a given experiment may partially derandomize the assignment to treatment. as shown in Table <ref type="table" target="#tab_0">A</ref>.3.1. The attenuation bias is stronger among respondents who accurately perceived their level of pre-post change in four studies but stronger among those who were inaccurate in five, and the difference in ATEs between these two subsamples is statistically significant in none (metaanalytic estimated difference -0.001, p = 0.990).<ref type="foot" target="#foot_22">30</ref> Across all nine between-groups experiments, the meta-analytic design effect (expressed as proportional to the respective post-only ATE) among the recall-accurate respondents is -0.433 (p = 0.008) and a very similar -0.302 (p = 0.004) among recallinaccurate respondents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Simulation Results</head><p>To simulate the impact of the design effect of repeated measure designs on several considerations of experimental design, we simulate 1,000 repeated measure experiments and 1,000 post-only experiments at each of several sample sizes (from N = 200 to N = 2000, in increments of 200),<ref type="foot" target="#foot_23">31</ref> , true effect sizes (from 0 to 0.4, in increments of 0.1) expressed in terms of Cohen's d, and design effect attenuation assumptions (10 percent, 20 percent, or 30 percent).<ref type="foot" target="#foot_24">32</ref> </p><p>For each experiment, we assume that each observation of the outcome variable is a random withinindividual draw from a true value plus random error (i.e., <ref type="bibr" target="#b16">Feldman 1989;</ref><ref type="bibr" target="#b58">Zaller and Feldman 1992)</ref>.</p><p>The true value for each individual is a random draw from a normal distribution with a mean of zero and a standard deviation of 1. The pre-treatment outcome measure (repeated measure experiments only)</p><p>is the true value plus error, which is a random draw from a normal distribution with a mean of zero and a standard deviation of 0.4. The post-treatment outcome measure is again the true value plus a random draw for error (again from a normal distribution with a mean of zero and a standard deviation of 0.4). We randomly assign each individual to treatment or control with a random draw from a binomial distribution (p = 0.5). The post-treatment outcome value for those assigned to the treatment condition is the true (baseline) value of their inherent attitude plus random error, plus a homogeneous treatment effect (equal to the defined true effect) that is attenuated by the defined value in the repeated measure experiments.</p><p>After drawing these random values for each individual up to the defined sample size, we regress the post-treatment outcome variable on the treatment indicator, plus the pre-treatment outcome variable for the repeated measure experiments only. We then record the coefficient on the treatment indicator and its standard error in each experiment. Note that the choice of a standard deviation of 0.4 for the response error distribution provides a correlation between repeated measures of about 0.8 and a corresponding reduction in the standard error of the treatment coefficient of about 40 percent, which is in line with our observed data from the field, but slightly conservative.</p><p>We simulate 1,000 experiments for each design at each value of the sample size, true effect, and design effect parameters, or 300,000 experiments in total. For each combination of parameters, we then calculate the statistical power of the test (as 1 minus the proportion of false negatives, shown in Figure <ref type="figure" target="#fig_5">5</ref> in the main text), the mean absolute error between the treatment coefficient and the true treatment effect (shown in Figure <ref type="figure" target="#fig_6">6</ref> in the main text), the mean false discovery rate (the proportion of experiments in which the treatment coefficient is significant but negative when the true effect is positive, or the confidence does not include zero when the true effect is zero), and the mean coverage rate (the proportion of experiments in which the confidence interval includes the true effect parameter) for each design type.</p><p>The results on the latter two metrics are shown in Figures <ref type="bibr">A.4.1, and A.4</ref>.2. The false discovery rate is equivalent under both designs in most circumstances, but superior (lower) under a repeated measure design when the sample size and true effect size are both small. Finally, the coverage rate is similar for both designs when the true effect is zero or the true effect size and attenuation are both small, but otherwise the coverage rate under a repeated measure design quickly declines as the sample size, true effect size, and degree of attenuation increase.</p><p>completion time was 6.1 minutes. A total of 4,250 panelists entered the survey; as preregistered, we exclude 82 who failed to complete the survey, and a further 139 for either extreme speeding (less than 1/3 of the median completion time) or item non-response on at least half of the survey questions. This provides our analysis sample of N i = 4, 029. Although provided by NORC, we do not apply sample weights in our analyses to preserve statistical power <ref type="bibr" target="#b40">(Miratrix et al. 2018)</ref>.</p><p>The As with all survey research, the design and collection of data has limitations for all three samples, and resulting estimates may involve unmeasured error that limits representativeness to the target population. The upper panel provides two hypothetical example randomizations executed at each of five stages to determine the display order of the survey for individual respondents. The bottom panel provides the resulting display order experienced by the respondent. One star (*) indicates that the item counts double for the purpose of determining distance between repeated measures; two stars (**) indicates that the item counts triple (the NFL blocks each include three questions). To identify the distance between repeated measures, sum the number of items between them plus the number of stars (*).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Sample Characteristics</head><p>As shown in Table <ref type="table">B</ref>.3.1, all respondents first answers pre-treatment questions for all four repeated measure experiments. Respondent A then answers post-treatment questions for all six experiments (including the post-only experiments) in a completely random order, with the unrelated NFL content also included in the mix. In contrast, Respondent B, assigned to the "forced-short" procedure, completes the post-treatment questions for the two repeated measure experiments that appeared last in the pre-treatment block (the GMO and prescription drug experiments, in this case), guaranteeing that some repeated measure content appears close together, before answering all remaining post-treatment questions in a random order (again with the NFL content mixed in). Table <ref type="table">B</ref>.3.1 also illustrates how we measure distance between repeated measures: the count of items in between the pre-and post-treatment measures, plus the count of stars in between, indicating that an item that counts twice (*, because the question is preceded by a longer paragraph) or thrice (**, because it denotes a block of three questions).</p><p>For example, the calculated distance between measures in the prescription drugs experiment is 13 for Respondent A, and 0 for Respondent B because the measures appear back-to-back. In this section, we provide the question wording and response options for all experimental content for studies 1-6. We specify the standard TESS unit length of each item (1 unit for most items, 2 units for those whose question is preceded by a longer paragraph). Note that the order of items was randomized as discussed in the main text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Foreign Aid (Study 1)</head><p>Foreign Aid Pretreatment/Control (1 TESS unit): "Do you think spending on foreign aid should be increased or decreased?"</p><p>• Greatly increased </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Professionalization Measures</head><p>"To the best of your memory, how many other online surveys have you completed in the past 30 days, not including this one?" [Open-ended] "To the best of your memory, in the past 30 days, how many different online survey companies have you completed one or more surveys for, not including this one?" <ref type="bibr">[Open-ended]</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Histogram of distances between repeated measures. Figure shows the observed distances (counts of survey items) separating the pre-and post-treatment measures for observations in the repeated measure design setting. Data includes pooled observations from all experiments in all samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Internal Meta-Analyses. Figure displays estimated design effects from internal meta-analyses of experiments within each sample and across all three samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Estimated design effect by distance between repeated measures. Figure displays the estimated ATE at each distance between pre-and post-treatment measures (in counts of survey items, x-jittered for visual clarity) in each experiment in each sample, standardized to the respective observed post-only ATE. The red line indicates the fitted values from a linear regression on these ATE point estimates on distance; the shaded areas indicate 95 percent confidence intervals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Histogram of design effects. Figure displays a histogram of observed design effects in terms of percentage change in estimated ATE (left panel) and standard error (right panel) in bootstrapped models with equal sample size across designs. The solid red line in each panel indicates the median percentage change in each statistic across all 18 experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>ments per design at each of several sample sizes, true ATEs (expressed as Cohen's d ), and true attenuation in the estimated ATE from the repeated measure design effect. 21 From these simulated experiments, we estimate the statistical power of the test (defined as one minus the observed proportion of false negatives, shown in Figure5) and the mean absolute error in the estimated ATE (versus the true ATE) under each design (Figure6). Each figure shows how the respective statistic changes as the sample size increases (within-panel), the true effect size increases (across columns), and the design effect attenuation of the estimated ATE increases (across rows). The means for each statistic is shown by the solid grey line for post-only experiments and by the dashed black line for repeated measure experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>TrueFigure 5 :</head><label>5</label><figDesc>Figure 5: Statistical power in simulated experiments. Figure displays mean statistical power in simulated experiments at varying sample sizes, true effect sizes (Cohen's d), and true design effect (attenuation) of a repeated measure experiment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Absolute error in simulated experiments. Figure displays mean absolute error between the estimated and true ATE in simulated experiments at varying sample sizes, true effect sizes (Cohen's d), and true design effect (attenuation) of a repeated measure experiment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure A. 2</head><label>2</label><figDesc>Figure A.2.1: Meta-Analytic design effect by respondent professionalization. The top (bottom) row of panels indicates the design effect among above (below) median respondents within each sample on each professionalization measure (columns).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure A. 3</head><label>3</label><figDesc>Figure A.3.1 shows the percentage of repeated measure experiment observations by observed stability or change versus self-reported perceived stability or change. We find that respondents in most repeated measure observations (69.9 percent) provide the same response both pre-and post-treatment, as shown in the center column of Figure A.3.1, and most of these (81.0 percent) self-report that their attitudes stayed about the same. Among the remaining 30.1 percent of participants whose observed responses did change (left and right columns), only 39.1 percent accurately perceived that change, with half (50.0 percent) incorrectly reporting no change and the remaining 10.9 percent reporting a change in the opposite direction from their actual change.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure</head><label></label><figDesc>Figure A.3.1: Respondent perception of attitude change. Figure shows a contingency table of pooled respondents (all between-groups repeated measure observations in all samples) by actual observed pre-post change in responses (columns) and self-reported perceived pre-post change (rows). Frequency counts for each cell are shown in parentheses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure</head><label></label><figDesc>Figure A.3.2 offers an exploratory report of differences in perceived attitude change across sam-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure</head><label></label><figDesc>Figure A.3.2: Respondent perception of attitude change by sample and experiment. Figure shows contingency tables of respondents in each between-groups repeated measure experiment (panel columns) in each sample (panel rows) by actual observed pre-post change in responses (within-panel columns) and self-reported perceived pre-post change (within-panel rows). Frequency counts for each cell are shown in parentheses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table A</head><label>A</label><figDesc></figDesc><table><row><cell>.5.1</cell></row><row><cell>for the full list of studies). While most citations are from political science journals, the cita-</cell></row><row><cell>tions span 83 journals in a range of fields, including communication, criminology, economics,</cell></row><row><cell>education, and environmental studies. The article's broad influence on experimental prac-</cell></row><row><cell>tice is already clear and likely to grow as disciplines become more critical of underpowered</cell></row><row><cell>experiments (Arel-Bundock et al. 2022; Ioannidis, Stanley, and Doucouliagos 2017; Open</cell></row><row><cell>Science Collaboration 2015).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>, ran-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Summary of Replicated Survey Experiments</figDesc><table><row><cell></cell><cell>Topic</cell><cell>Design</cell><cell>Treatment</cell><cell>Control</cell></row><row><cell>1</cell><cell>Foreign Aid</cell><cell>Between-Groups</cell><cell>Foreign Aid 1% of</cell><cell>No Information</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Budget</cell><cell></cell></row><row><cell>2</cell><cell>Drug Imports</cell><cell>Between-Groups</cell><cell>DEM Favors, REP</cell><cell>No Party Cues</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Opposes</cell><cell></cell></row><row><cell>3</cell><cell>GMOs</cell><cell>Between-Groups</cell><cell>Pro: Prevents</cell><cell>Con: Uncertain</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Blindness</cell><cell>Health Effects</cell></row><row><cell>4</cell><cell>Anti-poverty</cell><cell>Within-Subject</cell><cell>Assistance to the</cell><cell>Welfare Spending</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Poor</cell><cell></cell></row><row><cell>5</cell><cell>Affirmative Action</cell><cell>Within-Subject</cell><cell>Target Women</cell><cell>Target Racial</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Minorities</cell></row><row><cell cols="2">6 Opioid Clinic Policy</cell><cell>Within-Subject</cell><cell>Clinic 2 Miles Away</cell><cell>Clinic 1/4 Mile</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Away</cell></row><row><cell cols="5">experiments from Wilson et al. (2008, denoted Study 5) and de Benedictis-Kessner and</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Sample and Median Respondent Characteristics</figDesc><table><row><cell></cell><cell>Survey</cell><cell>Sampling</cell><cell>Median R:</cell><cell>Median R:</cell><cell>Median R:</cell></row><row><cell></cell><cell>Vendor</cell><cell>Method</cell><cell>Survey</cell><cell>Surveys per</cell><cell>Panel</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Duration</cell><cell>Month</cell><cell>Memberships</cell></row><row><cell>1</cell><cell>AmeriSpeak</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Summary of Experimental Results</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Post-Only</cell><cell cols="2">Repeated</cell><cell cols="2">Design Effect</cell></row><row><cell>Experiment</cell><cell>Sample</cell><cell cols="2">Est. ATE Obs.</cell><cell cols="2">Est. ATE Obs.</cell><cell>Estimate</cell><cell>SE</cell><cell>∆ in ATE</cell></row><row><cell>Fgn. Aid</cell><cell cols="2">AmeriSpeak 0.089  *  *  *</cell><cell cols="2">1,272 0.065  *  *  *</cell><cell cols="2">2,662 -0.023</cell><cell>0.015</cell><cell>-26.1%</cell></row><row><cell></cell><cell>Prolific</cell><cell>0.111  *  *  *</cell><cell cols="2">1,433 0.068  *  *  *</cell><cell cols="2">2,828 -0.043  *  *</cell><cell>0.014</cell><cell>-38.6%</cell></row><row><cell></cell><cell>Lucid</cell><cell>0.063  *  *  *</cell><cell cols="2">1,616 0.056  *  *  *</cell><cell cols="2">3,252 -0.008</cell><cell>0.016</cell><cell>-12.0%</cell></row><row><cell>Drug Imp.</cell><cell cols="2">AmeriSpeak 0.125  *  *  *</cell><cell cols="2">1,360 0.055  *  *  *</cell><cell cols="2">2,580 -0.070  *</cell><cell>0.029</cell><cell>-56.0%</cell></row><row><cell></cell><cell>Prolific</cell><cell>0.096  *  *  *</cell><cell cols="2">1,261 0.072  *  *  *</cell><cell cols="2">2,485 -0.024</cell><cell>0.032</cell><cell>-25.1%</cell></row><row><cell></cell><cell>Lucid</cell><cell>0.110  *  *  *</cell><cell cols="2">1,375 0.077  *  *  *</cell><cell cols="2">2,699 -0.033</cell><cell>0.032</cell><cell>-30.4%</cell></row><row><cell>GMOs</cell><cell cols="2">AmeriSpeak 0.162  *  *  *</cell><cell cols="2">1,283 0.129  *  *  *</cell><cell cols="2">2,660 -0.033  *</cell><cell>0.017</cell><cell>-20.2%</cell></row><row><cell></cell><cell>Prolific</cell><cell>0.180  *  *  *</cell><cell cols="2">1,396 0.162  *  *  *</cell><cell cols="2">2,865 -0.017</cell><cell>0.016</cell><cell>-9.7%</cell></row><row><cell></cell><cell>Lucid</cell><cell>0.144  *  *  *</cell><cell cols="2">1,650 0.124  *  *  *</cell><cell cols="2">3,218 -0.021</cell><cell>0.016</cell><cell>-14.2%</cell></row><row><cell>Anti-pov.</cell><cell cols="2">AmeriSpeak 0.202  *  *  *</cell><cell cols="2">1,351 0.159  *  *  *</cell><cell cols="2">5,172 -0.044  *</cell><cell>0.020</cell><cell>-21.3%</cell></row><row><cell></cell><cell>Prolific</cell><cell>0.165  *  *  *</cell><cell cols="2">1,481 0.110  *  *  *</cell><cell cols="3">5,560 -0.055  *  *  *  0.017</cell><cell>-33.1%</cell></row><row><cell></cell><cell>Lucid</cell><cell>0.169  *  *  *</cell><cell cols="2">1,637 0.135  *  *  *</cell><cell cols="2">6,457 -0.033  †</cell><cell>0.019</cell><cell>-20.0%</cell></row><row><cell cols="3">Afrm. Act. AmeriSpeak 0.095  *  *  *</cell><cell cols="2">1,310 0.079  *  *  *</cell><cell cols="2">5,430 -0.017</cell><cell>0.022</cell><cell>-17.3%</cell></row><row><cell></cell><cell>Prolific</cell><cell>0.094  *  *  *</cell><cell cols="2">1,420 0.053  *  *  *</cell><cell cols="2">5,682 -0.040  †</cell><cell>0.022</cell><cell>-43.2%</cell></row><row><cell></cell><cell>Lucid</cell><cell>0.094  *  *  *</cell><cell cols="2">1,593 0.079  *  *  *</cell><cell cols="2">6,546 -0.014</cell><cell>0.020</cell><cell>-15.0%</cell></row><row><cell>Clinic</cell><cell cols="2">AmeriSpeak 0.113  *  *  *</cell><cell cols="2">1,361 0.101  *  *  *</cell><cell cols="2">5,328 -0.012</cell><cell>0.018</cell><cell>-10.8%</cell></row><row><cell></cell><cell>Prolific</cell><cell>0.071  *  *  *</cell><cell cols="2">1,361 0.126  *  *  *</cell><cell cols="2">5,800 0.055  *  *</cell><cell>0.019</cell><cell>+76.5%</cell></row><row><cell></cell><cell>Lucid</cell><cell>0.047  *  *</cell><cell cols="2">1,624 0.050  *  *  *</cell><cell cols="2">6,489 0.004</cell><cell>0.016</cell><cell>+7.6%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>† p&lt;0.10; * p&lt;0.05; * * p&lt;0.01; * * * p&lt;0.001 Note: Table displays the estimated ATE under each design in each experiment in each sample, followed by the repeated measure design's estimated design effect and percentage change from the ATE of the post-only design.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Bootstrapped Experimental Results Table displays the estimated ATE and bootstrapped standard error under each design in each experiment in each sample, each estimated 1,000 times with a number of sampled respondents equal to the number of observations in the respective postonly design (see column four of Table3).</figDesc><table><row><cell>Post-Only</cell><cell>Repeated Measure</cell><cell>Increased Precision</cell></row></table><note><p>* p&lt;0.05; * * p&lt;0.01; * * * p&lt;0.001 Note:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Table A.1.2 (p = 0.049), suggesting that fielding multiple repeated measure experiments in a single survey may exacerbate the design effect for experiments later in the survey. Notably, however, focusing strictly on design effects in the first repeated measures experiment respondents encountered-equivalent to CSP's approach of separate surveys-we still find significant attenuation. Repeated Measure Results by Order of Repeated Measure Design Encountered Tabledisplaysthe results of internal meta-analyses (k = 18 for each) of the repeated measure design effect, subset by the order of repeated measure experiments in the survey for each individual respondent. The total number of respondents included in each meta-analysis is provided in parentheses.</figDesc><table><row><cell cols="3">Repeated Measure Experiment Encountered Design Effect Estimate Std. Error</cell><cell>95% CI</cell><cell>p-value</cell></row><row><cell>First (N = 38,628)</cell><cell>-0.131  *</cell><cell>0.050</cell><cell>[-0.236,-0.025]</cell><cell>0.018</cell></row><row><cell>Second (N = 38,664)</cell><cell>-0.206  *  *</cell><cell>0.065</cell><cell>[-0.344,-0.068]</cell><cell>0.006</cell></row><row><cell>Third (N = 38,673)</cell><cell>-0.183  *  *</cell><cell>0.048</cell><cell>[-0.285,-0.082]</cell><cell>0.001</cell></row><row><cell>Fourth (N = 38,665)</cell><cell>-0.266  *  *  *</cell><cell>0.041</cell><cell cols="2">[-0.353,-0.180] &lt; 0.001</cell></row><row><cell>† p&lt;0.10;</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>* p&lt;0.05; * * p&lt;0.01; * * * p&lt;0.001 Note:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table A</head><label>A</label><figDesc></figDesc><table><row><cell cols="6">.1.1: Estimated Meta-analytic Design Effects by Design Type and Sample</cell><cell></cell></row><row><cell>Experiment Type</cell><cell>Sample</cell><cell>k</cell><cell>Estimate</cell><cell>Std. Error</cell><cell>95% CI</cell><cell>p-value</cell></row><row><cell>Both Types</cell><cell>AmeriSpeak</cell><cell>6</cell><cell>-0.222  *</cell><cell>0.057</cell><cell>[-0.368,-0.076]</cell><cell>0.011</cell></row><row><cell></cell><cell>Prolific</cell><cell>6</cell><cell>-0.148</cell><cell>0.171</cell><cell>[-0.588, 0.292]</cell><cell>0.426</cell></row><row><cell></cell><cell>Lucid</cell><cell>6</cell><cell>-0.162  †</cell><cell>0.068</cell><cell>[-0.336, 0.011]</cell><cell>0.061</cell></row><row><cell></cell><cell>Total</cell><cell cols="2">18 -0.200  *  *  *</cell><cell>0.040</cell><cell cols="2">[-0.285,-0.115] &lt; 0.001</cell></row><row><cell cols="2">Between-groups AmeriSpeak</cell><cell>3</cell><cell>-0.259  †</cell><cell>0.082</cell><cell>[-0.614, 0.095]</cell><cell>0.088</cell></row><row><cell></cell><cell>Prolific</cell><cell>3</cell><cell>-0.226</cell><cell>0.118</cell><cell>[-0.733, 0.281]</cell><cell>0.195</cell></row><row><cell></cell><cell>Lucid</cell><cell>3</cell><cell>-0.157</cell><cell>0.097</cell><cell>[-0.575, 0.262]</cell><cell>0.249</cell></row><row><cell></cell><cell>Total</cell><cell>9</cell><cell>-0.210  *  *</cell><cell>0.049</cell><cell>[-0.322,-0.097]</cell><cell>0.003</cell></row><row><cell>Within-subject</cell><cell>AmeriSpeak</cell><cell>3</cell><cell>-0.187</cell><cell>0.079</cell><cell>[-0.525, 0.150]</cell><cell>0.140</cell></row><row><cell></cell><cell>Prolific</cell><cell>3</cell><cell>-0.020</cell><cell>0.377</cell><cell>[-1.640, 1.601]</cell><cell>0.963</cell></row><row><cell></cell><cell>Lucid</cell><cell>3</cell><cell>-0.169</cell><cell>0.094</cell><cell>[-0.572, 0.236]</cell><cell>0.216</cell></row><row><cell></cell><cell>(Total -Outlier)</cell><cell>8</cell><cell>-0.227  *  *</cell><cell>0.051</cell><cell>[-0.348,-0.107]</cell><cell>0.003</cell></row><row><cell></cell><cell>Total</cell><cell>9</cell><cell>-0.149</cell><cell>0.094</cell><cell>[-0.367, 0.069]</cell><cell>0.153</cell></row><row><cell></cell><cell>† p&lt;0.10;</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>* p&lt;0.05; * * p&lt;0.01; * * * p&lt;0.001 Note: Table displays the results of internal meta-analyses of k studies by design type and sample.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table A .</head><label>A</label><figDesc>1.2: Internal Meta-analysis with Repeated Measure Experiment Order Fixed Effects</figDesc><table><row><cell>Fixed Effect</cell><cell cols="2">Design Effect Estimate Std. Error</cell><cell>95% CI</cell><cell>p-value</cell></row><row><cell>Second Encountered</cell><cell>-0.090</cell><cell>0.074</cell><cell>[-0.237, 0.058]</cell><cell>0.229</cell></row><row><cell>Third Encountered</cell><cell>-0.057</cell><cell>0.074</cell><cell>[-0.206, 0.091]</cell><cell>0.442</cell></row><row><cell>Fourth Encountered</cell><cell>-0.149  *</cell><cell>0.074</cell><cell>[-0.297,-0.000]</cell><cell>0.049</cell></row><row><cell cols="2">Constant (First Encountered) -0.123  *</cell><cell>0.053</cell><cell>[-0.228,-0.019]</cell><cell>0.022</cell></row><row><cell></cell><cell cols="2">† p&lt;0.10;  *  p&lt;0.05;  *  *  p&lt;0.01;  *  *  *  p&lt;0.001</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table A</head><label>A</label><figDesc>Tabledisplaysthe results of internal meta-analyses (k = 18 for each) of the repeated measure design effect as a function of each respondent's first encounter with an attitude recall (perceived change) question.</figDesc><table><row><cell>Meta-analyses of Design Effects by Respondent Professionalization</cell></row><row><cell>.1.3: Internal Meta-analysis with Repeated Measure Experiment Relative to First Attitude Recall Question Subset Design Effect Estimate Std. Error 95% CI p-value First RM Experiment (Pre-recall) -0.131  *  0.050 [-0.236,-0.025] 0.018 Subsequent Pre-recall RM Experiments -0.145  *  0.055 [-0.260,-0.030] 0.017 All Pre-recall RM Experiments -0.132  *  0.050 [-0.238,-0.026] 0.017 All Post-recall RM Experiments -0.249  *  *  *  0.049 [-0.351,-0.146] &lt; 0.001  † p&lt;0.10;  *  p&lt;0.05;  *  *  p&lt;0.01;  *  *  *  p&lt;0.001 Panel Memberships Above Median Below Median -0.06 -0.03 0.00 0.03 -0.06 -0.03 0.00 0.03 Prolific Lucid AmeriSpeak Prolific Lucid AmeriSpeak Meta-analytic Design Effect Note: Count of Surveys Completed Sample AmeriSpeak Lucid Sample Prolific</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table A .</head><label>A</label><figDesc>3.1: Repeated Measure Results by Accuracy in Perceived Attitude Change Table displays the results of each experiment under the repeated measure design, conditional on whether the respondent accurately reported the direction of change in their attitude pre-post (or attitude stability) on a prior between-groups repeated measure experiment. The final two columns report the respective design effects (vs. post-only).</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Accurate</cell><cell></cell><cell cols="2">Inaccurate</cell><cell></cell><cell cols="2">Design Effect vs.</cell></row><row><cell></cell><cell></cell><cell cols="2">Perception</cell><cell></cell><cell cols="2">Perception</cell><cell></cell><cell cols="2">Post-only (∆ in ATE)</cell></row><row><cell cols="2">Experiment Sample</cell><cell>Est. ATE</cell><cell>SE</cell><cell>Obs.</cell><cell>Est. ATE</cell><cell>SE</cell><cell cols="2">Obs. Accurate</cell><cell>Inaccurate</cell></row><row><cell>Fgn. Aid</cell><cell cols="2">AmeriSpeak 0.052  *  *  *</cell><cell>0.009</cell><cell>905</cell><cell>0.056  *  *  *</cell><cell>0.015</cell><cell>428</cell><cell>-0.036  *</cell><cell>-0.033</cell></row><row><cell>Fgn. Aid</cell><cell>Prolific</cell><cell>0.071  *  *  *</cell><cell cols="3">0.008 1,032 0.061  *  *  *</cell><cell>0.017</cell><cell>338</cell><cell>-0.041  *  *</cell><cell>-0.050  *</cell></row><row><cell>Fgn. Aid</cell><cell>Lucid</cell><cell>0.062  *  *  *</cell><cell>0.012</cell><cell>942</cell><cell>0.039  *</cell><cell>0.018</cell><cell>675</cell><cell>-0.001</cell><cell>-0.023</cell></row><row><cell>Drug Imp.</cell><cell cols="2">AmeriSpeak 0.016</cell><cell>0.019</cell><cell>866</cell><cell>0.068  *</cell><cell>0.035</cell><cell>444</cell><cell cols="2">-0.109  *  *  *  -0.056</cell></row><row><cell>Drug Imp.</cell><cell>Prolific</cell><cell>0.031  *</cell><cell>0.014</cell><cell>964</cell><cell>0.115  *  *</cell><cell>0.037</cell><cell>270</cell><cell>-0.065  †</cell><cell>0.019</cell></row><row><cell>Drug Imp.</cell><cell>Lucid</cell><cell>0.066  *  *</cell><cell>0.024</cell><cell>819</cell><cell>0.040</cell><cell>0.033</cell><cell>552</cell><cell>-0.044</cell><cell>-0.070</cell></row><row><cell>GMOs</cell><cell cols="2">AmeriSpeak 0.098  *  *  *</cell><cell>0.011</cell><cell>906</cell><cell>0.122  *  *  *</cell><cell>0.019</cell><cell>403</cell><cell cols="2">-0.064  *  *  *  -0.040</cell></row><row><cell>GMOs</cell><cell>Prolific</cell><cell>0.157  *  *  *</cell><cell cols="3">0.009 1,212 0.151  *  *  *</cell><cell>0.022</cell><cell>280</cell><cell>-0.022</cell><cell>-0.029</cell></row><row><cell>GMOs</cell><cell>Lucid</cell><cell>0.153  *  *  *</cell><cell>0.012</cell><cell>973</cell><cell>0.114  *  *  *</cell><cell>0.019</cell><cell>634</cell><cell>0.009</cell><cell>-0.030</cell></row><row><cell></cell><cell></cell><cell cols="5">† p&lt;0.10;  *  p&lt;0.05;  *  *  p&lt;0.01;  *  *  *  p&lt;0.001</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Note:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>second and third samples are non-probability convenience samples recruited via quota sampling from the Prolific (n i = 4, 261) and Lucid (now Cint, n i = 4, 869) opt-in online panels. This component of the study was funded by the Rapoport Family Foundation and by Bass Connections at Duke University. 5% Democrat, 27.7% Republican, 42.8% Independent). The Lucid sample was recruited with joint quotas on sex, age, and race/ethnicity as shown in TableB.1.1 (note that the "Other" category was not an explicit quota, but includes anyone who opted not to report their sex, age, or race/ethnicity to Lucid in the prescreen phase). These surveys were fielded from July 3 rd to July 15 th , 2024. Prolific respondents received $1.00 for completing the study; Lucid provided participants with an incentive to participate in our study, but these incentives differ by respondent and are not disclosed to the researcher. The median completion time for Prolific participants was 7.2 minutes and 7.3 minutes for Lucid participants. After consenting to participate the study, participants were screened for eligibility to confirm that they were at least 18 years of age and resided in the United States. We recruited a total of 4,398 eligible Prolific participants and 6,094 eligible Lucid participants into the study. As preregistered, we exclude 94 participants in the Prolific sample and 354 in the Lucid sample who failed to complete the respective survey, as well as 5Prolific participants and 190 Lucid participants who failed an explicit attention check during screening (failing to select either "B" or "D" when asked to identify the second and fourth letters of the English alphabet). Finally, we exclude 38 Prolific participants and 681 Lucid participants for extreme speeding (completing the survey in less than 1/3 of the within-sample median time) or failing at least two of the</figDesc><table><row><cell cols="6">The Prolific sample recruited with the following quotas: sex (50.9% female, 49.1% male), age (11.8% age</cell></row><row><cell cols="6">18-24, 17.5% age 25-34, 17.0% age 35-44, 15.8% 45-54, and 37.9% age 55 or above), and party affiliation</cell></row><row><cell cols="5">(29.Table B.1.1: Lucid Demographic Quotas</cell><cell></cell></row><row><cell>Sex</cell><cell cols="3">Age Race/Ethnicity Quota Sex</cell><cell cols="2">Age Race/Ethnicity Quota</cell></row><row><cell>Male</cell><cell>18-24 White</cell><cell>2.9%</cell><cell>Male</cell><cell>35-44 Black</cell><cell>1.2%</cell></row><row><cell cols="2">Female 18-24 White</cell><cell>3.0%</cell><cell cols="2">Female 35-44 Black</cell><cell>1.2%</cell></row><row><cell>Male</cell><cell>18-24 Hispanic</cell><cell>1.2%</cell><cell>Male</cell><cell>35-44 Other Race</cell><cell>0.6%</cell></row><row><cell cols="2">Female 18-24 Hispanic</cell><cell>1.3%</cell><cell cols="2">Female 35-44 Other Race</cell><cell>0.6%</cell></row><row><cell>Male</cell><cell>18-24 Black</cell><cell>0.8%</cell><cell>Male</cell><cell>45-54 White</cell><cell>4.2%</cell></row><row><cell cols="2">Female 18-24 Black</cell><cell>0.8%</cell><cell cols="2">Female 45-54 White</cell><cell>4.4%</cell></row><row><cell>Male</cell><cell>18-24 Other Race</cell><cell>0.5%</cell><cell>Male</cell><cell>45-54 Hispanic</cell><cell>1.5%</cell></row><row><cell cols="2">Female 18-24 Other Race</cell><cell>0.5%</cell><cell cols="2">Female 45-54 Hispanic</cell><cell>1.5%</cell></row><row><cell>Male</cell><cell>25-34 White</cell><cell>4.4%</cell><cell>Male</cell><cell>45-54 Black</cell><cell>1.0%</cell></row><row><cell cols="2">Female 25-34 White</cell><cell>4.5%</cell><cell cols="2">Female 45-54 Black</cell><cell>1.1%</cell></row><row><cell>Male</cell><cell>25-34 Hispanic</cell><cell>1.8%</cell><cell>Male</cell><cell>45-54 Other Race</cell><cell>0.6%</cell></row><row><cell cols="2">Female 25-34 Hispanic</cell><cell>1.9%</cell><cell cols="2">Female 45-54 Other Race</cell><cell>0.6%</cell></row><row><cell>Male</cell><cell>25-34 Black</cell><cell>1.2%</cell><cell>Male</cell><cell>55+ White</cell><cell>12.8%</cell></row><row><cell cols="2">Female 25-34 Black</cell><cell>1.3%</cell><cell cols="2">Female 55+ White</cell><cell>13.3%</cell></row><row><cell>Male</cell><cell>25-34 Other Race</cell><cell>0.7%</cell><cell>Male</cell><cell>55+ Hispanic</cell><cell>2.0%</cell></row><row><cell cols="2">Female 25-34 Other Race</cell><cell>0.8%</cell><cell cols="2">Female 55+ Hispanic</cell><cell>2.0%</cell></row><row><cell>Male</cell><cell>35-44 White</cell><cell>4.2%</cell><cell>Male</cell><cell>55+ Black</cell><cell>2.0%</cell></row><row><cell cols="2">Female 35-44 White</cell><cell>4.6%</cell><cell cols="2">Female 55+ Black</cell><cell>2.0%</cell></row><row><cell>Male</cell><cell>35-44 Hispanic</cell><cell>1.7%</cell><cell>Male</cell><cell>55+ Other Race</cell><cell>1.1%</cell></row><row><cell cols="2">Female 35-44 Hispanic</cell><cell>1.7%</cell><cell cols="2">Female 55+ Other Race</cell><cell>1.1%</cell></row><row><cell>Other</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5.3%</cell></row></table><note><p><p>Recruited panelists entered (separate) omnibus surveys hosted directly by the authors on the Qualtrics platform.</p>following preregistered quality checks: self-reported age and birth year do not correspond, within a tolerance of +/-2 years; self-reported state of residence and zip code do not match; speeding (completing the survey in less than 1/2 of the median time); scoring less than 0.65 on Qualtrics' internal reCaptcha measure; partially failing the pretreatment attention check by selecting either "B" or "D" but not both; or failing a second explicit pre-treatment attention check question about activities in the past 30 days (by self-reporting unlikely activities like purchasing an airline company, climbing a mountain on Mars, or having a fatal heart attack, or failing to self-report likely activities like eating a meal and using electricity). All of the screening and exclusion criteria were preregistered. The exclusions reduce the final analysis samples to n i = 4, 261 Prolific respondents and n i = 4, 869 Lucid participants. Appendix B.2 provides descriptive statistics for all samples. The observations are not weighted.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>Table B.2.1: Sample Characteristics by Vendor (Unweighted) Table reports unweighted percentages of respondents included in the final analysis samples.</figDesc><table><row><cell>Category Male Mean Age White Black Hispanic Multi-race Other Race or Ethnicity Less than high school degree High school diploma or equivalent Some college/Associate degree Bachelor's degree Postgraduate degree Less than $60,000 $60,000-$99,999 $100,000-$149,999 $150,000-$199,999 $200,000 or more Democrat Independent Republican Randomization Stage Respondent A AmeriSpeak Lucid 48% 46% 49.47 50.08 66.16% 63.13% 11.74% 13.70% 13.88% 8.95% 2.81% 7.01% 5.41% 7.21% 4.39% 4.87% 18.82% 29.83% 38.21% 33.99% 22.37% 20.44% 16.21% 10.87% 43.87% 64.46% 24.95% 20.94% 17.50% 8.97% 7.42% 3.14% 6.26% 2.49% 46.06% 44.48% 18.10% 16.31% 35.84% 39.21% Respondent B Prolific 48% 46.68 69.67% 12.91% 4.06% 6.13% 7.23% 0.77% 12.20% 33.11% 35.79% 18.12% 42.86% 27.06% 18.27% 6.83% 4.98% 49.43% 12.09% 38.48% Stage 1: Assign Post-only Clinic, Poverty Foreign, Clinic Stage 2: Assign Treatment Vector {0, 0, 0, 1, 1, 0} {1, 0, 1, 0, 0, 1} Stage 3: Order Pre-treat Block Drugs, Affirm, GMOs, Foreign Affirm, Poverty, GMOs, Drugs Stage 4: Select Procedure Full-random Forced-short Stage 5: Order Post-block Poverty, NFL-B, Clinic, GMOs, Drugs, GMOs, Foreign, NFL-A, Drugs, Affirm, Foreign, NFL-A Affirm, NFL-B, Poverty, Clinic Respondent Experience Drugs: Pre Affirm: Pre Affirm: Pre Poverty: Pre GMOs: Pre GMOs: Pre Foreign: Pre Drugs: Pre Poverty: Treatment &amp; Post Drugs: Control &amp; Post **NFL-B Drugs: Attitude Change *Clinic: Control &amp; Post *GMOs: Treatment &amp; Post Clinic: Covariate GMOs: Attitude Change *GMOs: Control &amp; Post Foreign: Treatment &amp; Post GMOs: Attitude Change Foreign: Attitude Change Drugs: Control &amp; Post **NFL-A Drugs: Attitude Change Affirm: Control &amp; Post Affirm: Treatment &amp; Post **NFL-B Foreign: Control &amp; Post Poverty: Control &amp; Post Foreign: Attitude change *Clinic: Treatment &amp; Post **NFL-A Clinic: Covariate Note: Table B.3.1: Example Randomization Outcomes Note:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>Which of the following describes your total annual household income from 2023-that is, the total income everyone living in your household made together, before taxes, in 2023?• Less than $20,000• $20,000 to $39,999 • $40,000 to $59,999 • $60,000 to $79,999 • $80,000 to $99,999 • $100,000 to $119,999 • $120,000 to $149,999 • $150,000 to $199,999 • $200,000 or more Year Born: In what year were you born? Please enter a 4-digit number. [Open-ended] Zip Code: In which ZIP code do you currently reside? Please enter a 5-digit number. [Open-ended] Attention Check 2: Which of the following have you done in the past 30 days? Please check all that apply.</figDesc><table><row><cell>• Unemployed</cell></row><row><cell>• Retired</cell></row><row><cell>• Full-time homemaker</cell></row><row><cell>• Student</cell></row><row><cell>• Something else</cell></row><row><cell>Household Income: • Eaten a meal</cell></row><row><cell>• Purchased an airline company</cell></row><row><cell>• Read a book</cell></row><row><cell>• Climbed the Olympus Mons</cell></row><row><cell>• Had a fatal heart attack</cell></row><row><cell>• Used electricity</cell></row><row><cell>B.4.2 Experimental Content</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>• Slightly increased • Kept about the same • Slightly decreased • Greatly decreased Foreign Aid Treatment (1 TESS unit): "Spending on foreign aid currently makes up about 1% of the federal budget. Do you think federal spending on foreign aid should be increased or decreased?" Drug Imports Treatment (1 TESS unit): "Democrats tend to favor and Republicans tend to oppose allowing individuals to import prescription drugs from Canada. Do you support or oppose this policy?" Anti-GMO Control (2 TESS units): "As you may know, opponents of genetically modified foods point out that there have not been studies on the long-term health effects of genetically modified foods on humans. And a recent study on animals found that genetically modified potatoes damaged the digestive tracts of rats. How strongly do you favor or oppose the production and consumption of genetically modified foods?" Pro-GMO Treatment (2 TESS units): "As you may know, supporters of genetically modified foods point out that a recent study on genetically modified foods found that a type of rice ("golden rice") can be produced with a high content of vitamin A, which is used to prevent blindness. How strongly do you favor or oppose the production and consumption of genetically modified foods?"Recall Previous Attitude (1 TESS unit): "As you may remember, we also asked you about your support or opposition to [foreign aid / importing subscription drugs from Canada / genetically modified foods (GMOs)] earlier in the survey. To the best of your memory, how have your preferences about [foreign aid / importing subscription drugs from Canada / genetically modified foods (GMOs)] changed since then?"• Much more supportive • A little more supportive • Stayed about the same • A little more opposed • Much more opposed Anti-poverty (Study 4) Welfare (1 TESS unit): "Generally speaking, do you think we're spending too much, too little or about the right amount on welfare?" • Too much • About the right amount • Too little Assistance to the Poor (1 TESS unit): "Generally speaking, do you think we're spending too much, too little or about the right amount on assistance to the poor?" • Too much • About the right amount • Too little B.4.3 Post-Experimental Content This content was included on our surveys following the experimental content.</figDesc><table><row><cell>• Strongly favor</cell></row><row><cell>• Favor</cell></row><row><cell>• Slightly favor</cell></row><row><cell>• Greatly increased • Neither favor nor oppose</cell></row><row><cell>• Slightly increased • Slightly oppose</cell></row><row><cell>• Kept about the same • Oppose</cell></row><row><cell>• Slightly decreased • Strongly oppose</cell></row><row><cell>• Greatly decreased</cell></row><row><cell>Drug Imports (Study 2)</cell></row><row><cell>Drug Imports Pretreatment/Control (1 TESS unit): "Do you support or oppose allowing indi-</cell></row><row><cell>viduals to import prescription drugs from Canada?" • Strongly favor • Strong support • Favor • Somewhat support • Slightly favor • Slightly support • Neither favor nor oppose • Neither support nor oppose • Slightly oppose • Slightly oppose • Oppose • Somewhat oppose • Strongly oppose • Strongly oppose</cell></row><row><cell>Perceived Attitude Change (Studies 1-3 Only)</cell></row><row><cell>• Strong support</cell></row><row><cell>• Somewhat support</cell></row><row><cell>• Slightly support</cell></row><row><cell>• Neither support nor oppose</cell></row><row><cell>• Slightly oppose</cell></row><row><cell>• Somewhat oppose</cell></row><row><cell>• Strongly oppose</cell></row><row><cell>GMOs (Study 3)</cell></row><row><cell>GMO Pretreatment (1 TESS unit): "How strongly do you favor or oppose the production and</cell></row><row><cell>consumption of genetically modified foods?"</cell></row><row><cell>• Strongly favor</cell></row><row><cell>• Favor</cell></row><row><cell>• Slightly favor</cell></row><row><cell>• Neither favor nor oppose</cell></row><row><cell>• Slightly oppose</cell></row><row><cell>• Oppose</cell></row><row><cell>• Strongly oppose</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>This research was approved by the Institutional Review Board of [REDACTED] under protocol[REDACTED]. We further affirm that this research adheres to the American Political Science Association's Principles and Guidance for Human Subjects Research.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>That is, while repeated measure designs are appropriate and even advantageous for studying small treatment effects because of increased power, we opted for studies with more powerful treatments to minimize risk that floor effects could cause a false negative in our estimation of design effects.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>Studies 1-4 described here correspond to studies 2,</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3"><p>5, 6, and 1 in CSP, respectively.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_4"><p>NORC discourages attention checks, out of concern that AmeriSpeak respondents are unaccustomed to the practice and may discontinue participation, so we preregistered a more limited procedure for dropping AmeriSpeak respondents (see Appendix B.2). We assume that retained AmeriSpeak respondents are sufficiently high quality, given the quality metrics we have (e.g., completion times) and NORC's rigorous recruitment and management for AmeriSpeak panelists.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_5"><p>For Study 2, we interact the treatment indicator with an indicator for Democratic party identification. The coefficient of interest is on the interaction term. We exclude respondents who do not lean toward either party from this analysis.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_6"><p><ref type="bibr" target="#b4">Blair et al. (2019)</ref> show that, relative to using difference scores as the outcome, adjusting for the pre-treatment measure reduces noise when adjustment bias is minimal, as in randomized experiments<ref type="bibr" target="#b36">(Lin 2013)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_7"><p>Note that observations for within-subject repeated measure models reflect two observations per repeated measures respondent and one observation per post-only respondent (less non-response).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_8"><p>This is similar to the approach taken by<ref type="bibr" target="#b49">Sheagley and Clifford (2025)</ref> and is primarily intended to ease interpretation of the resulting analysis.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_9"><p>The meta-analysis of between-groups experiments in the AmeriSpeak sample is the slight exception here, which detects a design effect significant at the 0.10 level (estimate -0.259, p = 0.088).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_10"><p>Although we observe distances up to 20 items between repeated measures, we have very few observations at the largest possible distance for each experiment (between 4 and 52 observations per experiment) and therefore exclude these imprecise estimates from our analysis.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_11"><p>These analyses deviate from our preanalysis plan of using spline regressions to assess non-linear effects of distance on the design effect. We estimated spline regressions (interacting the treatment indicator with indicator variables for each discrete TESS distance observed) for each experiment in each sample, and found few significant interactions at all and no consistent pattern across the studies-that is, no clear evidence of non-linearity, as the point estimates in Figure3also suggest. We therefore opted for this alternate analysis for ease of presentation and interpretation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16" xml:id="foot_12"><p>This estimated effect is sufficiently small that it may best be considered negligible<ref type="bibr" target="#b45">(Rainey 2014)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17" xml:id="foot_13"><p>For these analyses, we preregistered excluding respondents who reported completing more than 1,000 surveys in the past 30 days or over 100 active panel memberships, as these responses are likely not genuine.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="18" xml:id="foot_14"><p>We suggest that failure to recognize attitude change may also indicate a ceiling on design effects from priming and even demand pressures, as it signals fuzziness about the pre-treatment question itself.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="21" xml:id="foot_15"><p>Simulation details are provided in Appendix A.4. We assume a correlation between repeated measures of 0.8 and a corresponding reduction in the ATE standard error of 0.4, which is slightly conservative relative to the reductions we observe in our actual experiments.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="22" xml:id="foot_16"><p>Appendix A.4 also examines the false discovery rate and coverage rate for the true ATE. When the true ATE is zero, the two designs perform similarly on both metrics. When the true ATE is non-zero, repeated measure designs have a lower false discovery rate for smaller samples and true effect sizes, while the coverage rate drops as the sample, true ATE, and degree of attenuation increase.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="23" xml:id="foot_17"><p>We note that CSP also acknowledge this potential exception to their general recommendation(2021, 1062).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="24" xml:id="foot_18"><p>That said, recent scholarship suggests that the sensitivity of items for the respondent is often overestimated<ref type="bibr" target="#b31">(Kramon and Weghorst 2019)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="26" xml:id="foot_19"><p>Note that our surveys are somewhat short overall-between 5 and 10 minutes for a majority of respondents. Our results cannot speak to the relative design effects of placing measures far apart on much longer surveys, or on separate surveys completed days or weeks apart.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="27" xml:id="foot_20"><p>See also<ref type="bibr" target="#b49">Sheagley and Clifford (2025)</ref> for similar findings regarding the placement of moderators.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="28" xml:id="foot_21"><p>We caution that our interpretation of the center-top and center-bottom cells as "inaccurate" is on less firm ground, in that a respondent's views may have shifted slightly but not by enough to merit a change in response on a coarse close-ended scale.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="30" xml:id="foot_22"><p>The design effect (relative to the post-only design) is statistically significant more frequently among recall-accurate respondents, but this is primarily a function of power, as recall-accurate respondents outnumber recall-inaccurate respondents by more than two to one.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="31" xml:id="foot_23"><p>Note that we simulate two-arm experiments, meaning our simulated sample sizes correspond to 100 to 1,000 respondents per cell in expectation, in increments of 100.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="32" xml:id="foot_24"><p>Note that the mean observed attenuation in our fielded experiments is around 20 percent; the 10 percent and 30 percent attenuation rates reflect the possibility that our observed rate may be an over-estimate or under-estimate, respectively, in some experimental settings.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments: This research was generously supported by the <rs type="funder">Time-sharing Experiments for the Social Sciences (TESS)</rs> as part of a Special Competition on Replications, by the <rs type="funder">Rapoport Family Foundation</rs>, and by <rs type="funder">Bass Connections at Duke University</rs>. The authors thank <rs type="institution">NORC</rs> staff for their assistance with data collection, particularly <rs type="person">Dan Costanzo</rs> and <rs type="person">Alyssa Kahle</rs>. The authors thank <rs type="person">Scott Clifford</rs>, <rs type="person">Gustavo Diaz</rs>, <rs type="person">Don Green</rs>, <rs type="person">Jon Green</rs>, <rs type="person">Sunshine Hillygus</rs>, <rs type="person">Christopher Johnston</rs>, <rs type="person">Molly Offer-Westort</rs>, <rs type="person">Carlisle Rainey</rs>, <rs type="person">Geoff Sheagley</rs>, <rs type="person">Mallory SoRelle</rs>, <rs type="person">Anton Strezhnev</rs>, <rs type="person">Matthew Tyler</rs>, and two anonymous TESS reviewers, as well as participants at PolMeth 2024, TexMeth <rs type="grantNumber">2025</rs>, <rs type="grantNumber">MPSA 2025</rs>, <rs type="grantNumber">APSA 2025</rs>, the <rs type="institution">UT Austin Junior Methodologist Workshop</rs>, the <rs type="institution">Worldview Lab at Duke University</rs>, and the <rs type="institution">Graduate Research Workshop at the Sanford School of Public Policy</rs> for helpful comments that improved the research. Authorship for this work is in alphabetical order.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_PQ3bXa8">
					<idno type="grant-number">2025</idno>
				</org>
				<org type="funding" xml:id="_rgxvGdz">
					<idno type="grant-number">MPSA 2025</idno>
				</org>
				<org type="funding" xml:id="_ZSucUkr">
					<idno type="grant-number">APSA 2025</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Respondent Professionalization</head><p>In Figure <ref type="bibr">A.2</ref>.1, we report the results of internal meta-analyses of design effects conditional on degree of respondent professionalization (above or below within-sample median). These results are also provided in tabular format in Table <ref type="table">A</ref>.2.1. We operationalize respondent professionalization two ways, using the self-reported counts of surveys completed in the past 30 days (survey count) or the self-reported number of survey companies the respondent has completed surveys for in the past 30 days (panel memberships). We observe no substantive differences between respondents who are more or less professionalized in each sample; the estimated design effects are uniformly negative (from -0.009 to -0.043) and rarely differ from each other significantly.  A.5 Evidence of the Influence of <ref type="bibr" target="#b10">Clifford, Sheagley, and Piston (2021)</ref> In the first four years since publication (through April 2025), <ref type="bibr" target="#b10">Clifford, Sheagley, and Piston (2021)</ref> has garnered citations from 118 original peer-reviewed published studies spanning 83 different journals.</p><p>Of the 118, 83 are original studies referencing CSP to justify using repeated measure designs. In Table <ref type="table">A</ref>.5.1 below, the pre-post designs have bolded titles and within-subject designs do not. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Study Information</head><p>The study was approved by [REDACTED UNIVERSITY]'s Institutional Review Board under protocol <ref type="bibr">[REDACTED]</ref>. Anonymized preregistration materials for this study are available here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Sampling Procedure</head><p>The data for this study come from a three omnibus surveys of the U.S. general adult population (combined N i = 13, 163) recruited from three vendors. We describe the sampling procedure for each sample in turn.</p><p>The first sample (n i = 4, 029) was drawn from the probability-based AmeriSpeak panel. This component of the study was funded by the National Science Foundation via the Time-Sharing Experiments for the Social Sciences (TESS) maintained by the University of Rochester. The AmeriSpeak panel, operated by NORC, is a probability-based panel designed to be representative of the US household population.</p><p>Randomly selected US households are sampled using area probability and address-based sampling, with a known, non-zero probability of selection from the NORC National Sample Frame. These sampled households are then contacted by US mail, telephone, and field interviewers (face to face). The panel provides sample coverage of approximately 97 percent of the U.S. household population. Those excluded from the sample include people with P.O. Box only addresses, some addresses not listed in the USPS Delivery Sequence File, and some newly constructed dwellings. While most AmeriSpeak allows noninternet households can participate in AmeriSpeak surveys by telephone, this option was not included for this study; this study was also available only in English. Households without conventional internet access but having web access via smartphones are allowed to participate in AmeriSpeak surveys by web.</p><p>More information about the panel and sampling design is available at AmeriSpeak.norc.edu.</p><p>For this study, NORC invited consented AmeriSpeak panelists to participate in the omnibus survey hosted directly by the authors on the Qualtrics platform. This survey was fielded from June 27 th to July 15 th , 2024. NORC invited 19,024 total panelists to participate, sending email reminders 3 days after initial invitation and every 5 days thereafter, plus a final email reminder on July 9 th . The survey completion rate among invited participants was 21.2 percent. The weighted cumulative response rate (which accounts for panel recruitment, panel retention, and survey completion) is 3.7 percent. AmeriSpeak panelists were offered the cash equivalent of $2.00 for completing the survey. The median</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Randomization Procedure</head><p>We randomized design conditions, treatment conditions, and the order of the experimental content for each respondent with a complex, multi-stage randomization procedure. In the first randomization stage, two of the six experiments (at the respondent level) were randomly assigned post-only designs, with the remaining four experiments assigned to repeated measure designs. In the second stage, we randomized assignment to treatment or control stimuli for each experiment. For the within-subject experiments, this assignment dictated which wording appeared first and which appeared second (if in the repeated measure condition). In the third stage, we randomly ordered the "pre-treatment" questions (or first wordings) for the four repeated measure experiments. These four questions (the "pre-treatment" block) were displayed to respondents sequentially.</p><p>All remaining experimental content was randomized as part of the "post-treatment" block. This block included eight sub-blocks: a treatment/control stimulus and immediate post-treatment measurement for each of the six experiments, plus six unrelated questions about attitudes regarding the National Football League (NFL) that were split into two sub-blocks of three questions each. The sub-blocks for the three between-groups experiments additionally included a question about perceived change in attitude (only if assigned to the repeated measures design), and the sub-block for Study 6 additionally included a post-treatment covariate measure about personal exposure to opioid addiction.</p><p>In the fourth randomization stage, we randomly assigned each respondent to one of two orderrandomization procedures for the overall post-treatment block: either a "full-random" or a "forcedshort" procedure, which was then executed as the final fifth stage. In the full-random procedure, all sub-blocks in the post-treatment block were displayed to respondents in a random order. In the forcedshort procedure, the sub-blocks for the two experiments whose pre-treatment content appeared last (that is, the third and fourth pre-treatment items) were forced to appear immediately following the pretreatment block, either in the same order or the inverse, with equal probability. All other sub-blocks were randomly ordered and were displayed to respondents subsequently in that order. This alternate procedure ensured that more repeated measure experiments appeared close together than was likely if we fully randomized the order of the sub-blocks.</p><p>To illustrate this complex randomization design, Table <ref type="table">B</ref>.3.1 shows the realized randomization outcomes and respondent experience for two hypothetical respondents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Survey Questionnaire</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4.1 Screening and Demographics</head><p>This content was included prior to the experimental content in the Prolific and Lucid surveys only. This content was not included on the AmeriSpeak survey. Opioid Clinic Personal Exposure (1 TESS unit): "Do you personally know anyone who has ever been addicted to opioids, including prescription painkillers or heroin?"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Screening</head><p>• Yes, I personally know someone who has been addicted to opioids (such as a family member, a friend, an acquaintance, or myself) • No, I do not know anyone who has ever been addicted to opioids Unrelated Items (Distractor Content) NFL Block 1 (3 TESS units): "We're interested in what people do in their spare time. How much attention would you say you pay to football games in the National Football League (NFL)?"</p><p>• A lot • Some • None "Without consulting any sources, do you happen to know if any of the following slogans are associated with the NFL? It's OK if you don't know or aren't sure, just tell us that."</p><p>• "Intercept Cancer"</p><p>• "End Racism"</p><p>• "Inspire Change"</p><p>• "Salute to Service"</p><p>• "End Concussions"</p><p>• "It Takes All of Us"</p><p>• "Play It Safe" "Should the NFL encourage people to do any of the following things?"</p><p>• Register to vote in upcoming elections </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Real, but Limited: A Meta-analytic Assessment of Framing Effects in the Political Domain</title>
		<author>
			<persName><forename type="first">Eran</forename><surname>Amsalem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alon</forename><surname>Zoizner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British Journal of Political Science</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="237" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><surname>Arel-Bundock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">C</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hristos</forename><surname>Briggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Doucouliagos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">D</forename><surname>Mendoza Aviña</surname></persName>
		</author>
		<author>
			<persName><surname>Stanley</surname></persName>
		</author>
		<ptr target="https://osf.io/7vy2f" />
		<title level="m">Quantitative Political Science Research Is Greatly Underpowered. OSF Preprints</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Combining List Experiment and Direct Question Estimates of Sensitive Behavior Prevalence</title>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">M</forename><surname>Aronow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Coppock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Forrest</forename><forename type="middle">W</forename><surname>Crawford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><forename type="middle">P</forename><surname>Green</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Survey Statistics and Methodology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="66" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Using screeners to measure respondent attention on self-administered surveys: Which items and how many?</title>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">J</forename><surname>Berinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Michele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Margolis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Sances</surname></persName>
		</author>
		<author>
			<persName><surname>Warshaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political Science Research and Methods</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="430" to="437" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Use change scores or control for pre-treatment outcomes? Depends on the true data generating process</title>
		<author>
			<persName><forename type="first">Graeme</forename><surname>Blair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasper</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Coppock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Macartan</forename><surname>Humphreys</surname></persName>
		</author>
		<ptr target="https://declaredesign.org/blog/posts/use-change-scores-or-control.html" />
	</analytic>
	<monogr>
		<title level="j">DeclareDesign</title>
		<imprint>
			<date type="published" when="2019-01-15">2019. January 15. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Minimum Detectable Effects: A Simple Way to Report the Statistical Power of Experimental Designs</title>
		<author>
			<persName><forename type="first">Howard</forename><forename type="middle">S</forename><surname>Bloom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evaluation Review</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="547" to="556" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Research on Interviewing Techniques</title>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">F</forename><surname>Cannell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lois</forename><surname>Oksenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sociological Methodology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="389" to="437" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">National Surveys Via RDD Telephone Interviewing Versus the Internet: Comparing Sample Representativeness and Response Quality</title>
		<author>
			<persName><forename type="first">Linchiat</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><forename type="middle">A</forename><surname>Krosnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Public Opinion Quarterly</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="641" to="678" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Experimental Methods: Between-Subject and Within-subject Design</title>
		<author>
			<persName><forename type="first">Gary</forename><surname>Charness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Gneezy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Economic Behavior &amp; Organization</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Preference for Consistency: The Development of a Valid Measure and the Discovery of Surprising Behavioral Implications</title>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">B</forename><surname>Cialdini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><forename type="middle">R</forename><surname>Trost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">T</forename><surname>Newsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">318</biblScope>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improving Precision without Altering Treatment Effects: Repeated Measure Designs in Survey Experiments</title>
		<author>
			<persName><forename type="first">Scott</forename><surname>Clifford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Sheagley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Spencer</forename><surname>Piston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Political Science Review</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1048" to="1065" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The Limits (and Strengths) of Single-Topic Experiments</title>
		<author>
			<persName><forename type="first">Scott</forename><surname>Clifford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlisle</forename><surname>Rainey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political Analysis</title>
		<imprint>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generalizing Survey Experiments Using Topic Sampling: An Application to Party Cues</title>
		<author>
			<persName><surname>Clifford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">J</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlisle</forename><surname>Leeper</surname></persName>
		</author>
		<author>
			<persName><surname>Rainey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political Behavior</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1233" to="1256" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generalizability of Heterogeneous Treatment Effect Estimates Across Samples</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Coppock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Leeper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Mullinix</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="12441" to="12446" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A biometrics invited paper with discussion. some aspects of analysis of covariance</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Mccullagh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="page" from="541" to="561" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Concentrated Burdens: How Self-Interest and Partisanship Shape Opinion on Opioid Treatment Policy</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>De Benedictis-Kessner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Hankinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Political Science Review</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1078" to="1084" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Measuring Issue Preferences: The Problem of Response Instability</title>
		<author>
			<persName><forename type="first">Stanley</forename><surname>Feldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political Analysis</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="60" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Evaluating Effect Size in Psychological Research: Sense and Nonsense</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">C</forename><surname>Funder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><surname>Ozer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Methods and Practices</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="156" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A Tale of Two Countries: The Effectiveness of List Experiments to Measure Drug Consumption in Opposite Contexts</title>
		<author>
			<persName><forename type="first">Miguel</forename><surname>García-Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosario</forename><surname>Queirolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Public Opinion Research</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="255" to="272" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Beyond Power Calculations: Assessing Type S (Sign) and Type M (Magnitude) Errors</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Carlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perspectives on Psychological Science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="641" to="651" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Testing for Publication Bias in Political Science</title>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">S</forename><surname>Gerber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><forename type="middle">P</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Nickerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political Analysis</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="385" to="392" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Effect Size Guidelines for Individual Differences Researchers</title>
		<author>
			<persName><forename type="first">Gilles</forename><forename type="middle">E</forename><surname>Gignac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eva</surname></persName>
		</author>
		<author>
			<persName><surname>Szodorai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Personality and Individual Differences</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="74" to="78" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Political Ignorance and Collective Policy Preferences</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Gilens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Political Science Review</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="379" to="396" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Diagnosing Survey Response Quality</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hillygus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Sunshine</surname></persName>
		</author>
		<author>
			<persName><surname>Lachapelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook on Politics and Public Opinion</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Professional Respondents in Nonprobability Online Panels</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hillygus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalie</forename><surname>Sunshine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mckenzie</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Online Panel Research: Data Quality Perspective</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="219" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">FC. Designing Survey Experiments</title>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">H</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Experimental Methodology</title>
		<imprint>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
	<note>Leeat Yariv and Erik Snowberg</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">How Effective is Nudging? A Quantitative Review on the Effect Sizes and Limits of Empirical Nudging Studies</title>
		<author>
			<persName><forename type="first">Dennis</forename><surname>Hummel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Maedche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Behavioral and Experimental Economics</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="47" to="58" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The Power of Bias in Economics Research</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">P A</forename><surname>Ioannidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hristos</forename><surname>Doucouliagos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Economic Journal</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">605</biblScope>
			<biblScope unit="page" from="236" to="F265" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Are Nonprobability Surveys Fit for Purpose?</title>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Jerit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Barabas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Public Opinion Quarterly</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="816" to="840" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Courtney</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Keeter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Hatley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyley</forename><surname>Mcgeeney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alejandra</forename><surname>Gimenez</surname></persName>
		</author>
		<ptr target="https://www.pewresearch.org/methods/2016/05/02/evaluating-online-nonprobability-surveys/" />
		<title level="m">Evaluating Online Nonprobability Surveys. Pew Research Center</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Studying Identities with Experiments: Weighing the Risk of Posttreatment Bias Against Priming Effects</title>
		<author>
			<persName><forename type="first">Samara</forename><surname>Klar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Leeper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Robison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Political Science</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="56" to="60" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">(Mis) measuring sensitive attitudes with the list experiment: Solutions to list experiment breakdown in Kenya</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Kramon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><surname>Weghorst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Public Opinion Quarterly</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">S1</biblScope>
			<biblScope unit="page" from="236" to="263" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cross-Sample Comparisons and External Validity</title>
		<author>
			<persName><forename type="first">Yanna</forename><surname>Krupnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Seth Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Political Science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="80" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Publication Bias in Psychology: A Diagnosis Based on the Correlation Between Effect Size and Sample Size</title>
		<author>
			<persName><forename type="first">Anton</forename><surname>Kühberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Astrid</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Scherndl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS One</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">105825</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Investigating Interviewer Effects and Confounds in Survey-Based Experimentation</title>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">J</forename><surname>Lavrakas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenny</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colleen</forename><surname>Mcclain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Experimental Methods in Survey Research: Techniques that Combine Random Sampling with Random Assignment</title>
		<editor>
			<persName><forename type="first">Paul</forename><surname>Lavrakas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michael</forename><surname>Traugott</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Courtney</forename><surname>Kennedy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Allyson</forename><surname>Holbrook</surname></persName>
		</editor>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Accounting for the Effects of Accountability</title>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">S</forename><surname>Lerner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">E</forename><surname>Tetlock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">255</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Agnostic Notes on Regression Adjustment to Experimental Data: Re-examining Freedman&apos;s Critique</title>
		<author>
			<persName><forename type="first">Winston</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="295" to="318" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The Experimentalist Looks Within: Toward an Understanding of Within-Subject Experimental Designs</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">A</forename><surname>List</surname></persName>
		</author>
		<ptr target="https://www.nber.org/papers/w33456" />
	</analytic>
	<monogr>
		<title level="j">NBER</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Measurement Error and the Replication Crisis</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Loken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Gelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">355</biblScope>
			<biblScope unit="issue">6325</biblScope>
			<biblScope unit="page" from="584" to="585" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The Accuracy of Measurements with Probability and Nonprobability Survey Samples: Replication and Extension</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Macinnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><forename type="middle">A</forename><surname>Krosnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Annabell</forename><forename type="middle">S</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu-Jung</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Public Opinion Quarterly</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="707" to="744" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Worth Weighting? How to Think About and Use Weights in Survey Experiments</title>
		<author>
			<persName><forename type="first">Luke</forename><forename type="middle">W</forename><surname>Miratrix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jasjeet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Sekhon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><forename type="middle">F</forename><surname>Theodoridis</surname></persName>
		</author>
		<author>
			<persName><surname>Campos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political Analysis</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="275" to="291" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Demand Effects in Survey Experiments: An Empirical Assessment</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Mummolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Peterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Political Science Review</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="517" to="529" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Population-Based Survey Experiments</title>
		<author>
			<persName><forename type="first">Diana</forename><forename type="middle">C</forename><surname>Mutz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Princeton University Press</publisher>
			<pubPlace>Princeton</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Estimating the Reproducability of Psychological Science</title>
		<author>
			<orgName type="collaboration">Open Science Collaboration</orgName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">349</biblScope>
			<biblScope unit="issue">6251</biblScope>
			<biblScope unit="page">4716</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Why Most Experiments in Psychology Failed: Sample Sizes Required for Randomization to Generate Equivalent Groups as a Partial Solution to the Replication Crisis</title>
		<author>
			<persName><forename type="first">Gjalt-Jorn</forename><surname>Peters</surname></persName>
		</author>
		<ptr target="osf.io/preprints/38vfn" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Arguing for a Neglible Effect</title>
		<author>
			<persName><forename type="first">Carlisle</forename><surname>Rainey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Political Science</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="773" to="1091" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Power Rules: Practical Statistical Power Calculations</title>
		<author>
			<persName><forename type="first">Carlisle</forename><surname>Rainey</surname></persName>
		</author>
		<idno>OSF Preprints</idno>
		<ptr target="https://osf.io/preprints/osf/5am9q" />
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Voters, Emotions, and Race in 2008: Obama as the First Black President</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">P</forename><surname>Redlawsk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caroline</forename><forename type="middle">J</forename><surname>Tolbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Franko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political Research Quarterly</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="875" to="889" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">College Sophomores in the Laboratory: Influences of a Narrow Data Base on Social Psychology&apos;s View of Human Nature</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">O</forename><surname>Sears</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="515" to="530" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">No Evidence that Measuring Moderators Alters Treatment Effects</title>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Sheagley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Clifford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Political Science</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="63" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">That Which We Call Welfare Would Smell Sweeter: An Analysis of the Impact of Question Wording on Response Patterns</title>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">W</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Public Opinion Quarterly</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="75" to="83" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">N</forename><surname>Stagnaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Druckman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">J</forename><surname>Berinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><forename type="middle">A</forename><surname>Arechar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robb</forename><surname>Willer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">G</forename><surname>Rand</surname></persName>
		</author>
		<title level="m">Representativeness versus Response Quality: Assessing Nine Opt-In Online Survey Samples</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Cognitive Processes Underlying Context Effects in Attitude Measurement</title>
		<author>
			<persName><forename type="first">Roger</forename><surname>Tourangeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><forename type="middle">A</forename><surname>Rasinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">299</biblScope>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Prevalence and Prevention of Large Language Model Use in Crowd Work</title>
		<author>
			<persName><forename type="first">Veniamin</forename><surname>Veselovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manoel</forename><surname>Horta Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Cozzolino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Rothschild</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>West</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/2310.15683" />
	</analytic>
	<monogr>
		<title level="j">ArXiv ArXiv preprint</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Artificial Artificial Artificial Intelligence: Crowd Workers Widely Use Large Language Models for Text Production Tasks</title>
		<author>
			<persName><forename type="first">Veniamin</forename><surname>Veselovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manoel</forename><surname>Horta Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>West</surname></persName>
		</author>
		<ptr target="https://arxiv.org/html/2306.07899" />
	</analytic>
	<monogr>
		<title level="j">ArXiv ArXiv preprint</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Self-reported Alcohol Consumption and Sexual Behavior in Males and Females: Using the Unmatched-Count Technique to Examine Reporting Practices of Socially Sensitive Subjects in a Sample of University Students</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">A</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Braithwaite</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Alcohol and Drug Education</title>
		<imprint>
			<biblScope unit="page" from="49" to="72" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Fact-Checking: A Meta-analysis of What Works and for Whom</title>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Lance</forename><surname>Holbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasmin</forename><surname>Morag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political Communication</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="350" to="375" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Affirmative Action Programs for Women and Minorities: Expressed Support Affected by Question Order</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">R</forename><surname>Mckay</surname></persName>
		</author>
		<author>
			<persName><surname>Avery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Public Opinion Quarterly</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="514" to="522" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A Simple Theory of the Survey Response: Answering Questions versus Revealing Preferences</title>
		<author>
			<persName><forename type="first">John</forename><surname>Zaller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanley</forename><surname>Feldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Political Science</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="579" to="616" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Experimenter Demand Effects in Economic Experiments</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Zizzo</surname></persName>
		</author>
		<author>
			<persName><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Experimental Economics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="75" to="98" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
