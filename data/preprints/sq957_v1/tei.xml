<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Human Enough to Be Kind A Study Measuring Human Preferences for Robotic Acts of Kindness</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Benjamin</forename><forename type="middle">G</forename><surname>Goodhart</surname></persName>
							<email>benjamingoodhart@gmail.com</email>
						</author>
						<title level="a" type="main">Human Enough to Be Kind A Study Measuring Human Preferences for Robotic Acts of Kindness</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8C36631EACEFBE69BEABC9812B674FA9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-10-21T13:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Human-robot interaction</term>
					<term>kindness</term>
					<term>prosocial behavior</term>
					<term>robot design</term>
					<term>user preferences</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This study develops one of the first instruments to assess human preferences for concrete acts of kindness performed by social robots. In Phase 1 (N = 101; ~1,000 responses), participants completed digital forced-choice surveys in which they compared 65 robot-to-human kindness scenarios drawn from four categories-Emotional Support, Practical Help, Social Awareness, and Family &amp; Child Support-or ranked actions within a single category. Phase 2 (N = 918; ~9,086 responses; U.S.-only) replicated all scenarios and added four extensions: (a) a human-to-human comparison condition for every action, (b) Likert ratings (1-7) of perceived kindness alongside preferences, (c) random assignment to one of four between-subject survey versions (Robot→Human Preference; Robot→Human Likert; Human→Human Preference; Human→Human Likert), and (d) gender demographics. Across phases, results show that choices are not statistically random; clear patterns emerge at both the category and scenario levels. Phase 2 reveals robust actor effects (robots vs. humans), category-level differences, and systematic convergences and divergences between perceived kindness and choice. Together, these findings refine how kindness is evaluated when enacted by robots versus humans and surface design targets for emotionally intelligent, user-centered robots.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>We are on the brink of a world where humanoid robots will be all around us-integrated into homes, workplaces, and public spaces. According to a recent report by Morgan Stanley Research, the number of humanoid robots is projected to reach nearly 1 billion globally by 2050, with rapid acceleration in adoption expected during the late 2030s and 2040s <ref type="bibr" target="#b1">[1]</ref>. While most of these robots will initially be deployed in industrial and commercial roles, tens of millions are expected to become a fixture in everyday household life as technology matures and costs decline.</p><p>As this transformation unfolds, designing robots that can enact kindness will not only foster positive emotional connections but also ensure that these technologies support human well-being and societal cohesion. The way robots act toward humans-especially in moments requiring social awareness, empathy, and practical support-will play a critical role in shaping acceptance, trust, and the quality of human-robot relationships.</p><p>While much existing work focuses on robot capabilities <ref type="bibr" target="#b4">[4,</ref><ref type="bibr">5]</ref>, there is growing research on the actions people prefer from robots in everyday contexts. Kiesler and Forlizzi, along with DiSalvo and Gemperle <ref type="bibr" target="#b5">[6]</ref>, investigated how the physical design of robot heads influences perceived humanness. They showed that the presence and arrangement of features such as eyes, mouths, and eyelids account for a substantial proportion of how human-like a robot appears. Their findings underscore the importance of visual and morphological design in creating robots that people perceive as credible social partners and highlight the delicate balance between making robots appear human enough to support social interaction without crossing into discomfort or the uncanny valley.</p><p>Beyond physical appearance, Malle and Thapa Magar <ref type="bibr" target="#b2">[2]</ref> explored the mental capacities people desire in robots, finding consistent preferences for agency-related skills such as logical reasoning and moral deliberation, paired with ambivalence toward emotional capabilities. Building on this work, Nääs, Thellman, and Ziemke <ref type="bibr" target="#b11">[12]</ref> demonstrated that preferences for cognitive and emotional capabilities vary by role. For example, while participants generally valued high-agency capabilities-like planning and understanding goals-they expressed stronger desire for experience-related abilities (e.g., feeling happiness) in social companionship robots than in household cleaning robots. Qualitative analyses further revealed that although many people preferred robots to function objectively and logically, some still wanted robots to recognize and respond to human emotions without actually experiencing them. These nuanced attitudes suggest that robot capabilities should be tailored to application domain and user expectation.</p><p>Haring et al. <ref type="bibr">[7]</ref> further emphasized alignment: a robot's visual appearance strongly shapes people's initial perceptions and expectations about its behavior. Their comparison of an android, a humanoid, and a non-biomimetic robot showed that mismatches between how a robot looks and how it acts can trigger negative reactions and rejection. Effective design requires careful alignment of appearance, abilities, and intended use to enable intuitive interaction. In parallel, Wallach and Allen <ref type="bibr" target="#b7">[8]</ref> argue that as robots assume more responsibility, they must be programmed with moral decision-making abilities for our safety. Despite these advances, most research has focused on high-level capacity preferences <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b10">11]</ref> or isolated design features <ref type="bibr" target="#b5">[6]</ref>, rather than systematically cataloging specific, concrete prosocial acts robots can perform in everyday life. Our central questions are: What types of "kind" acts do people prefer from robots-and when does actor identity (robot vs. human) change that preference? And does the perceived kindness rating (Likert) predict the probability that an action is preferred? In Phase 1, we operationalized preference as a forced choice between two robot actions and documented patterned choices across categories and scenarios. Phase 2 extends the instrument in three ways: it adds a human-to-human comparison for every scenario, collects Likert ratings of perceived kindness alongside preferences, and introduces random assignment and gender demographics within a U.S.-only sample to minimize cultural variance. This extension allows us to ask not just what people prefer, but when actor identity matters, how perceived kindness aligns with choice, and where category-level differences emerge.</p><p>Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author. Copyright is held by the owner/author(s).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Participants</head><p>Phase 1. 101 crowd workers completed the study (self-reported gender: 50% female, 50% male; age not collected). One additional respondent exited early (99 % completion).</p><p>Phase 2. 918 participants were recruited from the United States only to control for cultural variation (self-reported gender: 50% female, 50% male). Age was collected only for participants completing the Likert-scale versions of the survey; no other demographics were collected. All procedures and content matched Phase 1 except where described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Design and Procedure</head><p>Both phases used the same 65 scenarios spanning four kindness categories: Emotional Support, Practical Help, Social Awareness, and Family &amp; Child Support. Each trial displayed two concise scenario texts with an illustrative image; respondents clicked the action they preferred. A "Not sure" option permitted abstention. Phase 1 surveys. Participants were randomly assigned to one of two 10-question, forced-choice surveys:</p><p>1. Category-Comparison: each item paired scenarios from two different categories.</p><p>2. Within-Category Ranking: each item paired two scenarios drawn from the same category, allowing fine-grained ranking of actions within that domain.</p><p>Phase 2 surveys. Participants were randomly assigned (between subjects) to exactly one of four versions:</p><p>1. Robot→Human Preference (forced choice over robot actions)</p><p>2. Robot→Human Likert (1-7 perceived-kindness ratings for robot actions)</p><p>3. Human→Human Preference (forced choice over human actions)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Human→Human Likert (1-7 perceived-kindness ratings for human actions)</head><p>This design balances exposure, avoids carryover, and enables direct tests of actor effects and preference-kindness alignment.</p><p>Robot Tested in Scenarios</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Measures</head><p>• Preference (forced choice). For each scenario, Choice % is the proportion of times it was selected when paired (primary analyses exclude "Not sure"; robustness checks treat them as non-choices).</p><p>• Perceived kindness (Likert). Mean kindness rating per scenario on a 1-7 scale.</p><p>• Category aggregates. Scenario-level metrics aggregated by Category × Actor.</p><p>• Demographics. Gender (Phase 2 only). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Data-Quality Safeguards</head><note type="other">Safeguard</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robot → Human Categorical Preferences</head><p>Category effects (Robot→Human). Selection rates differed by category, χ²(3, N = 3,824) = 85.77, p &lt; .001, Cramér's V = .15. Practical Help was chosen 61.3% of the time (623/1,016), clearly above a 50% chance level (binomial p &lt; .001). Emotional Support (45.8%, 555/1,211) and Family &amp; Child Support (42.3%, 407/963) were below 50% (ps ≤ .01), and Social Awareness (46.4%, 294/634) was close to 50% (p = .07). Post-hoc pairwise tests (Holm-corrected) showed Practical Help was selected more often than Emotional Support, Social Awareness, and Family &amp; Child Support (all ps &lt; .001). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human → Human Categorical Preferences</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Category-Level Patterns and Actor Effects</head><p>• In Robot→Human Preference, Practical Help leads (e.g., carry groceries, rake leaves, take out trash).</p><p>• In Human→Human Preference, Emotional Support and Social Awareness rise, with Family &amp; Child Support comparatively balanced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Representative robot-strong scenarios (high Preference):</head><p>• Helps elderly person carry groceries -83.6% (N = 67)</p><p>• Rakes leaves for neighbor who can't -83.0% (N = 100)</p><p>• Takes out full trash without being asked -82.4% (N = 108)</p><p>• Helps someone who is lost by giving directions -81.1% (N = 106)</p><p>• Helps elderly woman load item into car -81.7% (N = 71)</p><p>Representative human-strong scenarios (high Preference):</p><p>• Stays to help a sick friend -82.1% (N = 78)</p><p>• Offers hand to help an elderly person -82.4% (N = 68)</p><p>• Helps elderly neighbor carry groceries -81.9% (N = 72)</p><p>• Holds umbrella in rain -76.3% (N = 97)</p><p>• Comforts someone in distress -74.1% (N = 112)</p><p>These patterns align with the intuition that instrumental, low-ambiguity assistance is readily accepted from robots, whereas affect-heavy or socially interpretive moments are more human-preferred.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Perceived Kindness vs. Preference</head><p>Perceived kindness generally tracks preference but not perfectly, revealing design-relevant divergences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convergences (high Likert, high Preference).</head><p>• Robot: Rakes leaves for neighbor who can't (Likert ≈ 6.22; Pref = 83.0%); Helps fix flat tire (Likert ≈ 6.27; Pref = 73.2 %).</p><p>• Human: Comforts someone in distress (Likert ≈ 6.58; Pref = 74.1%); Holds umbrella (Likert ≈ 6.62; Pref = 76.3%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Divergences (high Likert, lower Preference).</head><p>• Robot: Quietly sits and holds hand of a sad person (Likert ≈ 6.00; Pref = 37.1%); Leaves motivational note (Likert ≈ 5.88; Pref = 34.4%).</p><p>These are acts people recognize as kind yet hesitate to prefer from a robot-especially where touch, intimacy, or implicit social reading is required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Divergences (higher Preference, moderate Likert).</head><p>• Robot: Takes out trash (Pref = 82.4%; Likert ≈ 5.21); Finds lost keys (Pref = 66.4%; Likert ≈ 5.95).</p><p>Here, utility appears to outweigh warmth ratings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Within-Category Highlights</head><p>• Practical Help (Robot-leaning). Strong preferences for carrying/lifting, finding objects, outdoor chores; mixed for micro-tidying (e.g., straighten couch cushions 51.1%).</p><p>• Emotional Support (Human-leaning).</p><p>Touch/soothing actions rate highly kind for humans (e.g., comfort in distress Likert ≈ 6.58) while robots are rated kind but less preferred (hold hand Likert ≈ 6.00; Pref = 37.1%).</p><p>• Social Awareness. Directional help and inclusive gestures favor humans; simple signaling (e.g., hold elevator door Robot Likert ≈ 5.64; Human Pref = 63.2%) is more actor-neutral.</p><p>• Family &amp; Child Support. Mixed: safety-adjacent tasks (fasten bike helmet, math help) are broadly accepted; intimacy-coded grooming (braiding hair) favors humans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Gender Exploratory Analyses</head><p>Gender (Likert ratings only). Mean kindness ratings clustered tightly across genders (≈5.4-5.8 on a 1-7 scale). For robot scenarios, males rated actions slightly higher than females (5.56 vs. 5.38). For human scenarios, females were slightly higher than males (5.63 vs. 5.55), and the non-binary group showed the highest mean (5.80) but likely with a small N; "prefer not to say" had no usable data. These differences are modest and should be interpreted descriptively rather than as inferentially significant.</p><p>Age (Likert ratings only). For robot scenarios, younger adults (18-24 = 5.66; 25-34 = 5.69) rated actions slightly kinder than midlife respondents (45-54 = 5.13), with a rebound among 55+ (5.55). For human scenarios, means were relatively flat from 18-44 (5.58-5.64), dipped at 45-54 (5.37), and peaked in 55+ (5.75). Again, effects are small (≤ ~0.6 Likert points) and reported as descriptive patterns.</p><p>Note: Demographics were collected only in the Phase-2 Likert modules; we did not analyze demographic differences for forced-choice preference.</p><p>For a confirmatory analysis, future work should model Likert ~ Actor × (Gender + Age) with random effects for scenario and participant, and report effect sizes with multiple-comparison control.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Cross-Phase Comparison</head><p>Phase 2 replicates the central Phase-1 result: preferences are structured, not random. With greater power, Phase 2 clarifies that robots are strongest in Practical Help, whereas humans are favored in Emotional Support and Social Awareness. Phase 2 also introduces the preference-kindness alignment tests: some actions rated kind for robots are not preferred from robots (touch/intimacy), while some moderately rated actions are strongly preferred (high-utility tasks). These patterns were not measurable in Phase 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">DISCUSSION</head><p>Across two phases, participants gravitate toward instrumental assistance from robots-actions with clear goals, low ambiguity, and obvious utility (carrying, fetching, clearing, finding). By contrast, affective or interpretive moments (comforting touch, inclusion decisions, subtle etiquette) remain human-preferred, even when the same actions are rated "kind" in the abstract. This actor asymmetry suggests that the hurdle for robots is not recognizing kindness but performing it in contexts saturated with social nuance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Design implications.</head><p>1. Lean into high-utility helps. Retrieval, carrying, outdoor chores, object-finding, and simple environmental adjustments are reliable wins.</p><p>2. Consent-first social touch. When touch or intimacy is implicated, default to offer/consent patterns ("Would you like me to…?") or handoff to a human caregiver.</p><p>3. Make kindness legible. Pair ambiguous acts with brief explanatory cues ("I noticed your hands were full, so I held the door").</p><p>4. Actor-aware policies. Treat some actions as robot-primary, others human-primary, and some shared with consent gating and context checks.</p><p>Preference vs. perception. Where people rate an act as kind but do not prefer it from a robot (e.g., hand-holding, motivational notes), the barrier appears relational, not moral. Users are not rejecting kindness per se; they are calibrating who should deliver it.</p><p>Generalizability and limits. Phase 2's U.S.-only sampling improves internal coherence at the cost of cross-cultural reach; future phases should vary culture explicitly. Gender analyses are preliminary and warrant larger, balanced samples. Finally, stated preferences may diverge from behavior; in-situ HRI studies are needed to test whether these patterns translate to action and long-term engagement. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">FUTURE WORK</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Outcome</head><label></label><figDesc>Randomized scenario order &amp; left/right position ✔ Equalized pairing frequency ✔ Minimum dwell time ≥ 2 s Mean = 11s (SD = 4.2 s) Duplicate IP / bot screen check None detected</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="17,72.00,129.63,468.00,273.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="17,72.00,468.24,468.00,275.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="18,72.00,105.12,468.00,276.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="18,72.00,429.24,468.00,276.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="19,-8.25,117.63,315.00,180.00" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>6. ACKNOWLEDGEMENTS I am deeply grateful to <rs type="person">Professor Malte Jung</rs> (<rs type="affiliation">Cornell University</rs>), <rs type="person">Professor Shuran Song</rs> (Stanford), <rs type="person">Professor Michael Goldstein</rs> (<rs type="affiliation">Cornell University</rs>), <rs type="person">Professor Bertram Malle</rs> (<rs type="affiliation">Brown University</rs>), <rs type="person">Professor Vanessa Bohns</rs> (<rs type="affiliation">Cornell University</rs>), <rs type="person">Professor Anita Allen</rs> (<rs type="affiliation">University of Pennsylvania</rs>), <rs type="person">Professor Nikolas Martelarro</rs> (<rs type="affiliation">Carnegie Mellon University</rs>), <rs type="person">Professor Justine Cassell</rs> (<rs type="affiliation">Carnegie Mellon University</rs>), and <rs type="person">Linas Nasvytis, Ph.D.</rs> (<rs type="affiliation">Stanford University</rs>) for sharing their expertise and offering thoughtful guidance. Their encouragement and insights significantly enriched this project.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tables &amp; Figures</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<ptr target="https://www.morganstanley.com/insights/articles/humanoid-robot-market-5-trillion-by-2050" />
		<title level="m">Humanoids: A $5 Trillion Market</title>
		<imprint>
			<publisher>Morgan Stanley Research</publisher>
			<date type="published" when="2025-05-14">2025. May 14</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">What kind of mind do I want in my robot? Developing a measure of desired mental capacities in social robots</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">F</forename><surname>Malle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thapa Magar</surname></persName>
		</author>
		<idno type="DOI">10.1145/3029798.3038378</idno>
		<ptr target="https://doi.org/10.1145/3029798.3038378" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Companion of the 2017 ACM/IEEE International Conference on Human-Robot Interaction</title>
		<meeting>the Companion of the 2017 ACM/IEEE International Conference on Human-Robot Interaction</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="195" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Matching robot appearance and behavior to tasks to improve human-robot cooperation. RO-MAN 2003</title>
		<author>
			<persName><forename type="first">J</forename><surname>Goetz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kiesler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Powers</surname></persName>
		</author>
		<idno type="DOI">10.1109/ROMAN.2003.1251796</idno>
		<ptr target="https://doi.org/10.1109/ROMAN.2003.1251796" />
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Advancements in humanoid robots: A comprehensive review and future prospects</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/JAS.2023.124140</idno>
		<ptr target="https://doi.org/10.1109/JAS.2023.124140" />
	</analytic>
	<monogr>
		<title level="j">IEEE/CAA Journal of Automatica Sinica</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="301" to="328" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">All robots are not created equal: The design and perception of humanoid robot heads</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Disalvo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gemperle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Forlizzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kiesler</surname></persName>
		</author>
		<idno type="DOI">10.1145/778712.778756</idno>
		<ptr target="https://doi.org/10.1145/778712.778756" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Designing Interactive Systems</title>
		<meeting>the Conference on Designing Interactive Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002">2002. 2002</date>
			<biblScope unit="page" from="321" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">How people perceive different robot types: A direct comparison of an android, humanoid, and non-biomimetic robot</title>
		<author>
			<persName><forename type="first">K</forename><surname>Haring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silvera-Tawil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Velonaki</surname></persName>
		</author>
		<idno type="DOI">10.1109/KST.2016.7440504</idno>
		<ptr target="https://doi.org/10.1109/KST.2016.7440504" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Knowledge and Smart Technology</title>
		<meeting>the 8th International Conference on Knowledge and Smart Technology</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="265" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Allen</surname></persName>
		</author>
		<idno type="DOI">10.1093/acprof:oso/9780195374049.001.0001</idno>
		<idno>acprof:oso/97801953 74049.001.0001</idno>
		<ptr target="https://doi.org/10.1093/" />
		<title level="m">Moral machines: Teaching robots right from wrong</title>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Acceptance of socially assistive humanoid robot by preschool and elementary school teachers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fridin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Belokopytov</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.chb.2013.12.016</idno>
		<ptr target="https://doi.org/10.1016/j.chb.2013.12.016" />
	</analytic>
	<monogr>
		<title level="j">Computers in Human Behavior</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="23" to="31" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Are friends electric? The benefits and risks of human-robot relationships</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Prescott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Robillard</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.isci.2020.101993</idno>
		<ptr target="https://doi.org/10.1016/j.isci.2020.101993" />
	</analytic>
	<monogr>
		<title level="j">iScience</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">101993</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Human preferences for cognitive and emotional capabilities in robots across different application domains</title>
		<author>
			<persName><forename type="first">H</forename><surname>Nääs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thellman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ziemke</surname></persName>
		</author>
		<idno type="DOI">10.3389/frobt.2025.1511549</idno>
		<ptr target="https://doi.org/10.3389/frobt.2025.1511549" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Robotics and AI</title>
		<imprint>
			<biblScope unit="page" from="12" to="1511549" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Introduction to Psychology: Social Psychology and Influences on Behavior</title>
		<author>
			<persName><forename type="first">Lumen</forename><surname>Learning</surname></persName>
		</author>
		<ptr target="https://courses.lumenlearning.com/waymaker-psychology/chapter" />
		<imprint/>
	</monogr>
	<note>what-is-social-psycholo gy/ 35 Person braids a child&apos;s hair 5.48 36 Person refills someone&apos;s water bottle</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
