<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Word predictability in Portuguese: Cloze norming study vs. LLMs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Jane</forename><surname>Aristia</surname></persName>
							<email>j.aristia@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Modal&apos;X</orgName>
								<orgName type="institution">Université Paris Nanterre</orgName>
								<address>
									<settlement>Nanterre</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Orcid</forename><surname>Id</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Modal&apos;X</orgName>
								<orgName type="institution">Université Paris Nanterre</orgName>
								<address>
									<settlement>Nanterre</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Word predictability in Portuguese: Cloze norming study vs. LLMs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8868DF4E7426E25123C7BCEC85A0A3EB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-10-21T13:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the rise of large language models (LLM), there has been deemed a possible alternative to human participants in many scientific domains, including linguistic studies, the cloze study. Cloze probability is used to inform researchers as to how predictable a word is within a certain sentential context. It is a common tool in linguistic studies to understand language production and processing. Previous studies (e.g., <ref type="bibr" target="#b12">Jacobs et al., 2022;</ref><ref type="bibr" target="#b16">Lopes Rego et al., 2024)</ref> have compared LLM performance with traditional cloze studies and their results are promising. Nonetheless, these studies were done in English. Hence, we would like to know LLM performance in the Portuguese language. Here, we compared results from a traditional cloze study with two LLM, such as: Grevásio <ref type="bibr" target="#b20">(Santos et al., 2024) and</ref> Tucano (Corrêa et al.,   2024)  then performed a correlation analysis to investigate their performance. The results show a moderate and weak correlation between the cloze probability from human participants and the LLMs. These results highlight the gap between human performance and LLM, specifically in cloze probability.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The growing interest in understanding language prediction is accompanied by the increased need for cloze norming study as it gives us information about the predictability of a word within a sentence. Traditionally, this norming study requires participants to complete a sentence (e.g., "The students are studying hard because tomorrow they will have an ___ "). It measures the predictability of a target word based on the sentential context. The cloze probability is obtained by calculating the proportion of participants' word responses for each sentence. However, with the advancement of the large language model (LLM), we observe the shift towards LLM as an alternative. Some researchers argued that LLM is comparable to human response and can be used to understand human cognition <ref type="bibr" target="#b10">(Hu et al., 2022</ref><ref type="bibr" target="#b11">(Hu et al., , 2024))</ref>.</p><p>However, this idea is still debatable, as those who oppose it argue that LLMs use different mechanisms and it is premature to use them to understand human cognition <ref type="bibr" target="#b14">(Katzir, 2023;</ref><ref type="bibr" target="#b15">Leivada et al., 2024)</ref>. Despite these debates, researchers still try to use LLM to understand linguistic processing. For instance, it has been used for linguistic tasks such as cloze norming study <ref type="bibr" target="#b12">(Jacobs et al., 2022;</ref><ref type="bibr" target="#b16">Lopes Rego et al., 2024)</ref>. In a traditional cloze norming study we need to collect human participant data, which takes time and sometimes we also need to provide financial compensation for participants. In short, this traditional method is time-consuming and costly. On the other hand, LLM enables us to obtain data efficiently and faster. LLM allows us to skip this data collection step because the cloze probability is calculated through the tokenization of sub-words of each word in a sentence. Therefore, LLM could be deemed as a promising alternative to the traditional cloze norming study.</p><p>To make sure that LLM could provide a good word probability, there have been studies that compare cloze norming studies with LLM (e.g., <ref type="bibr" target="#b12">Jacobs et al., 2022;</ref><ref type="bibr" target="#b16">Lopes Rego et al., 2024)</ref> and the results showed that they are indeed comparable. For instance, Lopes <ref type="bibr" target="#b16">Rego et al. (2024)</ref>, compared the traditional cloze norming study with cloze results from LLMs such as GPT-2 and Llama to investigate the reading models such as OB-1 reader <ref type="bibr" target="#b21">(Snell et al., 2018)</ref>. Their study showed that LLMs' results are more accurate in predicting human eye movement towards the anticipated words than the traditional cloze study. These studies seem to suggest that it is indeed promising to use LLM as an alternative to traditional cloze study. Nonetheless, we need to be cautious with the possibility of overfitting from LLM.</p><p>Looking at how LLM is comparable to traditional cloze study, we are interested to know if this is also applied in European Portuguese because most of the previous studies were done in English. So, this study aims to see if cloze probability from LLMs is comparable to Portuguese human cloze probability. For this purpose, we used cloze norming datasets from <ref type="bibr">Aristia et al. (in preparation)</ref> and for the LLMs we used two Portuguese text generation models such as Grevásio <ref type="bibr" target="#b20">(Santos et al., 2024)</ref> and Tucano <ref type="bibr" target="#b5">(Corrêa et al., 2024)</ref>.</p><p>These were the most recent openly accessible text generation models we could find.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Cloze norming study</head><p>The data were taken from cloze norming studies in <ref type="bibr">Aristia et al. (in preparation)</ref> wherein they were adapted from the sentence pool in <ref type="bibr">Frade et al (2021)</ref>. <ref type="bibr">Aristia et al. (in preparation)</ref> conducted two cloze studies; the first study was to get the probability of the article in each sentence and the second to get the probability of the noun. In this present study, we only used 117 sentences that were used in their experiment and were obtained from the second cloze study. The average age of the participants is 27.69 years old (age range: 20-50 years old). There were 45 female participants from 125 participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Cloze probability with LLM</head><p>For the LLMs, we used Grevásio <ref type="bibr" target="#b20">(Santos et al., 2024)</ref> and Tucano <ref type="bibr" target="#b5">(Corrêa et al., 2024)</ref>. Grevásio is an open-source trained decoder model from the LLaMA family. This model was trained with a supervised fine-tuning method wherein the dataset was labelled.</p><p>The dataset was from GLUE and SuperGLUE that were machine translated into European</p><p>Portuguese. The other model that we used here is Tucano, a Transformer-based model that is pre-trained in Portuguese. Nonetheless, here we used the supervised fine-tuning model of Tucano, which is called Tucano-2b4-Instruct. It is trained with several datasets such as the Portuguese version of 1 million GPT4, Orca word math problems <ref type="bibr" target="#b17">(Mitra et al., 2024)</ref> in Portuguese, and the Aira dataset <ref type="bibr" target="#b4">(Corrêa, 2024)</ref>. Nonetheless, Tucano, unlike Grevásio, was trained not only in European Portuguese but also in Brazilian Portuguese.</p><p>Further, to obtain the target word's probability from the LLM, we adapted the code from GPT-2-for-Psycholinguistic-Applications developed by Samer Nour Eddine which allows us to use LLM for the Portuguese language. The sentences were parsed into sub-word tokens and each word position was marked. The cloze probability of each word was obtained by calculating the conditional probabilities of each word's sub-word token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1</head><p>Average Cloze probability from the Cloze norming study, Grevásio PT and Tucano BR Note. The bar reflects the standard error (SE).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Cloze probability</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Cloze norming study with participants</head><p>The average cloze probability of the sentences was .73 (range= .41 -1, SD = .17), depicted in Figure <ref type="figure">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Cloze obtained through LLM</head><p>The average target word probability (see Figure <ref type="figure">1</ref>) from Grevásio was .23 (range = 7.79E-08 -.99, SD = .32), and Tucano was .24 (range = 8.55E-05 -.85, SD = .19).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data similarity analysis</head><p>Figure <ref type="figure">1</ref> illustrates the distribution of the data. To verify the similarity between the distributions of data obtained from human participants and LLM distance analysis using Jensen-Shannon method <ref type="bibr" target="#b3">(Cha, 2007)</ref> was performed. To run this analysis the 'philentropy' package <ref type="bibr" target="#b7">(Drost, 2018)</ref> in R (2000) was used. The results showed that the LLM data of both Grevásio (p&lt;.001) and Tucano (p&lt;.001) was significantly different from the traditional cloze study using human participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Correlation analysis</head><p>To investigate if there are correlations between traditional cloze study and LLMs, we conducted Spearman correlation <ref type="bibr" target="#b22">(Wissler, 1905)</ref> in R. It is a non-parametric test that looks at a monotonic relationship, which is less restrictive than assuming linearity in the data. It also determines the strength and direction of this observed correlation. Through this analysis, we found that Tucano performed better than Grevásio when their results were compared with the traditional cloze study. We observed significant and weak positive correlation (See Figure <ref type="figure">2</ref>) between the word probability from traditional cloze study with Grevásio, r(115) = .25, p =.007, 95%CI [.10, .45]; and, moderate significant correlation with Tucano, r(115) = .36, p &lt; .001, 95%CI <ref type="bibr">[.19, .51</ref>]. These confidence intervals (CI) were obtained through the bootstrapping technique, which is a method that can estimate the uncertainty in the data through random resampling, without assuming normal distribution in the data. We can see in Figure <ref type="figure">3</ref> that the correlation values for both Grevásio and Tucano fall within the CI range.</p><p>Although their values were overlapping, Tucano performed slightly better than Grevásio as the lowerband of the CI was higher for Tucano. For Grevásio, the CI is between .10 and .45. For Grevásio, the CI for Spearman is between .19 and .51.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion</head><p>The purpose of this study is to see if LLMs can be used as an alternative to traditional Cloze studies. Visually, in Figure <ref type="figure">1</ref>, we can see that there are huge differences between the distribution of cloze probability obtained by human participants versus those using LLMs. This observation is confirmed by the similarity analysis wherein the cloze probabilities obtained through LLM are significantly different from the human participants data. Despite that, the statistical results show there are significant correlations between them. Nonetheless, it needs to be noted that they are not strongly correlated as seen in Figure <ref type="figure">2</ref>. This indicates that for Portuguese language, the LLMs are not yet ready to be used as an alternative to traditional cloze study with human participants.</p><p>Differences in the mean probability between the cloze probability from human participants and LLM, as seen in Figure <ref type="figure">1</ref>, and weak to moderate correlations in Figure <ref type="figure">2</ref> RoBERTa <ref type="bibr">(Liu et al., 2019), and</ref><ref type="bibr">Pythia (Biderman et al., 2023)</ref>. In the first experiment, they compared probabilities from the cloze study with the model probabilities and they found no linearity in the correlation. In the second experiment, they used the rank of probable responses from both the cloze study and LLM, and again the results showed weak correlation between them. In the third experiment, they aimed to assess if the model training affects the fitting to the human data. In the fourth experiment, they conducted a clustering analysis to evaluate the semantic production of humans and LLM. In short, their first two studies focused on showing differences between human cloze probability and LLM.</p><p>LLM calculates the probability between words in a sentence differently, that is why we could observe differences between data from the human participant data and LLM in the present study. <ref type="bibr" target="#b9">Günther and Cassani (2025)</ref> argued that LLM uses probability between tokens to predict the upcoming words. Therefore, in a sentence completion task like a cloze study, LLM tends to use a frequent word that may be less grammatical, or a less probable word that may be more grammatical <ref type="bibr" target="#b14">(Katzir, 2023)</ref>. For instance, <ref type="bibr" target="#b14">Katzir (2023)</ref> showed that to continue a sentence such as: "The little duck that met the horses with the blue spots yesterday __", the GPT model preferred to use 'are' rather than 'destroys' as a continuation of this sentence.</p><p>On the other hand, human participants are able to make grammatical sentences with a less probable word because they can make use of the sentential context. In the same vein, <ref type="bibr" target="#b2">Cai et al. (2023)</ref> study also showed that LLMs do not really rely on context to resolve syntactic ambiguity in a sentence as it relies more on sub-word tokens. Another possible reason for these weak correlations is perhaps due to the way the LLMs are fine-tuned, as they tend to perform well on the task related to their fine-tuning focus <ref type="bibr" target="#b6">(Denning et al., 2025)</ref>.</p><p>Nevertheless, the weak to moderate correlation between human participants' cloze probability and LLMs could also be interpreted to indicate that there is a possibility to use them as complementary data with human participants instead of an alternative. Caution not to use LLM as an alternative also comes from a recent Event-Related-Potentials (ERP) study <ref type="bibr" target="#b1">(Arkhipova et al., 2025)</ref>. ERP is a common method in psycho/neurolinguistics studies that investigate linguistic information that is used by the brain during language comprehension. Apart from that, in the future, it is not impossible to use LLM as an alternative to human data. To achieve this, improvements are needed. For instance, additional data training and fine-tuning in Portuguese text is needed, specifically with cloze task. Also, Portuguese variance need to be taken into account, for instance there are differences in the word choice between Brazilian Portuguese and European Portuguese. Thus, taking this into account will increase the accuracy of LLM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>All in all, this study is an early attempt to compare traditional cloze study using human participants with Portuguese LLM. Portuguese LLM has not yet achieved the same level of performance in traditional cloze tests as in English. Nevertheless, it did show a similar trend as seen in the correlation results, wherein we observed there is an increase of LLM probability for words with higher cloze probability and particularly more stable and promising results from Tucano than Grevásio. Hence, it is favorable to recommend LLM as complementary data with human participants' data rather than as a replacement and it needs to be kept in mind that LLM computes words' probability differently than human. To enhance human-like performance of LLM, further studies and improvements are required.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 3</head><label>23</label><figDesc>Figure 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>, are in line with the findings of Jacobs et al. (2024). They conducted four experiments comparing the cloze study from Peele et al. (2020) with three LLMs: GPT-2 (Radford et al., 2019),</figDesc></figure>
		</body>
		<back>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Competing interests</head><p>The author declares that there are no competing interests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Data accessibility statement</head><p>The data, codes, and materials that support this study are available in https://osf.io/f8jrv/ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Declaration of use of AI</head><p>The author declares that ChatGPT was used for information search, grammar checking and sentence rephrasing to make it clearer under the supervision of the author. It was also used to assist the coding part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Ethics and consent</head><p>Data from cloze study that involved human participants was part of an EEG study (Aristia et al., in preparation)  that has been approved by the ethic committee of Faculdade Psicologia, Universidade de Lisboa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Author's role</head><p>Jane Aristia : conceptualization, methodology, software, validation, formal analysis, investigation, data curation, visualization, writing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">Acknowledgement</head><p>The cloze data from this study was part of an EEG study (Aristia et al., in preparation)    that was conducted at Prof AP's lab, VoicES lab, Universidade de Lisboa, in collaboration with SF, LAPSO-ISCTE. The author also thanked SNE for his advice on the comparison between human cloze and LLM; and, PZ who proofread the manuscript and check the grammar of this manuscript.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Link to preprint</head><p>https://doi.org/10.31234/osf.io/e8fzj_v2</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Prediction by production in spoken sentence processing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Aristia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Frade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pinheiro</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in preparation</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">When Meaning Matters Most: Rethinking Cloze Probability in N400 Research</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Arkhipova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lopopolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vasishth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rabovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page" from="2025" to="2029" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">G</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Haslett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Pickering</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.08014</idno>
		<title level="m">Do large language models resemble humans in language use?</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Comprehensive survey on distance/similarity measures between probability density functions</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Cha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">City</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Dynamic normativity: Necessary and sufficient conditions for value alignment</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Corrêa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.11039</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Corrêa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Falk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fatimah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.07854</idno>
		<title level="m">Tucano: Advancing Neural Text Generation for Portuguese</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Denning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Snefjella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Blank</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2504.16884</idno>
		<title level="m">Do Large Language Models know who did what to whom?</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Philentropy: information theory and distance quantification with R</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Drost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Open Source Software</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">26</biblScope>
			<biblScope unit="page">765</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Is second best good enough? An EEG study on the effects of word expectancy in sentence comprehension</title>
		<author>
			<persName><forename type="first">S</forename><surname>Frade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Santi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raposo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language, Cognition and Neuroscience</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="209" to="223" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Günther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Günther</surname></persName>
		</author>
		<title level="m">Large Language Models in psycholinguistic studies</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A fine-grained comparison of pragmatic language understanding in humans and language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Floyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Jouravlev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fedorenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gibson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.06801</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Language models align with human judgments on key grammatical constructions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mahowald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lupyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ivanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="issue">36</biblScope>
			<biblScope unit="page">2400917121</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Masked language models directly encode linguistic uncertainty</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Federmeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Society for Computation in Linguistics 2022</title>
		<meeting>the Society for Computation in Linguistics 2022</meeting>
		<imprint>
			<date type="published" when="2022-02">2022, February</date>
			<biblScope unit="page" from="225" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Large-scale cloze evaluation reveals that token prediction tasks are neither lexically nor semantically aligned</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Grobol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tsang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.12057</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Why large language models are poor theories of human linguistic cognition: A reply to Piantadosi</title>
		<author>
			<persName><forename type="first">R</forename><surname>Katzir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biolinguistics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Evaluating the language abilities of Large Language Models vs. humans: Three caveats</title>
		<author>
			<persName><forename type="first">E</forename><surname>Leivada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dentella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Günther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biolinguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Language models outperform cloze predictability in a cognitive model of reading</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Lopes Rego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Meeter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">1012117</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Khanpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Awadallah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.14830</idno>
		<title level="m">Orca-math: Unlocking the potential of slms in grade school math</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Completion norms for 3085 English sentence contexts</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Peelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Spehar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Sommers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Van Engen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="1795" to="1799" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">R language definition</title>
		<author>
			<orgName type="collaboration">R Core Team.</orgName>
		</author>
	</analytic>
	<monogr>
		<title level="m">R foundation for statistical computing</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">116</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Branco</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.18766</idno>
		<title level="m">Advancing Generative AI for Portuguese with Open Decoder Gervásio PT</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">OB1-reader: A model of word recognition and eye movements in text reading</title>
		<author>
			<persName><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van Leipsig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Grainger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Meeter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">969</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The Spearman correlation formula</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wissler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">558</biblScope>
			<biblScope unit="page" from="309" to="311" />
			<date type="published" when="1905">1905</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
