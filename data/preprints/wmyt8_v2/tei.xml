<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Salient features of task-irrelevant continuous speech distort subjective time</title>
				<funder ref="#_vtQx9ds">
					<orgName type="full">National Institutes of Health</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><roleName>Ph.D</roleName><forename type="first">Ashley</forename><forename type="middle">Royal</forename><surname>Symons</surname></persName>
							<email>ashley.symons@rhul.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution" key="instit1">Royal Holloway</orgName>
								<orgName type="institution" key="instit2">University of London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fred</forename><surname>Dick</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Experimental Psychology</orgName>
								<orgName type="institution">University College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Adam</forename><surname>Tierney</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Psychological Sciences</orgName>
								<orgName type="institution">University of London</orgName>
								<address>
									<settlement>Birkbeck, London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Affiliations</surname></persName>
						</author>
						<author>
							<persName><surname>Holloway</surname></persName>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of London Egham Hill</orgName>
								<address>
									<postCode>TW20 0EX</postCode>
									<settlement>Egham</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Salient features of task-irrelevant continuous speech distort subjective time</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EB4766497EC944DBB922C46BB9E97C16</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-10-21T13:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Attention</term>
					<term>speech</term>
					<term>time</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Computational models of auditory salience predict that acoustic change and divergence from prediction increase the salience of sound streams. Confirming these predictions, prior research has shown that acoustic change and unpredictable sound features are linked to increases in physiological arousal and disruption of concurrent task performance. However, it remains unclear whether linguistic features, such as phonemic and lexical/semantic surprisal, help drive attentional orienting, or whether instead attentional capture takes place prior to linguistic analysis. To address this question, we introduce a new technique for assessing attentional capture by naturalistic taskirrelevant speech. In this paradigm, participants tap to a metronome while ignoring a spoken passage from an audiobook. Salient features of the task-irrelevant speech capture attention, increase arousal, and expand subjective time, leading to shifts in tap timing. We show that distortions of subjective time are driven not only by acoustic change but also by phonemic surprisal. Thus, attentional orienting to sound takes place after the initial stages of linguistic analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Significance statement</head><p>We measured the speed of time perception while participants ignored distracting speech. We find that when listeners' predictions about upcoming speech sounds fail, the subjective passage of time slows down. This suggests that people make linguistic predictions even when ignoring speech and that prediction errors capture attention.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Imagine that you are in a coffee shop, trying to work on a grant proposal. The ambient noise of silverware clinking and coffee being assembled recedes into the background as you focus your mind. Then, behind you, a conversation turns heated: a couple begins to argue, their voices suddenly louder, higher in pitch, and spiked with emotional words. Despite your best intentions, your attention drifts away from your proposal and you begin to eavesdrop. As this emotional conversation captures your attention, your physiological arousal increases: your pupils dilate, your skin sweats, your pulse quickens, and your perception of time expands. This is a common experience because speech is particularly good at capturing our attention. However, it remains an open question which factors cause speech to capture attention and which cause speech to fade into the background.</p><p>Researchers have developed several computational models of the factors which drive attentional capture by sound. However, these models have been developed to apply to sound in general and so cannot capture speech-specific factors such as phonology or lexical semantics. Instead, these models have focused on modelling changes in salience over time in complex sound streams due to acoustic factors. For example, some bottom-up models have created salience maps with center-surround inhibition in time-frequency "space" <ref type="bibr" target="#b43">(Kayser et al., 2005;</ref><ref type="bibr" target="#b40">Kalinli &amp; Narayanan, 2007;</ref><ref type="bibr" target="#b27">Duangudom &amp; Anderson, 2007)</ref>, inspired by vision research using eye tracking data as ground truth <ref type="bibr" target="#b62">(Niebur et al., 2002)</ref>. These models predict that sudden acoustic changes across multiple dimensions (amplitude, pitch, spectral shape) will be linked to transient increases in attentional capture. Other contextual models track dynamic changes in feature-specific deviance from prediction relative to local and longer-term statistics <ref type="bibr" target="#b87">(Tsuchida &amp; Cottrell, 2012;</ref><ref type="bibr" target="#b41">Kaya &amp; Elhilali, 2014)</ref>. These models predict that moments of high unpredictability within a speech stream will be followed by short-term capture of attention.</p><p>Predictions of auditory salience models have been tested via behavior and physiology. One simple method of assessing salience used to validate computational models is to ask participants how salient a given sound or auditory scene is <ref type="bibr" target="#b40">(Kalinli &amp; Narayanan, 2007;</ref><ref type="bibr" target="#b27">Duangudom &amp; Anderson, 2007)</ref> or to ask participants which of two competing scenes is more salient <ref type="bibr" target="#b34">(Huang &amp; Elhilali, 2017)</ref>. This research has found that salience ratings are greater after change along several acoustic dimensions, including loudness, pitch, and spectral shape, as well as when a sound stream's acoustic characteristics diverge from the distributional statistics of the surrounding context. However, this approach relies upon participants having consistent and valid interpretations of the word/concept "salience". An alternate approach is to examine the effects of presentation of a sound stream on performance of a concurrent task, such as serial visual shortterm memory <ref type="bibr" target="#b38">(Jones &amp; Macken, 1993)</ref>. Visual memory is disrupted more by sound streams featuring acoustic change, and larger changes are linked to greater disruption <ref type="bibr" target="#b39">(Jones et al., 2000;</ref><ref type="bibr" target="#b74">Schlittmeier et al., 2012)</ref>. Moreover, unpredictable changes cause more disruption of performance than predictable changes <ref type="bibr" target="#b8">(Bell et al., 2012;</ref><ref type="bibr" target="#b9">Bell et al., 2019)</ref>. However, disruption of performance is not a pure measure of attentional capture, because it can potentially reflect interference with pre-conscious automatic processes (such as processing of serial order) as well as divergence of attention <ref type="bibr" target="#b36">(Hughes 2014</ref>).</p><p>An alternate approach to studying capture of attention by sound streams is to measure the physiological components of attentional orienting. Capture of attention by a particularly salient sound is accompanied by an increase in arousal which prepares the listener for action <ref type="bibr" target="#b80">(Sokolov 1963)</ref>. These arousal effects can be a confounding factor when investigating attentional capture by assessing disruption of behavior, as distraction and arousal can have opposite effects on performance <ref type="bibr" target="#b13">(Bonmassar et al., 2023;</ref><ref type="bibr" target="#b56">Masson &amp; Bidet-Caulet, 2019)</ref>. However, arousal can be assessed more directly by measuring physiological responses such as pupil dilation, skin conductance, and MEG/EEG. For example, sound intensity is linked to the degree of pupil dilation <ref type="bibr" target="#b4">(Antikainen &amp; Niemi, 1983;</ref><ref type="bibr" target="#b50">Liao et al., 2016)</ref> and the amplitude of the galvanic skin response <ref type="bibr" target="#b7">(Barry 1975)</ref>. The degree of acoustic modulation is also linked to the extent of microsaccadic inhibition <ref type="bibr">(Zhao et al., 2019a)</ref>, pupil dilation <ref type="bibr" target="#b6">(Bala &amp; Takahashi 2000;</ref><ref type="bibr" target="#b55">Marois et al., 2018)</ref>, involuntary peripheral muscle responses <ref type="bibr" target="#b75">(Schultz et al., 2021)</ref>, decreased neural phase-locking to target stimuli <ref type="bibr">(Huang &amp; Elhilali, 2020)</ref>, and the size of the P3a, an ERP component thought to reflect attentional orienting <ref type="bibr" target="#b12">(Berti et al., 2004;</ref><ref type="bibr" target="#b70">Rinne et al., 2006)</ref>. Moreover, these physiological responses are not only driven by acoustic change but also factor in the surrounding context: unpredictable stimuli lead to greater changes in pupil dilation <ref type="bibr" target="#b30">(Friedman et al., 1973;</ref><ref type="bibr" target="#b51">Liao et al., 2018;</ref><ref type="bibr" target="#b58">Milne et al., 2021a;</ref><ref type="bibr" target="#b66">Qiyuan et al., 1985;</ref><ref type="bibr" target="#b81">Southwell et al., 2017;</ref><ref type="bibr">Zhao et al. 2019b</ref>) and larger neural responses <ref type="bibr" target="#b42">(Kaya et al., 2020)</ref>.</p><p>In summary, computational modelling and behavioral/physiological research have demonstrated that acoustic change and unpredictability are linked to disruption of behavior and increased arousal. Acoustic factors alone, however, may not be sufficient to explain why certain sounds capture attention. Task-irrelevant comprehensible speech, for example, interferes with task performance more than acoustically matched non-speech sounds <ref type="bibr" target="#b23">(Dorsi et al., 2018;</ref><ref type="bibr" target="#b49">Le Compte et al., 1997;</ref><ref type="bibr" target="#b52">Little et al., 2010;</ref><ref type="bibr" target="#b88">Viswanathan et al., 2014)</ref>, suggesting that certain linguistic factors additionally play a role in driving attentional orienting. One possible explanation for why speech can so effectively capture attention is that it contains probabilistic regularities on many different levels, including phonemic and semantic patterns, leading to predictions which capture attention when not fulfilled. However, modelling has not addressed the question of whether unpredictability of linguistic features can lead to attentional orienting. This question has also largely not been addressed experimentally, either using physiological or behavioral measures. An important exception is <ref type="bibr" target="#b71">Röer et al. (2019)</ref>, who found that semantically unexpected words can interfere with visual short-term recall. However, as mentioned above, interference of an auditory stimulus with visual recall can reflect either attentional capture or interference-by-process. For example, as suggested by <ref type="bibr" target="#b71">Röer et al. (2019)</ref>, the sequence processing necessary for chunking during visual recall could overlap cognitively with the process of integrating a word with its preceding semantic context. Moreover, <ref type="bibr" target="#b72">Röer and Cowan (2021)</ref> found that unexpected words in a distractor stream do not interfere with comprehension of a target speech stream. It remains, therefore, an open question whether linguistic surprisal in a task-irrelevant stream of speech can lead to attentional orienting.</p><p>Here we demonstrate a method of tracking attentional capture by task-irrelevant speech which can be used to assess the salience of phonemic and semantic surprisal while ruling out the influence of interference-by-process. This approach takes advantage of a well-documented link between increased arousal and expansion of subjective time. Expanded subjective time has been demonstrated due to a wide variety of experimental manipulations of arousal, including administration of methamphetamine to rats <ref type="bibr" target="#b54">(Maricq et al., 1981)</ref>, emotional content of stimuli <ref type="bibr" target="#b24">(Droit-Volet et al., 2004;</ref><ref type="bibr" target="#b25">Droit-Volet &amp; Meck, 2007;</ref><ref type="bibr" target="#b32">Gil &amp; Droit-Volet 2012;</ref><ref type="bibr" target="#b48">Lake et al., 2016)</ref>, breath-holding <ref type="bibr" target="#b76">(Schwarz et al., 2013)</ref>, artificially raised body temperature <ref type="bibr" target="#b89">(Wearden &amp; Penton-Voak 2007)</ref>, and presentation of simple fluctuating stimuli such as clicks and flashes <ref type="bibr" target="#b18">(Buffardi, 1971;</ref><ref type="bibr" target="#b26">Droit-Volet &amp; Wearden, 2002;</ref><ref type="bibr" target="#b63">Ortega &amp; López, 2008;</ref><ref type="bibr" target="#b64">Penton-Voak et al., 1996;</ref><ref type="bibr" target="#b90">Wearden et al., 1999)</ref>. Moreover, the rate of subjective passage of time and pupil size have been shown to correlate in monkeys <ref type="bibr" target="#b83">(Suzuki et al., 2016)</ref>. These findings are compatible with models of subsecond time perception featuring an internal central clock (or clocks) which can vary in speed due to changes in internal state <ref type="bibr" target="#b31">(Gibbon, Church, &amp; Meck, 1984;</ref><ref type="bibr" target="#b1">Allman &amp; Meck, 2012;</ref><ref type="bibr" target="#b57">Merchant, Harrington, &amp; Meck, 2013;</ref><ref type="bibr" target="#b2">Allman, Teki, Griffiths, &amp; Meck, 2014)</ref>. Assessing subjective time, therefore, enables measurement of arousal-induced task bias separately from task performance, which reflects a complex combination of attention, arousal, and process-based interference.</p><p>Prior research on arousal and bias in internal timing has presented single short sound events and assessed retrospective time perception. However, we have developed a technique that enables the assessment of ongoing subjective time throughout presentation of a complex sound stream. Participants are asked to tap to the beat of a 2-Hz click track while ignoring the presentation of distracting sounds. An auditory rather than visual pacing signal is used-i.e. clicks rather than flashes-because participants have been reported to tap more consistently to auditory stimuli <ref type="bibr" target="#b19">(Chen, Repp, &amp; Patel, 2002)</ref>, and so this choice minimizes noise in our data due to intrinsic synchronization variability. Synchronized tapping requires participants to keep track of time so that an upcoming movement can be planned to align with the next click. In a previous paper <ref type="bibr" target="#b84">(Symons et al., 2024)</ref>, we showed that presentation of distracting sounds and sound changes led to an expansion of subjective time, causing participants to wait for less time before making their next movement. This finding is conceptually similar to the filled duration illusion, in which silent intervals are perceived as being shorter in duration than intervals filled with sensory events <ref type="bibr" target="#b18">(Buffardi, 1971;</ref><ref type="bibr" target="#b63">Ortega &amp; López, 2008;</ref><ref type="bibr">Wearden et al., 2007)</ref>. A likely explanation is that unexpected sounds and sound changes lead to increased arousal, speeding up internal pacemakers and expanding subjective time <ref type="bibr" target="#b31">(Gibbon et al., 1984)</ref>. Importantly, larger acoustic changes led to greater temporal distortions: a one-semitone pitch change, for example, did not affect tap timing, but a six-semitone pitch change did, suggesting that the shift in timing was driven by sound salience rather than simple perception of acoustic change. These findings were consistent across online and in-lab participant samples, suggesting that this online paradigm can be used to accurately measure subtle tapping shifts.</p><p>This previous study used relatively simple sounds (e.g., complex tones and white noise bursts) that allowed for precise control over variations in the acoustic features of interest. The use of synthesized sounds enabled us to vary individual acoustic dimensions while keeping the stimuli otherwise constant. However, it remains an open question to what extent those results generalize to more naturalistic listening scenarios where sounds vary across multiple acoustic and linguistic features simultaneously. To address this question, here we used this synchronized tapping paradigm to investigate capture of attention by task-irrelevant naturalistic speech. Participants were asked to tap to the beat of a 2-Hz click track while ignoring a continuous stream of narrative speech (an audiobook recording of "The Old Man and the Sea"; Di <ref type="bibr" target="#b21">Liberto et al., 2015;</ref><ref type="bibr" target="#b15">Broderick et al., 2018;</ref><ref type="bibr" target="#b85">Teoh et al., 2019)</ref>. The degree to which listeners' tapping deviated from the beat of the click track (tapping asynchrony) provided a measure of temporal distortion. Based on prior work <ref type="bibr" target="#b84">(Symons, Dick, &amp; Tierney, 2024)</ref>, we predicted that salient acoustic changes, including changes in intensity, pitch, and spectral shape, would increase autonomic arousal, leading to an overestimation of the passage of time and more negative asynchronies (earlier tapping). To test whether temporal distortions could be elicited by linguistic unpredictability, we computed measures of word frequency, phoneme surprisal, and semantic surprisal, features that have been shown to elicit changes in neural tracking of continuous speech <ref type="bibr" target="#b15">(Broderick et al., 2018</ref><ref type="bibr" target="#b16">(Broderick et al., , 2022;;</ref><ref type="bibr" target="#b33">Gillis et al., 2021;</ref><ref type="bibr" target="#b92">Weissbart et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 1</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>A sample of 101 participants between the ages of 20 and 41 (M = 28.34 years, SD = 5.83; 65 female, 36 male, 0 non-binary) was recruited from the Prolific (prolific.co) online recruitment platform. Due to the National Institutes of Health (NIH) funding requirements, data on race and ethnicity were collected. We placed no geographic restrictions on Prolific, and therefore, racial and ethnic categories may not have applied to participants outside of the United States. However, we report them here for completeness: From the original sample, 99 participants reported their ethnicity as not Hispanic or Latino and 2 preferred not to report. Forty-five participants were Black or African American, 43 were White, 5 were Asian, 7 were more than one race, and 1 participant preferred not to report. Automated screening procedures were set to ensure that participants spoke English as their native language and had no known hearing impairments. The experiment was conducted using the online experiment platform Gorilla Experiment Builder <ref type="bibr" target="#b5">(Anwyl-Irvine et al., 2020)</ref>. Participants were required to complete the experiment on a desktop or laptop with Google Chrome as the web browser and instructed to wear headphones for the duration of the experiment. All experimental procedures were approved by the Ethics Committee in the Department of Psychological Sciences at Birkbeck, University of London. Each participant provided informed consent and received monetary compensation for their participation at a standard rate.</p><p>Data from participants who did not report English as one of their native languages on a subsequent questionnaire were excluded from analysis (N = 15). To ensure that participants were complying with task instructions to tap along to the click track, we imposed an additional set of criteria for exclusion: did not tap at all during one or more run (N = 2), failed to synchronize with the clicks (N = 9) meaning they showed no significant clustering of phases across taps according to circ_rtest in the Matlab Circular Statistics Toolbox <ref type="bibr" target="#b11">(Berens, 2009)</ref>, or whose tapping variability (standard deviation of intervals between tap and click) was greater than 100 ms (N = 17). We also removed any participant whose responses were coarsely quantized (&gt; 15 ms quantization) due to the use of Bluetooth keyboards (which participants were explicitly requested not to use). Compared to wired keyboards, Bluetooth keyboards bin responses in much longer intervals, and do not permit the temporal precision needed to measure small tapping shifts (~4-5 ms; <ref type="bibr" target="#b84">Symons et al., 2024)</ref>. To identify participants showing coarsely quantized responses, we binned the intertap intervals with an 0.1 Hz resolution, computed the autocorrelation function (0-100 ms lags), and then identified peaks in the autocorrelation function (minimum prominence = 0.3). This resulted in the removal of 1 additional participant. Lastly, we removed 1 participant who had &lt; 70% of valid taps for one or more excerpt. Valid taps were those occurring within 250 ms of a click (so if participants stopped tapping temporarily, these missed taps would be considered invalid) and falling within 3 standard deviations of their mean. The final sample consisted of 55 participants ages 20 to 40 (M = 29.09, SD = 6.17, 35 female, 20 male; racial/ethnic status using NIH reporting groupings: 55 not Hispanic or Latino, 20 Black or African American, 29 White, 6 more than one racial grouping). Of this sample, 26 participants reported receiving musical training (ranging from 1-20 years). However, only 10 participants could be classed as musicians based on the 6-year criterion suggested by prior research <ref type="bibr" target="#b97">(Zhang et al., 2020)</ref>. Therefore, musical training was not factored into the analysis. 1 Additionally, 24 participants reported experience with other languages (with age of second language acquisition ranging from age 1 to 35 years).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stimuli</head><p>Tapping sequences. The continuous speech consisted of an audiobook recording of "The Old Man and the Sea" spoken by a professional male narrator with an American English accent (see <ref type="bibr" target="#b15">Broderick et al., 2018;</ref><ref type="bibr" target="#b85">Teoh et al., 2019)</ref>. The audiobook was divided into four excerpts, each 2-3 minutes in duration. Each excerpt was presented simultaneously with a 2-Hz isochronous click sequence (Figure <ref type="figure">1</ref>). Clicks were broadband impulses spanning 10 time points (0.23 ms in duration with a 44.1 kHz sample rate). To ensure that the clicks were audible against the continuous speech, the peak amplitude of the speech was set to be 70% of the click amplitude. Prior to the onset of the continuous speech, 10 clicks presented against silence were added to allow participants time to synchronize their tapping with the click track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1 Speech Features and Tapping Analysis</head><p>Note. On the left, examples of speech feature vectors, which were extracted across the full duration of each excerpt. Acoustic and linguistic features were extracted from the audiobook and represented as vectors that were time-aligned with the tapping responses. On the right, a schematic of the tapping analysis. Listeners tapped to the beat of a click track (dark grey lines) while ignoring continuous speech in the background (light grey). Taps are represented in dotted red vertical bars. Using the mTRF toolbox, a decoder was trained to predict variations in speech features (amplitude envelope shown here) based on tapping asynchrony, which was represented 1 Given that this experiment was not designed to test effects of musicianship, we did not factor years of musical training into our primary analyses. However, because musical training affects beat synchronization (e.g., <ref type="bibr" target="#b86">Thompson et al., 2015)</ref> and time perception (e.g., <ref type="bibr" target="#b60">Mittal et al., 2024)</ref>, we have included a preliminary analysis of musical training in the Supplementary Materials that can inform future work.</p><p>as a single vector with values at each click time representing the difference between tap and click time.</p><p>Acoustic features. The amplitude envelope and relative pitch of the speech recordings were obtained from previous research using this audiobook <ref type="bibr" target="#b85">(Teoh et al., 2019)</ref>. The amplitude envelope was extracted by filtering the speech waveform between 80 -2,800 Hz and computing the absolute value of the Hilbert transform. The envelope was then low-pass filtered (cut-off = 30 Hz) and down-sampled to 128 Hz. This provided a measure of amplitude level. In addition, we computed the change in amplitude across successive time points by calculating the derivative over time. Relative pitch was computed by extracting the fundamental frequency (F0) of the speech signal at 128 Hz and applying a z-transform to normalize the pitch based on the speaker's vocal range. In addition, we computed the change in relative pitch across successive time points by calculating the derivative over time. Spectral centroid and spectral flux were extracted from each speech recording using the spectralCentroid and spectralFlux functions in the Audio Toolbox implemented in Matlab (version 2021a). Spectral centroid describes the center of gravity in the spectrum while spectral flux measures the change in the spectrum over time. Both spectral measures were computed with a 23.4-ms window and 15.6-ms overlap between successive windows, resampled to a 128 Hz sampling rate, and z-scored.</p><p>Linguistic features. All linguistic features were based off written transcripts of the audiobook. Phoneme surprisal was computed using a similar procedure to that reported in <ref type="bibr">Brodbeck et al. (2018)</ref>. First, phoneme onsets were automatically marked using the Gentle forced aligner (https://lowerquality.com/gentle/). Missing or incorrectly marked phonemes were adjusted by hand. Next, a phonetic dictionary with linked lexical frequencies was assembled by combining information from the SUBTLEX subtitle database <ref type="bibr" target="#b17">(Brysbaert &amp; New, 2009)</ref> and the CMU Pronouncing Dictionary (http://www.speech.cs.cmu.edu/cgi-bin/cmudict). Any words in the text that were not present in SUBTLEX were given a frequency equal to the lowest possible value. For each phoneme, we calculated two "cohort frequency values": first, we calculated the summed frequency of all words in the dictionary matching the set of phonemes spanning the beginning of the word to the current phoneme. Second, we calculated the sum of the frequency of all words matching the set of phonemes from the beginning of the word to the previous phoneme. Phoneme surprisal was calculated as the negative log2 of the ratio between the first and the second value. For the first phoneme of a word, phoneme surprisal was the negative log2 of the ratio between the summed frequency of all words in the dictionary beginning with that phoneme and the summed frequency of all words in the dictionary. Finally, phoneme surprisal was stored as a vector with values equal to the surprisal of each phoneme across the duration of the phoneme and zeros elsewhere. Non-zero portions of this vector were z-scored such that the mean of the non-zero values was 0 and the standard deviation was 1, to make sure that phoneme surprisal analysis did not simply reflect the difference between the presence versus absence of phonemes.</p><p>Word frequency was extracted from the SUBTLEX-US database <ref type="bibr" target="#b17">(Brysbaert &amp; New, 2009)</ref>. Word onsets were marked using Prosodylab-Aligner <ref type="bibr">(Gorman et al., 2011)</ref>. These markings were obtained from previous studies using this audiobook <ref type="bibr" target="#b15">(Broderick et al., 2018)</ref>. For each speech recording, we downsampled the recording to 128 Hz and extracted the time points corresponding to each word onset and offset. A custom Matlab script then searched for each word in the database. Word frequency was stored as a vector with values equal to the frequency of each word, and the value only changing with the onset of the next word. Non-zero portions of the vector were z-scored prior to analysis. Words that were not found in the database were set to the minimum word frequency value in the database. Contractions such as "aren't" are included in the database without the apostrophe (e.g., "aren't" is listed as "arent"). However, it was not clear how contractions that form words when taking out the apostrophe (e.g., "I'll" or "we're") are represented in the database. Since these instances were rare in the speech recordings we used, these words were ignored in the analysis. A full list of words not found in the database or excluded from analysis here can be found in the Supplementary Materials (Table <ref type="table">S2</ref>.1).</p><p>The semantic surprisal measure was obtained from previous research using this audiobook <ref type="bibr" target="#b3">(Anderson et al., 2024)</ref>. The text corresponding to each speech recording was passed to Open AI's GPT-2, which computed a single surprisal value for each word based on the preceding context (up to 1024 words). Each value represented the negative log probability estimate of each word. Semantic surprisal values were time-aligned with word onset and stored as a vector with surprisal values lasting the duration of the word, with the value changing at the onset of the next word. Non-zero portions of the vector were z-scored prior to analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure</head><p>Upon signing up to the study, participants were provided with a link to the experiment. After providing informed consent, participants completed a demographics questionnaire in which they reported their age, gender, language background, and musical experience. On-screen instructions were then provided. Participants were told that they would hear a series of clicks against some background sounds and instructed to tap to the beat of the clicks by pressing the 'j' key on the keyboard while ignoring the background speech. An example sequence of clicks presented against silence was provided to allow participants to practice tapping to the clicks. During the main task, participants heard each excerpt with the order of the four runs (and thus the order of stimulus presentation) randomized across participants. At the end of the experiment, participants were asked whether they experienced any technical issues that could have affected their performance on the task. No technical issues were reported. This experiment lasted approximately 20 minutes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data processing and analysis</head><p>Tapping asynchrony. Sound timing and participant response times were recorded in Gorilla (gorilla.sc). Custom Matlab scripts extracted the sound offset (relative to the start of the run) and subtracted this from the known sound duration to measure the sound delay for each run (with each run consisting of one 2-3-minute speech recording with clicks) and each individual. Participants' taps were extracted for each run and the difference between participants' tap time and the nearest click onset (tap-click asynchrony) was computed. The true asynchrony between participants' tap time and the click onsets could not be reliably recorded due to variations in the computer setup. To account for this variability, we subtracted the tap-click asynchrony at each time point from the mean tap-click asynchrony across the entire run. Instances in which there was no tap within +/-250 ms of a click onset were classed as missing taps and excluded from analysis. Likewise, taps greater than 3 standard deviations from the participant's mean tapping asynchrony for a given run were removed from analysis. Of the participants included, the percentage of taps removed on this basis ranged from 0.28 -5.41% (M = 1.99%, SD = 1.42%). Only participants with &gt; 70% of valid taps were included in the analysis. Taps within the first 5 seconds (10 clicks prior to the onset of the speech) were not included in the analysis.</p><p>Stimulus reconstruction. To determine the relationship between the features of continuous speech and tapping asynchrony, a linear model was trained to reconstruct an estimate of each feature separately based on tapping asynchrony using the multivariate temporal response function (mTRF) toolbox <ref type="bibr" target="#b20">(Crosse et al., 2016)</ref> implemented in Matlab (Version 2023b). To do this, we treated the tapping data as a vector consisting of a series of impulses at the click time points and zeros at all other time points, with the amplitude of each impulse equal to the corresponding tapping asynchrony. Non-zero portions of the tapping vector were z-scored prior to analysis. This tapping vector was then used to predict the speech features in the two seconds preceding the click using the 'backwards' option in the mTRF toolbox. A doubly-nested cross-validation procedure was used to identify the optimal regularization parameter and then test the model. First, we divided the data from the four runs into a training set consisting of 3 runs and a test set consisting of 1 run. Then a leave-one-out cross-validation procedure was conducted on the training data over time lags from zero to a maximum of two seconds, with the time lags selected based on previous research <ref type="bibr" target="#b84">(Symons, Dick, &amp; Tierney, 2024)</ref>. This procedure was used to obtain the optimal regularization parameter within the range of 2 1 to 2 10 . The model was then trained using the optimal regularization parameter. To evaluate the performance of the model, we determined the accuracy with which the model predicted speech features based on tapping asynchrony in the test data. The similarity between the predicted and observed data was computed using Pearson's correlation coefficient. This process was repeated four times, with each run taking its turn serving as the test data and the remaining three runs serving as the training data (e.g., model 1 was trained on runs 2-4 and tested on run 1). The prediction accuracy (r value) of the model and model coefficients were averaged across all four models.</p><p>Statistical analysis. To test whether variations in tapping asynchrony predicted the stimulus features of interest, we conducted a Monte Carlo analysis. During each of 1000 iterations, we randomly shuffled the tapping data within each excerpt for each participant. We then ran the temporal response function analysis reported above on the shuffled tapping data, computing the resulting prediction accuracy for each participant. Next, we took the median prediction accuracy across participants, resulting in a null distribution of median cross-participant prediction accuracy over the 1000 iterations. Finally, we compared the median of the prediction accuracy across participants for the original data to this null distribution. P-values, therefore, represent the probability of the observed r-value given the distribution of r-values obtained when shuffling the tapping data.</p><p>Prior work shown that temporal distortions occur between 250-750 ms following salient acoustic changes <ref type="bibr" target="#b84">(Symons, Dick &amp; Tierney, 2024)</ref>. To examine the time course of temporal distortions elicited by variations in continuous speech, we examined the behavioral temporal response function (behavioral TRF), which represents the degree to which tapping asynchrony predicted variations in each feature at each time point (within the 2-second time window) preceding the tap. One-sample rank sum tests compared model coefficients to zero across participants at each time point within the 2-second time window preceding the tap to establish the time course of the tapping shift. P-values were corrected for multiple comparisons across time (257 time points; <ref type="bibr" target="#b10">Benjamini &amp; Hochberg, 1995)</ref>.</p><p>There was a significant correlation between amplitude envelope and many of the other stimulus features (see Supplementary Materials, Figure <ref type="figure">S1</ref>.1). Therefore, any effects observed for the other stimulus features could be partially driven by amplitude. To ensure that the effects observed for other features were not solely driven by amplitude, we ran a follow-up analysis covarying out amplitude. To do this, we constructed a linear regression using amplitude to predict each of the other features and extracted the residuals from the model. We then conducted the same statistical analyses as above, but with the residuals from the regression model as the dependent variable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transparency and Openness</head><p>This study was not preregistered. The sample size was determined based on previous research <ref type="bibr" target="#b84">(Symons, Dick, &amp; Tierney, 2024)</ref>. All data inclusion criteria, manipulations, and measures are reported here. Stimulus vectors, anonymous data, and analysis code are available on OSF (https://osf.io/pc5tw/).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Tapping asynchrony significantly predicted amplitude, amplitude change, pitch change, spectral flux, and phoneme surprisal (Figure <ref type="figure" target="#fig_0">2A</ref>). Across features, tapping asynchrony was most consistently linked to speech features occurring 550-600 ms before the click (Figure <ref type="figure" target="#fig_0">2B</ref>). Moments in the speech featuring high amplitude, for example, were linked to earlier tapping roughly half a second later. Earlier tapping was also linked to moments of high acoustic change in the speech half a second earlier, including changes in amplitude, pitch, and spectral shape (frequency content). These results suggest that acoustic change increases physiological arousal, with a lag of around 500 ms, causing participants to experience an expansion of subjective time and, therefore, tap earlier.</p><p>Importantly, effects of speech characteristics on tapping asynchrony were not limited to acoustic measures but extended to linguistic characteristics: low phonemic predictability also led participants to tap sooner. Therefore, failure of linguistic prediction led to faster tapping roughly a half second later, suggesting an expansion of subjective time due to increased arousal. shuffling the tapping data. B) Behavioral temporal response function (TRF). Coefficients (in arbitrary units) represent the degree to which tapping asynchrony predicts the speech feature at each time lag. Along the x-axis, the zero time lag indicates the onset of the click to which participants were attempting to synchronize. Along the y-axis, a positive coefficient indicates that a higher value (e.g., larger amplitude) is associated with later tapping while a negative coefficient indicates that a higher value is associated with earlier tapping. Thick blue lines represent lags at which the coefficients significantly differ from zero (with FDR-correction for multiple comparisons).</p><p>To ensure that the links between speech features and tapping speed were not simply driven by variations in amplitude, we ran a follow-up analysis covarying out amplitude. Figure <ref type="figure" target="#fig_1">3</ref> shows the median prediction accuracy versus a histogram of the null distribution of prediction accuracies, as well as model coefficients, when covarying for amplitude. Effects of amplitude and pitch change (both highly correlated with amplitude, see Supplementary Materials) were no longer significant. However, even when covarying out amplitude, tapping asynchrony was significantly linked to both preceding spectral flux and phoneme surprisal. Both acoustic change and linguistic surprisal, therefore, continued to predict tapping speed, even once the effects of amplitude were controlled for. Coefficients (in arbitrary units) represent the degree to which tapping asynchrony predicts the speech feature after removing the contribution of amplitude at each time lag. Along the x-axis, the zero time lag indicates the onset of the click to which participants were attempting to synchronize. Along the y-axis, a positive coefficient indicates that a higher residual value (e.g., higher pitch after accounting for amplitude) is associated with later tapping while a negative coefficient indicates that a higher residual value is associated with earlier tapping. Thick blue lines represent lags at which the coefficients significantly differ from zero (with FDR-correction for multiple comparisons).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>We find that acoustic changes in task-irrelevant speech, including changes in amplitude, pitch change, and spectral shape, are linked to distortions in subjective time, as measured with a synchronized tapping paradigm. Our prediction, based on the results of our previous experiment <ref type="bibr" target="#b84">(Symons et al. 2024)</ref>, was that acoustic changes would be linked to earlier tapping 250-750 ms later. The functions relating tapping asynchrony to acoustic change are roughly consistent with these predictions: for amplitude change, pitch change, and spectral flux, greater change was linked to earlier tapping between 500 and 1250 ms later. This pattern suggests that acoustic change led to an increase in arousal arising within around 500 ms, expanding subjective time for around 750 milliseconds before returning to baseline. We also find that sounds with greater amplitude are linked to earlier tapping in the same time range, suggesting that louder sounds are more salient, leading to greater attentional orienting, increased arousal, and expanded subjective time.</p><p>Importantly, we find that the link between tapping asynchrony and characteristics of taskirrelevant speech is not limited to acoustic features. There was a robust relationship between phonemic surprisal and asynchrony, such that greater surprisal was linked to earlier tapping. The time course of this effect closely aligned with the time course of the effect of amplitude on tapping; however, phoneme surprisal and amplitude only weakly correlated (rs = 0.07), and the phoneme surprisal effect remained significant even after covarying for amplitude. This finding suggests that phonemic surprisal captures attention, increasing arousal and expanding subjective time. We did not find any significant relationship between semantic surprisal and time perception; however, this null result could also reflect a lack of statistical power and so should be interpreted with caution . Our finding that semantic surprisal does not affect tapping performance conflicts somewhat with the finding of <ref type="bibr" target="#b71">Röer et al. (2019)</ref> that semantic unpredictability in speech can interfere with the performance of a concurrent visual serial memory task, but as the authors of that paper suggest, this could reflect interference by process between semantic integration and tracking of serial order. Our results also conflict somewhat with <ref type="bibr" target="#b45">Kothinti &amp; Elhilali (2023)</ref>, who found that semantic surprisal in non-linguistic auditory scenes was a predictor of perceptual salience, as measured via salience ratings.</p><p>Our primary framework for explaining our results is that they reflect fluctuations in arousal, which have been shown to be linked to expansions and contractions in subjective time <ref type="bibr" target="#b54">(Maricq et al., 1981;</ref><ref type="bibr" target="#b25">Droit-Volet &amp; Meck, 2007;</ref><ref type="bibr" target="#b89">Wearden &amp; Penton-Voak 2007;</ref><ref type="bibr">Schwartz et al., 2013)</ref>. However, an alternate possible explanation is that acoustic events in the period just before a click are perceptually fused with the click onset, resulting in a hybrid percept with an earlier time of onset. This could cause participants to perceive that their tapping is later than the hybrid perceived click, leading them to make their next movement earlier in time. Similar effects of integration of auditory events on the phase of synchronized tapping are demonstrated in <ref type="bibr" target="#b68">Repp (2004)</ref>, with a fixed window for temporal integration of around 120 ms. This perceptual fusion account could explain why the functions relating amplitude change, spectral flux, and amplitude level all peak just before the previous click (at 500 ms). However, this explanation would have difficulty explaining why phoneme surprisal is linked to tapping asynchrony, given that correlations between phoneme surprisal and acoustic features were rather weak (rs = 0.10 or lower; see Figure <ref type="figure">S1</ref>.1 in the Supplementary Materials). Nevertheless, to rule out this explanation we ran a follow-up experiment in which the clicks and the task-irrelevant speech were presented in separate ears, preventing perceptual fusion. This additional experiment also enabled us to determine the replicability of the shape of the functions relating speech features to tapping over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 2</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overview</head><p>Our results from Experiment 1 showed consistent shifts in synchronized tapping across multiple acoustic and linguistic features. The time course of this effect aligned with our previous research using simpler sounds (250-750 ms; <ref type="bibr" target="#b84">Symons et al., 2024)</ref>. However, because the clicks and speech were presented in the same ear, we could not rule out the possibility that the observed tapping shifts were driven by perceptual fusion of the speech with clicks as opposed to increases in arousal. Therefore, we conducted a second experiment aimed at (i) ruling out the possibility that tapping shifts were driven by perceptual fusion and (ii) determining the replicability and generalizability of the relationship between stimulus dynamics and tapping shifts. To this end, Experiment 2 aimed to replicate the results of Experiment 1 in a new sample of participants and audiobook recordings. Listeners tapped to the beat of a click track while ignoring excerpts from "The Old Man and the Sea" (different from the excerpts used in Experiment 1). To determine whether the effects observed in Experiment 1 were driven by perceptual fusion, Experiment 2 presented clicks and speech in opposite ears. Half of the participants heard clicks in the left ear and speech in the right, while the other half of the participants heard clicks in the right ear and speech in the left. If the previous results were driven by perceptual fusion, there should be no relationship between tapping asynchrony and the features present in continuous speech. However, if variations in continuous speech distort internal timekeeping by changing physiological arousal, tapping asynchrony should predict acoustic and linguistic features even when the two streams are present in opposite ears.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>A sample of 194 participants between the ages of 18 and 40 (M = 30.91, SD = 5.99, 70 female, 120 male, 4 non-binary) was recruited from the Prolific (prolific.co) online recruitment platform. Data on race and ethnicity were not recorded since this experiment was not conducted with NIH funding. Automated screening procedures were set to ensure that participants spoke English as their native language and had no known hearing impairments. The experiment was conducted using the online experiment platform Gorilla Experiment Builder <ref type="bibr" target="#b5">(Anwyl-Irvine et al., 2020)</ref>. Participants were required to complete the experiment on a desktop or laptop with Google Chrome as the web browser. All participants were instructed to wear headphones for the duration of the experiment, and completed a headphone screening test <ref type="bibr" target="#b59">(Milne et al., 2021b)</ref> to ensure that they were wearing headphones. The headphone test was needed in Experiment 2 because, unlike in Experiment 1, it was essential that clicks and speech were presented in opposite ears. All experimental procedures were approved by the Ethics Committee in the Department of Psychological Sciences at Birkbeck, University of London. All participants provided informed consent and received monetary compensation for their participation at a standard rate.</p><p>Data from participants who did not report English as one of their native languages on a questionnaire were excluded from analysis (N = 3). Participants who experienced technical issues with sound loading (N = 20) were excluded. Since the use of headphones was essential for this experiment, participants who achieved less than 6/6 on the headphone screening test <ref type="bibr" target="#b59">(Milne et al., 2021b)</ref> were also removed (N = 26). Following the same procedure as Experiment 1, participants who did not tap at all during one or more runs (N = 1), showed tapping variability &gt; 100 ms (N = 37), failed to synchronize with the clicks (N = 7), had keyboard quantization &gt; 15 ms (N = 4), or had fewer than 70% of valid taps during one or more run (N = 2) were excluded. The final sample consisted of 94 participants ages 19 to 40 (M = 32.05, SD = 5.46, 33 female, 59 male, 2 non-binary). Of this sample, 44 participants reported receiving musical training (ranging from 1-15 years), with only 10 participants reporting at least 6 years of training <ref type="bibr" target="#b97">(Zhang et al., 2020)</ref>. For this reason, musical training was not factored into the analysis. Twenty-eight participants reported experience with other languages (with age of second language acquisition ranging from age 1 to 35 years).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stimuli</head><p>As in Experiment 1, tapping sequences consisted of four 2-Hz isochronous click sequences (2-3 minutes in duration). The properties of the clicks were identical to Experiment 1. Each sequence was presented simultaneously with continuous speech excerpts from the same audiobook as Experiment 1 ("The Old Man and the Sea"), but consisted of four different excerpts from those used previously. The peak amplitude of the continuous speech was set to be 70% of the click amplitude. In this experiment, clicks and speech were presented in opposite ears, with the ear in which clicks/speech were presented counterbalanced across participants. Acoustic (amplitude, amplitude change, relative pitch, pitch change, spectral centroid, spectral flux) and linguistic (word frequency, phoneme surprisal, semantic surprisal) features were extracted from the continuous speech following the same procedure as Experiment 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure</head><p>The procedure for Experiment 2 was identical to Experiment 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data processing and analysis</head><p>The data processing and analysis protocol was identical to Experiment 1. Of the participants included, the percentage of taps removed on our pre-defined exclusion criteria ranged from 0 -8.08 % (M = 1.92%, SD = 1.34%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>When clicks and speech were presented in opposite ears, tapping asynchrony was most consistently linked to variations in the speech signal 550 ms before the click (Figure <ref type="figure" target="#fig_2">4</ref>). As in Experiment 1, moments in the speech featuring high amplitude were linked to earlier tapping roughly half a second later. Earlier tapping was also preceded by time points in which the speech acoustics rapidly changed, including changes in amplitude, pitch, and spectral shape. As in Experiment 1, the relationship between speech features and tapping speed was not limited to acoustic factors. Linguistic surprisal, including both phonemic and semantic surprisal, led to earlier tapping, suggesting that failure of prediction during speech listening is linked to increased arousal and, therefore, expansion of subjective time.</p><p>Broadly, then, we replicated the Experiment 1 results, even though in this experiment the click and speech were in separate ears. There were some minor differences between the patterns of results in Experiment 2 versus Experiment 1: contrary to Experiment 1, relative pitch and semantic surprisal were linked to tapping asynchrony in Experiment 2. Coefficients (in arbitrary units) represent the degree to which tapping asynchrony predicts the speech feature at each time lag. Along the x-axis, the zero time lag indicates the onset of click to which participants were attempting to synchronize. Along the y-axis, a positive coefficient indicates that a higher value (e.g., larger amplitude) is associated with later tapping while a negative coefficient indicates that a higher value is associated with earlier tapping. Thick blue lines represent lags at which the coefficients significantly differ from zero (with FDR-correction for multiple comparisons).</p><p>To ensure that the relationships between speech features and tapping asynchrony were not simply driven by variations in amplitude, we ran a follow-up analysis covarying out amplitude (Figure <ref type="figure">5</ref>). Tapping asynchrony was significantly linked to preceding variations in amplitude change, relative pitch, pitch change, spectral flux, and semantic surprisal after accounting for variations in amplitude. Both acoustic change and linguistic surprisal, therefore, continued to predict tapping speed, even once the effects of amplitude were controlled for, though the effect of phoneme surprisal were no longer significant in this analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 5</head><p>Experiment 2: Relationship Between Tapping Asynchrony and Features of Task-Irrelevant Speech (Covarying for Amplitude) Note. (A) Prediction accuracy when covarying out amplitude. Amplitude was covaried out by regressing each speech feature against amplitude and extracting the residuals. The dashed line shows the correlation coefficient (Pearson's R) representing the relationship between the time series of each speech feature (with amplitude removed) and the predicted time series based on tapping asynchrony as estimated by the mTRF model. Histograms show the permutationgenerated null distribution of R values, representing the relationship between the time series of each speech feature (with amplitude removed) and the time series predicted by the shuffled tapping data. P-values represent the probability of the observed r-value given the distribution of r-values obtained when shuffling the tapping data. B) Behavioral temporal response function (TRF). Coefficients (in arbitrary units) represent the degree to which tapping asynchrony predicts the speech feature after removing the contribution of amplitude at each time lag. Along the x-axis, the zero time lag indicates the onset of the click to which participants were attempting to synchronize. Along the y-axis, a positive coefficient indicates that a higher residual value (e.g., higher pitch after accounting for amplitude) is associated with later tapping while a negative coefficient indicates that a higher value is associated with earlier tapping. Thick blue lines represent lags at which the coefficients significantly differ from zero (with FDR-correction for multiple comparisons).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>We find that when participants listen to speech in one ear while tapping to a click track in the other, moments in the speech that feature high amplitude, rapid acoustic change, and linguistic surprisal are followed by earlier tapping. That the link between speech features and tapping speed is present even when the target clicks and distracting speech are in opposite ears suggests that this relationship cannot be driven by perceptual fusion between the clicks and sound changes within a nearby temporal window <ref type="bibr" target="#b68">(Repp, 2004)</ref>. Nevertheless, in this experiment we once again find that the function relating speech characteristics to tapping peaks at around 550 ms, just before the presentation of the previous click. Why are acoustic and linguistic factors particularly salient just before the presentation of a click?</p><p>In this paradigm, the task-relevant stimulus (i.e., the click) was perfectly predictable. As a result, participants knew that the time between clicks could only contain task-irrelevant sound. Participants may, therefore, have manipulated temporal attention to diminish the salience of any sounds presented in between clicks. One potential mechanism for this attentional weighting could be periodic modification of arousal. <ref type="bibr" target="#b79">Shalev and Nobre (2022)</ref>, for example, demonstrated that when temporally predictable stimuli were presented, tonic arousal was overall lowered, but increased briefly just before upcoming stimuli. In our experiment, then, participants may have lowered tonic arousal between clicks, tamping down the response to task-irrelevant speech. However, just before the onset of the next click, they may have increased arousal, making themselves vulnerable to attentional capture by speech features, including acoustic and linguistic change.</p><p>In Experiment 2, tapping shifts were linked to both phoneme surprisal and semantic surprisal. The effects for phoneme surprisal replicate effects observed in Experiment 1, though phoneme surprisal was not significant in the most conservative analysis in which clicks and speech were presented in opposite ears and amplitude was covaried out. By contr ast, in Experiment 2, we found that semantic surprisal was significantly linked to subsequent tapping speed. Effects of semantic surprisal were not observed in Experiment 1 and should therefore be interpreted with caution. Given that the link between phonemic surprisal and tapping speed was weaker in Experiment 2, we ran one further experiment to determine whether this relationship replicated in a different talker reading a different text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 3</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overview</head><p>Results from Experiment 2 largely replicated those from Experiment 1, suggesting that acoustic and linguistic features of continuous speech can increase physiological arousal, resulting in a distortion of internal timekeeping. However, there were subtle differences across experiments and analyses as to which features were predicted by tapping shifts. Experiment 3 aimed to identify (i) which features were most consistently linked to shifts in tapping asynchrony and (ii) whether the effects observed in Experiments 1 and 2 generalized to a novel speaker and a different narrative. In this experiment, a new sample participants heard an audiobook recording of "The Northern Lights" <ref type="bibr" target="#b65">(Pullman 1995)</ref> spoken by a female narrator. To determine which effects were most consistent across experiments, we focused our analysis on the features that were consistently linked to tapping asynchrony across both the primary analyses of Experiments 1 and 2: amplitude, amplitude change, pitch change, spectral flux, and phoneme surprisal. Based on the results of Experiments 1 and 2, we predicted that shifts in tapping asynchrony would predict variation in each of these features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>A sample of 98 participants between ages 20-66 (M = 38.52, SD = 9.69; 56 female, 42 male, 0 non-binary) was recruited from the Prolific (prolific.co) and SONA recruitment platforms. No data on race or ethnicity were collected because the study was not conducted under NIH funding. All participants completed the study via Gorilla Experiment Builder <ref type="bibr" target="#b5">(Anwyl-Irvine et al., 2020)</ref>. Experimental procedures were approved by the Ethics Committee in the Department of Psychological Sciences at Birkbeck, University of London, and participants were provided with monetary compensation or course credit for their participation at a standard rate.</p><p>Data from participants who did not report English as their native language (N = 8) were excluded from analysis. Additionally, participants who did not tap during one or more run (N = 0), tapped out of phase with the clicks (N = 5), had tapping variability &gt; 100 ms (N = 14), showed substantial (&gt; 15 ms) keyboard quantization (N = 4), or fewer than 70% of valid taps (N = 2) were removed.</p><p>The final sample consisted of 65 participants (M = 39.92, SD = 10.13, 37 female, 28 male). Of this sample, 25 participants reported receiving musical training (ranging from 1-15 years). Only 6 could be classed as musicians based on a 6-year criterion <ref type="bibr" target="#b97">(Zhang et al., 2020)</ref>. Fifteen participants reported experience with other languages with the age of second language acquisition ranging from 1-28 years.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stimuli</head><p>The continuous speech consisted of an audiobook recording of "The Northern Lights" <ref type="bibr" target="#b65">(Pullman 1995)</ref> spoken by a female narrator with a southern British English accent. The audiobook was divided into four excerpts presented simultaneously with a 2-Hz isochronous click sequence. All other aspects of stimulus generation were identical to Experiments 1 and 2.</p><p>Only features that were significantly linked to tapping asynchrony in the primary analyses for both Experiments 1 and 2 were extracted and analyzed. This included: amplitude, amplitude change, pitch change, spectral flux, and phoneme surprisal. All features were extracted and processed according to the same steps as Experiments 1 and 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure</head><p>The procedure for Experiment 3 was identical to Experiments 1 and 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data processing and analysis</head><p>The data processing and analysis protocol was identical to Experiments 1 and 2. Of the participants included, the percentage of taps removed on our pre-defined exclusion criteria ranged from 0.14 -9.97 % (M = 1.81%, SD = 1.70%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Results using a new audiobook with a different talker show that shifts in tapping asynchrony are linked to higher amplitude, greater changes in amplitude, and increased phonemic surprisal (Figure <ref type="figure" target="#fig_3">6</ref>). The time course of the effects appears broadly consistent with Experiments 1 and 2, with higher amplitude, greater amplitude change, and phonemic surprisal variations linked to tapping shifts approximately 550 ms later. However, significant effects were not observed for pitch change or spectral flux in this experiment, suggesting that the extent to which these features elicit tapping shifts may be speaker-specific. Coefficients (in arbitrary units) represent the degree to which tapping asynchrony predicts the speech feature at each time lag. Along the x-axis, the zero time lag indicates the onset of the click to which participants were attempting to synchronize. Along the y-axis, a positive coefficient indicates that a higher value (e.g., larger amplitude) is associated with later tapping while a negative coefficient indicates that a higher value is associated with earlier tapping. Thick blue lines represent lags at which the coefficients significantly differ from zero (with FDR-correction for multiple comparisons).</p><p>A with Experiments 1 and 2, we ran a follow-up analysis covarying out amplitude to test if the relationship between tapping asynchrony and phoneme surprisal was driven by variations in amplitude (Figure <ref type="figure">7</ref>). Even when covarying for amplitude, tapping asynchrony significantly predicted variations in phoneme surprisal. Effects of amplitude change, pitch change, and spectral flux were not significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 7</head><p>Experiment 3: Relationship Between Tapping Asynchrony and Features of Task-Irrelevant Speech (Covarying for Amplitude) Note. (A) Prediction accuracy when covarying out amplitude. Amplitude was covaried out by regressing each speech feature against amplitude and extracting the residuals. The dashed line shows the correlation coefficient (Pearson's r) representing the relationship between the time series of each speech feature (with amplitude removed) and the predicted time series based on tapping asynchrony as estimated by the mTRF model. Histograms show the permutation-generated null distribution of r-values, representing the relationship between the time series of each speech feature (with amplitude removed) and the time series predicted by the shuffled tapping data. P-values represent the probability of the observed r-value given the distribution of r-values obtained when shuffling the tapping data. B) Behavioral temporal response function (TRF). Coefficients (in arbitrary units) represent the degree to which tapping asynchrony predicts the speech feature after removing the contribution of amplitude at each time lag. Along the x-axis, the zero time lag indicates the onset of the click to which participants were attempting to synchronize. Along the y-axis, a positive coefficient indicates that a higher residual value (e.g., greater phoneme surprisal after accounting for amplitude) is associated with later tapping while a negative coefficient indicates that a higher value is associated with earlier tapping. Thick blue lines represent lags at which the coefficients significantly differ from zero (with FDR-correction for multiple comparisons).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Experiment Comparison</head><p>To give the reader an overview of the results, Table <ref type="table">1</ref> shows a summary of the findings from Experiments 1, 2, and 3. Some features showed inconsistent results, and were potentially driven by differences in the participant sample or specific excerpts used. However, variations in amplitude, amplitude change, and phoneme surprisal were consistently linked to changes in tapping asynchrony across all experiments. Therefore, we compared prediction accuracies and behavioral TRFs across experiments for each of these 3 features. We first used a Kruskal-Wallis test to compare prediction accuracies for each feature between Experiments 1-3. There was no difference in prediction accuracies across experiments for amplitude, H(2) = 3.87, p = 0.144, amplitude change, H(2) = 2.40, p = 0.302, or phoneme surprisal, H(2) = 0.33, p = 0.849. We then used Spearman's correlations to compare the TRF shapes for the 3 features that showed significant effects across experiments. For each feature, p-values were corrected for multiple correlations (3 tests) using Bonferroni correction. As shown in Figure <ref type="figure">8</ref>, the shapes of the TRFs were remarkably similar across experiments. TRF shape correlations were significant for all three features: amplitude (Experiment 1 vs 2: rs = 0.90, p &lt; 0.001; Experiment 1 vs 3: rs = 0.87, p &lt; 0.001; Experiment 2 vs 3: rs = 0.91, p &lt; 0.001), amplitude change (Experiment 1 vs 2: rs = 0.84, p &lt; 0.001; Experiment 1 vs 3: rs = 0.72, p &lt; 0.001; Experiment 2 vs 3: rs = 0.75, p &lt; 0.001), and phoneme surprisal (Experiment 1 vs 2: rs = 0.49, p &lt; 0.001; Experiment 1 vs 3: rs = 0.28, p &lt; 0.001; Experiment 2 vs 3: rs = 0.32, p &lt; 0.001).</p><p>Overall, our results suggest that the TRF shape for amplitude and phoneme surprisal features are highly replicable across different speakers and participant samples. Moreover, the shape of the TRF is unaffected by whether the clicks and the speech are presented in the same ear versus opposite ears (Experiment 1 vs 2), suggesting that the link between speech features and tapping speed is for the most part not driven by perceptual fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 8</head><p>Behavioral Temporal Response Functions from Experiments 1-3 Note. Behavioral TRFs from Experiment 1 (clicks and speech in the same ear, blue line) and Experiment 2 (clicks and speech in opposite ears, black line), and Experiment 3 (different speaker and audiobook, red line). Coefficients (in arbitrary units) represent the degree to which tapping asynchrony predicts the speech feature at each time lag, with positive coefficients indicating a higher value (e.g., larger amplitude) is associated with later tapping and negative coefficients indicating that a higher value is associated with earlier tapping. The zero time lag indicates the onset of the click to which participants were attempting to synchronize. For visualization purposes only, amplitude measures were re-scaled by dividing coefficients by the maximum value so that Experiment 3 coefficients could be viewed on the same plot as Experiments 1 and 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>With a new speaker and audiobook, Experiment 3 replicates the previous two experiments, showing that salient variations in acoustic and linguistic features lead to earlier tapping. These effects were consistently observed for amplitude measures as well as phoneme surprisal. However, contrary to Experiments 1 and 2, tapping shifts were not linked to pitch change or spectral flux. One simple explanation for these null results in Experiment 3 is that the speech excerpts in Experiment 3 contained less pitch and spectral variation. In support of this explanation, Experiment 1 excerpts contained larger pitch changes and greater variation in the spectrum compared to Experiment 3 (p &lt; 0.001; see Supplementary Materials for further details). In line with our prior work showing no significant tapping shifts following small (1 semitone) pitch changes <ref type="bibr" target="#b84">(Symons et al., 2024)</ref>, it may be that a certain threshold must be reached in order for acoustic variations to elicit the increases in arousal thought to underpin shifts in tapping asynchrony.</p><p>Nonetheless, cross-experiment comparisons suggest that, when speech features are sufficiently salient, the effects on tapping behavior are remarkably stable across different participant samples and speech excerpts. For amplitude and phoneme surprisal measures, the time course of tapping shifts significantly correlated across all three experiments. Taken together, results from all three experiments suggest that acoustic and linguistic features of continuous natural speech can increase arousal and disrupt internal timekeeping. However, the degree of behavioral disruption in response to a given speech sample likely depends on the amount of acoustic or linguistic variation in the signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>General Discussion</head><p>Across three experiments, we find that as listeners tap to a series of clicks, dynamic features of task-irrelevant speech continuously distort subjective time, causing tap timing to drift forwards and backwards. This distortion is found even when the speech and clicks are presented in separate ears. The best-fitting explanation for this effect is that changes in salience of taskirrelevant speech are linked to changes in arousal which, in turn, modulate the speed of internal timekeeping processes <ref type="bibr" target="#b31">(Gibbon et al., 1984)</ref>, biasing listeners' estimates of when the next click will occur. Importantly, we find that tap timing is related not only to acoustic characteristics of speech, most notably amplitude, but also phonemic surprisal. These results suggest that attentional orienting to sound takes place after the initial stages of linguistic analysis.</p><p>These results are broadly consistent with the predictions made by computational models of the salience of auditory scenes. For example, a common prediction across many models is that sudden changes in acoustic features will capture attention <ref type="bibr" target="#b43">(Kayser et al., 2005;</ref><ref type="bibr" target="#b40">Kalinli &amp; Narayanan, 2007;</ref><ref type="bibr" target="#b27">Duangudom &amp; Anderson, 2007)</ref>, inspired by vision research using eye tracking data as ground truth <ref type="bibr" target="#b62">(Niebur et al., 2002)</ref>, and indeed, we find here that subjective time expands roughly 500 ms after acoustic change. Our results also confirm the role of predictability in salience, as postulated by other models <ref type="bibr" target="#b87">(Tsuchida &amp; Cottrell, 2012;</ref><ref type="bibr" target="#b41">Kaya &amp; Elhilali, 2014)</ref>. On the other hand, our results suggest that computational models of salience designed to apply to a broad class of auditory scenes may not capture some of the factors driving attentional capture by taskirrelevant speech. In particular, future computational models of the salience of speech should incorporate phonemic predictability. Future research could also investigate whether there are additional domain-specific factors driving the salience of auditory scenes, such as the predictability of melody <ref type="bibr" target="#b22">(Di Liberto et al., 2020)</ref> and rhythm <ref type="bibr" target="#b96">(Zalta et al., 2024)</ref> in music.</p><p>Our leading explanation for why salient speech distorts synchronization timing is that changes in the salience of ongoing sounds modulate arousal, which in turn expands and contracts subjective time. This explanation fits our finding that the effects of task-irrelevant speech on synchronization are maintained even when the speech and clicks are presented in alternate ears. Direct evidence for this explanation, however, would require concurrent measurement of one or more physiological measures of arousal. While internal timing distortions have been linked to pupil dilation in monkeys <ref type="bibr" target="#b83">(Suzuki et al., 2016)</ref>, the link between physiological arousal and subjective time remains inconclusive in humans <ref type="bibr" target="#b93">(Williams et al. 2020)</ref>. Future research, therefore, should measure interference with synchronization by task-irrelevant sounds alongside physiological assessments such as pupil dilation or the galvanic skin response, to determine whether physiological arousal and distortion of subjective time correlate across time.</p><p>Precise perception of time is vitally important for perceiving speech and music. Differences in voice onset time between voiced and unvoiced consonants, for example, are around 40 ms on average <ref type="bibr" target="#b61">(Morris et al., 2008)</ref>, and timing also helps convey phrase boundaries <ref type="bibr" target="#b82">(Streeter, 1978)</ref>, linguistic focus <ref type="bibr" target="#b37">(Ip &amp; Cutler, 2022)</ref>, and lexical stress <ref type="bibr" target="#b78">(Severijnen et al., 2024)</ref>. Although music listening tends to place less stringent demands upon temporal processing than speech listening <ref type="bibr" target="#b0">(Albouy et al., 2020)</ref>, during performance musicians must synchronize with each other by rapidly correcting differences in timing between movement and sound that can be as small as 3 ms <ref type="bibr" target="#b69">(Repp, 2000;</ref><ref type="bibr" target="#b53">Madison &amp; Merker, 2004)</ref>. Given the importance of precise perception and production of timing for human communication, it is surprising and somewhat disconcerting that time perception is constantly subject to distortion by task-irrelevant sound. However, the effect sizes we report here and in <ref type="bibr" target="#b84">Symons et al. (2024)</ref> are small enough that these distortions are unlikely to affect conscious perception. In <ref type="bibr" target="#b84">Symons et al. (2024)</ref>, for example, we reported that the sudden onset of a highly aggravating drill sound distorted tap timing by only 4-5 ms. This is well below the average threshold for conscious perception of differences in interval timing, which is around 20 ms <ref type="bibr" target="#b53">(Madison &amp; Merker, 2004)</ref>. Here, we show that the relationship between speech salience and temporal distortion is relatively weak, with mean correlations between predicted and actual amplitude of around 0.02. Most of the variability in participants' tapping, therefore, was driven by other factors, such as intrinsic motor and timekeeper variability, as well as the accuracy of auditory-motor error correction <ref type="bibr" target="#b94">(Wing &amp; Kristofferson, 1973;</ref><ref type="bibr" target="#b77">Semjen et al., 1998;</ref><ref type="bibr" target="#b47">Krause et al., 2010)</ref>. However, there are reliable individual differences in the extent to which timing is distorted by the salience of task-irrelevant sounds <ref type="bibr" target="#b84">(Symons et al. 2024)</ref>, with a few participants' tapping distorted by as much as 20 ms, an effect potentially strong enough to interfere with conscious perception of timing. Future research could investigate whether fluctuations in the salience of naturalistic sound streams can have meaningful impacts on the ability to carry out perceptualmotor tasks requiring fine temporal precision. For example, analysis of live jazz performances could investigate whether the acoustic and statistical characteristics of improvised solos are linked to small distortions in the timing of the rhythm section.</p><p>The paradigm we introduce here for assessing attentional orienting to task-irrelevant speech has several advantages, making it a useful tool for answering outstanding questions about auditory salience. First, it is highly reliable, with cross-condition correlations of up to r = 0.88 <ref type="bibr" target="#b84">(Symons et al., 2024)</ref>. Second, as demonstrated here, it can be used to simultaneously investigate the role of many different attributes of complex, naturalistic sound signals in driving attentional capture. Third, it can be conducted online and in a relatively short amount of time (total experiment length was 20 minutes on average). Finally, synchronization is a task that measures a natural behavior <ref type="bibr" target="#b73">(Savage et al., 2015)</ref>, is simple to explain, and is accessible to almost any experimental population, including children as young as three years old <ref type="bibr" target="#b44">(Kirschner &amp; Tomasello, 2009;</ref><ref type="bibr" target="#b95">Woodruff-Carr et al., 2014)</ref>. Nevertheless, because it does not rely on responses being correct or incorrect, even typically-developing adults do not perform at ceiling <ref type="bibr">(Thomson et al., 2015)</ref>.</p><p>One weakness of these studies is that they were conducted online, and therefore we cannot say anything definite about the environment in which the participants carried out the experiments. Some participants, for example, may have completed the experiments in a distracting listening environment, which could lessen the effect of task-irrelevant speechparticipants who are already distracted by fluctuations in ambient background noise may be less affected by the addition of yet another sound stream. Our study may, therefore, under-estimate the effects of task-irrelevant speech on subjective time, and the null effects we report here (such as the lack of influence of semantic surprisal in Experiment 1) should be interpreted with caution. Nevertheless, our prior study using a simpler version of the paradigm in which distracting sounds and sound events occurred between clicks found highly similar results when comparing in-lab versus online participants, suggesting that our results here are likely to generalize to the laboratory.</p><p>In summary, we use a synchronization paradigm to show that subjective time is destabilized by the presence of distracting naturalistic speech, constantly speeding up and slowing down in response to fluctuations in speech salience. These distortions are driven by both acoustic and linguistic factors in task-irrelevant speech, suggesting that attentional orienting takes place after at least the early stages of linguistic analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Constraints on Generality</head><p>All participants were native speakers of English. It remains an open question, therefore, whether individuals are more or less distracted by speech in their native language compared to a less familiar language. This question could have important practical consequences for immigrants attempting to concentrate and resist distraction in, for example, university classrooms, but prior research on this topic has not reached a consensus. While some research has found that disruption of visual serial memory is greater when participants are exposed to their native language as opposed to an unfamiliar language <ref type="bibr" target="#b28">(Ellermeier et al., 2015;</ref><ref type="bibr" target="#b29">Ellermeier &amp; Zimmer, 2014)</ref>, this effect is relatively small and has not been consistently replicated <ref type="bibr" target="#b46">(Komar et al., 2024)</ref>. Moreover, the underlying mechanisms remain unclear, potentially reflecting either attentional orienting or interference with pre-conscious processes <ref type="bibr" target="#b36">(Hughes, 2014)</ref>. On the other hand, EEG research has found instead that cortical tracking of acoustic features of speech is greater for nonnative compared to native listeners <ref type="bibr" target="#b67">(Reetzke et al., 2021;</ref><ref type="bibr" target="#b100">Zou et al., 2019)</ref>. One possible explanation of these conflicting results is that linguistic familiarity and proficiency modulate distraction by different speech features in different ways, which could be tested using the current paradigm.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2</head><label>2</label><figDesc>Figure 2 Experiment 1: Relationship Between Tapping Asynchrony and Features of Task-Irrelevant Speech</figDesc><graphic coords="11,72.00,97.30,426.00,539.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3</head><label>3</label><figDesc>Figure 3 Experiment 1: Relationship Between Tapping Asynchrony and Features of Task-Irrelevant Speech (Covarying for Amplitude)</figDesc><graphic coords="13,72.00,122.60,426.00,471.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4</head><label>4</label><figDesc>Figure 4 Experiment 2: Relationship Between Tapping Asynchrony and Features of Task-Irrelevant Speech</figDesc><graphic coords="17,75.06,97.30,344.00,493.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6</head><label>6</label><figDesc>Figure 6 Experiment 3: Relationship Between Tapping Asynchrony and Features of Task-Irrelevant Speech</figDesc><graphic coords="22,72.00,215.14,346.00,432.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="6,72.00,349.63,468.00,187.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="23,72.00,309.68,257.00,342.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="25,72.00,324.98,462.35,157.90" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The authors thank the participants. This research is funded by the <rs type="funder">National Institutes of Health</rs> (<rs type="grantNumber">NIH R01DC017734</rs>). The authors also thank <rs type="person">Ed Lalor</rs> for providing the stimuli and stimulus information vectors used in Experiments 1 and 2.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_vtQx9ds">
					<idno type="grant-number">NIH R01DC017734</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Distinct sensitivity to spectrotemporal modulation supports brain asymmetry for speech and melody</title>
		<author>
			<persName><forename type="first">P</forename><surname>Albouy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Benjamin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Morillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zatorre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">376</biblScope>
			<biblScope unit="page" from="1043" to="1047" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pathophysiological distortions in time perception and timed performance</title>
		<author>
			<persName><forename type="first">M</forename><surname>Allman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Meck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain</title>
		<imprint>
			<biblScope unit="volume">135</biblScope>
			<biblScope unit="page" from="656" to="677" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Properties of the internal clock; first-and second-order principles of subjective time</title>
		<author>
			<persName><forename type="first">M</forename><surname>Allman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Teki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Meck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. Rev. Psychol</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="743" to="771" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep-learning models reveal how context and listener attention shape electrophysiological correlates of speech-to-language transformation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Lalor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">1012537</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neuroticism and the pupillary response to a brief exposure to noise</title>
		<author>
			<persName><forename type="first">J</forename><surname>Antikainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Psychology</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="131" to="135" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Gorilla in our midst: An online behavioral experiment builder</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Anwyl-Irvine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Massonnié</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Flitton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kirkham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Evershed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="388" to="407" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pupillary dilation response as an indicator of auditory discrimination in the barn owl</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Takahashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Comparative Physiology</title>
		<imprint>
			<biblScope unit="volume">186</biblScope>
			<biblScope unit="page" from="425" to="434" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Low-intensity auditory stimulation and the GSR orienting response</title>
		<author>
			<persName><forename type="first">R</forename><surname>Barry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physiological Psychology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="98" to="100" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Habituation of the irrelevant sound effect: evidence for an attentional theory of short-term memory disruption</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Röer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dentale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Buchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1542" to="1557" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distraction by steady-state sounds: evidence for a graded attentional model of auditory distraction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Röer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Buchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="500" to="512" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Controlling the false discovery rate: a practical and powerful approach to multiple testing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Benjamini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hochberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="289" to="300" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">CircStat: a MATLAB toolbox for circular statistics</title>
		<author>
			<persName><forename type="first">P</forename><surname>Berens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bottom-up influences on working memory: behavioral and electrophysiological distraction varies with distractor strength</title>
		<author>
			<persName><forename type="first">S</forename><surname>Berti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Roeber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Schröger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="249" to="257" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On the relationship of arousal and attentional distraction by emotional novel sounds</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bonmassar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Scharf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Widmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wetzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">237</biblScope>
			<biblScope unit="page">105470</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Salience is in the eye of the beholder: increased pupil size reflects acoustically salient variables</title>
		<author>
			<persName><forename type="first">V</forename><surname>Boswijk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Loerts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hilton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">100061</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Electrophysiological correlates of semantic dissimilarity reflect the comprehension of natural, narrative speech</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Broderick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Di Liberto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Crosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Lalor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Biology</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="803" to="809" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">More than words: Neurophysiological correlates of semantic dissimilarity depend on comprehension of the speech narrative</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Broderick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Zuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Lalor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="5201" to="5214" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Moving beyond Kučera and Francis: A critical evaluation of current word frequency norms and the introduction of a new and improved word frequency measure for American English</title>
		<author>
			<persName><forename type="first">M</forename><surname>Brysbaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>New</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="977" to="990" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Factors affecting the filled-duration illusion in the auditory, tactual, and visual modalities</title>
		<author>
			<persName><forename type="first">L</forename><surname>Buffardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="292" to="294" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spectral decomposition of variability in synchronization and continuation tapping: comparisons between auditory and visual pacing and feedback conditions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Repp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Movement Science</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="515" to="532" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The multivariate temporal response function (mTRF) toolbox: a MATLAB toolbox for relating neural signals to continuous stimuli</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Crosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Di Liberto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bednar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Lalor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Human Neuroscience</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">604</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Low-frequency cortical entrainment to speech reflects phoneme-level processing</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Di Liberto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>O'sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Lalor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Biology</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page" from="2457" to="2465" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Cortical encoding of melodic expectations</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Di Liberto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pelofi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bianco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Herrero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>De Cheveigné</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>in human temporal cortex. eLife, 9, e51784</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The role of speech fidelity in the irrelevant sound effect: insights from noise-vocoded speech backgrounds</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dorsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Viswanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rosenblum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quarterly Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="2152" to="2161" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Perception of the duration of emotional events</title>
		<author>
			<persName><forename type="first">S</forename><surname>Droit-Volet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brunot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niedenthal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition and Emotion</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="849" to="858" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">How emotions colour our perception of time</title>
		<author>
			<persName><forename type="first">S</forename><surname>Droit-Volet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Meck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="504" to="513" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Speeding up an internal clock in children? Effects of visual flicker on subjective duration</title>
		<author>
			<persName><forename type="first">S</forename><surname>Droit-Volet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wearden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quarterly Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="193" to="211" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Using auditory saliency to understand complex auditory scenes</title>
		<author>
			<persName><forename type="first">V</forename><surname>Duangudom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th European Signal Processing Conference</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Memory disruption by irrelevant noise-vocoded speech: effects of native language and the number of frequency bands</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ellermeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kattner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ueda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Duomoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nakajima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="page" from="1561" to="1569" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The psychoacoustics of the irrelevant sound effect</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ellermeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zimmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acoustical Science &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Effect of stimulus uncertainty on the pupillary dilation response and the vertex evoked potential</title>
		<author>
			<persName><forename type="first">D</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hakerem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fleiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electroencephalography and Clinical Neurophysiology</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="475" to="484" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Scalar timing in memory</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gibbon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Meck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of the New York Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">423</biblScope>
			<biblScope unit="page" from="52" to="78" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Emotional time distortions: the fundamental role of arousal</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Droit-Volet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition and Emotion</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="847" to="862" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Neural markers of speech comprehension: measuring EEG tracking of linguistic speech representations, controlling the speech acoustics</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gillis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanthornhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Francart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Brodbeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">50</biblScope>
			<biblScope unit="page" from="10316" to="10329" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Auditory salience using natural soundscapes</title>
		<author>
			<persName><forename type="first">N</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elhilali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="page" from="2163" to="2176" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Push-pull competition between bottom-up and top-down auditory attention to natural soundscapes</title>
		<author>
			<persName><forename type="first">N</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elhilali</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">52984</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Auditory distraction: a duplex-mechanism account</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hughes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PsyCh Journal</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="30" to="41" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">In search of salience: Focus detection in the speech of different talkers</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H K</forename><surname>Ip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cutler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language and Speech</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="650" to="680" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Irrelevant tones produce an irrelevant speech effect: implications for phonological coding in working memory</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Macken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="369" to="381" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Interference from degraded auditory stimuli: linear effects of changing-state in the irrelevant sequence</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Alford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Macken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Banbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tremblay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="1082" to="1088" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A saliency-based auditory attention model with applications to unsupervised prominent syllable detection in speech</title>
		<author>
			<persName><forename type="first">O</forename><surname>Kalinli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Interspeech</title>
		<imprint>
			<biblScope unit="page" from="1941" to="1944" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Investigating bottom-up auditory attention</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elhilali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Human Neuroscience</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">327</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pitch, timbre and intensity interdependently modulate neural responses to salient sounds</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elhilali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroscience</title>
		<imprint>
			<biblScope unit="volume">440</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Mechanisms for allocating auditory attention: an auditory saliency map</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kayser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Petkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lippert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Logothetis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Biology</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1943" to="1947" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Joint drumming: Social context facilitates synchronization in preschool children</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kirschner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tomasello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Child Psychology</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="299" to="314" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Are acoustics enough? Semantic effects on auditory salience in natural scenes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kothinti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elhilali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">1276237</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Evidence of a metacognitive illusion in stimulus-specific prospective judgments of distraction by background speech</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Komar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Buchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mieth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van De Vijver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">24111</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Perception in action: The impact of sensory information on sensorimotor synchronization in musicians and non-musicians</title>
		<author>
			<persName><forename type="first">V</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pollok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schnitzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Psychologica</title>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="28" to="37" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Emotional modulation of interval timing and time perception</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lebar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Meck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroscience and Biobehavioral Reviews</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="403" to="420" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Irrelevant speech and irrelevant tones: the relative importance of speech to the irrelevant speech effect</title>
		<author>
			<persName><forename type="first">D</forename><surname>Le Compte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Neely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="472" to="483" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Correspondences among pupillary dilation response, subjective salience of sounds, and loudness</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kidani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yoneya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kashino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Furukawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="412" to="425" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Pupillary dilation response reflects surprising moments in music</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yoneya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kashino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Furukawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Eye Movement Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Speech versus non-speech as irrelevant sound: controlling acoustic variation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Thomson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Psychology</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="62" to="70" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Human sensorimotor tracking of continuous subliminal deviations from isochrony</title>
		<author>
			<persName><forename type="first">G</forename><surname>Madison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Merker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroscience Letters</title>
		<imprint>
			<biblScope unit="volume">370</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="73" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Methamphetamine and time estimation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Maricq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Church</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Animal Behavior Processes</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="18" to="30" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Eyes have ears: indexing the orienting response to sound using pupillometry</title>
		<author>
			<persName><forename type="first">A</forename><surname>Marois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Labonté</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Parent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Vachon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Psychophysiology</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page" from="152" to="162" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Fronto-central P3a to distracting sounds: an index of their arousing properties</title>
		<author>
			<persName><forename type="first">R</forename><surname>Masson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bidet-Caulet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">185</biblScope>
			<biblScope unit="page" from="164" to="180" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Neural basis of the perception and estimation of time</title>
		<author>
			<persName><forename type="first">H</forename><surname>Merchant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harrington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Meck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. Rev. Neurosci</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="313" to="336" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Sustained pupil responses are modulated by predictability of auditory sequences</title>
		<author>
			<persName><forename type="first">A</forename><surname>Milne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tampakaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chait</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="6116" to="6127" />
			<date type="published" when="2021">2021a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">An online headphone screening test based on dichotic pitch</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Milne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bianco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Oxenham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Billig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chait</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="1551" to="1562" />
			<date type="published" when="2021">2021b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A matter of time: how musical training affects time perception</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Juneja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saumya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shukla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neuroscience</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">1364504</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Voice onset time differences between adult males and females: Isolated syllables</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Mccrea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Herring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Phonetics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="308" to="317" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Controlling the focus of visual selective attention</title>
		<author>
			<persName><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Models of Neural Networks IV</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="247" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Effects of visual flicker on subjective time in a temporal bisection task</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>López</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioural Processes</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="380" to="386" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Speeding up an internal clock in humans? Effects of click trains on subjective duration</title>
		<author>
			<persName><forename type="first">I</forename><surname>Penton-Voak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Percival</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wearden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Animal Behavior Processes</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="307" to="320" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Pullman</surname></persName>
		</author>
		<title level="m">The Northern Lights. Scholastic Children&apos;s Books</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">The pupil and stimulus probability</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qiyuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Richer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wgoner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Beatty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychophysiology</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="530" to="534" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Neural tracking of the speech envelope is differentially modulated by attention and language experience</title>
		<author>
			<persName><forename type="first">R</forename><surname>Reetzke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">N</forename><surname>Gnanateja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chandrasekaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain and Language</title>
		<imprint>
			<biblScope unit="volume">213</biblScope>
			<biblScope unit="page">104891</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Adaptation to tempo changes in sensorimotor synchronization: Effects of intention, attention, and awareness</title>
		<author>
			<persName><forename type="first">B</forename><surname>Repp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quarterly Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="499" to="521" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Compensation for subliminal timing perturbations in perceptual-motor synchronization</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Repp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Research</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="106" to="128" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Two separate mechanisms underlie auditory change detection and involuntary control of attention</title>
		<author>
			<persName><forename type="first">T</forename><surname>Rinne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Särkkä</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Degerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Schröger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Alho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain Research</title>
		<imprint>
			<biblScope unit="volume">1077</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="135" to="143" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Auditory distraction in short-term memory: Stable effects of semantic mismatches on serial recall</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Röer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Buchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Auditory Perception &amp; Cognition</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="143" to="162" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">A preregistered replication and extension of the cocktail party phenomenon: One&apos;s name captures attention, unexpected words do not</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Röer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cowan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">234</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Statistical universals reveal the structures and functions of human music</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Savage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Currie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">29</biblScope>
			<biblScope unit="page" from="8987" to="8992" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Algorithmic modelling of the irrelevant sound effect (ISE) by the hearing sensation fluctuation strength</title>
		<author>
			<persName><forename type="first">S</forename><surname>Schlittmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weißgerber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kerber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fastl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hellbrück</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception, &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="194" to="203" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>Attention</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Dynamic acoustic salience evoked motor responses</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kotz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cortex</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="page" from="320" to="332" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">The heart beat does not make us tick: the impact of heart rate and arousal on time perception</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Winkler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sedlmeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception, &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="182" to="193" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>Attention</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Getting synchronized with the metronome: comparisons between phase and period correction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Semjen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vorberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-H</forename><surname>Schulze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Research</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="44" to="55" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Your &quot;VOORnaam&quot; is not my &quot;VOORnaam&quot;: An acoustic analysis of individual talker differences in word stress in Dutch</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Severijnen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Bosker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mcqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Phonetics</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page">101296</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Eyes wide open: regulation of arousal by temporal expectations</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shalev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nobre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">224</biblScope>
			<biblScope unit="page">105062</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Higher nervous functions: the orienting reflex</title>
		<author>
			<persName><forename type="first">E</forename><surname>Sokolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Physiology</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="545" to="580" />
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Is predictability salient? A study of attentional capture by auditory patterns</title>
		<author>
			<persName><forename type="first">R</forename><surname>Southwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Baumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Barascud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Friston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chait</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society B</title>
		<imprint>
			<biblScope unit="volume">372</biblScope>
			<date type="published" when="2017">2017. 20160105</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Acoustic determinants of phrase boundary perception</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Streeter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1582" to="1592" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Correlation between pupil size and subjective passage of time in non-human primates</title>
		<author>
			<persName><forename type="first">T</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kunimatsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tanaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="11331" to="11337" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Salient sounds distort time perception and production</title>
		<author>
			<persName><forename type="first">A</forename><surname>Symons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tierney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="137" to="147" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Prosodic pitch processing is represented in delta-band EEG and is dissociable from the cortical tracking of other acoustic and phonetic features</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Teoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Cappelloni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Lalor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3831" to="3842" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Beat synchronization across the lifespan: Intersection of development and musical experience</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>White-Schwoch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tierney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kraus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">128839</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Auditory saliency using natural statistics</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tsuchida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Cognitive Science Society</title>
		<meeting>the Annual Meeting of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1048" to="1053" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">The role of speech-specific properties of the background in the irrelevant sound effect</title>
		<author>
			<persName><forename type="first">N</forename><surname>Viswanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dorsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>George</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Quarterly Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="581" to="589" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Feeling the heat: body temperature and the rate of subjective time, revisited</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wearden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Penton-Voak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quarterly Journal of Experimental Psychology Section B: Comparative and Physiological Psychology</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="129" to="141" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Speeding up and (...relatively...) slowing down an internal clock in humans</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wearden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Philpott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Win</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioural Processes</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="63" to="76" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Internal clock processes and the filled-duration illusion</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Wearden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Norton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Montford-Bebb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="716" to="729" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Cortical tracking of surprisal during continuous speech comprehension</title>
		<author>
			<persName><forename type="first">H</forename><surname>Weissbart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Kandylaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Reichenbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cognitive Neuroscience</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="155" to="166" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Do click trains dilate time perception due to physiological arousal?</title>
		<author>
			<persName><forename type="first">E</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Solodow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<idno type="DOI">10.31234/ios.io/78w43</idno>
	</analytic>
	<monogr>
		<title level="j">PsyArXiv</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Response delays and the timing of discrete motor responses</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Wing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Kristofferson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="12" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Beat synchronization predicts neural speech encoding and reading readiness in preschoolers</title>
		<author>
			<persName><forename type="first">K</forename><surname>Woodruff Carr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>White-Schwoch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Tierney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Strait</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kraus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">40</biblScope>
			<biblScope unit="page" from="14559" to="14564" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Neural dynamics of predictive timing and motor engagement in music listening</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zalta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Large</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schön</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Morillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science Advances</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">2525</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">The definition of a musician in music psychology: A literature review and the six-year rule</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Susino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Mcpherson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Schubert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychology of Music</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="389" to="409" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Rapid ocular responses are modulated by bottom-up-driven auditory salience</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bengamin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Benhamou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yoneya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Slaney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chait</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="7703" to="7714" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Pupil-linked phasic arousal evoked by violation but not emergence of regularity within rapid sound sequences</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chait</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">I</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4030</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Auditory and language contributions to neural encoding of speech features in noisy environments</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">192</biblScope>
			<biblScope unit="page" from="66" to="75" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
