<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Beyond independent latent classes: Testing the limits of human flexibility</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Pieter</forename><surname>Verbeke</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Experimental Psychology</orgName>
								<orgName type="institution">Ghent University</orgName>
								<address>
									<settlement>Ghent</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Maurice</forename><surname>De Walsche</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Experimental Psychology</orgName>
								<orgName type="institution">Ghent University</orgName>
								<address>
									<settlement>Ghent</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pauline</forename><surname>Maelfait</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Experimental Psychology</orgName>
								<orgName type="institution">Ghent University</orgName>
								<address>
									<settlement>Ghent</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tom</forename><surname>Verguts</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Experimental Psychology</orgName>
								<orgName type="institution">Ghent University</orgName>
								<address>
									<settlement>Ghent</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Beyond independent latent classes: Testing the limits of human flexibility</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B57D1933A513A72842F3F38BCC8F6D52</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-10-22T06:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Generalization</term>
					<term>cognitive flexibility</term>
					<term>learning</term>
					<term>latent classes</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A hallmark of human intelligence is the ability to flexibly adapt to novel situations. This flexibility relies crucially on the appropriate generalization of previously acquired knowledge. One influential theoretical framework argues that humans organize their knowledge in a collection of latent classes. Humans could then assign any novel situation to one of the latent classes (or construct a new one if it is too dissimilar), and thus generalize based on older knowledge.</p><p>However, this framework is not sufficiently flexible to explain human generalization. In particular, we argue that at least three important features are missing: dependency, compositionality, and tuning. To empirically test this, we developed a novel behavioral task, which required adapting knowledge across tasks in order to generalize appropriately in the test phase. Across three experiments, dependency, compositionality, and tuning requirements were increasingly added to the basic task. Results demonstrate that humans are sensitive to all three types of structure.</p><p>We discuss how current models must be extended to capture human generalization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>General introduction</head><p>Humans demonstrate a remarkable flexibility in dealing with novel challenges. A key aspect of this flexibility is to efficiently combine and adapt previously learned knowledge for the current task <ref type="bibr" target="#b30">(Tomov et al., 2021;</ref><ref type="bibr" target="#b31">Vaidya &amp; Badre, 2022;</ref><ref type="bibr" target="#b33">Verbeke &amp; Verguts, 2024)</ref>. For instance, while learning to drive a car is a complex learning process that requires practice, one can still partially rely on what was learned to navigate traffic as a cyclist or pedestrian (e.g., several traffic rules). This generalization across tasks significantly speeds up learning.</p><p>One way to transfer information efficiently between situations is to organize knowledge in a collection of latent classes, as formalized in the Dirichlet or Chinese restaurant process.</p><p>Here, latent classes cluster together contexts, tasks or stimuli that share underlying structure.</p><p>Originally applied in cognitive science to categorization <ref type="bibr" target="#b0">(Anderson, 1991)</ref>, and later to classical and operant conditioning <ref type="bibr" target="#b12">(Gershman &amp; Niv, 2012)</ref>, this approach was more recently extended to clustering stimulus-action rules (tasks; <ref type="bibr" target="#b4">Collins et al., 2014;</ref><ref type="bibr" target="#b5">Collins &amp; Frank, 2013)</ref>. In the context of stimulus-action rules, a latent class is often also referred to as task set since it clusters several tasks into a single class or set. For instance, one could create a latent class for "driving a car on the European mainland". This allows to generalize the stimulus-action sequences (i.e., the policy) that one learned in one car and one country to all cars and all countries on the European mainland. Different classes can be made for driving a bicycle (other action sequences) or driving a car in the UK (other traffic rules).</p><p>Critically, the latent class framework assumes that tasks are discretely assigned to one class from a collection of independent classes <ref type="bibr" target="#b5">(Collins &amp; Frank, 2013;</ref><ref type="bibr" target="#b12">Gershman &amp; Niv, 2012;</ref><ref type="bibr" target="#b26">Razmi &amp; Nassar, 2022;</ref><ref type="bibr" target="#b32">Vaidya et al., 2021)</ref>. Information is then generalized to all tasks in the same class but not to other classes. In the current paper, we argue that this implementation can only account for a very narrow range of human generalization abilities. Below, we challenge three specific assumptions of the current latent class theory. Each of these challenges will be empirically tested in three different experiments.</p><p>A first assumption that we challenge is that latent classes are independent, with tasks being either fully correlated (when they are in the same class) or uncorrelated (when they are in a different class). Several real-world tasks exhibit a more complex relational structure. For example, the main difference between traffic rules in the UK and European mainland is simply that one should inverse the left-right dimension (driving left instead of right). Moreover, solving math problems often relies on understanding the anti-correlated nature of mathematical operations (e.g., addition versus subtraction). Independent latent classes do not allow such reasoning.</p><p>Second, we challenge the all-or-none nature of generalization. In the latent class framework, task rules are generalized to all tasks in the same class and not generalized when tasks are in different classes. However, policies for real-world tasks can rarely be transferred as a whole. For instance, while one can use a lot of knowledge from driving on the European mainland to driving in the UK, it remains critical to invert the left-right dimension for several traffic rules. To extend the latent class framework to such tasks, previous work proposed compositionality <ref type="bibr" target="#b8">(Dekker et al., 2022;</ref><ref type="bibr" target="#b10">Franklin &amp; Frank, 2018;</ref><ref type="bibr" target="#b21">Liu &amp; Frank, 2022;</ref><ref type="bibr" target="#b27">Reverberi et al., 2012)</ref>. Here, task rules are divided in multiple components that are each assigned to a latent class. This effectively allows generalizing the action sequences of driving a car to the European mainland while avoiding interference from the UK traffic rules. Note that this approach still presumes independent classes, which would not allow to transfer the traffic rules in an inverted manner. This adds an empirically untested layer of complexity. Thus, we build on previous work proposing compositionality and extend this to anti-correlated compositional sets.</p><p>Third, we further elaborate on the all-or-none nature of generalization. Specifically, we challenge that (sub)sets of task rules are generalized in a rigid manner. Even when only generalizing a subset of the task rules, an adaptive agent may want to implement at least a minor tuning or transformation when generalizing to a different task <ref type="bibr" target="#b33">(Verbeke &amp; Verguts, 2024)</ref>. For example, when transitioning from a small to a big car, most of the required action sequences are very similar. Nevertheless, a bigger car has a larger radius for making maneuvers, so some tuning of the learned action sequences is required. Thus, we describe several arguments that challenge the current implementation of the latent class theory on human generalization <ref type="bibr" target="#b4">(Collins et al., 2014;</ref><ref type="bibr" target="#b5">Collins &amp; Frank, 2013;</ref><ref type="bibr" target="#b12">Gershman &amp; Niv, 2012)</ref>. Nevertheless, empirical investigations that test the limits of human flexibility in generalization are lacking. Therefore, we developed a novel paradigm and employed this in three different experiments, each one further increasing the complexity of task contexts.</p><p>First, we challenge the notion of independence in the latent class framework. For this purpose, we test whether humans can generalize to anti-correlated task contexts (e.g., inverting the leftright dimension between UK and European mainland). A second experiment builds on previous work challenging the all-or-none nature of latent classes <ref type="bibr" target="#b10">(Franklin &amp; Frank, 2018</ref><ref type="bibr">, 2020;</ref><ref type="bibr" target="#b21">Liu &amp; Frank, 2022)</ref>. Specifically, we test compositional generalization. In contrast to previous work, this compositional generalization is tested in an environment combining both fully correlated and anti-correlated tasks, thus requiring subjects to decide what to simply transfer and what to invert. A third experiment further challenges the rigid nature of latent class generalization by testing more subtle tuning of task rules such as an expansion or shrinkage of the action space (comparable to switching from a small to big car).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 1: Generalization to anti-correlated task contexts</head><p>In a first experiment, we challenge the notion of independent latent classes. More specifically, we test whether humans can spontaneously (without explicit training) generalize across anti-correlated tasks by learning about relationships between classes.</p><p>To test this, we developed a novel task paradigm, which will be adapted for the second and third experiment. The task is framed as a fishing contest and is visualised in Fig. <ref type="figure" target="#fig_0">1</ref>. Here, a boat brings participants to a fishing site located around an island (Fig. <ref type="figure" target="#fig_0">1a</ref> and Fig. <ref type="figure" target="#fig_0">1e</ref>). When arriving at this fishing site, participants should try to catch an underwater animal (fish, squid, crab, ‚Ä¶). This can be done by positioning a cage (on a continuous left-to-right axis) and dropping it at the correct location (see Fig. <ref type="figure" target="#fig_0">1b</ref>). After a couple of trials, the boat brings participants to another fishing site. The participants must thus learn the hiding spots of the animals. Each animal had a fixed hiding spot which depended on the fishing site. Critically, there was a relational structure between fishing sites. While the hiding spots at some fishing sites were fully correlated (animals were hidden at the same location), they were uncorrelated or even anticorrelated (hiding spots were mirrored along the vertical axis) with other fishing sites. Inference about this relational structure allows to significantly speed up learning. A full overview of the hiding spots (on a left (-1) to right (+1) axis) is given in Fig. <ref type="figure" target="#fig_2">2b</ref>. Note. Overview of the task procedure. In each experimental round, participants visit multiple fishing sites. a) Each visit starts with the presentation of a boat moving to the relevant fishing site. Once the boat arrives at the fishing site it is presented at this location for a couple of seconds after which a trial is started. b) On each trial, participants can see the location of the fishing site on the radar, the animal that they need to catch (presented four times, at the top of the screen) and the cage. The cage is always initialized at the middle of the screen; a laser pointer indicates where it would land if it were dropped. c-d) After participants drop the cage, they receive feedback. Here, the animal would appear from underneath the sand. If the cage drops on the animal, it remains presented in the cage (d). If the cage does not catch the animal, the animal disappears, and a small heap of sand remains where it was hidden (c). Note that in the test rounds, participants do not get feedback. In this case, the cage will just drop on the sand but neither the animal or the heap are presented. This trial procedure is repeated until each animal is caught two or three times, depending on the experiment and round within the experiment. e) Then, the screen with the boat is presented again, which brings the participant to the next fishing site.</p><p>In all experiments, there were four types of experimental rounds which reflect a crossing of two animal sets (initial set and generalization set) and two types of feedback (learning rounds with feedback and testing rounds without feedback). In initial learning rounds, the participant needs to learn the hiding spot of four animals (initial set) for each fishing site. Initial test rounds investigate what participants have learned about this initial set. In these rounds, no feedback is given, meaning that the animal will not reveal itself after a cage drop. In generalization training rounds, a novel set of (generalization) animals is introduced. Participants again must learn the hiding spot of each animal at each fishing site. Importantly, only 2 fishing sites are visited during these generalization training rounds. These 2 fishing sites are referred to as trained sites.</p><p>Critically, if participants have correctly inferred the relationship between fishing sites during the initial learning rounds, the 2 trained sites provide sufficient information to know where the generalization animals are hidden at the other fishing sites. This is what is tested in the generalization test rounds. Here, participants visit all fishing sites and must try to catch animals from the generalization set introduced in the generalization training rounds. Again, no feedback is provided during these rounds. An extra overview of the difference between experimental rounds is provided in Table <ref type="table" target="#tab_0">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Participants. Fifty-three participants were recruited from the Sona participant pool at Ghent University. Two participants were excluded because they did not complete all experimental sessions. Technical problems caused data loss of another 2 participants. This resulted in a final sample of forty-nine participants (12 male, 37 female) with a mean age of 22.6</p><p>(complete age range: 18-32). Each participant had normal or corrected-to-normal vision. All participants gave their informed written consent before the experiment. To participate in three one-hour long sessions of which the last one was in the fMRI scanner, participants received a payment of 60 euros. The study was approved by the local ethics committee of Ghent University Hospital.</p><p>Apparatus and stimuli. The web-based experiment used in the first two sessions (procedure described below) was programmed using JsPsych <ref type="bibr">(de Leeuw, 2015)</ref>. For these sessions, participants could use their own pc if their screen had a standard 1.78 aspect ratio and 60 Hz refresh rate. Participants could position the cage by using the left and right arrow keys on their keyboard and drop the cage by pressing the spacebar. The task for the on-site data collection of the third session was programmed in PsychoPy <ref type="bibr" target="#b25">(Peirce et al., 2019)</ref>. Here, participants observed the screen via the mirror in the head coil and responded via a Cedrus response box. The cage could be positioned by using 2 response buttons at their right hand and dropped by pressing a response button with their left hand. The visual stimuli (e.g., underwater animals) were freely available on vecteezy.com.</p><p>Procedure. Participants had to complete three sessions of about one hour each. The first two sessions were performed in the participant's home environment via a web-based experiment. In these sessions, initial training rounds were alternated with initial test rounds. In the initial training round, participants visited all fishing sites. During a visit, participants were first presented with an island group and a boat (see Fig. <ref type="figure" target="#fig_0">1a</ref>). After one second, this boat would move around the island group until it reached the relevant fishing site. The boat remained at this position for 2 seconds. Then, another screen is presented (Fig. <ref type="figure" target="#fig_0">1b</ref>). Here, a radar at the top of the screen indicates at which fishing site the participant currently is. Additionally, four instances of an underwater animal are shown at the top of the screen (2 on each side of the radar) to inform participants which animal they need to catch. Additionally, a cage was on every trial initialised at the middle of the screen (just below the radar). This cage spanned 10% of the screen width.</p><p>Importantly, participants could never catch an animal by dropping it at its initialized (middle) location. Participants used button presses to move the cage. A single button press would shift the cage for 3% of the screen width. Alternatively, participants could keep the response button pressed to move the cage faster to the left or right. A different button press was needed to drop the cage and catch the animal. If the response limit (6 seconds in Experiment 1, and 4 seconds in Experiment 2 and 3) was exceeded, the cage would automatically be dropped at its current position. Once the cage was dropped, an animal would appear from beneath the sand, providing feedback about its hiding spot. It took the cage 250 milliseconds to drop to the bottom. During this time, the animal was already presented at its hiding spot. If the animal was caught (Fig. <ref type="figure" target="#fig_0">1c</ref>), it would be presented in the cage for another 750 milliseconds. If the cage missed the animal (Fig. <ref type="figure" target="#fig_0">1d</ref>), the animal would disappear, and the empty cage was presented for 750 milliseconds together with a small heap of sand where the animal was hidden. Then, the next trial would start, meaning that the cage was initialized again at the middle of the screen and a different animal had to be caught. During each visit of a fishing site, all four animals had to be caught three times. After this, the boat would appear again and bring participants to the next fishing site (Fig. <ref type="figure" target="#fig_0">1e</ref>). Once all fishing sites were visited (in random order) participants could take a short break. This was repeated for three times after which the initial test round was introduced. Here, participants again visited all fishing sites in random order. In each visit, all four animals had to be caught two times. Critically, they did not receive trial-by-trial feedback anymore. Here, the cage would drop but neither the animal nor the heap of sand would appear. They were informed about the total number of caught animals at the end of the round. This sequence of initial learning round and initial test round was repeated twice in each session.</p><p>The third session was performed at Ghent University Hospital while participants were lying in the MRI scanner. Current manuscript only discusses behavioral results. Here, participants were first provided a short reminder of what was learned before by means of another initial training round. Again, all fishing sites were visited. In each visit, all four animals had to be caught two times. Each fishing site was visited only once. Then, participants performed a long generalization training round. This round introduced a generalization set of three novel animals and visited only the two trained fishing sites. During each visit, participants needed to catch all three generalization animals three times. Both trained sites were visited 6 times in an alternating fashion. After this, a generalization test round was introduced. Here, participants were tested on the novel set of three animals. However, in these rounds the participants visited all fishing sites without receiving trial-by-trial feedback. At the end of each round, participants were always informed about the total number of animals they caught. In Experiment 1, the generalization test rounds followed a specific sequence of fishing site visits. Here, each time one of the trained sites was visited after which one of the associated (fully correlated or anti-correlated) fishing sites were visited. As a result, trained sites were visited twice as often in these generalization test rounds. On each visit, each animal had to be caught twice. In total there were six generalization test rounds, but these were intermixed with four generalization training rounds (one visit of both trained sites) and two repetitions of the initial learning round (one visit of all fishing sites). An overview of the sequence of rounds in all sessions (and the performance in each round) is given in Fig. <ref type="figure" target="#fig_2">2c</ref>.</p><p>Analyses. To evaluate performance, we computed a baselined error score. (1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ùêµùëéùë†ùëíùëôùëñùëõùëíùëë ùê∏ùëüùëüùëúùëüùë†ùëêùëúùëüùëí ùëù</head><p>Here, on each trial, we compute the error score by taking the absolute distance (in screen width %) between the location where the participant (p) dropped the cage and the hiding spot of the animal on that trial (t). As baseline (denominator of Equation ( <ref type="formula">1</ref>)), we take the average of 1000 replications of a random agent. On each replication (r), a random value Rp,t,r is drawn from a uniform distribution between 0 and 1. Also hiding spots and cage drop locations are recoded on a 0 to 1 axis. Similar to the cage drop location, this random value is compared to the animal hiding spot on that trial. The average of 1000 replications was considered to reflect the error score of a random agent and is used to baseline the error score of the participant (see Equation ( <ref type="formula">1</ref>)). As a result, the baselined error score gives an indication of performance where 0 is perfect performance (no distance between cage drop and hiding spot) and a value of 1 or more reflects random or worse than random performance.</p><p>To provide an overview of performance, we used a repeated measures ANOVA with this baselined error score as dependent variable and round number, feedback (test vs learning round) and animal set (initial versus generalization) as independent variables.</p><p>However, our main analyses are focused on performance in the generalization test rounds. Therefore, we also performed a repeated measures ANOVA on this subset of the data with the baselined error score as dependent variable. The independent variables were round number and type of fishing site (trained, fully correlated, or anti-correlated). We first describe general performance analyses across all experimental rounds. These revealed a main effect of round number (F(1,48) = 55.68, p &lt; .0001, ÔÅ® 2 = .54), indicating that the error scores significantly decreased over rounds (see Fig. <ref type="figure" target="#fig_2">2c</ref>). Also the effect of feedback was significant (F(1,48) = 114.5, p &lt; .0001, ÔÅ® 2 = .70), showing that error scores were significantly higher in the test rounds (no feedback; M = .387) than training rounds (with feedback; M = .184).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Additionally, the effect of animal set (initial or generalization) also reached significance (F(1,48) = 7.036, p = .011, ÔÅ® 2 = .13). This revealed that on average the error scores were lower for the generalization animals (M = .268) than for the initial animals (M = .285). Also the interactions between round number and feedback (F(1,48) = 12.58, p = .0008, ÔÅ® 2 = .21) and between feedback and animals (F(1,48) = 47.97, p &lt; .0001, ÔÅ® 2 = .50) reached significance. The interaction between round number and animals did not reach significance (F(1,48) = .032, p = .859, ÔÅ® 2 &lt; .0001). The three-way interaction between round number, feedback and animals did reach significance (F(1,48) = 63.33, p &lt; .0001, ÔÅ® 2 = .57). Together, these interactions indicates that the difference between test and training performance decreased over rounds and that this decrease was stronger for the initial animals than for the generalization animals.</p><p>We next investigated whether participants could generalize to full and anti-correlated fishing sites. For this purpose, we zoomed in on performance during the generalization test rounds (Figure <ref type="figure" target="#fig_2">2d</ref>). Analyses of the performance (baselined error score; see Equation ( <ref type="formula">1</ref>)) in the generalization test rounds, revealed again a main effect of round number (F(1,48) = 3.045, p &lt; .0001, ÔÅ® 2 = .36), indicating that the error score decreased over rounds (Fig. <ref type="figure" target="#fig_2">2d</ref>). Notice that participants never received feedback in these rounds but did receive feedback in the round(s) in between two generalization test rounds. Hence, this learning effect reflects transfer from what was learned in between two generalization test rounds. Also the effect of fishing site type reached significance (F(2, 96) = 31.82, p &lt; .0001, ÔÅ® 2 = .45). Here, paired t-tests revealed a clear order in performance. Specifically, the baselined error score was lowest in the trained sites (M = .187, SD = .162). This was significantly better (t(48) = 5.668, p &lt; .0001, d = .81) than the fully correlated fishing sites (M = .411, SD = .343). This was again significantly better (t(48) = 3.982, p = .0002, d = .57) than the baselined error score in the anti-correlated fishing sites (M = .652, SD = .401).</p><p>Importantly, performance was significantly better than random (i.e., baselined error score smaller than 1) in all three types of fishing sites (all p &lt; .0001). The interaction between round number and fishing site type did not reach significance (F(2, 96) = .025, p = .976, ÔÅ® 2 &lt; .0001), revealing that the order in performance across types of fishing sites was stable across rounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>Experiment 1 demonstrates that humans can learn about more complex task relations than the latent class framework allows. Their generalization was much better than random in the fully correlated fishing sites (as predicted by the latent class framework), but also in the anticorrelated fishing sites (which is not predicted by this framework). Presumably, extracting not only the latent classes but also their relational structure allowed them to generalize efficiently.</p><p>Previous experimental work only tested generalization in fully correlated task contexts <ref type="bibr" target="#b4">(Collins et al., 2014;</ref><ref type="bibr" target="#b5">Collins &amp; Frank, 2013;</ref><ref type="bibr" target="#b32">Vaidya et al., 2021)</ref>. We replicated this finding but also went beyond previous work by introducing anti-correlated tasks. Here, participants had to invert the mappings that they learned in one of the trained tasks. Although performance was significantly worse in the anti-correlated tasks compared to the fully correlated ones, performance within the anti-correlated tasks was still significantly better than random. Hence, human generalization can profit from dependence between latent classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 2: Compositional generalization in anti-correlated task contexts</head><p>In a second experiment, we challenge the all-or-none nature of human generalization.</p><p>Consistently, previous work proposed a compositional extension of the latent class framework <ref type="bibr" target="#b10">(Franklin &amp; Frank, 2018;</ref><ref type="bibr" target="#b21">Liu &amp; Frank, 2022)</ref>. This allows to transfer part of the task rules from one context to another while not generalizing other parts of the task rules. In Experiment 2, we explored a more complex task environments in which fully and anti-correlated transfers interacted with compositional generalization.</p><p>Specifically, we adapted the experimental paradigm from Experiment 1 but added two compositional fishing sites. In one of the two compositional fishing sites, the hiding spots of the outer two animals (out of four) were inverted compared to the trained sites and the inner two animals had the same hiding spot as in the trained sites (e.g., S5 relative to S1; see Fig. <ref type="figure" target="#fig_4">3b</ref>). In the second compositional fishing site, the hiding spots of the inner two animals were inverted while the hiding spots of the outer two animals were the same as in the trained sites (e.g., S4 relative to S1; see Fig. <ref type="figure" target="#fig_4">3b</ref>). The other 6 fishing sites were the same as in Experiment 1. This allowed us to replicate our findings. Fig. <ref type="figure" target="#fig_4">3a</ref> and Fig. <ref type="figure" target="#fig_4">3b</ref> provide an overview of the fishing sites (S1-S6) and the animal (A1-A8) hiding spots at each fishing site.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Participants. Forty-five students from Ghent University were recruited in exchange for course credits. Four participants were excluded because they did not complete all experimental sessions. Additionally, four participants were removed before the analyses because they performed worse than the random baseline in the last initial test round. Note that the qualitative patterns did not change when we included these participants. This resulted in a final sample of thirty-seven participants (6 male, 31 female). Each participant had normal or corrected-tonormal vision. All participants gave their informed written consent before the experiment.</p><p>Apparatus and stimuli. The same visual stimuli as for Experiment 1 were used. Here, all sessions used a web-based experiment, which was programmed by using JsPsych (de <ref type="bibr">Leeuw, 2015)</ref>. Again, participants could use their own pc if their screen had a standard 1.78 aspect ratio and 60 Hz refresh rate. Participants could position the cage by using the left and right arrow keys on their keyboard and drop the cage by using the spacebar. The second session consisted of three repetitions of the following sequence: initial learning round, generalization learning round, generalization test round. In initial learning rounds, participants visited all fishing sites. In each visit, all four animals had to be caught two times. In generalization learning rounds, a generalization set of four novel animals was used. Here, only the two trained sites were visited. Each of these fishing sites were visited three times in alternating order. On each visit, all four generalization animals needed to be caught two times.</p><p>In the generalization test rounds, participants visited each fishing site once. Also here, all four generalization animals had to be caught two times during each visit. Critically, as in Experiment 1, participants received no feedback during these generalization test rounds.</p><p>Analyses. To evaluate performance, we again computed the baselined error score (Equation ( <ref type="formula">1</ref>)). To provide an overview of general performance, we used a repeated measures ANOVA with this baselined error score as dependent variable and round number, feedback and animal set as independent variables.</p><p>Similar to Experiment 1, our main analyses are focused on performance in the generalization test rounds. Here, we used a repeated measures ANOVA with the baselined error score as dependent variable. Again, the independent variables were round number and fishing site type. However, in Experiment 2, there were four types of fishing sites: trained, fully correlated, anti-correlated and compositional.</p><p>Additionally, we performed more detailed analyses to explore a possible interaction between anti-correlated and compositional generalization. For this purpose, we decomposed each fishing site in 2 task rules (see shading in Fig. <ref type="figure" target="#fig_4">3b</ref>). More specifically, the hiding spots of the outer animals of the first trained site were coded as rule A + and the hiding spot of the inner animals as rule B + . For the second trained site, the hiding spots of the outer animals were coded as rule C + and the hiding spots for the inner animals as rule D + . As a result, fully correlated fishing sites followed rules A + B + or C + D + and anti-correlated fishing sites followed A -B -and C -D -. The two compositional sites followed task rules A -B + and A + B -(for a full overview see Fig. <ref type="figure" target="#fig_4">3b</ref>). We computed the baselined error score for each task rule in each fishing site. This score was then used as dependent variable in a repeated measure ANOVA which included two independent variables. first factor was inversion (yes for anti-correlated ( -) task rules and no for fully correlated ( + ) task rules). The second factor was compositionality (yes for compositional fishing sites and no for fully-and anti-correlated fishing sites). The performance in the trained contexts was omitted for these analyses. rounds. e) illustrates the interaction between compositionality and inversion. Specifically, mean baselined error scores are presented based on whether the task rule required inversion (-) or not (+) and whether the fishing site was compositional or not (full or anti; trained was omitted here). Again, error bars provide 95% confidence intervals and the horizontal black dashed line reflects error scores for a random agent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>General performance analyses again revealed a main effect of experimental round (F(1,36) = 6.33, p = .0165, ÔÅ® 2 = .15). Indeed, Fig. <ref type="figure" target="#fig_4">3c</ref> illustrates that the error scores significantly decreased over rounds. Also the effect of feedback was significant (F(1,36) = 223.5, p &lt; .0001, ÔÅ® 2 = .86), showing that error scores were significantly higher in the test rounds (M = .677) than training rounds (M = .263). The effect of animals (initial versus generalization) did not reach significance in Experiment 2 (F(1,36) = 0.098, p = .756, ÔÅ® 2 &lt; .0001). In contrast to Experiment 1, the interaction between round number and feedback (F(1,36) = 1.215, p = .278, ÔÅ® 2 = .03) did not reach significance. However, the interaction between round number and animals (F(1,36) = 11.38, p = .002, ÔÅ® 2 = .24), as well as the interaction between feedback and animals (F(1,36) = 31.02, p &lt; .0001, ÔÅ® 2 = .46) reached significance. Additionally, the three-way interaction between round number, feedback and animals did reach significance (F(1,36) = 23.93, p &lt; .0001, ÔÅ® 2 = .40).</p><p>Thus, the difference between the initial and generalization animals decreased over rounds. This decrease was mainly driven by the generalization learning rounds.</p><p>More detailed analyses on the generalization test rounds (see Fig. <ref type="figure" target="#fig_4">3d</ref>), also demonstrated a main effect of round number (F(1,36) = 13.06, p = .0009, ÔÅ® 2 = .27). More importantly however, the effect of fishing site type reached significance (F(3, 108) = 16.15, p &lt; .0001, ÔÅ® 2 = .31). Here, paired t-tests revealed the same order in performance as for Experiment 1. Specifically, the baselined error score was lowest in the trained sites (M = .503, SD = .352). This was significantly better (t(36) = 4.329, p = .0001, d = .71) than the fully correlated fishing sites (M = .683, SD = .341). This was again, significantly better (t(36) = 2.501, p = .017, d = .41) than the baselined error score in the anti-correlated fishing sites (M = .792, SD = .331). Performance in the two compositional sites (M = .775, SD = .307) did differ significantly from the trained sites (t(36) = 5.469, p &lt; .0001, d = .9), but did not show significant differences with the fully correlated sites (t(36) = 1.909, p = .064, d = .31) or the anti-correlated fishing sites (t(36) = .355, p = .725, d = .06).</p><p>Note that performance was more comparable to the anti-correlated fishing sites. Importantly, performance was significantly better than random (i.e., baselined error score smaller than 1) in all four context types (all p &lt; .0003). The interaction between round number and context type did not reach significance (F(3, 108) = .302, p = .824, ÔÅ® 2 &lt; .0001).</p><p>We next investigated an interaction between compositionality and the nature of task transformations (inversion (in anti-correlated rules) vs no inversion (in fully correlated rules)).</p><p>Here, we find a strong effect of inversion (F(1,36) = 13.59, p = .0007, ÔÅ® 2 = .27) but also a significant effect of compositionality (F(1,36) = 4.899, p = .033, ÔÅ® 2 = .12). Interestingly, also the interaction between inversion and compositionality reached significance (F(1,36) = 5.703, p = .022, ÔÅ® 2 = .14).</p><p>As shown in Fig. <ref type="figure" target="#fig_4">3e</ref>, these results indicate that performance is worse when inversion (anticorrelation) is needed. Performance is also worse in compositional sites but only for the fully correlated task rule. There is no added effect of compositionality for the anti-correlated task rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>Experiment 2 replicated the findings of Experiment 1. Again, we found generalization across fully-and anti-correlated task contexts. As in Experiment 1, generalization was better for the fully correlated fishing sites than for the anti-correlated fishing sites.</p><p>Furthermore, we found that also when compositional generalization is required, participants can exploit anti-correlations between task rules. In general, both compositional and anti-correlated generalization tended to be more difficult than generalization across fully correlated task rules. However, there appears to be no additive difficulty effect when compositionality and anti-correlation are combined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 3: Testing the limits of task rule tuning</head><p>The previous experiments suggest that human generalization is more flexible than the all-or-none process described in the latent class framework. In Experiment 3, we aimed to further test the limits of these generalization abilities. Specifically, we test whether humans can also learn about, and tune learned task rules, such as would be required to switch from a big to a small car and vice versa.</p><p>For this purpose, we add two novel types of fishing sites. One fishing site investigates expansion of the stimulus space while the other fishing site investigates shrinkage of the stimulus space. In total, there were 8 fishing sites (Fig. <ref type="figure" target="#fig_5">4a</ref>). Again, two fishing sites function as trained sites. Two other fishing sites are fully correlated with one of these trained sites. A fifth fishing site is anti-correlated to one of the trained sites. A sixth fishing site substitutes the hiding spots of two animals. This is similar to one of the compositional sites in Experiment 2. The seventh fishing site expands the hiding spots in the trained sites (i.e., putting the animals further apart). The eighth fishing site shrinks the hiding spots in the trained site (i.e., putting the animals closer together).</p><p>Critically, even in the shrinkage condition, no animal can be caught without moving the cage. An overview of the animal hiding spots at each fishing site is provided in Fig. <ref type="figure" target="#fig_5">4b</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Participants. Forty-five students from Ghent University were recruited in exchange for course credits. Five participants were excluded because they did not complete all experimental sessions. Additionally, one participant was omitted from the analyses because they performed worse than baseline in the last initial test round. This resulted in a final sample of thirty-nine participants (2 Male, 37 Female). Each participant had normal or corrected-to-normal vision. All participants gave their informed written consent before the experiment.</p><p>Apparatus and stimuli. The same visual stimuli as for Experiment 1 and 2 were used.</p><p>Here, all sessions used a web-based experiment, which was programmed by using JsPsych (de <ref type="bibr">Leeuw, 2015)</ref>. We used the same requirements and response buttons as in Experiment 2.</p><p>Procedure. The procedure was exactly the same as in Experiment 2. Participants had to complete two sessions of about one hour each. In the first session, initial training rounds were intermixed with initial test rounds. The second session consisted of three repetitions of the following sequence: initial learning round, generalization learning round, generalization test round.</p><p>Analyses. To evaluate performance, we used the baselined error score (Equation ( <ref type="formula">1</ref>)).</p><p>We again performed general performance analyses in the form of a repeated measures ANOVA with the baselined error score as dependent variable and round number, feedback and animal set as independent variables.</p><p>Similar to Experiment 1 and 2, our main analyses focused on performance in the generalization test rounds. Here, we performed a repeated measures ANOVA with the baselined error score as dependent variable. The independent variables were round number and type of fishing site (trained, fully correlated, anti-correlated, compositional, expansion and shrinkage).</p><p>Additional analyses were done to test whether participants performed an expansion or shrinkage in the two novel contexts and not simply used a fully correlated transfer from the trained context. For this purpose, we computed a confusion score. Specifically, we computed the absolute distance between the cage drop location and the hiding spot of the animal in the trained site. This reflects what the (unbaselined) error score would be if the participant performed on the trained site. We compared this confusion score to the unbaselined error score, which is just the absolute distance between the cage drop location and the hiding spot of the animal in the current (expansion or inversion) fishing site. If the unbaselined error score is lower than the confusion score, this indicates that participants tuned their behavior in the correct manner. General performance analyses (Fig. <ref type="figure" target="#fig_5">4c</ref>) again revealed a learning effect across experimental rounds (F(1,38) = 40.71, p &lt; .0001, ÔÅ® 2 = .52). Also the effect of feedback was significant (F(1,38) = 115.7, p &lt; .0001, ÔÅ® 2 = .75), showing that error scores were significantly higher in the test rounds (M = .517) than training rounds (M = .228). The effect of animals (initial or generalization) did not reach significance in Experiment 3 (F(1,38) = 0.006, p = .941, ÔÅ® 2 &lt; .0001).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Again, the interaction between round number and feedback (F(1,38) = 2.069, p = .159, ÔÅ® 2 = .05)</p><p>did not reach significance. Also the interaction between round number and animals (F(1,38) = .197, p = .66, ÔÅ® 2 &lt; .0001) was not significant. However, the interaction between feedback and animals (F(1,38) = 25.45, p &lt; .0001, ÔÅ® 2 = .40) as well as the three-way interaction (F(1,38) = 39.27, p &lt; .0001, ÔÅ® 2 = .51) reached significance. Together, this illustrates that the difference between initial and generalization animals decreased over rounds, and this decrease was mainly driven by the generalization learning rounds.</p><p>When performing more detailed analyses on the data from the generalization test (Fig. <ref type="figure" target="#fig_5">4d</ref>) rounds, we observed that, even in this very complex environment, performance was significantly better than random (i.e., baselined error score smaller than 1) in all fishing sites (all p &lt; .0001). Consistent with the previous experiments, the ANOVA demonstrated a main effect of round number (F(1,38) = 18.50, p = .0001, ÔÅ® 2 = .33). Moreover, the effect of the type of fishing site reached significance (F(5, 190) = 6.86, p &lt; .0001, ÔÅ® 2 = .15). Again, performance was best in the trained sites (M = .332, SD = .275). This was significantly better than all other fishing sites (all p &lt; .005). In contrast to the previous experiments, there was no significant difference in performance between the fully correlated fishing sites and the anti-correlated fishing sites (t(38) = 0.943, p = .352, d = .15). Nevertheless, overall performance was still better in the fully correlated fishing site (M = .483, SD = .328) than in the anti-correlated site (M = .522, SD = .348). Notably, performance was worst in the compositional fishing site (M = .653, SD = .292). This was significantly worse than all other fishing sites (all p &lt; .006) except the expansion site (M = .575, SD = .570). The interaction between round number and context type again did not reach significance (F(5, 190) = 2.12, p = .065, ÔÅ® 2 = .05).</p><p>As described in the Analyses section, we used a confusion score to evaluate whether participants adapted behavior from the trained contexts to perform the task in the Expansion and Shrinkage contexts. As shown in Fig. <ref type="figure" target="#fig_5">4e</ref>, the confusion score in the shrinkage context (M = 21.078, SD = 6.235) was significantly (t(38) = 3.856, p = .0004, d = .62) higher (worse) than the unbaselined error score (M = 13.264, SD = 8.709). Also in the Expansion fishing site, the confusion score (M = 42.127, SD = 14.441) was significantly (t(38) = 3.186, p = .003, d = .51) higher than the unbaselined error score (M = 23.565, SD = 23.349). Hence, instead of simply transferring task rules, participants tuned behavior to make adaptive changes to the learned task rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>Experiment 3 went one step beyond Experiments 1 and 2 and investigated whether, compared to the all-or-none generalization process described in the latent classes framework, humans would also tune existing task rules to behave more adaptively. Results indeed suggest that participants did so. More generally, participants demonstrated a remarkable flexibility in learning about a wide variety of complex task relations and exploiting this for generalization.</p><p>Notably, we found that, compared to all other fishing site, the compositional sites were significantly harder to learn in Experiment 3. However, this is probably due to our design choices.</p><p>Since we used a wide variety of fishing site types, we chose to limit the complexity of the learning process by reducing the number of unique hiding spots. As can be observed in Figure <ref type="figure" target="#fig_5">4b</ref>, some animals were hidden at the same spot within one fishing site (both .5 or both -.5), which probably caused participants to cluster these two animals together during learning and made it harder to tear them apart again for generalization in the compositional fishing sites.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>General Discussion</head><p>Across three experiments, we robustly demonstrate three features of human generalization that are incompatible with traditional latent class theory, namely dependency, compositionality and tuning. Each experiment added a novel test to the previous ones, so that our results also yield internal replications. We argue that latent class models should be made sufficiently expressive to capture human generalization.</p><p>First, we challenged the notion of independence between latent classes. We demonstrated that, on top of generalizing across fully correlated tasks, humans can also generalize across anti-correlated tasks. Interestingly, there was a significant difference in generalization performance between fully correlated and anti-correlated tasks. This suggests that there is an added difficulty in learning about anti-correlated task relations compared to fully correlated task relations. Such difficulties could be explained by assuming a hierarchical learning process in which learning about relationships between latent classes happens at a higher level of abstraction than the construction of latent classes itself (e.g., <ref type="bibr" target="#b19">Kemp et al., 2010)</ref>.</p><p>Second, we challenged the all-or-none nature of the latent class framework. Here, we built on previous work proposing compositionality <ref type="bibr" target="#b10">(Franklin &amp; Frank, 2018</ref><ref type="bibr">, 2020;</ref><ref type="bibr" target="#b21">Liu &amp; Frank, 2022</ref>), but we combined this with the anti-correlations described in Experiment 1. Results indicated that also when compositional generalization is required, participants can exploit anticorrelations between task rules. Interestingly, both compositional and anti-correlated generalization tended to be more difficult than generalization across fully correlated task rules.</p><p>Moreover, there appears to be no additive difficulty effect when compositionality and anticorrelation are combined. This suggests that both types of generalization share a similar level of abstraction, which is higher than the generalization across fully correlated task contexts.</p><p>In a third step, we elaborated on the all-or-none nature of generalization in the latent class framework. Here, we tested whether humans would be able to learn about, and apply, task rule tuning. Specifically, we added two fishing sites that required to either expand or shrink the geometrical space of the hiding spots. Participants were able to generalize such expansion and shrinkage as well.</p><p>As proposed by the latent class framework, it is important to sample from existing (classes of) knowledge when encountering a novel challenge. Indeed, previous empirical work has demonstrated that people prioritize past solutions in novel situations <ref type="bibr" target="#b15">(Hall-McMaster et al., 2024)</ref>. Nevertheless, this is often just an optimal starting point for learning <ref type="bibr" target="#b30">(Tomov et al., 2021)</ref>.</p><p>In most cases, considerable updates are still required to behave adaptively in the novel context.</p><p>Moreover, novel situations often require combining knowledge from multiple classes that one learned before. As we mentioned before, a partial solution is compositionality, which allows to combine parts of information from different classes for generalization. However, also this solution is not sufficiently flexible to explain the tuning (inversion, shrinkage and expansion) that we tested in current work.</p><p>One conceptual extension that allows for dependence between classes is to use an Indian buffet process as prior in assigning stimuli to classes <ref type="bibr" target="#b13">(Griffiths &amp; Ghahramani, 2011)</ref> instead of the Chinese restaurant process <ref type="bibr" target="#b14">(Griffiths et al., 2003)</ref>. This approach removes the constraint that a collection of stimulus-action mappings (a task) can only belong to one class;</p><p>this allows for compositional generalization in a more flexible manner than with traditional latent classes <ref type="bibr" target="#b10">(Franklin &amp; Frank, 2018)</ref>. Nevertheless, also this type of model has difficulties in explaining why performance was consistently better for fully correlated than for anti-correlated fishing sites. Another extension of the latent class framework is to construct hierarchical classes <ref type="bibr" target="#b11">(Franklin &amp; Frank, 2020;</ref><ref type="bibr" target="#b19">Kemp et al., 2010;</ref><ref type="bibr" target="#b21">Liu &amp; Frank, 2022)</ref>. Here, latent classes would be clustered themselves in even higher order classes. As we briefly mentioned before, such an approach would allow to cluster fully correlated task contexts in one class and then on a more abstract level also represent anti-correlations between classes. However, to our knowledge no implementation of hierarchical classes exist that allow for anti-correlations between or within classes. Furthermore, it is unclear how one can implement classes that allow for subtle tuning as in our expansion and shrinkage task contexts.</p><p>In the latent class framework, each object (task in our case) is assigned to a single class.</p><p>Other inference procedures have been proposed, such as inferring the values on several continuous latent dimensions for each object (as in variational Gaussian inference; <ref type="bibr" target="#b20">Kingma &amp; Welling, 2022)</ref>, or assigning each object to several classes simultaneously (as in the Helmholtz machine; <ref type="bibr" target="#b6">Dayan et al., 1995)</ref>. Nevertheless, it is also not clear whether these approaches are sufficiently expressive to capture the rich structure that we observed. Moreover, they have rarely been used to explain human categorization or generalization across tasks.</p><p>A modelling framework with more expressivity than latent classes, are artificial neural networks. However, an often-articulated fear is that their representations are not sufficiently structured to capture human generalization <ref type="bibr" target="#b9">(Fodor &amp; Pylyshyn, 1988;</ref><ref type="bibr" target="#b23">Marcus, 2018)</ref>.</p><p>Interestingly, however, compositional representations have been shown to emerge naturally in neural networks that are trained to perform many tasks <ref type="bibr" target="#b17">(Johnston &amp; Fusi, 2023;</ref><ref type="bibr" target="#b35">Yang et al., 2019)</ref>.</p><p>Moreover, compositionality in neural networks emerges also for anti-correlated tasks <ref type="bibr" target="#b35">(Yang et al., 2019)</ref>. Other researchers even demonstrated that it allows to generalize in an anti-correlated manner <ref type="bibr" target="#b28">(Riveland &amp; Pouget, 2024)</ref>. Another approach that implements compositionality in a more explicit manner are mixture of expert networks. Here, an agent can adapt behavior by linearly combining different existing expert networks <ref type="bibr" target="#b16">(Jacobs et al., 1991;</ref><ref type="bibr" target="#b18">Jordan &amp; Jacobs, 1994)</ref>.</p><p>Hence, instead of an all-or-none process, a weighted integration of information from different experts can be used to create novel behavior. Interestingly, it has been proposed that the ventrolateral prefrontal cortex of the human brain would function as a gating region, integrating the weighted contributions of all experts in other brain areas <ref type="bibr" target="#b24">(O'Doherty et al., 2021)</ref>. Another interesting and relevant neural network concept is the adapter from Artificial Intelligence <ref type="bibr" target="#b36">(Zhang et al., 2021)</ref>. Here, extra processing layers are introduced in a pretrained network and only the extra processing layers are trained for a novel task. Such an approach could be particularly useful to capture the task-specific tunings that we observed in Experiment 3 <ref type="bibr" target="#b22">(Lu et al., 2024)</ref>.</p><p>Yet another way to organize tasks (or any other objects) are cognitive maps. Here, one maximizes the distance between orthogonal objects and clusters objects that share sufficient similarities <ref type="bibr" target="#b2">(Behrens et al., 2018;</ref><ref type="bibr" target="#b29">Schuck et al., 2016;</ref><ref type="bibr" target="#b34">Wilson et al., 2014)</ref>. Like the latent class framework, this approach will thus cluster similar task contexts together. However, it allows for more flexibility in the sense that the distance between two tasks can be parametrically modulated (allowing for more graded dependence between tasks), representations can be partially overlapping (not all-or-none), and other neural regions can still tune their influence on behavior. Previous work has proposed that cognitive maps are mainly present in human hippocampus, ventromedial prefrontal cortex and orbitofrontal cortex <ref type="bibr" target="#b1">(Baram et al., 2021;</ref><ref type="bibr" target="#b3">Bernardi et al., 2020;</ref><ref type="bibr" target="#b29">Schuck et al., 2016;</ref><ref type="bibr" target="#b32">Vaidya et al., 2021;</ref><ref type="bibr" target="#b34">Wilson et al., 2014)</ref>.</p><p>In sum, current work demonstrated that the flexibility of human generalization goes well beyond what can be captured by latent classes. Here, we specifically focused on dependency, compositionality, and tuning. To understand how humans generalize, a major challenge will be combining the structure of latent classes with the expressivity of neural networks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The task.</figDesc><graphic coords="4,72.00,189.18,451.23,246.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>,ùë° = |ùê∂ùëéùëîùëí ùëëùëüùëúùëù ùëôùëúùëêùëéùë°ùëñùëúùëõ ùëù,ùë° -ùê¥ùëõùëñùëöùëéùëô ‚Ñéùëñùëëùëñùëõùëî ùë†ùëùùëúùë° ùëù,ùë° | ‚àë |ùëÖ ùëù,ùë°,ùëü -ùê¥ùëõùëñùëöùëéùëô ‚Ñéùëñùëëùëñùëõùëî ùë†ùëùùëúùë° ùëù,ùë° |/1000 1000 ùëü=1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Experiment 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Procedure.</head><label></label><figDesc>Participants had to complete two sessions of about one hour each. Similar as the first two sessions of Experiment 1, the first session of Experiment 2 implemented two alternations of an initial training round (each context visited three times with feedback) with an initial test round (each context visited once without feedback).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Experiment 2.</figDesc><graphic coords="11,72.00,324.67,451.30,253.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Experiment 3.</figDesc><graphic coords="14,72.00,425.38,451.30,253.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="8,72.00,72.00,451.26,211.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 . Experimental rounds.</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell>Animal set</cell><cell cols="2">Feedback Visited fishing sites</cell></row><row><cell>Initial learning round</cell><cell>Initial set</cell><cell>Yes</cell><cell>All</cell></row><row><cell>Initial test round</cell><cell>Initial set</cell><cell>No</cell><cell>All</cell></row><row><cell cols="3">Generalization learning round Generalization set Yes</cell><cell>Trained (2)</cell></row><row><cell>Generalization test round</cell><cell cols="2">Generalization set No</cell><cell>All</cell></row></table></figure>
		</body>
		<back>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>All data</head><p>(https://osf.io/uynb3/) and code (https://github.com/CogComNeuroSci/PieterV_public/tree/master/Generalization_Behavioral) will be made available upon submission of the manuscript.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The adaptive nature of human categorization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<idno type="DOI">10.1037/0033-295X.98.3.409</idno>
		<ptr target="https://doi.org/10.1037/0033-295X.98.3.409" />
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="409" to="429" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Entorhinal and ventromedial prefrontal cortices abstract and generalize the structure of reinforcement learning problems</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Baram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Garvert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E J</forename><surname>Behrens</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuron.2020.11.024</idno>
		<ptr target="https://doi.org/10.1016/j.neuron.2020.11.024" />
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="713" to="723" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E J</forename><surname>Behrens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C R</forename><surname>Whittington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Baram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Stachenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kurth-Nelson</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuron.2018.10.002</idno>
		<ptr target="https://doi.org/10.1016/j.neuron.2018.10.002" />
	</analytic>
	<monogr>
		<title level="m">What Is a Cognitive Map? Organizing Knowledge for Flexible Behavior</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="490" to="509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The Geometry of Abstraction in the Hippocampus and Prefrontal Cortex</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Benna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rigotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Munuera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fusi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Salzman</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cell.2020.09.031</idno>
		<ptr target="https://doi.org/10.1016/j.cell.2020.09.031" />
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">183</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="954" to="967" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Human EEG Uncovers Latent Generalizable Rule Structure during Learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G E</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cavanagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Frank</surname></persName>
		</author>
		<idno type="DOI">10.1523/JNEUROSCI.3900-13.2014</idno>
		<ptr target="https://doi.org/10.1523/JNEUROSCI.3900-13.2014" />
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="4677" to="4685" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cognitive control over learning: Creating, clustering, and generalizing task-set structure</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G E</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Frank</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0030852</idno>
		<ptr target="https://doi.org/10.1037/a0030852" />
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="190" to="229" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The Helmholtz Machine</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1995.7.5.889</idno>
		<ptr target="https://doi.org/10.1162/neco.1995.7.5.889" />
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="889" to="904" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">jsPsych: A JavaScript library for creating behavioral experiments in a web browser</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>De</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13428-014-0458-y</idno>
		<ptr target="https://doi.org/10.3758/s13428-014-0458-y" />
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Curriculum learning for human compositional generalization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Dekker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Summerfield</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.2205582119</idno>
		<ptr target="https://doi.org/10.1073/pnas.2205582119" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="page" from="119" to="141" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Connectionism and cognitive architecture: A critical analysis</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Fodor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">W</forename><surname>Pylyshyn</surname></persName>
		</author>
		<idno type="DOI">10.1016/0010-0277</idno>
		<ptr target="https://doi.org/10.1016/0010-0277" />
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="90031" to="90035" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Compositional clustering in task structure learning</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">T</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Frank</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pcbi.1006116</idno>
		<ptr target="https://doi.org/10.1371/journal.pcbi.1006116" />
	</analytic>
	<monogr>
		<title level="j">PLoS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generalizing to generalize: Humans flexibly switch between compositional and conjunctive structures during reinforcement learning</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">T</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Frank</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pcbi.1007720</idno>
		<ptr target="https://doi.org/10.1371/journal.pcbi.1007720" />
	</analytic>
	<monogr>
		<title level="j">PLoS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="33" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploring a latent cause theory of classical conditioning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Niv</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13420-012-0080-8</idno>
		<ptr target="https://doi.org/10.3758/s13420-012-0080-8" />
	</analytic>
	<monogr>
		<title level="j">Learning and Behavior</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="255" to="268" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The Indian buffet process: An introduction and review</title>
		<author>
			<persName><forename type="first">T</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1185" to="1224" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hierarchical Topic Models and the Nested Chinese Restaurant Process</title>
		<author>
			<persName><forename type="first">T</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper_files/paper/2003/hash/7b41" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
	<note>bfa5085806d fa24b8c9de0ce567f-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Neural Prioritisation of Past Solutions Supports Generalisation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hall-Mcmaster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Tomov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">W</forename><surname>Schuck</surname></persName>
		</author>
		<idno type="DOI">10.1101/2024.06.10.598294</idno>
		<ptr target="https://doi.org/10.1101/2024.06.10.598294" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>p. 2024.06.10.598294</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adaptive Mixtures of Local Experts</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Nowlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1991.3.1.79</idno>
		<ptr target="https://doi.org/10.1162/neco.1991.3.1.79" />
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="87" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Abstract representations emerge naturally in neural networks trained to perform multiple tasks</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fusi</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41467-023-36583-0</idno>
		<ptr target="https://doi.org/10.1038/s41467-023-36583-0" />
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hierarchical Mixtures of Experts and the EM Algorithm</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Jacobs</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1994.6.2.181</idno>
		<ptr target="https://doi.org/10.1162/neco.1994.6.2.181" />
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="181" to="214" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to Learn Causal Models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kemp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1551-6709.2010.01128.x</idno>
		<ptr target="https://doi.org/10.1111/j.1551-6709.2010.01128.x" />
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1185" to="1243" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1312.6114</idno>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<ptr target="https://doi.org/10.48550/arXiv.1312.6114" />
		<title level="m">Auto-Encoding Variational Bayes</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hierarchical clustering optimizes the tradeoff between compositionality and expressivity of task structures for flexible reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Frank</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.artint.2022.103770</idno>
		<ptr target="https://doi.org/10.1016/j.artint.2022.103770" />
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="page">312</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Hasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Zacks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Norman</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2312.08519</idno>
		<idno type="arXiv">arXiv:2312.08519</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2312.08519" />
		<title level="m">Reconciling Shared versus Context-Specific Information in a Neural Network Model of Latent Causes</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Marcus</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1801.00631</idno>
		<idno type="arXiv">arXiv:1801.00631</idno>
		<ptr target="https://doi.org/10.48550/arXiv.1801.00631" />
		<title level="m">Deep Learning: A Critical Appraisal</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Why and how the brain weights contributions from a mixture of experts</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>O'doherty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tadayonnejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cockburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Iigaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Charpentier</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neubiorev.2020.10.022</idno>
		<ptr target="https://doi.org/10.1016/j.neubiorev.2020.10.022" />
	</analytic>
	<monogr>
		<title level="j">Neuroscience &amp; Biobehavioral Reviews</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page" from="14" to="23" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">PsychoPy2: Experiments in behavior made easy</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peirce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Macaskill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>H√∂chenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sogo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kastman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Lindel√∏v</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13428-018-01193-y</idno>
		<ptr target="https://doi.org/10.3758/s13428-018-01193-y" />
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="195" to="203" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adaptive Learning through Temporal Dynamics of State Representation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Razmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Nassar</surname></persName>
		</author>
		<idno type="DOI">10.1523/JNEUROSCI.0387-21.2022</idno>
		<ptr target="https://doi.org/10.1523/JNEUROSCI.0387-21.2022" />
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2524" to="2538" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Compositionality of Rule Representations in Human Prefrontal Cortex</title>
		<author>
			<persName><forename type="first">C</forename><surname>Reverberi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>G√∂rgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-D</forename><surname>Haynes</surname></persName>
		</author>
		<idno type="DOI">10.1093/cercor/bhr200</idno>
		<ptr target="https://doi.org/10.1093/cercor/bhr200" />
	</analytic>
	<monogr>
		<title level="j">Cerebral Cortex</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1237" to="1246" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Natural language instructions induce compositional generalization in networks of neurons</title>
		<author>
			<persName><forename type="first">R</forename><surname>Riveland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pouget</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41593-024-01607-5</idno>
		<ptr target="https://doi.org/10.1038/s41593-024-01607-5" />
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="988" to="999" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Human Orbitofrontal Cortex Represents a Cognitive Map of State Space</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">W</forename><surname>Schuck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Niv</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuron.2016.08.019</idno>
		<ptr target="https://doi.org/10.1016/j.neuron.2016.08.019" />
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1402" to="1412" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-task reinforcement learning in humans</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Tomov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41562-020-01035-y</idno>
		<ptr target="https://doi.org/10.1038/s41562-020-01035-y" />
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Abstract task representations for inference and control</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Vaidya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Badre</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2022.03.009</idno>
		<ptr target="https://doi.org/10.1016/j.tics.2022.03.009" />
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="484" to="498" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Neural representation of abstract task structure during generalization</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Vaidya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Badre</surname></persName>
		</author>
		<idno type="DOI">10.7554/eLife.63226</idno>
		<ptr target="https://doi.org/10.7554/eLife.63226" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Reinforcement learning and meta-decision-making</title>
		<author>
			<persName><forename type="first">P</forename><surname>Verbeke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Verguts</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cobeha.2024.101374</idno>
		<ptr target="https://doi.org/10.1016/j.cobeha.2024.101374" />
	</analytic>
	<monogr>
		<title level="j">Current Opinion in Behavioral Sciences</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page">101374</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Orbitofrontal cortex as a cognitive map of task space</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Schoenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Niv</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuron.2013.11.005</idno>
		<ptr target="https://doi.org/10.1016/j.neuron.2013.11.005" />
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="267" to="279" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Task representations in neural networks trained to perform many cognitive tasks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Joglekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Newsome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41593-018-0310-2</idno>
		<ptr target="https://doi.org/10.1038/s41593-018-0310-2" />
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="297" to="306" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2111.00667</idno>
		<idno type="arXiv">arXiv:2111.00667</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2111.00667" />
		<title level="m">Unsupervised Domain Adaptation with Adapter</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
