<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Social processing of dynamic naturalistic social interactions</title>
				<funder ref="#_7AcF7Bh">
					<orgName type="full">European Research Council</orgName>
					<orgName type="abbreviated">ERC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Katie</forename><surname>Daughters</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">University of Essex</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Simona</forename><surname>Skripkauskaite</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Experimental Psychology</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Psychiatry</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kami</forename><surname>Koldewyn</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">School of Human and Behavioural Sciences</orgName>
								<orgName type="institution">Bangor University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Social processing of dynamic naturalistic social interactions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EC8DE495951E220541E31016EA0210B1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-10-22T02:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Social interaction</term>
					<term>social attention</term>
					<term>naturalistic</term>
					<term>dyna</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Research suggests that static depictions of social interactions preferentially capture our attention compared to non-interactions. Research also suggests that motion captures attention. To date, therefore, it is unknown whether dynamic social interactions preferentially capture attention relative to non-interactions, over and above motion cues. The present study captured 81 participants' eye-gaze and facial movements when viewing 4-second video clips of socialinteractions compared to motion-matched non-interactions. We hypothesised that participants would spend more time looking at the two people in the videos relative to the background when viewing social interactions compared to non-interactions. Results confirmed our hypothesis and demonstrated that this effect was stronger for individuals with greater empathy and lower autistic traits. These results add to the growing body of research investigating the processing of social interactions in complex, naturalistic stimuli and demonstrate that social interactions do preferentially capture attention, even when motion cues are present.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Humans are an inherently social species. As such, researchers have sought to understand how we make sense of and engage with our social world. This has often meant focusing on one particular facet of social perception and conducting tightly controlled laboratory experiments using simplified stimuli. There is, however, a growing demand to move away from this approach to better understand how we process more complex and naturalistic social information, and in particular, whether this processing is specialised to or different for social interaction perception.</p><p>A large body of research has investigated visual attention as an important mechanism underlying social perception. It is known that social stimuli quickly and effectively capture attention relative to nonsocial stimuli (e.g., <ref type="bibr" target="#b11">End &amp; Gamer, 2017)</ref>. More recently, studies have demonstrated a visual preference for 'interactive' or facing human dyads and triads compared to non-facing dyads and triads <ref type="bibr" target="#b12">(Fratino et al., 2022;</ref><ref type="bibr">Papeo et al., 2019;</ref><ref type="bibr" target="#b21">Papeo &amp; Abassi, 2019;</ref><ref type="bibr" target="#b29">Vestner et al., 2019</ref><ref type="bibr" target="#b28">Vestner et al., , 2022))</ref>. However, these studies typically use simple silhouettes and therefore do not encompass other important indicators to social interaction, such as eye-gaze or gestures.</p><p>A few studies have included more complex stimuli. For example, <ref type="bibr" target="#b30">Villani et al. (2015)</ref> presented participants with paintings depicting two individuals either acting independently or interacting with one another. They found that participants spent more time looking at the bodies of individuals when they were interacting and more time looking at the face when acting independently. However, this study did not include a direct comparison of attention to social versus non-social information and thus it is not possible to ascertain the moderating impact of social interaction on this contrast; they also used paintings, not photographs depicting real life. A more recent study <ref type="bibr" target="#b24">(Skripkauskaite et al., 2022)</ref> expanded on this by investigating visual attention to interacting and non-interacting dyads in real life photographs. They found that participants spent more time looking at individuals compared to the background for interacting stimuli, and there was no difference for non-interacting stimuli, supporting the idea that social interactions, specifically, capture social attention.</p><p>These studies, however, have all utilised static images. Dynamic stimuli, on the other hand, not only enable the use of more ecologically valid stimuli but also incorporate additional indicators to interaction such as actions. Indeed, research demonstrates that movement also quickly and effectively captures attention <ref type="bibr" target="#b0">(Atkinson et al., 2018)</ref>. Introducing movement therefore provides an additional challenge as to whether social interactions capture attention differently or preferentially as previous studies would suggest. There is some preliminary evidence to support this hypothesis. For instance, observation of dynamic, compared to static, interactions give rise to different neural representation patterns <ref type="bibr" target="#b19">(Landsiedel et al., 2022)</ref>. Furthermore, <ref type="bibr" target="#b4">Chevallier et al. (2015)</ref> found that children spent more time looking at faces compared to objects during dynamic videos that depicted multiple people and showed both social interactions and non-interactions. This analysis, however, focused on faces exclusively, not whole individuals, and did not explicitly analyse differences between interactions and non-interactions. Thus, is remains unknown whether social interactions drive visual attention even in complex dynamic scenes where motion cues and other 'low level' visual features may compete for attention.</p><p>Visual attention is not the only mechanism through which we attempt to process and understand complex social information. As some of the previous studies suggest <ref type="bibr" target="#b4">(Chevallier et al., 2015;</ref><ref type="bibr" target="#b30">Villani et al., 2015)</ref>, the face is an important tool for navigating the social world. It can provide vital information, including affect, which is important when inferring the social context of an interaction. It is therefore important to correctly interpret facial cues and in particular other's emotions. Facial mimicry, the spontaneous copying of other's facial expressions, has been proposed to enhance emotion recognition <ref type="bibr" target="#b13">(Hatfield et al., 1993;</ref><ref type="bibr" target="#b25">Stel et al., 2016)</ref> and empathy <ref type="bibr" target="#b8">(Drimalla et al., 2019)</ref>. It also facilitates social interactions through social learning <ref type="bibr" target="#b16">(Kavanagh &amp; Winkielman, 2016)</ref> and a desire to affiliate with others <ref type="bibr" target="#b3">(Bourgeois &amp; Hess, 2008)</ref>. There is clear evidence of facial mimicry occurring when viewing another person's face in static <ref type="bibr" target="#b7">(Dimberg et al., 2011)</ref> or dynamic <ref type="bibr" target="#b5">(Daughters et al., 2021;</ref><ref type="bibr" target="#b14">Hess &amp; Blairy, 2001)</ref> stimuli on a computer screen and when participating in live social interactions <ref type="bibr" target="#b15">(Hess &amp; Bourgeois, 2010;</ref><ref type="bibr" target="#b26">Stel et al., 2013)</ref>. To the best of our knowledge, however, no study to date has examined facial movements in response to witnessing others in social interactions. The present study therefore aimed to fill this gap in the literature, to understand whether facial movements assist in the processing of complex social interactions.</p><p>Finally, it is important to consider individual differences that may bias one's visual attention and facial movements. Individuals who are higher in trait empathy have been found to be faster to look to human figures <ref type="bibr" target="#b30">(Villani et al., 2015)</ref> and display greater facial movements <ref type="bibr" target="#b7">(Dimberg et al., 2011)</ref> in response to static stimuli. In contrast, autistic individuals have been found to spend less time looking at human information compared to neurotypical individuals (e.g., <ref type="bibr" target="#b4">Chevallier et al., 2015;</ref><ref type="bibr" target="#b23">Shi et al., 2015)</ref>, while facial mimicry research has found both lower <ref type="bibr" target="#b31">(Yoshimura et al., 2015)</ref> and similar <ref type="bibr" target="#b27">(Van der Donck et al., 2021)</ref> facial movements in autistic compared to neurotypical individuals. Thus, although research suggests there are differences in both visual attention and facial movements for those with higher trait empathy and autistic traits, this is yet to be examined in naturalistic dynamic stimuli and has not been investigated with respect to social interaction perception.</p><p>The primary aim of the current study was to investigate visual attention and facial movements as two potential mechanisms underlying social perception of dynamic social scenes, and to assess to what extent the presence of a social interaction may moderate this effect. The secondary aim was to investigate the effect of individual differences on both visual attention to and facial movement in response to observed social interactions, contrasted with visually similar non-interactions. This was examined across four hypotheses: 1) Participants will spend more time looking at social (versus non-social) information and this will be moderated by the presence of social interactions, such that participants will spend more time looking at social (versus non-social) information in social interactions compared to non-interactions; 2) Participants will display more facial movement (specifically, frowning) in response to social interactions, compared to non-interactions; 3) Participants higher in trait empathy will display greater visual attention and facial movements in response to social interactions (compared to noninteractions); 4) Participants lower in autistic traits will display greater visual attention and facial movements in response to social interactions (compared to non-interactions).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>An a priori power calculation determined that 70 participants would be sufficient to attain 80% power with an ⍺ = 0.05 for a three-way interaction, however, due to anticipated data loss, the study aimed to recruit 85 participants. Eighty-one participants (19-84, M = 30.78, SD = 13.85; 45 females) took part in the in-person study at ESSEXLabs. Participants were recruited via an online booking system and received financial compensation for taking part. The data were pre-processed and analysed in accordance with pre-registration (https://aspredicted.org/blind.php?x=VGV_Y4C). Data from seven participants were removed due to true outliers (e.g., technical issues); none of the participants were excluded due to missing eye-tracking data or missing facial analysis data; 15 participants had missing questionnaire data and were therefore not included in the individual differences analyses. The study was approved by the University of Essex Department of Psychology Ethics Committee (ETH2122-1116).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Materials Social Perception Task</head><p>Participants were asked to watch 60 video clips depicting various everyday scenarios. Each 4-second video clip depicted a scenario with two individuals either engaging in a social interaction or acting independently (non-interaction). In total, there were 30 interaction and 30 non-interaction scenarios where each interaction scene was matched as closely as possible to a non-interaction scene, such that they both contained the same people and objects, and where people performed similar actions and (where possible) maintained the same physical distance between the two individuals (e.g., Figure <ref type="figure">1</ref>). Matched interaction and non-interaction scenes were filmed from the same camera angle in the same setting and did not differ in overall motion energy (see <ref type="bibr">Landsiedel et al., 2022 for analysis)</ref>. Interaction videos were previously subjectively rated as more social, more positive, and more visually interesting than non-interaction videos <ref type="bibr" target="#b19">(Landsiedel et al., 2022)</ref>. Scenarios were acted out by four different actor pairs (two female/female pairs, one male/male pair and one female/male pair) captured in eight different locations (e.g., an office, lobby waiting area etc.).</p><p>The experimental task was programmed using iMotions (www.imotions.com), a biometric research platform that can be used to synchronize multiple psychophysical measures. This enabled automated facial coding, eye tracking, and stimulus presentation to be precisely coordinated. Eye-tracking data was recorded using a portable Tobii x2-120 compact eye-tracker sampling at 120Hz with a screen resolution of 1920x1080. An I-VT fixation filter was applied, and data were sampled from both eyes to produce information on eye position and latency. Participants' facial movements were recorded via a Logitech HD webcam and post-processed using the AFFDEX algorithm for automatic facial coding developed by Affectiva Inc. <ref type="bibr" target="#b10">(El Kaliouby &amp; Robinson, 2005;</ref><ref type="bibr" target="#b20">McDuff et al., 2010)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Questionnaires</head><p>Autistic Traits. The Autism Quotient Short version (AQ-S; <ref type="bibr" target="#b17">Kloosterman et al., 2011)</ref> is an adapted 28-item version of the original 50-item Autism Quotient <ref type="bibr" target="#b1">(Baron-Cohen et al., 2001)</ref>. Items relate to five subscales: social skills, mind reading/communication, restricted and repetitive behaviour, imagination and attention to detail. Participants were asked to rate to what extent they agree/disagree with each item (1 = "Definitely Agree"; 4 = "Definitely Disagree"). The measure includes reverse item scoring such that higher scores represent more autistic traits. A total score and mean scores for each subscale were calculated. The total score achieved good internal reliability (Cronbach's ⍺ = 0.78).</p><p>Trait Empathy. The Interpersonal Reactivity Index (IRI; <ref type="bibr" target="#b6">Davis, 1983</ref>) is an established questionnaire, with items pertaining to four subscales: empathic concern, fantasy, personal distress and perspective taking. There are 28 items in total, seven for each subscale (including reverse coded items). For each item, participants are asked to indicate on a 5-point scale to what extent the statement can be applied to them (1 = "does not describe me very well"; 5 = "describes me very well"). A total score and mean scores for each subscale were calculated; higher scores indicated greater empathy. The total score achieved good internal reliability (Cronbach's ⍺ = 0.86).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure</head><p>Small groups (between 5-10) of participants took part in the study in a large testing room, with individual participants spaced around the room in private computer cubicles. The blinds were drawn and the same lighting was used for each testing session. Before data collection, participants read the study information sheet and provided written informed consent. Participants were then seated 60-65cm from the screen and completed a 9-point calibration. If the calibration quality was poor, the process was repeated. No participant was asked to complete the calibration more than three times. They then completed both questionnaires via the online survey software, Qualtrics, before starting the social perception task. Participants were instructed to watch the videos in a free viewing task. Each of the 60 trials consisted of a grey background with a black fixation cross presented in the middle of the screen for 10 seconds<ref type="foot" target="#foot_0">1</ref> before the 4-second video. The videos began automatically or were manually triggered earlier by the participants, who were instructed to focus on the fixation cross before beginning each video. The videos were presented in random order and occupied the full screen. Participants' eye and facial movements were recorded throughout the task. At the end of the study, participants were fully debriefed before leaving.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Analysis</head><p>All statistical analyses were conducted in R (version 1.4.1717) using the lme4 package <ref type="bibr" target="#b2">(Bates et al., 2015)</ref>. Satterthwaite's approximate method was used for significance testing (lmerTest package; <ref type="bibr" target="#b18">Kuznetsova et al., 2017)</ref>. The data and analysis code are available on OSF (https://osf.io/dqjpr/?view_only=01bdb55be14148e4ae4121b5f64489fa).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Eye tracking</head><p>Dynamic Areas of Interest (AOIs) were drawn around the two individuals in each video (see Figure <ref type="figure">1</ref>) using iMotions. These AOIs were adjusted on a frame-by-frame basis to ensure that all the human figure was captured in each frame. A static AOI was also created for each video capturing the whole scene. Dwell times (time the participants were fixated in an AOI while the stimulus was on the screen) was then exported for all AOIs. Thus, by subtracting the dwell time for both individuals from the scene AOI we could create a measure of time spent looking at the background or non-social AOI in each video.</p><p>Linear mixed-effect (multilevel) modelling was carried out to assess whether participants spent more time looking at social AOIs compared to non-social AOIs, and whether looking time differences were moderated by the presence of a social interaction. Thus, modelling focused on a 2(Stimulus: interacting=1 vs. non-interacting=0) x 2(AOI: social=1 vs. non-social=0) design. Working from the null/unconditional model, then adding random then fixed effects, models assessed the impact of individual differences in empathy and autistic traits. Participants were modelled as a random intercept effect; stimulus, AOI, IRI scores and AQ scores as fixed predictors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Facial movements</head><p>Participants' facial movements were scored in terms of 20 'channels' <ref type="bibr" target="#b9">(Ekman &amp; Friesen, 1975)</ref> based on the Facial Action Coding System (FACS) action units in AFFDEX. Scores ranged from 0 to 100 representing the probability that that channel was being expressed (0channel not expressed, 100 -channel expressed). Because AFFDEX offers a large number of output metrics, to avoid type 1 errors due to multiple tests, careful consideration is required to identify relevant metrics in advance. In line with our pre-registration, metrics for the 'smile' and 'brow furrow' AFFDEX channels for each video were exported for analysis. However, the smile channel did not produce enough data within reliable levels of activation (less than 1% after thresholding) to warrant further investigation. Therefore, no analysis will be reported for smiling. There was more variation in activation across the frown channel (5% after thresholding), however, this was heavily skewed such that most participants displayed very little frowning across the study. Therefore, in line with iMotions recommendations and to avoid significant skewness in the data, a threshold of 25 was applied to the data <ref type="bibr" target="#b5">(Daughters et al., 2021)</ref>, such that any datapoints above 25 were deemed to represent that channel being expressed; anything below 25 and the channel was deemed not to be expressed.</p><p>Logistic linear mixed-effect (multilevel) modelling was carried out to assess whether participants displayed more frowning during social interactions compared to non-interactions. Due to the dynamic nature of the stimuli, and that social interaction typically takes place in the middle of the 4-second videos, time was also added as a predictive factor into the analysis. Thus, these models focused on 2(Stimulus: interacting=1 vs. non-interacting=0) x 4(Time:0-3 second) design. Models also assessed the impact of individual differences in empathy and autistic traits. Participants and video identifiers were modelled as crossed random effects (because we anticipated that there would be some random variation in valence/affective tone across videos which might impact frowning); stimulus, AOI, IRI scores and AQ scores as fixed predictors. IRI and AQ scores were centred and scaled, however, all models including these factors failed to converge and will therefore not be reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Eye Tracking</head><p>Both stimulus and AOI significantly predicted dwell time (see Table <ref type="table">1</ref> for statistics), however, there was also a significant interaction in the anticipated direction. Participants spent more time looking at social AOIs (compared to non-social AOIs) when viewing social interactions (compared to non-interactions); see Figure <ref type="figure">2</ref>.</p><p>The model achieved an overall better fit, however, when individual differences in empathy and autistic traits were added in. Again, AOI significantly predicted participants dwell time, but there were also significant interactions between AOI and both IRI and AQ scores (see Table <ref type="table">2</ref> for statistics). Participants with greater trait empathy spent longer looking at social AOIs (see Figure <ref type="figure">3</ref>) and participants with more autistic traits spent longer looking at non-social AOIs. Indeed, participants with particularly high autistic traits in our sample crossed over to spend more time looking at non-social AOIs than social (see Figure <ref type="figure">4</ref>). These trait related effects, however, were not further moderated by the stimulus type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Facial Movements: Frowning</head><p>Both stimulus and time bin significantly predicted the presence of frowning (see Table <ref type="table">3</ref> for statistics). There was also a significant interaction. In contrast to our hypothesis, participants displayed more frowning during non-interactions compared to interactions, an effect that was very slightly more pronounced during the first three seconds of the videos (see Figure <ref type="figure">5</ref>). However, this effect was small and all the fixed effects in the model explained only 0.2% of frowning variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>The present study investigated two potential mechanisms that might facilitate social interaction perception: visual attention and facial movements. Crucially, the study sought to extend existing research by using dynamic naturalistic stimuli, thereby examining whether visual attention, in particular, is still captured preferentially by social interactions even when motion cues -which also drive visual attentionare present. We also sought to explore the effect of social interactions on facial movements when witnessing "everyday" dyadic interactions. The findings support our first hypothesis; participants spent longer looking at the two individuals in the video relative to the background when they were interacting compared to not interacting. In addition, and partially supporting hypotheses three and four, there were also individual differences in the extent to which social information captured attention. Higher scores in trait empathy were associated with increased social bias (i.e., more time spent longer looking at the people in the video and less time looking at the background). In contrast, greater levels of autistic traits were associated with reduced social bias. Interestingly, there was a cut-off point on the AQ-S where individuals' preference for looking swapped from people to the background (a score of approximately 76-77 in our sample). However, contrary to our expectations, there was no significant interaction between either empathy or autistic traits and stimulus type, suggesting that these differences in social bias were not moderated by the presence of social interactions.</p><p>Our findings support and extend the small body of research investigating the visual processing of social interactions (e.g., <ref type="bibr" target="#b4">Chevallier et al., 2015;</ref><ref type="bibr" target="#b12">Fratino et al., 2022;</ref><ref type="bibr" target="#b21">Papeo &amp; Abassi, 2019;</ref><ref type="bibr" target="#b24">Skripkauskaite et al., 2022;</ref><ref type="bibr" target="#b29">Vestner et al., 2019)</ref>. Specifically, we replicate an earlier finding that social interactions capture visual attention to a greater extent than non-interactions in naturalistic stimuli <ref type="bibr" target="#b24">(Skripkauskaite et al., 2022)</ref>, but extend this finding by demonstrating this in dynamic videos that may provide both additional cues to interaction (e.g., coordinated or reciprocal actions) but also greater distraction (due to 'low level' motion cues capturing attention) in the non-interaction condition. We also replicate and extend findings from the only study to date to assess visual attention to social (faces) versus non-social (objects) AOIs in naturalistic dynamic stimuli <ref type="bibr" target="#b4">(Chevallier et al., 2015)</ref>. In their study, participants demonstrated greater looking to faces versus objects, and thus our findings 1) confirm that whole bodies also capture visual attention to a greater extent than background objects in naturalistic dynamic stimuli, and 2) extend this by demonstrating this effect to be moderated by the presence of a social interaction. We note that <ref type="bibr" target="#b4">Chevallier et al. (2015)</ref> did include interaction versus non-interaction conditions in their task. However, as this factor was not included in their analysis, it is not possible to draw any conclusions regarding social interaction perception from their study.</p><p>Why might individuals spend more time looking at two individuals when they are interacting as opposed to when they are engaged in pursuing separate goals, particularly when scenes contain the same number of people? On the surface, scenes where agents pursue separate goals might require more attention and processing to parse than scenes where individuals engage in joint actions. However, as discussed in prior work (e.g., <ref type="bibr" target="#b24">Skripkauskaite et al., 2022)</ref> social interactions constitute more than the sum of their parts; for example, they provide information on the social context and the relationship between two individuals. In addition, and as a potential explanation for why we observed greater visual attention to interacting people in dynamic stimuli, as the interaction unfolds over time individuals may spend more time looking back and forth between the interactants (compared to non-interacting individuals). Future research may wish to investigate this hypothesis using scan path analysis. Lastly, people find interactive scenes -even when they are low-emotive everyday scenes -to be more interesting and more positive than non-interactive scenes, even when these scenes are visually similar, matched for motion energy, and include the same people performing similar actions. In our view, this is a feature of social interactions rather than a design flaw. Social interactions may draw visual attention because humans are inherently social and pursue understanding interactions and relationships even when doing so serves no obvious purpose for the observer.</p><p>Our findings are also broadly in line with previous research investigating individual differences in visual attention. While <ref type="bibr" target="#b30">Villani et al. (2015)</ref> found that individuals with greater empathic concern (a subscale of the IRI) were faster (fewer fixations prior) to look to the face, they found no effect of any IRI subscale on looking time. In contrast, the present study found that individuals higher in trait empathy (represented by the whole IRI scale) spent longer looking at people. The findings across studies are broadly compatible -individuals with higher empathy show a stronger attentional bias towards social information -and differences between studies are potentially explained by methodological differences. Villani et al. presented static images of artwork for 15 seconds, while the current study presented naturalistic dynamic videos for 4 seconds. First, time-to-first-fixation or similar metrics are not relevant in stimuli that unfold over time and thus were not calculated in the current study. Secondly, the longer presentation time used by Villani and colleagues may have resulted in general processing of the image towards the end, potentially masking possible biases in looking time that might have been present during early processing. <ref type="bibr" target="#b4">Chevallier et al. (2015)</ref>'s primary interest was to examine differences in visual processing of social information between autistic and non-autistic individuals. Indeed, and in line with our findings, they found that autistic individuals spent less time looking at social AOIs and more time looking at object AOIs. Thus, our results confirm that individuals with higher levels of autistic traits also display differences in social information processing in naturalistic dynamic stimuli. We note, however, that our individual difference findings were not moderated by the presence of social interactions. This may reflect a genuine finding, suggesting that individual differences could explain a significant portion of variance in our main finding. However, we suggest this is unlikely due to the within subject design. Our study was powered for a three way rather than a 4way interaction, and so it is possible that we may not have had enough power to detect higher order effects in our more complex model incorporating individual differences. Future work, perhaps manipulating stimulus type to include stimuli that induce empathic responses, could delve more deeply into individual differences that might specifically influence attention specifically to social interactions.</p><p>The available data for facial movements did not support our second hypothesis: participants displayed more frowning in response to noninteractions compared to interactions, albeit this effect was very small. It may be that rather than participants utilising their own facial movements to make sense of the social interactions <ref type="bibr" target="#b3">(Bourgeois &amp; Hess, 2008;</ref><ref type="bibr" target="#b8">Drimalla et al., 2019;</ref><ref type="bibr" target="#b16">Kavanagh &amp; Winkielman, 2016)</ref>, some participants were employing greater facial movements, in particular frowning, during non-interactions in an effort to make sense of the information (i.e., they were searching for social interactions). We acknowledge, however, that our results are based on a low (25% probability) threshold and very small effect sizes and thus caution is advised when interpreting the results. Unfortunately, models looking at facial movements incorporating trait empathy and autistic traits failed to converge and thus no conclusions can be drawn about the possible impact of individual differences on facial activity in response to witnessing social interactions. We believe the strongest argument for why we failed to observe reasonable levels of facial movements in the current study is due to the webcam-based iMotions algorithm, which may not be sensitive enough to detect the subtle facial movements elicited by the stimuli. Indeed, the authors have encountered this issue in a previous study in which participants were ultimately deliberately instructed to mimic the study stimuli in order to achieve reliable measures of facial activity <ref type="bibr" target="#b5">(Daughters et al., 2021)</ref>. Future research could investigate the impact of social interactions on facial movements using more sensitive measurement techniques such as facial electromyography and/or include social scenes with more emotive content in the stimulus set.</p><p>Although failing to acquire adequate facial data is a significant limitation of the current study, we highlight several strengths as well. First, the study recruited a good sample size (N = 85, eye tracking observations = 7018) and utilised mixed-effect modelling to control for random variation across participants. Second, the study used naturalistic dynamic videos and, as such, provides novel insight into the processing of social interactions in more ecologically valid stimuli. Third, the stimulus set, although naturalistic, is tightly controlled such that important factors such as proximity between agents, props, actions, and level of motion is equivalent across specific interaction and noninteraction contexts. Fourth, although the mean age (30.78) of the current study is close to the typical student populations utilised in many psychological experiments, our age range (19-84) incorporated a much wider proportion of the population and achieved a roughly 50/50 sex split, thereby increasing the generalisability and inclusivity of the study.</p><p>In conclusion, the present study investigated two potential mechanisms through which individuals may process social interactions. Importantly, the current study used naturalistic dynamic stimuli and therefore provides novel insights into social interaction processing. The study found preferential processing of people in social interactions versus non-interactions, measured via longer looking times. There were also individual differences in this processing, such that individuals higher in trait empathy spent longer looking at people relative to the background, while individuals higher in autistics traits spent longer looking at the background. Future studies may wish to explore whether these longer looking times are driven by continual looking back and forth between interactants as the social interaction unfolds over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declaration of Conflicting Interests</head><p>The authors declare that there is no conflict of interest.</p></div>			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>This was due to an iMotions programming limitation of a minimum presentation time of 10 seconds.</p></note>
		</body>
		<back>

			<div type="funding">
<div><head>Funding</head><p>This work has received funding form the <rs type="funder">European Research Council</rs> under the European Union's <rs type="programName">Horizon 2020 research and innovation programme</rs> (<rs type="grantNumber">ERC-2016 -STG-716974</rs>: <rs type="projectName">Becoming Social</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_7AcF7Bh">
					<idno type="grant-number">ERC-2016 -STG-716974</idno>
					<orgName type="project" subtype="full">Becoming Social</orgName>
					<orgName type="program" subtype="full">Horizon 2020 research and innovation programme</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Visual attention and action: How cueing, direct mapping, and social interactions drive orienting</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Cole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1585" to="1605" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The autism-spectrum quotient (AQ): Evidence from asperger syndrome/high-functioning autism, malesand females, scientists and mathematicians</title>
		<author>
			<persName><forename type="first">S</forename><surname>Baron-Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wheelwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Skinner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Clubley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Autism and Developmental Disorders</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="17" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fitting linear mixed-effects models using lme4</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mächler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bolker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<idno type="DOI">10.18637/jss.v067.i01</idno>
		<ptr target="https://doi.org/10.18637/jss.v067.i01" />
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The impact of social context on mimicry</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bourgeois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Hess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Psychology</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="343" to="352" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Measuring social attention and motivation in autism spectrum disorder using eye-tracking: Stimulus type matters</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chevallier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Parish-Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mcvey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Rump</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Sasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Herrington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autism Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="620" to="628" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Oxytocin and emotion recognition: Investigating the possible roles of instructed mimicry and eye gaze</title>
		<author>
			<persName><forename type="first">K</forename><surname>Daughters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S R</forename><surname>Manstead</surname></persName>
		</author>
		<author>
			<persName><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schalk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Research in Ecological and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">100019</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Measuring individual differences in empathy: evidence for a multidimensional approach</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">113</biblScope>
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Emotional empathy and facial reactions to facial expressions</title>
		<author>
			<persName><forename type="first">U</forename><surname>Dimberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Andréasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Thunberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Psychophysiology</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="26" to="31" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">From face to face: the contribution of facial mimicry to cognitive and emotional empathy</title>
		<author>
			<persName><forename type="first">H</forename><surname>Drimalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Landwehr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Hess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dziobek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition and Emotion</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1672" to="1686" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Measuring facial movement</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">V</forename><surname>Friesen</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF01115465</idno>
		<ptr target="https://doi.org/10.1007/BF01115465" />
	</analytic>
	<monogr>
		<title level="m">Pictures of facial affect</title>
		<imprint>
			<publisher>Consulting Psychologists Press</publisher>
			<date type="published" when="1975">1975</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Real-time inference of complex mental states from facial expressions and head gestures</title>
		<author>
			<persName><forename type="first">R</forename><surname>El Kaliouby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Real-time vision for human-computer interaction</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="181" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Preferential processing of social features and their interplay with physical saliency in complex naturalistic scenes</title>
		<author>
			<persName><forename type="first">A</forename><surname>End</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gamer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">418</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">From dyads to triads: Perceptual unity of social groups</title>
		<author>
			<persName><forename type="first">V</forename><surname>Fratino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Colombatto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ristic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page">4227</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Emotional contagion</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hatfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Cacioppo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Rapson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Directions in Psychological Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="96" to="100" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Facial mimicry and emotional contagion to dynamic emotional facial expressions and their influence on decoding accuracy</title>
		<author>
			<persName><forename type="first">U</forename><surname>Hess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Blairy</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0167-8760(00)00161-6</idno>
		<ptr target="https://doi.org/10.1016/S0167-8760(00)00161-6" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Psychophysiology</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="141" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">You smile-I smile: Emotion expression in social interaction</title>
		<author>
			<persName><forename type="first">U</forename><surname>Hess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bourgeois</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Psychology</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="514" to="520" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The functionality of spontaneous mimicry and its influences on affiliation: An implicit socialization account</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Kavanagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Winkielman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">458</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Kloosterman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">V</forename><surname>Keefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Summerfeldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D A</forename><surname>Parker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Evaluation of the factor structure of the Autism-Spectrum Quotient</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="310" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">lmerTest package: tests in linear mixed effects models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Brockhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H B</forename><surname>Christensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The role of motion in the neural representation of social interactions in the posterior temporal cortex</title>
		<author>
			<persName><forename type="first">J</forename><surname>Landsiedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Daughters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Downing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Koldewyn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">262</biblScope>
			<biblScope unit="page">119533</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Affect valence inference from facial action unit spectrograms</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mcduff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>El Kaliouby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kassam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition-Workshops</title>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Seeing social events: The visual specialization for dyadic human-human interactions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Papeo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Abassi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="877" to="888" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visual search for people among people</title>
		<author>
			<persName><forename type="first">L</forename><surname>Papeo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goupil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soto-Faraco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1483" to="1496" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Different visual preference patterns in response to simple and complex dynamic social stimuli in preschool-aged children with autism spectrum disorders</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS One</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">122280</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attentional bias towards social interactions during viewing of naturalistic scenes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Skripkauskaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mihai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Koldewyn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quarterly Journal of Experimental Psychology</title>
		<imprint>
			<date type="published" when="2022">2022. 17470218221140880</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The role of mimicry in understanding the emotions of others</title>
		<author>
			<persName><forename type="first">M</forename><surname>Stel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Hess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Emotional Mimicry in Social Context</title>
		<editor>
			<persName><forename type="first">U</forename><surname>Hess</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Fischer</surname></persName>
		</editor>
		<imprint>
			<publisher>University Press</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="27" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mimicry and just world beliefs: Mimicking makes men view the world as more personally just</title>
		<author>
			<persName><forename type="first">M</forename><surname>Stel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Van Den Bos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rispens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British Journal of Social Psychology</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="397" to="411" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Investigating automatic emotion processing in boys with autism via eye tracking and facial mimicry recordings</title>
		<author>
			<persName><forename type="first">S</forename><surname>Van Der Donck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vettori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dzhelyova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Mahdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Claes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Steyaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boets</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autism Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1404" to="1420" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Objects that direct visuospatial attention produce the search advantage for facing dyads</title>
		<author>
			<persName><forename type="first">T</forename><surname>Vestner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Over</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L H</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">161</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bound together: Social binding leads to faster processing, spatial distortion, and enhanced memory of interacting partners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Vestner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Tipper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Over</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-A</forename><surname>Rueschemeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1251" to="1268" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Visual exploration patterns of human figures in action: an eye tracker study with art paintings</title>
		<author>
			<persName><forename type="first">D</forename><surname>Villani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Morganti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cipresso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ruggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Riva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gilli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">1636</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Impaired overt facial mimicry in response to dynamic facial expressions in highfunctioning autism spectrum disorders</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yoshimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Uono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Toichi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Autism and Developmental Disorders</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="1318" to="1328" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
