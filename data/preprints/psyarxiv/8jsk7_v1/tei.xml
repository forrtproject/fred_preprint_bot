<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Feeling the Music: Measuring Enhanced Emotion in Musicians via CNN-based Sentiment Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Gavin</forename><surname>Warnakulasooriya</surname></persName>
							<email>gavinwarna99@gmail.com</email>
						</author>
						<title level="a" type="main">Feeling the Music: Measuring Enhanced Emotion in Musicians via CNN-based Sentiment Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BEAF61DF61C96EA67EEE4853E86EBD36</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-22T20:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Empathy has long been associated with musical expertise, but quantifying empathy remains a difficult task. This study investigates the correlation between musical training and emotional reactivity via webcam based facial expression recognition. By using a Convolutional Neural Network (CNN), facial expressions of musically trained and untrained participants were analyzed as they watched emotionally intense videos, with and without audio. Musically trained individuals, defined as those with over 400 hours of total musical training, had significantly higher emotional reactivity than those without training. The musically trained group also showed a greater difference in their emotional response to videos with audio compared to videos without audio, suggesting an increase in emotional sensitivity to audio. Statistical analyses, using Mann-Whitney U tests and Cohen's d effect size calculations confirmed that the differences were statistically significant, and had a large effect size, thereby reinforcing the accepted hypothesis that musical training enhances emotional processing. The findings aid research on the cognitive and emotional benefits of musical training, providing quantitative backing that musicians have higher emotional reactivity (empathy). Additionally, the study highlights potential use cases of AI in facial sentiment analysis as an objective measure for psychological research concerning emotions. Future research could further explore causal links behind musical training and empathy and implications for the technology in music education, media analytics, and therapeutic methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Past empirical studies have supported the idea that musical training can enhance one's empathy and emotional reactivity. Engaging with music fundamentally uses a level of emotional communication, which may train individuals to perceive and share emotions more effectively, a sign of empathy. Studies have often showed the positive social and emotional effects of musical training, like an improved recognition of emotional inflections in speech, an enhanced ability to recognize distress in infants, and higher scores on behavior scales <ref type="bibr" target="#b3">(Kirschner &amp; Tomasello, 2010;</ref><ref type="bibr">Rabinowitch et al., 2012;</ref><ref type="bibr">Wang et al., 2009)</ref>. The findings of these past studies suggest that interaction with music can foster emotional awareness and build an increased capacity for empathy.</p><p>Defining musical training can be ambiguous, so a practical threshold is 400 hours of cumulative practice, corresponding to an intermediate level of proficiency in music. Past study has shown that individuals with at least 400 hours of training in music demonstrate stronger auditory skills, regardless of if they identify as a musician <ref type="bibr" target="#b11">(Wu &amp; Lu, 2021)</ref>. The threshold ensures that the musician group has meaningful engagement with music, regardless of their own perceptions, allowing for a comparison to non-musicians with less than 400 hours of experience.</p><p>Facial Expression Recognition (FER) can classify emotions based on facial features using Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b4">(Kopalidis et al., 2024)</ref>. The networks often include multiple convolutional layers combined with pooling layers, allowing them to detect small patterns in the face, such as small muscle movements. Dropout layers in the network prevent overfitting while categorical cross-entropy loss functions help provide stronger emotion classification.</p><p>One example of FER is the VGG-based FER model, which adapts a deep CNN (VGG-19) for emotion recognition. This network takes a facial image and processes it through multiple layers, outputting a probability for each emotion in specified categories. The model was trained on large datasets like FER2013 and has optimized its weights to classify expressions in an accurate manner. Studies indicate the CNN-based FER systems like the VGG-19 based model can achieve classification accuracies exceeding 87% <ref type="bibr" target="#b7">(Liu et al., 2021)</ref>. Advancements in FER allow researchers to measure emotional reactivity in an objective manner with precision, making it a powerful tool for psychological research.</p><p>The primary hypothesis of the study is that the musically trained individuals will demonstrate a greater degree of emotional reactivity than their non-trained counterparts when exposed to video stimuli. As an exploratory measure, the study also investigates whether the musically trained group will exhibit a higher difference in reactivity to the videos when they view them with and without audio. If confirmed, the findings suggest that musical experience enhances both empathy and emotional responses to auditory stimulus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participant Sampling</head><p>This study employed a snowball sampling method, leading to a final group of 46 participants, divided into 23 musicians and 23 non-musicians. These participants ranged from 14 to 60 years old, with an equal distribution across gender, across various musical backgrounds. To validate groupings, participants completed a survey assessing their musical experience prior to participation in the experiment. Those with over 400 hours of cumulative musical training were classified as musically trained <ref type="bibr">(Wang et al., 2009)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure</head><p>Before exposure to stimuli, participants viewed a neutral gray screen for 10 seconds to establish baseline emotional levels. They then watched three emotionally provocative videos selected to elicit distinct emotional responses: a video of a distressed dog designed to evoke sadness and anger, a video of happy babies intended to elicit joy, and a jump-scare horror video to provoke surprise and fear. These videos were first presented without audio and then shown again with synchronized audio to examine differences in emotional reactivity based on auditory enhancement. A Python GUI using Tkinter and OpenCV controlled video playback and recorded facial expressions. The second round of video with audio serves as a repetition for all subjects as they are exposed to the same video and serves as the main category for additional exploration as to if musically trained individuals will change in reaction more than untrained individuals when exposed to audio. This second round is biased as in the second round, since the participants had already seen the videos, they would be less reactive by default. However, this means that any positive change in emotional reactivity was even more significant, thereby indicating a clear difference. The participants' video data was then analyzed by the python package "FER".  <ref type="figure" target="#fig_0">1A</ref>, all participants were in the same room, with blackout curtains to minimize variability from external sunlight. As seen in figure <ref type="figure" target="#fig_0">1B</ref>, participants then entered a custom Python GUI completely independent to minimize confounding factors via the presence of other people. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Variables</head><p>In the experiment, the independent variable was the participant's musical training status, categorizing individuals as either musically trained or untrained. The study included two dependent variables: the total empathy score, which measured the overall emotional reactivity of each participant, and the difference in empathy scores between the audio and non-audio conditions to assess changes in emotional responsiveness. The untrained group functioned as a negative control, allowing for the isolation of musical training as a factor influencing emotional reactivity. To maintain experimental rigor, several variables were held constant, including standardized procedures for all participants and a controlled environment in which all individuals were exposed to the same set of videos under identical conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Controls</head><p>To establish controls for the FER model, both negative and positive controls were used. For negative control, several stock image of a person without facial features were run through the FER, returning 0 probability for all emotions, For a positive control, several images of happy and sad people were run through the model, which returned positive values for the 'happy' and 'sad' categories of the emotional probabilities. Thus, the performance of the FER model was verifiably accurate, and the experiment could continue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Processing</head><p>The FER model extracted emotional data at 24 frames per second, classifying expressions into seven emotions: happiness, sadness, anger, surprise, fear, disgust, and neutrality <ref type="bibr" target="#b7">(Liu et al., 2021)</ref>. Baseline emotional scores (from the gray screen) were first averaged and then these average values were subtracted from video reaction scores to normalize individual differences. Then, a weighting algorithm was employed to ensure that the most significant emotional moments were given appropriate weighting within the dataset, meaning that participants' reactions during more significant moments during the videos were more heavily considered toward their total emotional score. This algorithm first segmented each participant's emotional response into discrete time intervals, collecting data at a rate of 24 frames per second. For each frame, the FER model classified emotions using a probability distribution across seven distinct emotional categories. From the FER's output data, the average baseline values were subtracted and then the result was multiplied by the weight (per emotional significance of moments). This process allowed for a precise, frame-by-frame quantification of emotional intensity, ensuring that the most emotionally salient moments contributed more significantly to the final reactivity score. Following the quantification, only target emotions from Table <ref type="table" target="#tab_0">1</ref> were isolated and then integrated to achieve an emotional score index for each video. To integrate the emotional responses over time, a Riemann sum integration method was applied, which allowed for the numerical approximation of the total emotional response by summing discrete emotional intensity values at finely spaced time intervals. Given the continuous nature of facial expressions, the Riemann sum approach was particularly effective in capturing fluctuations in emotional responses across time. The integration was performed using a right Riemann sum technique, where the function value at the right endpoint of each subinterval was used to approximate the area under the curve of emotional response over time. This method provided a rigorous way to quantify total emotional reactivity, effectively transforming discrete emotional intensity measurements into a numerical reactivity score that could be compared between groups. Emotional index scores for each video were added to obtain a single total empathy score per each participant. Finally, for the exploratory section of the study, the net differences between audio and non-audio reactions were taken (e.g. emotional score index for video 4 -emotional score index from video 1) and summed to generate a single final difference score for the participant.</p><p>After this step, each subject would have both a total empathy score and a total difference in reactivity between audio and non-audio videos score. All analysis was conducted in a Python 3.9 environment with a Nvidia GTX 1650 and an Intel Core i7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Primary</head><p>The total empathy scores of all 46 participants were analyzed and categorized into two groups: musically trained and untrained individuals. To visually compare the differences in emotional reactivity between these groups, a bar graph and histogram were generated, illustrating the distribution of emotional responses across participants. These visual representations provide insight into the extent of variation in emotional reactivity and allow for a comparative analysis between musically trained and untrained individuals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 3. Comparison of total emotional scores between musically trained and untrained individuals</head><p>The box plot and histogram provide a clear depiction of the differences in emotional reactivity between musically trained and untrained individuals. The box plot illustrates that musically trained participants exhibited higher median empathy scores, with a greater range of values and a few extreme outliers, suggesting that their emotional reactivity varied more widely compared to non-musicians. In contrast, the untrained group demonstrated lower overall scores with less variability, indicating a more restricted range of emotional responses. The histogram further supports this trend, showing that the distribution of empathy scores in the musically trained group is more spread out, with a higher frequency of participants scoring at the upper end of the scale, while the untrained group clusters more heavily around lower scores. These results suggest that musical training is associated with heightened emotional sensitivity, reinforcing the hypothesis that musicians exhibit stronger emotional engagement with stimuli. The increased spread of scores within the musically trained group could also indicate that different levels or types of musical training may further modulate emotional responsiveness, an avenue that warrants further exploration in future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exploratory Outcome</head><p>From the difference scores evaluated, a histogram and line chart were produced to evaluate the differences between musically trained and untrained groups in terms of their difference in reaction to videos with audio and without audio. The difference score distributions indicate that musically trained individuals show greater changes in emotional reactivity when exposed to media with added audio. This suggests a potential link between musical training and heightened sensitivity to emotional cues in auditory-enhanced stimuli.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistical Analysis</head><p>First, a Shapiro-Wilk test and Levene's test were applied to determine the type of inference test suitable for the study. Results of the tests can be seen below: Because of the results of these tests, the Mann-Whitney U test was deemed most appropriate, due to its nonparametric quality. Finally, Cohen's d test measured the effect size of musical training on both emotional reactivity and differences with auditory stimuli (the primary and exploratory outcomes respectively). Due to p-values well below any reasonable threshold, we reject the null hypothesis for both the primary and exploratory cases. Thus, we confirm that musical training is associated with increased emotional reactivity and increased response to auditory stimuli. Finally, Cohen's d effect size calculation revealed a large effect size in both primary and exploratory analyses (d=1.4874, and d=1.8679, both greater than 0.8 threshold), reinforcing the large significance of musical training. Overall, these results confirm the original hypothesis that musicians exhibit more sensitivity to auditory and emotional cues, by a substantial degree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>This study provides strong evidence that musical training enhances emotional reactivity, especially in response to emotional auditory stimuli. Musically trained individuals consistently showed significantly higher empathy scores than untrained participants, both in total emotional reactivity and in the degree of change when sound was introduced. These findings directly support the primary and exploratory hypotheses: first, that musically trained individuals would show greater emotional reactivity overall, and second, that they would exhibit a more pronounced difference between audio and non-audio conditions. The ability of trained musicians to respond more deeply to emotional content, particularly when sound is present, may stem from enhanced auditory-emotional integration. Research by <ref type="bibr" target="#b0">Coffey et al. (2017)</ref> demonstrates that musical training leads to increased connectivity between the auditory cortex and limbic structures like the amygdala. This neuroplasticity likely contributes to faster and more intense emotional processing of auditory cues. It also aligns with our observation that trained participants were not desensitized by video repetition but instead demonstrated greater reactivity when audio was added in the second round, an effect that would be unlikely unless they were particularly susceptible to sound-based emotional input.</p><p>These findings also expand the work of <ref type="bibr" target="#b9">Rabinowitch et al. (2013)</ref> and <ref type="bibr" target="#b11">Wu &amp; Lu (2021)</ref>, who found that musical engagement enhances empathy and prosocial behavior. By using an AI-driven, objective method (CNNbased FER), this study builds on their behavioral observations with real-time physiological evidence, strengthening the argument that musical training meaningfully alters emotional processing.</p><p>Beyond neural mechanisms, the implications for education and therapeutic practice are substantial. Structured musical instruction can serve as a medium for developing emotional intelligence and social-emotional learning, especially in early childhood or adolescent education <ref type="bibr" target="#b10">(Schellenberg et al., 2015)</ref>. The emotionally expressive and interpretive demands of music may help children learn to recognize and manage emotions -skills that are critical for lifelong mental well-being. Likewise, music therapy has shown efficacy in treating conditions like depression, anxiety, and autism spectrum disorder by offering patients a non-verbal channel to engage with their emotions <ref type="bibr" target="#b13">(Aalbers et al., 2017)</ref>.</p><p>From a technology standpoint, this study demonstrates how tools like FER can be used to quantify emotional sensitivity and measure emotional responses at scale. Prior studies on the topic were empirical, while this study created a more objective way to quantify emotional sensitivity, avoiding human biases. However, facial expression recognition remains imperfect: performance can be influenced by lighting conditions, camera angles, facial structures, and cultural display norms <ref type="bibr" target="#b6">(Li &amp; Deng, 2020;</ref><ref type="bibr" target="#b7">Liu et al., 2021)</ref>. Future versions of this study could benefit from multimodal inputs, such as physiological signals from an EEG or heart rate sensors to improve reliability across populations.</p><p>Of course, this research has limitations. The small sample size (n = 46) and snowball sampling method restrict generalizability and may introduce selection bias. Emotional responses were also measured solely through visible facial changes, potentially missing individuals with intense but less overt expressions. In addition, we cannot yet determine the directionality of the observed effect: it remains possible that individuals with naturally high emotional sensitivity are more drawn to music, rather than music itself creating that sensitivity. Longitudinal or randomized intervention studies would be necessary to establish causality of this bidirectional effect.</p><p>Finally, this study opens the door to several future questions: Do different types of musical training (e.g., vocal vs. instrumental) or different genres (e.g., jazz vs. classical) lead to distinct emotional profiles? Does longer or earlier training yield stronger effects? Could combining facial analysis with fMRI or EEG provide clearer insight into the precise mechanisms by which music alters the brain's emotional processing pathways?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>By employing CNN-based analysis, this research introduced a novel methodological approach by moving beyond observational techniques commonly used in psychological studies. Instead, it utilized objective, real-time facial expression data to quantify emotional reactivity with greater objectivity. This framework not only minimizes subjective bias but also offers a scalable and replicable method that can be applied in future psychological research, affective computing, and clinical assessments to evaluate emotional engagement in a more standardized manner.</p><p>In summary, this study confirms both aspects of our hypothesis: musically trained individuals exhibit greater emotional reactivity overall and demonstrate stronger sensitivity to auditory cues. These effects are statistically robust and supported by both behavioral and neurological literature, suggesting that musical training enhances not only how we hear, but how deeply we feel.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Setup. Desk, GUI. As seen in figure1A, all participants were in the same room, with blackout curtains to minimize variability from external sunlight. As seen in figure1B, participants then entered a custom Python GUI completely independent to minimize confounding factors via the presence of other people.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Processed emotional scores over time. A: All emotions, B: isolated emotion Reimann sum. As seen in figure 2A, emotions per the FER model can vary greatly over time, depending on the participants facial reaction to the stimuli. Per figure 2B, summing total values (in blue) can render a single score per each video.Finally, for the exploratory section of the study, the net differences between audio and non-audio reactions were taken (e.g. emotional score index for video 4 -emotional score index from video 1) and summed to generate a single final difference score for the participant.After this step, each subject would have both a total empathy score and a total difference in reactivity between audio and non-audio videos score. All analysis was conducted in a Python 3.9 environment with a Nvidia GTX 1650 and an Intel Core i7.</figDesc><graphic coords="4,75.14,99.01,543.93,407.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Comparison of Emotional difference scores for musically trained and untrained individuals.The difference score distributions indicate that musically trained individuals show greater changes in emotional reactivity when exposed to media with added audio. This suggests a potential link between musical training and heightened sensitivity to emotional cues in auditory-enhanced stimuli.</figDesc><graphic coords="6,12.18,171.88,597.22,447.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Six video overviews</figDesc><table><row><cell>Video #</cell><cell>Description</cell><cell>Length</cell><cell>Target Emotion(s)</cell></row><row><cell>1</cell><cell>A sad video of a dog</cell><cell>~30 seconds</cell><cell>sad, angry</cell></row><row><cell></cell><cell>crying, no audio</cell><cell></cell><cell></cell></row><row><cell>2</cell><cell>A happy video of babies</cell><cell>~30 seconds</cell><cell>happy</cell></row><row><cell></cell><cell>laughing, no audio</cell><cell></cell><cell></cell></row><row><cell>3</cell><cell>A scary/ominous video of</cell><cell>~20 seconds</cell><cell>surprise, fear</cell></row><row><cell></cell><cell>a chair with a figure</cell><cell></cell><cell></cell></row><row><cell></cell><cell>jumping out, no audio</cell><cell></cell><cell></cell></row><row><cell>4</cell><cell>Same as video 1, with sad</cell><cell>~30 seconds</cell><cell>sad, angry</cell></row><row><cell></cell><cell>music.</cell><cell></cell><cell></cell></row><row><cell>5</cell><cell>Same as video 2, with</cell><cell>~30 seconds</cell><cell>happy</cell></row><row><cell></cell><cell>happy music and audible</cell><cell></cell><cell></cell></row><row><cell></cell><cell>laughs</cell><cell></cell><cell></cell></row><row><cell>6</cell><cell>Same as video 3, with</cell><cell>~20 seconds</cell><cell>surprise, fear</cell></row><row><cell></cell><cell>scary music and a scream</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Results of Shapiro-Wilk and Levene's tests.</figDesc><table><row><cell>Case</cell><cell>Test</cell><cell>Null Hypothesis</cell><cell>Result</cell><cell>Conclusion</cell></row><row><cell>Primary</cell><cell>Shapiro-Wilk</cell><cell>Data is Normal</cell><cell>Trained:</cell><cell>Reject (Untrained is</cell></row><row><cell></cell><cell></cell><cell></cell><cell>W=0.9829</cell><cell>non-normal)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>p=0.961</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Untrained:</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>W=0.8519</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>p=0.00367</cell><cell></cell></row><row><cell>Exploratory</cell><cell>Shapiro-Wilk</cell><cell>Data is Normal</cell><cell>Trained:</cell><cell>Fail to Reject (Both</cell></row><row><cell></cell><cell></cell><cell></cell><cell>W=0.9724</cell><cell>are normal)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>p=0.745</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Untrained:</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>W=0.9283</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>p=0.101</cell><cell></cell></row><row><cell>Primary</cell><cell>Levene's</cell><cell>Groups have</cell><cell>F=0.2511</cell><cell>Fail to Reject (Equal</cell></row><row><cell></cell><cell></cell><cell>equal variance</cell><cell>p=0.619</cell><cell>variance)</cell></row><row><cell>Exploratory</cell><cell>Levene's</cell><cell>Groups have</cell><cell>F=1.0920</cell><cell>Fail to Reject (Equal</cell></row><row><cell></cell><cell></cell><cell>equal variance</cell><cell>p=0.301</cell><cell>variance)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Results of Mann-Whitney U and Cohen's d tests</figDesc><table><row><cell>Case</cell><cell>Test</cell><cell>Null Hypothesis</cell><cell>Result</cell><cell>Conclusion</cell></row><row><cell>Primary</cell><cell>Mann-Whitney U</cell><cell>There is no</cell><cell>U=395.00</cell><cell>Reject null</cell></row><row><cell></cell><cell></cell><cell>significant</cell><cell>p=7.11e-05</cell><cell>hypothesis (p&lt;0.05)</cell></row><row><cell></cell><cell></cell><cell>difference</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>between groups</cell><cell></cell><cell></cell></row><row><cell>Exploratory</cell><cell>Mann-Whitney U</cell><cell>There is no</cell><cell>U=470.00</cell><cell>Reject null</cell></row><row><cell></cell><cell></cell><cell>significant</cell><cell>p=6.679e-06</cell><cell>hypothesis (p&lt;0.05)</cell></row><row><cell></cell><cell></cell><cell>difference</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>between groups</cell><cell></cell><cell></cell></row><row><cell>Primary</cell><cell>Cohen's d</cell><cell>N/A</cell><cell>d=1.4874</cell><cell>There is a large effect</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>size of musical</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>training on empathy</cell></row><row><cell>Exploratory</cell><cell>Cohen's d</cell><cell>N/A</cell><cell>d=1.8679</cell><cell>There is a large effect</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>size of musical</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>training on empathy</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>score differences</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>with an without</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>audio</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cortical correlates of the auditory frequency-following and onset responses: EEG and fMRI evidence</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B J</forename><surname>Coffey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Musacchia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Zatorre</surname></persName>
		</author>
		<idno type="DOI">10.1523/JNEUROSCI.1265-16.2017</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="830" to="838" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Normality tests for statistical analysis: a guide for non-statisticians</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ghasemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zahediasl</surname></persName>
		</author>
		<idno type="DOI">10.5812/ijem.3505</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Endocrinology and Metabolism</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="486" to="489" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Are assumptions of well-known statistical techniques checked, and why (not)</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hoekstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A L</forename><surname>Kiers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2012.00137</idno>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">137</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Joint music-making promotes prosocial behavior in 4-year-old children</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kirschner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tomasello</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.evolhumbehav.2010.04.004</idno>
	</analytic>
	<monogr>
		<title level="j">Evolution and Human Behavior</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="354" to="364" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Advances in facial expression recognition: A survey of methods, benchmarks, models, and datasets</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kopalidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Solachidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vretos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Daras</surname></persName>
		</author>
		<idno type="DOI">10.3390/info15030135</idno>
	</analytic>
	<monogr>
		<title level="j">Information</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">135</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Calculating and reporting effect sizes to facilitate cumulative science: a practical primer for ttests and ANOVAs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lakens</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2013.00863</idno>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">863</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep facial expression recognition: A survey</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<idno type="DOI">10.1109/TAFFC.2020.2981446</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1195" to="1215" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The effectiveness of facial expression recognition in detecting emotional responses to sound interventions in older adults with dementia</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2021.642977</idno>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">642977</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The Mann-Whitney U: A test for assessing whether two independent samples come from the same population</title>
		<author>
			<persName><forename type="first">N</forename><surname>Nachar</surname></persName>
		</author>
		<idno type="DOI">10.20982/tqmp.04.1.p013</idno>
	</analytic>
	<monogr>
		<title level="j">Tutorials in Quantitative Methods for Psychology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="13" to="20" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long-term musical group interaction has a positive influence on empathy in children</title>
		<author>
			<persName><forename type="first">T.-C</forename><surname>Rabinowitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Burnard</surname></persName>
		</author>
		<idno type="DOI">10.1177/0305735612440609</idno>
	</analytic>
	<monogr>
		<title level="j">Psychology of Music</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="484" to="498" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Group music training and children&apos;s prosocial skills</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Schellenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Corrigall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Dys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Malti</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0141449</idno>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">141449</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Musical training in the development of empathy and prosocial behaviors</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2021.661769</idno>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">661769</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Statistical power analysis for the behavioral sciences</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
	<note>2nd ed.</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Music therapy for depression</title>
		<author>
			<persName><forename type="first">S</forename><surname>Aalbers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fusar-Poli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Spreen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C F</forename><surname>Ket</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Vink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maratos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Crawford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gold</surname></persName>
		</author>
		<idno type="DOI">10.1002/14651858.CD004517.pub3</idno>
		<ptr target="https://doi.org/10.1002/14651858.CD004517.pub3" />
	</analytic>
	<monogr>
		<title level="j">Cochrane Database of Systematic Reviews</title>
		<imprint>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">4517</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
