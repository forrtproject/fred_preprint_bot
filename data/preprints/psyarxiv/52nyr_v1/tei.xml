<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VocalNotes: Investigating the Perception of Note Pitch and Boundaries through Varying Transcriptions of Vocal Performances from Five Musical Cultures</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Polina</forename><surname>Proutskova</surname></persName>
							<email>proutskova@googlemail.com</email>
						</author>
						<author>
							<persName><forename type="first">Miranda</forename><surname>Crowdus</surname></persName>
							<email>miranda.crowdus@concordia.ca</email>
						</author>
						<author>
							<persName><forename type="first">Yulia</forename><surname>Nikolaenko</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yuto</forename><surname>Ozaki</surname></persName>
							<email>yuto_ozaki@keio.jp</email>
						</author>
						<author>
							<persName><forename type="first">Lawrence</forename><surname>Shuster</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Olga</forename><surname>Velichkina</surname></persName>
							<email>olga.velichkina@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Yannick</forename><surname>Wey</surname></persName>
							<email>yannick.wey@hslu.ch</email>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Yue</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Gabriel</forename><surname>Zuckerberg</surname></persName>
							<email>gabriel_zuckerberg@brown.edu</email>
						</author>
						<author>
							<persName><forename type="first">Yukun</forename><surname>Li</surname></persName>
							<email>yukun.li@qmul.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Andrew</forename><surname>Killick</surname></persName>
							<email>a.killick@sheffield.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">John</forename><forename type="middle">M</forename><surname>Mcbride</surname></persName>
							<email>jmmcbride@protonmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Elizabeth</forename><surname>Phillips</surname></persName>
							<email>phille10@mcmaster.ca</email>
						</author>
						<author>
							<persName><forename type="first">Patrick</forename><forename type="middle">E</forename><surname>Savage</surname></persName>
							<email>psavage@sfc.keio.ac.jp</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">BBC</orgName>
								<orgName type="institution" key="instit2">Queen Mary University of Lonson</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Gakuto Chiba</orgName>
								<orgName type="institution" key="instit2">Keio University</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Concordia University</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">independent researcher</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Keio University</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Cornell University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">French Society for Ethnomusicology</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="department">Zhaoxin Yu</orgName>
								<orgName type="institution">Lucerne University of Applied Sciences and Arts</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="institution">Shandong College of Arts</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff9">
								<orgName type="institution">Shandong College of Arts</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff10">
								<orgName type="institution">Brown University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff11">
								<orgName type="institution" key="instit1">Queen</orgName>
								<orgName type="institution" key="instit2">Mary University of London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff12">
								<orgName type="department">Center for Algorithmic and Robotized Synthesis</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff13">
								<orgName type="institution">Institute for Basic Science</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff14">
								<orgName type="institution">McMaster University</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff15">
								<orgName type="institution">Keio University</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff16">
								<orgName type="institution">University of Auckland</orgName>
								<address>
									<settlement>New Zealand</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">VocalNotes: Investigating the Perception of Note Pitch and Boundaries through Varying Transcriptions of Vocal Performances from Five Musical Cultures</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">253625B685F708570747C5BE83638A91</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-23T06:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The VocalNotes project investigated how expert traditional music listeners conceive of notes in vocal performances by studying similarities and differences in their transcriptions. Teams of experts from five musical traditions (Japanese folk song, Chinese bangzi opera, Russian traditional village singing, Alpine yodelling, and Romaniote Jewish chanting) each transcribed ~10 minutes of vocal recordings from their culture, where manual transcription consisted of segmentation and note pitch correction, starting from an automatically extracted pitch curve. The experts then compared their independent transcriptions and looked for factors which could have led to disagreements.</p><p>Western staff notation is not suitable for investigating such variances, because it does not represent sufficiently fine gradations of pitch and timing. We therefore used tools that allowed more precise annotations, namely Tony for segmentation, and Sonic Visualiser for note pitch correction and transcription comparison.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We found that overall agreement was prevalent and the concept of note was generally applicable for analysis of vocal performances. Yet in some contexts disagreements were abundant, with the note concept reaching its limits. We identified four primary contexts which led to disagreements across several musical cultures: 1) differences in cultural knowledge between the transcribers, 2) differences in interpreting syllabic boundaries, 3) intra-syllabic pitch changes, and 4) "voice splash" -abrupt pitch changes caused by vocal techniques or used as an expressive device.</p><p>The VocalNotes dataset, containing the audio of the musical fragments, annotations, and song documentation, has been published for replicability and further research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Motivation</head><p>Providing a sufficiently broad yet useful definition of "music" has challenged music researchers for decades <ref type="bibr">(Nettl 2015: 19-30;</ref><ref type="bibr" target="#b35">Jacoby et al. 2020)</ref>. Many would agree that, for the most part, music is an intentional temporal ordering of sounds following some cultural or stylistic form. A majority of musical analysis across fields has taken musical "sounds" to be "notes," each with a defined onset, duration, and pitch, but for many musical cultures and styles this model can be hard to apply. Are the ornaments of Baroque music distinct notes, or components of the main note that they decorate? How does one define a note among the continuously gliding pitches and timbres that comprise much of contemporary electronic music? One could argue that these are special musical cases. Yet, the most ancient and universal form of human music is song <ref type="bibr" target="#b9">(Brown and Jordania 2013;</ref><ref type="bibr" target="#b75">Patrick E Savage et al. 2015;</ref><ref type="bibr" target="#b52">Mehr et al. 2019)</ref>, and conspecific vocalizations (in our case, the human voice) are among the most privileged audio signals in our cognition. Thus the issues that arise when analyzing and defining song are due to fundamental limitations on our ability to analyze and define music. Across musical cultures and styles, the analysis of song presents several difficulties.</p><p>The human vocal apparatus is very agile. It has a large number of movable parts of various forms, sizes, and functions, which are in constant movement and mutual adjustment during vocal production; the voice produces unlimited gradations of pitch and more extensive timbral variation than most instruments, as vocal formants change with every syllable <ref type="bibr" target="#b83">(Sundberg 1987;</ref><ref type="bibr" target="#b86">Titze and Martin 1998;</ref><ref type="bibr" target="#b81">Steinhauer, McDonald, and Estill 2017)</ref>. This flexibility causes the voice to produce very complex signals with continuously fluctuating fundamental frequency curves (Fig. <ref type="figure">1</ref>). This makes vocal analysis more difficult to formalise <ref type="bibr" target="#b64">(Proutskova 2019</ref>) than the analysis of most instrumental music. Additionally, many singing techniques use continuous changes in pitch (e.g. vibrato, embellishments, glides) or harmonic irregularities (glottal sounds and rasp) which contribute to song's analytical ambiguity. When presented with vocal music, both humans and machines have a hard time coming to consensus about its constituent parts.</p><p>Fig. <ref type="figure">1</ref> Pitch curve of a Russian traditional song "Vy kumushki kumitesia" from Poozerie region, played on a keyboard and sung by the traditional performer Olga Sergeeva <ref type="bibr" target="#b68">(Razumovskaya 1997)</ref> Difficulties can arise at multiple stages in the cognitive processes of transcription, including perception and representation. There is some evidence that the perception of singing is more permissive to small differences in pitch than the perception of instrumental (non-human) sounds (aka vocal generosity effect <ref type="bibr" target="#b63">(Pfordresher et al. 2010;</ref><ref type="bibr" target="#b34">Hutchins, Roquet, and Peretz 2012;</ref><ref type="bibr" target="#b62">Pfordresher and Brown 2017;</ref><ref type="bibr" target="#b61">Ozaki et al. 2024)</ref>). On the other hand, humans (even infants) can be remarkably sensitive to minute changes in vocal pitch, timing, and timbre, which are important cues in emotional communication <ref type="bibr" target="#b76">(Scherer 2003)</ref>. This can lead even expert listeners to hear objectively different vocalizations as equivalent, and vice versa.</p><p>Ethnomusicologists have long recognised that two different but equally competent transcribers are liable to notate the same music differently <ref type="bibr" target="#b45">(List 1963;</ref><ref type="bibr" target="#b30">Herzog 1964;</ref><ref type="bibr" target="#b21">England et al. 1964;</ref><ref type="bibr">List 1974;</ref><ref type="bibr" target="#b2">Alekseev 1990;</ref><ref type="bibr" target="#b80">Stanyek 2014)</ref>: "Two different transcriptions of the same piece do not necessarily indicate varying competence; they may reflect differences in the purpose of the task at hand, in the conception of what constitutes a piece of music" <ref type="bibr">(Nettl 2015: 76)</ref>. While perceptual differences between transcribers play a role, this has been difficult to separate from differences of interpretation and representation, all the more so when a notation system originating in one musical culture is adapted to others.</p><p>Western staff notation is the most common system for music transcription, including in non-Western or cross-cultural contexts. However, Western staff notation is not optimal for representing the inherent complexity of singing that this project investigates. In particular, because it does not readily allow sufficiently fine gradations of pitch and timing, it is not able to capture minute differences between the perceptions of different transcribers. More nuanced, "close-to-data" tools are needed, which can document how transcribers construct "notes" while listening to the audio stream of a vocal performance <ref type="bibr" target="#b60">(Ozaki et al. 2021)</ref>. Multiple fields stand to benefit from better understanding our perception and transcription of song. Human transcriptions are used for training machine learning models of automated music transcription (that is, note-based segmentation, including but not limited to staff notation). It has been shown that automated methods perform poorly when transcribing global songs <ref type="bibr" target="#b60">(Ozaki et al. 2021)</ref>. Our project demonstrates that there is inherent subjectivity in the perception of singing, expressed in transcription differences, and therefore there is ambiguity in the ground truth used for training automated music analysis models. Improving these automatic models requires improved understanding and formalisation of human disagreements. Moreover, high-quality datasets of singing are a rarity, which is a barrier in computational music analysis and has held back research on singing. Even rarer are cross-cultural song datasets with high quality annotations of real-life performances.</p><p>Alongside providing a high-quality dataset, our exploratory project opens up avenues for more targeted experiments in singing perception. In musical analysis, it provides the basis for further studies of musical modes, rhythm and entrainment. We demonstrate the advantages and the limitations of the existing note segmentation and pitch correction tools for a variety of contexts and repertoires. Crucially, we provide the data, musical contexts and the practical experience to frame the advantages and limitations of a basic musical concept: the note.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Previous work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VocalNotes in the context of ethnomusicology and notation</head><p>The VocalNotes project sits at a particular juncture in the history of ethnomusicology and its methodologies of transcription and analysis. From the beginning, ethnomusicology and its parent discipline comparative musicology placed great emphasis on transcribing unwritten (or differently written) musics into a visual notation that would allow systematic analysis and comparison <ref type="bibr" target="#b0">(Abraham and Hornbostel 1909;</ref><ref type="bibr">translated in 1994)</ref>. The notation used was almost always based on Western staff notation, which necessarily involved breaking the sound down into discrete notes. But questions have long been raised about the validity of using a "prescriptive" notation from one tradition as a "descriptive" notation for others <ref type="bibr" target="#b77">(Seeger 1958;</ref><ref type="bibr">Hood 1971: 62-92)</ref>, and even about the assumption that music necessarily consists of "notes" at all. Veteran ethnomusicologist Bruno Nettl wrote: "The concept of the articulated note works well for certain musics, especially instrumental…. In other kinds of music, perhaps singing most of all, notes are useful prescriptive devices, but they are not particularly descriptive. Lines may be preferable, providing opportunities to show glides and other ornaments" <ref type="bibr">(Nettl 2015: 82)</ref>.</p><p>Dissatisfaction with transcription into staff notation led to a search for alternatives that would (supposedly) bypass both the limitations of a note-based system and the cultural bias of a human transcriber by using technology to generate graphs of sound automatically <ref type="bibr" target="#b55">(Metfessel 1928)</ref>. The "scientific" aspirations of these efforts are evident in Charles Seeger's wording: "As a descriptive science, musicology is going to have to develop descriptive music-writing that can be written and read with maximum objectivity. I believe that the graphing devices and techniques… show the way towards such an end" <ref type="bibr">(Seeger 1958: 194)</ref>. By minimising the role of human interpretation in the transcription process, this "objective," "scientific" musicology would be differentiated from the "interpretative" methods of existing historical musicology and music criticism. Something of the same desire for scientific replicability is implied in Wim van der Meer's introduction to his manual on graphing melodies with the current software Praat: "When a musicologist makes an analysis -for instance a transcription (yes, a transcription is an analysis), no one really knows how the transcription is made…. When it is done by a computer program we can at least know how it is done, especially since the source code is public" <ref type="bibr" target="#b51">(Meer 2023)</ref>.</p><p>Meanwhile, the drawbacks of automatic sound graphs, as of transcriptions in staff notation, have long been recognized: they can be difficult to read, they often fail to distinguish different "layers" in a musical texture, and even if they capture an "objective" record of the sound waves, this may not tell us much about the sound as experienced by human beings <ref type="bibr" target="#b36">(Jairazbhoy 1977)</ref>. Thus, recent extensions of the "melographic" idea, including Meer's own work, have retained an element of interpretation in the manual annotation of computer-generated graphs (Music in Motion 2023) or, in the case of Andrew Killick's Global Notation <ref type="bibr" target="#b37">(Killick 2020)</ref>, allowing for manual as well as automatic production of line-based notation.</p><p>The transcriptions produced for the VocalNotes project, in fact, work similarly to most applications of Global Notation in presenting human-determined note pitches, onsets, and continuations within a visual space that (unlike that of conventional staff notation) is pitch-proportional and time-proportional. Indeed, VocalNotes is just the kind of study that Killick anticipated when he suggested that Global notation could "represent differences in the way the same passage is perceived by different listeners," discovered through "ethnographic and/or experimental research" (2020: 249). However, as there is not yet a purpose-made software program for writing and playing back Global Notation, we have found it more efficient to adopt and adapt the existing tools Tony and Sonic Visualiser.</p><p>VocalNotes uses sound-graphing technology not to eliminate human interpretation, but to study it. In this case, the focus is on how different listeners, all of them familiar with the musical styles they are hearing, interpret a melody differently in terms of how it breaks down into notes. Like staff notation, this does assume that melodies consist of "notes"; but the project uses sound analysis software to allow a more precise description of the boundaries, pitch, and inflections of the perceived notes than staff notation can provide.</p><p>Thus, in the context of ethnomusicology, VocalNotes represents a new intersection between manual and automatic transcription practices. Whether listeners necessarily experience music as an arrangement of "notes" is perhaps more a question for music cognition research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VocalNotes in the context of music perception and cognition</head><p>Any analysis -especially close analysis from a subjective experience like music listeningwill inevitably reflect the cognitive state of the analyst <ref type="bibr" target="#b90">(Zbikowski 2002)</ref>. The intersection between ethnomusicology, theoretical analysis, and cognition has long been noted: in 1988, John Baily wrote "What is the cognitive role of music theory? [...] Such questions take us beyond anthropology and into the domain of psychology" <ref type="bibr">(Baily 1988: 114)</ref>. The VocalNotes project presents a rare opportunity to explore how individual differences in the processes underlying music cognition may affect musical transcription and analysis.</p><p>The most basic and essential process of music cognition is auditory perception. In the VocalNotes project, pitch discrimination and rhythmic segmentation are the most crucial perceptual skills; timbre perception, while necessary, is not as relevant for the current investigation. Pitch discrimination abilities have been widely studied, ranging from people with amusia (or tone deafness) to those with perfect pitch <ref type="bibr" target="#b69">(Reis et al. 2021)</ref>. A common test is finding an individual's "just-noticeable difference" (JND) pitch threshold, which is the smallest difference in pitch (or smallest interval) that they can detect. JNDs depend on numerous factors, including the task demands themselves, but for melodic intervals starting on a pure tone of 528 Hz (roughly C5), musicians had an average JND of 3 Hz (0.59% or ~10 cents), whereas non-musicians had an average JND of 12 Hz (2.23% or ~38 cents) <ref type="bibr" target="#b3">(Arndt, Schlemmer, and Van Der Meer 2020</ref>). In the timing domain, JNDs as low as 10 ms have been reported when participants indicate the absolute duration of a tone <ref type="bibr" target="#b43">(Levitin, Grahn, and London 2018)</ref>.</p><p>If we have such precise perception of pitch and timing, how can disagreements ever arise between musical transcriptions? Individual differences ranging from a participant's age to their primary instrument can affect pitch perception. Musical training -both generally and in terms of genre and instrument -has been shown to affect music perception in profound ways <ref type="bibr" target="#b7">(Besson et al. 2007;</ref><ref type="bibr" target="#b84">Tervaniemi et al. 2009;</ref><ref type="bibr" target="#b40">Kühnis et al. 2013</ref>). More broadly, several studies have investigated the effect of enculturation, especially of language learning, on the perception of both pitch and timing (Grannan-Rubenstein, Grannan-Rubenstein, and Thibodeau 2014; <ref type="bibr" target="#b28">Hannon and Trainor 2007)</ref>.</p><p>Moreover, low-level perception is not the only task involved in musical transcription. VocalNotes was particularly interested in the process of segmentation, which is the process that translates a continuous audio stream into distinct units. This task relies on a higher-level process known as categorical perception, where the listener chunks the incoming sensory information into sensory "objects" -these can be discrete notes with a pitch, onset, and offset, but need not be -and then looks for patterns, especially patterns that are already familiar to them <ref type="bibr" target="#b16">(Deutsch 2012;</ref><ref type="bibr" target="#b18">Dowling and Harwood 1987)</ref>. Exposure to a series of sensory objects leads to the emergence of rhythmic and tonal structure models, which form the listener's expectations of how the music will continue <ref type="bibr" target="#b42">(Lerdahl and Jackendoff 1983;</ref><ref type="bibr" target="#b33">Huron 2006;</ref><ref type="bibr" target="#b47">Margulis 2014)</ref>. The emergence of pitch and rhythm models is also underpinned by a number of factors, including the listener's enculturation and musical expertise; experts are more efficient at segmentation and categorization, and our cultural exposure drives what musical patterns we are familiar with and thus what categories we are likely to form <ref type="bibr" target="#b27">(Hannon 2009)</ref>. More flexible cognitive states, such as the listener's attention and familiarity with the song, can further filter which aspects of the audio stream are focused on and which are ignored <ref type="bibr" target="#b42">(Lerdahl and Jackendoff 1983;</ref><ref type="bibr" target="#b33">Huron 2006;</ref><ref type="bibr" target="#b20">Elhilali et al. 2009</ref>). So, even if two people could perceive the pitch and timing of a song the same way, they may not have the same cognitive interpretation, or internal experience, of the song.</p><p>Transcribers must then also face the task of representation, which externalises this internal experience. There are numerous cognitive tasks underpinning the process of visually representing an auditory melody in Tony and Sonic Visualiser (the digital tools used in the VocalNotes project). The field of embodied cognition has made it clear that we think through our tools, and they shape our cognition as well <ref type="bibr" target="#b39">(Kirsh 2013)</ref>. For example, although categorical perception occurs in both the auditory and the visual domain, the mechanisms differ <ref type="bibr" target="#b23">(Goldstone and Hendrickson 2010)</ref>. Wearing glasses inherently changes how one perceives visual stimulation; likewise, listening to audio while seeing its representation in Tony and Sonic Visualiser, or directly manipulating that representation, inherently changes how it is perceived.</p><p>In order to draw meaningful conclusions, the VocalNotes methodology attempted to control for many of the above variables, such as enculturation, musical training and expertise, the effect of the tools on the transcription, and the differences between the audio and visual domains (see Methodology paper in this issue). Digital tools allowed not only for a fine-grained documentation of these outcomes, but also for generating data, making these outcomes available for further analysis with computational approaches.</p><p>The VocalNotes project is an exploratory study which investigated disagreements in transcriptions as reflections of differences in the cognition (or at least representation) of notes between transcribers. In the course of this exploration, the authors were simultaneously the subjects and the explorers, documenting the outcomes of their own cognitive processes through the medium of transcription, and analysing and comparing these transcriptions to reflect back on their cognitive processes.</p><p>VocalNotes in the context of computational music analysis and MIR Computational music analysis (computational musicology) is an interdisciplinary research area at the intersection of musicology and computer science <ref type="bibr" target="#b53">(Meredith 2015;</ref><ref type="bibr" target="#b56">Mor, Garhwal, and Kumar 2020)</ref>. Music Information Retrieval or Music Information Research (MIR) investigates music as data with the goal of improving our understanding of music and providing automated tools for music-related tasks, including music transcription, musical instrument recognition and separation, and music classification <ref type="bibr" target="#b19">(Downie 2003;</ref><ref type="bibr" target="#b11">Casey et al. 2008;</ref><ref type="bibr" target="#b78">Serra et al. 2013;</ref><ref type="bibr" target="#b57">Müller 2015)</ref>. The VocalNotes project uses MIR tools Tony <ref type="bibr" target="#b48">(Mauch et al. 2015)</ref> and Sonic Visualiser <ref type="bibr" target="#b10">(Cannam, Landone, and Sandler 2010)</ref> for pitch extraction, note segmentation and note pitch correction. A new high-quality cross-cultural dataset was created as part of the VocalNotes project <ref type="bibr" target="#b65">(Proutskova et al. 2023)</ref>, contributing to music-as-data investigations, whereas the studies conducted by the teams fall into the area of computational music analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pitch extraction</head><p>Pitch is defined as a subjective quality of perceived sounds that closely corresponds to the fundamental frequency (f0) of a pure or complex tone <ref type="bibr" target="#b29">(Hartmann 1997)</ref>. There are deviations from the exact f0 and the pitch percept, but pitch and f0 are often used interchangeably outside psychoacoustical studies <ref type="bibr" target="#b38">(Kim et al. 2018)</ref>. Pitch extraction refers to the process of determining the fundamental frequency of a harmonic sound; in our case the sounds are a cappella singing sampled every 10 msec <ref type="bibr" target="#b4">(Babacan et al. 2013)</ref>. Pitch, or more precisely, the fundamental frequency, can be visually represented as a curve in the time/frequency space (Fig. <ref type="figure">1</ref>), which we call a pitch curve or f0 curve. While there are numerous pitch extraction algorithms, the VocalNotes project is limited to using the pYIN algorithm as it is directly implemented in Tony and Sonic Visualiser. pYIN is a variant of YIN (De Cheveigné and Kawahara 2002), an autocorrelation-based f0 extraction method of the previous generation, with the addition of a Hidden Markov Model to decode the most probable sequence of pitch values. pYIN has been shown to have comparable performance to other state-of-the-art algorithms on estimating f0 from unaccompanied monophonic singing <ref type="bibr" target="#b17">(Devaney 2020;</ref><ref type="bibr" target="#b70">Rosenzweig, Scherbaum, and Muller 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Onset detection and note segmentation</head><p>To identify a note, an algorithm needs to determine the beginning (onset) and end (offset) of the note as well as the note pitch. Automatic onset detection turns out to be a particularly difficult task for singing voice: according to the MIREX 2018 audio onset detection competition, the best F1-score (a statistical measure of accuracy) of singing onset detection is only 61.94%, which is at least 10% lower than the onset F1-scores of other musical instruments (X. <ref type="bibr" target="#b89">Wang et al. 2022)</ref>. Previous singing transcription systems were based on hidden Markov models, relying on musical features (such as pitch, amplitude or metre) within the voiced regions <ref type="bibr" target="#b48">(Mauch et al. 2015;</ref><ref type="bibr" target="#b72">Ryynänen and Klapuri 2004;</ref><ref type="bibr" target="#b87">Viitaniemi, Klapuri, and Eronen 2003)</ref>. Note pitch is usually determined automatically as the median of the pitch values within the segment. However, these methods performed poorly on notes with soft onsets and offsets, pitch oscillations within notes and glides between temporally adjacent pitches <ref type="bibr" target="#b44">(Li et al. 2021;</ref><ref type="bibr">X. Wang et al. 2022)</ref>. Despite recent improvements using deep learning approaches (Fu and Su 2019; X. <ref type="bibr" target="#b89">Wang et al. 2022)</ref>, algorithms still perform worse than human experts <ref type="bibr" target="#b60">(Ozaki et al. 2021</ref>).</p><p>The main focus in the development of new transcription algorithms is comparing performance with previous models on a few baseline datasets <ref type="bibr" target="#b6">(Benetos et al. 2019</ref>). These datasets are skewed towards Western music, involving fixed-pitch instruments, and are lacking culturally-diverse singing data. This might be the reason why such systems have not been adopted by musicologists. <ref type="bibr" target="#b31">Holzapfel et al. (2022)</ref> asked 18 musicologists from 5 European universities to transcribe 8 excerpts of sousta, a traditional Greek instrumental dance genre, either from scratch or starting from an automatic transcription, finding no quantitative advantage of using automatic music transcription <ref type="bibr" target="#b31">(Holzapfel et al. 2022)</ref>. Although computer-assisted transcription studies exist <ref type="bibr" target="#b24">(Gómez and Bonada 2013)</ref>, recent reviews by musicologists argued that computational tools for musical analysis are useful for only low-level analysis and not widely adopted within mainstream musicology <ref type="bibr" target="#b14">(Cottrell 2018;</ref><ref type="bibr" target="#b85">Tilley 2018)</ref>. Similarly, the VocalNotes methodology, while relying on automatically extracted fundamental frequency curves as the first pass, does not make use of automatic note segmentation suggestions: segmentation is performed manually from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground truth and datasets</head><p>A major challenge in MIR is the lack of annotated data, especially for singing. Recently, large datasets based on crowd-sourced non-expert annotations, improved by deep neural networks, have been introduced (DALI (Meseguer-Brocal, Cohen-Hadria, and Peeters 2018), MIR-ST500 (J.-Y. Wang and Jang 2021)), butthe resulting quality of the annotations is low or difficult to assess. While these datasets comprise mainly Western popular music, new high-quality corpora of non-Western vocal traditions have emerged more recently, including for Georgian <ref type="bibr" target="#b71">(Rosenzweig et al. 2020)</ref>, Korean <ref type="bibr" target="#b13">(Choi et al. 2020)</ref>, and Chinese songs <ref type="bibr" target="#b25">(Gong, Repetto, and Serra 2017)</ref>. Cross-cultural datasets have also been published outside the MIR field, e.g. by the Many Voices project <ref type="bibr" target="#b61">(Ozaki et al. 2024)</ref>. We expand on this by creating and publicly releasing a dataset of vocal performances from five different traditions with expert pitch and note annotations <ref type="bibr" target="#b65">(Proutskova et al. 2023)</ref>.</p><p>In MIR research, algorithms are typically developed with the aim of reproducing a correct, "ground truth" annotation. One of the pitfalls is the assumption that there is one "correct" way to transcribe music, which, as described above, has been refuted by ethnomusicologists; this assumption has also been questioned by MIR researchers <ref type="bibr" target="#b82">(Sturm and Flexer 2023)</ref>. Singing is particularly difficult to transcribe due to inherently unstable pitch curves (Fig. <ref type="figure">1</ref>) and vocal drift <ref type="bibr" target="#b50">(Mauch, Frieler, and Dixon 2014)</ref>. Following <ref type="bibr" target="#b8">Bittner et al. (2021)</ref> and <ref type="bibr" target="#b60">Ozaki (2021)</ref>, this project demonstrates that human annotations of singing show considerable disagreement, thus questioning the singularity of vocal annotations as ground truth. We provide a larger high-quality dataset with the advantage of multiple independent expert annotations for each excerpt, exploring the concept of a variable ground truth, which we hope will aid the development of more flexible automated transcription algorithms that can deal with ambiguities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The VocalNotes project</head><p>The VocalNotes project investigated how expert traditional music listeners conceive of notes in vocal performances by studying similarities and differences in their transcriptions. Teams of experts from five musical traditions (Japanese folk song, Chinese Hebei bangzi opera, Russian traditional village singing, Alpine yodelling, and Romaniote Jewish chanting) each annotated approximately 10 minutes of ethnomusicological recordings in their culture using Tony <ref type="bibr" target="#b48">(Mauch et al. 2015)</ref> and Sonic Visualiser (Cannam, Landone, and Sandler 2010) for segmentation and note pitch correction, starting from an automatically extracted fundamental frequency curve. The experts then compared their independent transcriptions and looked for factors which led to disagreement. A pilot project run by two co-authors, Polina Proutskova and Olga Velichkina, preceded the VocalNotes project. They attempted a corpus study on modes in Russian traditional singing. After standard MIR techniques for automatic mode extraction failed, they turned to manual transcription, using Tony and Sonic Visualiser to create ground truth for supervised machine learning models. They discovered numerous differences in their annotations, even in songs they knew well and sang together. It became obvious that these differences were not errors, but reflections of divergences in their perception of the song.</p><p>Was this difference of interpretation specific to Russian village singing? Would transcribers in other cultures experience similar disagreements? Are some vocal styles more difficult to transcribe (leading to more disagreements) than others? To answer these questions, they invited teams of ethnomusicologists to participate in a comparative study applying similar methodology to different traditions. The project started in December 2021 with eight teams, five teams reaching the finish (Fig. <ref type="figure" target="#fig_0">2</ref>). This project posed several organisational challenges: The participants are situated across 14 time zones, from Japan to North America. Some of our participants do not speak English. This project did not receive specific funding and participants contributed either in their free time or as part of their ongoing research. Teams were affected, and some had to drop out, due to the wars in Ukraine and in Israel/Gaza. Although these difficulties certainly delayed the overall completion, the success of the project is manifested in the publications of this special issue and the release of the VocalNotes dataset <ref type="bibr" target="#b65">(Proutskova et al. 2023</ref>).</p><p>VocalNotes is an example of a project that differs from "helicopter research" where "researchers from high-income settings, or who are otherwise privileged, conduct studies in lower-income settings or with groups who are historically marginalised, with little or no involvement from those communities or local researchers in the conceptualization, design, conduct or publication of the research" (Nature Editors 2022; cf. <ref type="bibr" target="#b35">Jacoby et al. 2020;</ref><ref type="bibr" target="#b73">Sauvé et al. 2023</ref>; P. E. Savage 2022). In VocalNotes, all teams were equal in their involvement in the methodology, the transcription process, the synthesis, the dataset and the publications (Polina <ref type="bibr">Proutskova et al. 2024 (in preparation)</ref>, see also the Author Contributions statement at the end of this paper). Each team's work contributed to their own research questions of interest (see the Outcomes section of this paper for a summary), while the project provided technological and organisational support and a platform for the synthesis of the outcomes. curve correction: the selected area of a yodel song was wrongly interpreted by pYIN to be in the same octave as preceding pitches. This octave error can be easily corrected in Tony by choosing one out of many candidate pitches suggested by pYIN. c) Segmentation: a "note" (a blue rectangle spans from the onset to the offset) can be easily added and further edited in Tony. Note pitch is automatically determined as the median of the pitch within the segment; there is no editing mechanism for note pitch in Tony. d) Note pitch correction: in Sonic Visualiser, note pitch can be edited by moving the blue note bar up or down (highlighted by the circle). Fig. <ref type="figure">4</ref> Comparing transcriptions. The area above and below notes is coloured by note pitch for easy comparison; note pitch is also shown in cents (relative to an automatically extracted tonal centre). This representation allows us to easily estimate intervals between notes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology</head><p>It must be noted that what is referred to here as transcription is not the same as musical transcription in traditional music analysis but is in fact a semi-automated annotation process which relies on automatically extracted pitch curve and automatically rendered note pitch suggestions. The annotators (who we also call transcribers for simplicity here) perform the segmentation and note pitch correction on the basis of these.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Outcomes</head><p>We find, perhaps unsurprisingly, that transcriptions by experts were mostly in agreement. Therefore, we can claim that in the majority of contexts the note concept was a reliable tool for analysing our repertoires. However disagreements were quite common. They arose in every transcription, and in some cases disagreements were as numerous as agreements (this of course depends on how strictly one evaluates "agreement"). To give one example, in Fig. <ref type="figure" target="#fig_2">5a</ref> transcribers disagreed about the location of the boundary between the first two notes, and disagreed on whether the ensuing sound was one or two notes. Particularly where transcribers disagreed about the number of notes they heard, the usefulness of the note concept was at its limits.</p><p>We found that some repertoires and individual performances were "notier" than others: they were easier to transcribe and led to fewer disagreements between transcribers. Specifically, the notes in Alpine yodel (Fig. <ref type="figure" target="#fig_2">5b</ref>) were much easier to agree about than in other analysed traditions <ref type="bibr">(Fig 5c)</ref>. Within the traditions, singers varied considerably in the "notiness" of their individual singing style; this could be clearly seen in Russian and Japanese collections which included different performances of the same songs (Fig <ref type="figure" target="#fig_2">5d</ref>). is one or two notes. b) In this example of Alpine yodel, there is comparatively strong agreement on where notes begin and end. Overall, the Alpine repertoire is "notey", with a high level of agreement between transcribers, compared to other analysed repertoires. c) Japanese folk songs are less "notey" than Alpine yodel, displaying more disagreements between transcriptions. d) Within a tradition, individual singing styles can differ in how "notey" they are. Here, transcriptions of two Russian singers singing the song "Po zoriushke" are compared: Lamanova's singing style is "notier" than Motorykina's, the latter presenting more disagreements between transcribers than the former.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary of Findings from Individual Groups</head><p>Each team will present an individual paper describing the process and the outcomes of the project for their specific culture, in the context of the team's specific research question related to their tradition.</p><p>The Japanese team <ref type="bibr" target="#b12">(Chiba, Ozaki, and Savage 2024)</ref> analysed 9 recordings (3 different singers x 3 different folk songs), concluding that the transcriber with extensive performance experience in the tradition transcribed much more detail than the transcriber with only listening experience.</p><p>The Russian team put together a corpus of ethnographic solo and group singing recordings (with one channel per singer) from a variety of local traditions and genres. Their findings demonstrate the subjectivity of mode perception in Russian traditional singing, challenging the established practice of how the mode of a song is determined.</p><p>The Jewish team transcribed 10 excerpts from the few existing recordings of Romaniote Torah cantillation. A strategic selection of the recitation of the same biblical texts chanted by different Romaniote practitioners (Genesis 1:1-11; Deuteronomy 6:5-9) allowed the team to analyse the transcription within a comparative framework; they identified similarities in cantorial practices in different diasporic locations and conditions while also identifying and analysing what they have understood as idiolects: each practitioner's personal aesthetic particularities within the flexible affordances of their traditional practice.</p><p>The Alpine team transcribed nine excerpts from the central European yodelling tradition. They analysed and interpreted the relationship between vocal expression, or "utterance" on the one hand, and its notated representation on the other. Drawing on Milton Babbitt's model of the threefold representation of sound (acoustic, auditory, and graphemic), the differences in two independent, semi-automatized transcriptions are discussed. The onset and end of notes lead to little disagreement and enable establishing a broader hypothesis of the use of consonants as start and transition points in singing without lexical words. The symbolic representation for music not built along hierarchical models of meter like yodel remains controversial and a semi-automatized visualization can offer a more balanced solution.</p><p>The Chinese research team transcribed 11 excerpts from Hebei bangzi (clapper opera), a genre of Chinese traditional opera, covering a range of tempos including slow, moderate, fast, and rubato patterns. Both transcribers agreed that their transcriptions shared notable similarities overall, though they found that a greater familiarity with the vocal style and a higher level of reliance on auditory perception led one transcriber to create more detailed transcriptions, capturing subtle variations in pitch, timbre, and loudness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary of Disagreements</head><p>Here we outline a few contexts where disagreements arose. Although this list is not completely exhaustive, we strive to present a cohesive overview of disagreements which were encountered by more than one team while analysing their repertoires. We group disagreements in the following section as due to "cultural knowledge," "syllabic interpretation," "intra-syllabic pitch change," or a broad category of utterance that we term "voice splash." Note that there can be overlap between groups, as some examples may fall into more than one category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cultural Knowledge</head><p>Disagreements due to cultural knowledge arise when the transcribers' awareness of local dialects, musical practices, social context etc. differs, thus leading to differences in their interpretation of the singing. Such disagreements were relatively rare, but offer an important view into the reasons why transcriptions might diverge even when transcribers' perception is in full agreement. We here present two cases (Fig. <ref type="figure">6</ref>). In an example from Japan (Fig. <ref type="figure">6a</ref>), the singer is given call cues by another person.The transcriber who can perform this song excluded the calls from the transcription, because he would not be making them as a singer, whereas the other transcriber with only listening experience included the calls. In a case from Jewish Romaniote chant (Fig. <ref type="figure">6b</ref>), transcribers disagreed over an utterance because in the local dialect of the singer, of which one of the transcribers was aware, the number of syllables differs from the dialect spoken by the other transcriber. Fig. <ref type="figure">6</ref> Difference in cultural knowledge: a) Red is used to highlight areas where one transcriber has an extra note compared to the other. In this Japanese folk song, Esashi-Oiwake, GC annotated many more notes, except for one point. This call, "Soi", is uttered by a different person than the singer. While both transcribers perceived the call, only YO transcribed it, not being aware of the original ensemble context; GC, as a singer, would not make the call, so did not consider it to be a note (video). b) Romaniote Jewish dialect in Greece is different from that in NYC. In this example of cantor Levis, Romaniote Hebrew's [h] &gt; Ø leads to vowel elision, such as in /vəɑhɑvtɑ/ pronounced as <ref type="bibr">[vəɑ:vtɑ]</ref>. MC segments phonetically with elision as <ref type="bibr">[vəɑ:vtɑ]</ref> and GZ segments phonemically (or without elision) as <ref type="bibr">[vəɑ.ɑvtɑ]</ref>. (video)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Syllabic Interpretation</head><p>Overall we found that a syllabic change almost always triggers a note boundary (see Fig. <ref type="figure">7c</ref> for a single exception). Yet in some contexts, what constitutes a syllabic change can be interpreted differently by different transcribers. Some examples include insertion of vowels (anaptyxis) in the Russian (Fig. <ref type="figure">7a</ref>) and Jewish (Fig. <ref type="figure">7b</ref>) performances, or differences in syllabic division depending on the dialect (Fig. <ref type="figure">6b</ref>). The only counterexample of a multi-syllable note we found was the ethnachta clause in the Jewish Romaniote tradition. The ethnachta clause is the opening of a Torah passage recitation, which is spoken rapidly before a more protracted, sung passage. It is perceived by the singer and the listeners as a marker which only makes sense as a whole, rather than a word that can be subdivided into syllables (Fig. <ref type="figure">7c</ref>). Fig. <ref type="figure">7</ref> Differences in syllabic interpretation: a) In the Russian language, syllables may include more than one consonant. In traditional village singing, insertion of vowels (anaptyxis) is common, when a tiny vowel is added to a consonant that was part of a syllable in speech, making this consonant into a syllable of its own. Here, inserted vowels are given in brackets. The sound /s(y)/ (/с(ы)/ in Cyrillic transliteration) is interpreted as a voiceless consonant (and therefore not a note) by OV, whereas PP assigns pitch to it, making it into a note. You can hear in the video how the singer creates many short syllables in her singing by inserting a vowel after each consonant. b) In Romaniote chanting some cantors' expressively insert vowels at phrase-final words that end in a consonant (Ø &gt; [ə] / C_#). See the rightmost segment in CZ's transcription (red circle), which is absent in MC. (video) c) Multi-syllabic units: The ethnachta clause in the Jewish tradition was sometimes considered as one unit by the transcribers owing to its declamatory character as well as its role as a divider between two sung parts of a verse. Therefore, MC subsumed several syllables into one note segment. The example is cantor Borbolis (Gen.) chanting the words "pe-nei te-hom" (video).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intra-syllabic Pitch Change</head><p>The most frequent disagreement across all groups involves intra-syllabic pitch change, when pitch changes within a syllable (Fig. <ref type="figure" target="#fig_3">8</ref>,<ref type="figure" target="#fig_4">9</ref>). Examples of this include glissando and melisma (Fig. <ref type="figure" target="#fig_3">8a-c</ref>), vibrato (Fig. <ref type="figure" target="#fig_3">8d</ref>), intra-syllabic embellishments and runs (Fig. <ref type="figure" target="#fig_3">8e-f</ref>), and glides (scoops) at the start and the end of the syllable (Fig. <ref type="figure" target="#fig_4">9</ref>). transcriptions differ in terms of the note boundaries and number of notes (video). c) A Russian example of a glissando (video); PP, who segmented less, commented that her transcription was based on vocal and expressive gestures. Intra-syllabic pitch change: vibrato. d) A Chinese example of vibrato; ZY annotated it as one note (with vibrato), while WY segmented in three notes (video).Intra-syllabic pitch change: embellishments. e) In this Russian traditional song, PP considered the pitch change to be embellishments, while OV did not (video). f) In this Chinese example WY annotated an ornament highlighted in red with the pitch value 270 cents (video). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>"Voice splash"</head><p>We use the term "voice splash" to denote fast, sharp and very short changes in pitch, which were clearly audible and also visible in the f0 curve. This could be caused by a number of vocal techniques which may or may not have been intentional, and may or may not change vocal register. They can be linguistic and non-linguistic sounds, may constitute a syllable or be part of a syllable. While a canonical example is the Indian gamaka, we encountered this phenomenon in all repertoires analysed in this study (Fig. <ref type="figure" target="#fig_5">10</ref>).</p><p>There are borderline cases where it is difficult to decide whether an intra-syllabic embellishment should be classified as voice splash, and Jewish Romaiote repertoire presented many such examples (Fig. <ref type="figure" target="#fig_5">10e</ref>). In such cases, we relied on our musical judgement; more specifically, if the sounds seemed very expressive and were shorter than other embellishments, we classified them as voice splash. considerably compared to the automatic suggestion. Due to its expressivity we decided to classify this embellishment as a voice splash.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>From these conversations, it became apparent that the differences we were finding between transcribers were not primarily perceptual, but representational. That is, discussion between the experts suggested that they had heard more or less the same pitch content, but nonetheless, they represented it differently when asked to segment that pitch content into discrete notes. Of course, the initial pitch transcriptions were machine-assisted, starting from an automatically-extracted fundamental frequency, rather than done purely by ear. As such, any potential differences in early pitch perception were controlled for -or perhaps merely hidden -by this automated step (see Methodology paper). On the other hand, a previous study that did quantify agreement among purely manual transcriptions using staff notation also found more disagreement in note segmentation than in pitch assignment <ref type="bibr" target="#b60">(Ozaki et al. 2021)</ref>, suggesting that this result is not simply an artefact of our methodology. As mentioned, however, it has been demonstrated that expert musicians tend to have finely-honed pitch perception, so purely perceptual differences may have been negligible regardless <ref type="bibr" target="#b3">(Arndt, Schlemmer, and Van Der Meer 2020)</ref>.</p><p>It is of greater interest that, despite hearing the same pitch content, equally enculturated and expert musicians still segmented it differently. For example, where one listener would represent a short pitch deviation as a separate, ornamental note, another would represent it as vibrato embellishing a long held note (see Fig <ref type="figure" target="#fig_3">8</ref>). These differences in audio representation and categorization comprise some of the most interesting outcomes of this project, and require much further study. It is impossible in the present study, for example, to disambiguate whether these differences reflect the transcribers' internal (cognitive) representational models of the melody, their external (visual) representational models of the melody, or a complex interaction of the two. It is still striking that one can ask two experts of a musical tradition "what are the notes in this piece?" and receive such different answers. Perhaps it provides some support for Nettl's claim that "notes" -as an undifferentiated, universal concept -may not be a wholly appropriate framework for representing vocal music, and we need to have a more nuanced discussion of how non-discrete vocal gestures fit into our models (both internal and external) of music.</p><p>Although VocalNotes focused on differences between transcribers, differences in cognitive processes even within a single transcriber were also noted during discussion. For example, when transcribing the melody, the analyst could select what portion of the audio to listen to, and every analyst agreed that the length of the selection -essentially, the amount of auditory context -made a difference in how they conceptualised what they were hearing. The effect of the context was particularly striking during the note pitch correction phase, although it doubtless affected segmentation as well. A certain pitch may sound more or less correct when heard individually as opposed to in a phrase, or in a stream of multiple phrases, because the varying context changes our categorical perception, our usage of auditory memory, and our understanding of larger patterns <ref type="bibr" target="#b84">(Tervaniemi et al. 2009)</ref>. The effect of context on musical representation within a single brain, while not the focus of this project, also deserves further study.</p><p>There are many other issues of cognitive import that could be explored through this project. For example, emotional attachment to music profoundly affects the musical experience, and many of the analysts in VocalNotes have deep emotional connections to their traditions <ref type="bibr" target="#b41">(Lamont 2012)</ref>. Most analysts were also singers within the tradition themselves, and some reported using singing to determine transcription decisions such as the correct pitch for the note. Therefore, we must understand how music perception interacts with music production. Individual differences in music production abilities may be playing a large, implicit role in this task, and they vary more widely than music perception abilities, and are even more shaped by expertise <ref type="bibr" target="#b79">(Sloboda 2000)</ref>. The evidence from the Japanese team seems to point towards practitioners having more detailed segmentations compared to musically-informed and enculturated non-singers. This might relate to findings that music performers have more enhanced sensory-motor coupling than non-musicians, especially when listening to music of their own instrument <ref type="bibr">(Proverbio and Bellini 2018,15-25)</ref>.</p><p>In short, the VocalNotes project provides an interesting case study for examining a number of key processes in music cognition. The difficulty is in disentangling which process(es) are responsible for any given difference in the produced transcription. Only expert musicians were selected for the project, only cultural experts were included within each team, and everyone was asked to use the same tools to control for some of the variables, but in practice significant differences still arose, and therefore we suggest stricter controls for future studies (Methodology paper).</p><p>To facilitate future replication and research, we have published the VocalNotes dataset<ref type="foot" target="#foot_0">foot_0</ref> containing the audio fragments and the experts' annotations in a simple, easily accessible machine readable format (f0, note onsets/durations, and note pitch as csv files, alongside extensive contextual documentation which may include the title, lyrics, performers, sub-cultural or geographical origin, genre or function, and/or recording metadata) <ref type="bibr" target="#b65">(Proutskova et al. 2023)</ref>. This cross-cultural corpus with high quality annotations can serve as a basis for further quantitative analysis of disagreements, and as ground truth for machine learning algorithms.</p><p>Aside from the final transcriptions, the transcribers discussed their process of transcription at length, providing data for qualitative analysis of different cognitive processes. This exploratory study is thus a rich example of the kind of cross-cultural, cross-disciplinary, mixed-methods approach to understanding music cognition that could produce fascinating insights in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig 2</head><label>2</label><figDesc>Fig 2 Map showing the VocalNotes teams and where the members were located.</figDesc><graphic coords="9,73.50,496.86,451.50,254.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><figDesc>Fig. 3 a) Tony automatically extracts the fundamental frequency curve using the pYIN algorithm. pYIN works quite well for solo singing, though errors are still possible. b) Pitch</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5</head><label>5</label><figDesc>Fig. 5 Examples of agreement and disagreement in transcription. a) Transcribers disagree about the boundary separating the first two notes, and on whether the final descending pitch</figDesc><graphic coords="14,73.50,73.42,451.50,653.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 8</head><label>8</label><figDesc>Fig. 8 Intra-syllabic pitch change: glissando and melisma (a-c), vibrato (d), embellishments (e-f). a) A Japanese example of a melisma (video), GC segmenting in much more detail than YO. b) this Chinese example shows a sung syllable ('啊(a)') with melisma; the two</figDesc><graphic coords="19,73.50,76.96,451.50,645.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 9</head><label>9</label><figDesc>Fig. 9 Intra-syllabic pitch change: glides. Glides at the start and end of the note (sometimes called 'scoops') were found in each culture: a) Chinese, b) Jewish, c) Russian, d) Japanese, and e) Alpine.</figDesc><graphic coords="20,73.50,218.96,451.50,165.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 10 "</head><label>10</label><figDesc>Fig. 10 "Voice splash": a) In Japanese folk song, there are names for various vocal techniques which produce sharp, short changes in pitch (video). b) An expressive burst in a Russian lament, which incorporates sounds of crying into singing (video). c) In Chinese opera, a similar expressive burst is employed. d) A voice splash in Alpine yodel. e) Orange note bars represent the automatic note pitch suggestions and blue bars show the note pitch manually corrected by the transcriber. Jewish Romaniote repertoire presented many borderline voice splash cases like this one from a performance by the cantor Kofinas: while MC represented it as a part of the encompassing note, GZ segmented it and raised the pitch</figDesc><graphic coords="21,73.50,73.50,451.50,555.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="3,73.50,73.50,360.75,182.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="12,73.50,204.42,451.50,240.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="16,73.50,494.99,451.50,171.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="17,73.50,369.20,451.50,396.75" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The annotations can be downloaded from Zenodo under the CC BY-NC-SA licence: https://zenodo.org/records/10065955; the complete VocalNotes Dataset including the audio fragments that were analysed is provided for research only via a request form: https://osf.io/4n5ry/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We would like to acknowledge the contribution of the teams who had to drop out of the project due to the war in Ukraine and for other reasons: <rs type="person">Anastasiia Mazurenko</rs> of the Ukrainian team, <rs type="person">Ieva Tihovska</rs>, <rs type="person">Ilze Cepurniece</rs>, <rs type="person">Zane Šmite</rs> of the Latvian team, <rs type="person">Nana Mzhavanadze</rs>, <rs type="person">Teona Rukhadze</rs> of the <rs type="affiliation">Georgian</rs> team.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Author contributions </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vorschläge Für Die Transkription Exotischer Melodien</title>
		<author>
			<persName><forename type="first">Otto</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><forename type="middle">M</forename><surname>Hornbostel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sammelbände Der Internationalen Musikgesellschaft</title>
		<imprint>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="1909">1909</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Suggested Methods for the Transcription of Exotic Music&apos;. Translated by George and Eve List</title>
		<author>
			<persName><forename type="first">Otto</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich M Von</forename><surname>Hornbostel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ethnomusicology</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="425" to="456" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Notnaya Zapis&apos; Narodnoi Muzyki</title>
		<author>
			<persName><forename type="first">Evgeny</forename><surname>Alekseev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Score Notation of Folk Music</title>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Sovetskiy Kompositor</publisher>
			<pubPlace>Moscow</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Same or Different Pitch? Effects of Musical Expertise, Pitch Difference, and Auditory Task on the Pitch Discrimination Ability of Musicians and Non-Musicians</title>
		<author>
			<persName><forename type="first">Christin</forename><surname>Arndt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathrin</forename><surname>Schlemmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elke</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Der</forename><surname>Meer</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00221-019-05707-8</idno>
		<ptr target="https://doi.org/10.1007/s00221-019-05707-8" />
	</analytic>
	<monogr>
		<title level="j">Experimental Brain Research</title>
		<imprint>
			<biblScope unit="volume">238</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="247" to="258" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Comparative Study of Pitch Extraction Algorithms on a Large Variety of Singing Sounds</title>
		<author>
			<persName><forename type="first">Onur</forename><surname>Babacan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Drugman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathalie</forename><surname>Nicolas D'alessandro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Henrich</surname></persName>
		</author>
		<author>
			<persName><surname>Dutoit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="7815" to="7819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Anthropological and Psychological Approaches to the Study of Music Theory and Musical Cognition</title>
		<author>
			<persName><forename type="first">John</forename><surname>Baily</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Yearbook for Traditional Music</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="114" to="124" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic Music Transcription: An Overview</title>
		<author>
			<persName><forename type="first">Emmanouil</forename><surname>Benetos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyao</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ewert</surname></persName>
		</author>
		<idno type="DOI">10.1109/MSP.2018.2869928</idno>
		<ptr target="https://doi.org/10.1109/MSP.2018.2869928" />
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="30" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Influence of Musical Expertise and Musical Training on Pitch Processing in Music and Language</title>
		<author>
			<persName><forename type="first">Mireille</forename><surname>Besson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniele</forename><surname>Schön</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andréia</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyrille</forename><surname>Magne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Restorative Neurology and Neuroscience</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="399" to="410" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Vocadito: A Dataset of Solo Vocals with &quot;F0&quot;, Note, and Lyric Annotations</title>
		<author>
			<persName><forename type="first">Rachel</forename><forename type="middle">M</forename><surname>Bittner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Pasalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">José</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Meseguer-Brocal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Rubinstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.05580</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Universals in the World&apos;s Musics</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Jordania</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychology of Music</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="229" to="248" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sonic Visualiser: An Open Source Application for Viewing, Analysing, and Annotating Music Audio Files</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Cannam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Landone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM International Conference on Multimedia</title>
		<meeting>the 18th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1467" to="1468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Content-Based Music Information Retrieval: Current Directions and Future Challenges</title>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Casey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remco</forename><surname>Veltkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masataka</forename><surname>Goto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Leman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Rhodes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malcolm</forename><surname>Slaney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="668" to="696" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">What Is a &quot;Note&quot;? Agreement and Disagreement in Transcriptions of Japanese Folk Songs</title>
		<author>
			<persName><forename type="first">Gakuto</forename><surname>Chiba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuto</forename><surname>Ozaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">E</forename><surname>Savage</surname></persName>
		</author>
		<idno type="DOI">10.31234/osf.io/nh9d2</idno>
		<ptr target="https://doi.org/10.31234/osf.io/nh9d2" />
	</analytic>
	<monogr>
		<title level="j">PsyArXiv Preprint</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Children&apos;s Song Dataset for Singing Voice Research</title>
		<author>
			<persName><forename type="first">Soonbeom</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wonil</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saebyul</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangeon</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juhan</forename><surname>Nam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMIR Late-Breaking Demo</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Big Music Data, Musicology, and the Study of Recorded Music: Three Case Studies</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Musical Quarterly</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="216" to="243" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">YIN, a Fundamental Frequency Estimator for Speech and Music</title>
		<author>
			<persName><forename type="first">Alain</forename><surname>De Cheveigné</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hideki</forename><surname>Kawahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1917" to="1930" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The Processing of Pitch Combinations</title>
		<author>
			<persName><forename type="first">Diana</forename><surname>Deutsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Psychology of Music</title>
		<title level="s">Cognition and Perception</title>
		<meeting><address><addrLine>United Kingdom</addrLine></address></meeting>
		<imprint>
			<publisher>Elsevier Science</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="249" to="325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An Empirical Evaluation of Note Segmentation and Automatic Pitch-Extraction Methods for the Singing Voice</title>
		<author>
			<persName><forename type="first">Johanna</forename><surname>Devaney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Routledge Companion to Interdisciplinary Studies in Singing. Volume I, Development</title>
		<editor>
			<persName><forename type="first">Frank</forename><forename type="middle">A</forename><surname>Russo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Beatriz</forename><surname>Senoi Ilari</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Annabel</forename><forename type="middle">J</forename><surname>Cohen</surname></persName>
		</editor>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Routledge</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="136" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Music Cognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Dowling</surname></persName>
		</author>
		<author>
			<persName><surname>Jay</surname></persName>
		</author>
		<author>
			<persName><surname>Harwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychomusicology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">91</biblScope>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Music Information Retrieval</title>
		<author>
			<persName><forename type="first">J</forename><surname>Downie</surname></persName>
		</author>
		<author>
			<persName><surname>Stephen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="295" to="340" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Interaction between Attention and Bottom-Up Saliency Mediates the Representation of Foreground and Background in an Auditory Scene</title>
		<author>
			<persName><forename type="first">Mounya</forename><surname>Elhilali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanjuan</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shihab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">Z</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName><surname>Simon</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pbio.1000129</idno>
		<ptr target="https://doi.org/10.1371/journal.pbio.1000129" />
	</analytic>
	<monogr>
		<title level="j">PLoS Biology</title>
		<editor>
			<persName><forename type="first">Timothy</forename><forename type="middle">D</forename><surname>Griffiths</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Symposium on Transcription and Analysis: A Hukwe Song with Musical Bow</title>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">M</forename><surname>England</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Garfias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mieczyslaw</forename><surname>Kolinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>List</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willard</forename><surname>Rhodes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Seeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ethnomusicology</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="223" to="277" />
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hierarchical Classification Networks for Singing Voice Segmentation and Transcription</title>
		<author>
			<persName><forename type="first">Zih</forename><forename type="middle">-</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Sing</surname></persName>
		</author>
		<author>
			<persName><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Society for Music Information Retrieval Conference (ISMIR 2019)</title>
		<meeting>the 20th International Society for Music Information Retrieval Conference (ISMIR 2019)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="900" to="907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Categorical Perception</title>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Goldstone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">T</forename><surname>Hendrickson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wiley Interdisciplinary Reviews: Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="78" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Towards Computer-Assisted Flamenco Transcription: An Experimental Comparison of Automatic Transcription Algorithms as Applied to a Cappella Singing</title>
		<author>
			<persName><forename type="first">Emilia</forename><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordi</forename><surname>Bonada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Music Journal</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="73" to="90" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Creating an a Cappella Singing Audio Dataset for Automatic Jingju Singing Evaluation Research</title>
		<author>
			<persName><forename type="first">Rong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Caro Repetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Workshop on Digital Libraries for Musicology</title>
		<meeting>the 4th International Workshop on Digital Libraries for Musicology</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="37" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Enculturation Effects of Musical Training on Pitch Discrimination</title>
		<author>
			<persName><forename type="first">Greta</forename><surname>Grannan-Rubenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Grannan-Rubenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Thibodeau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Cognitive Science Society</title>
		<meeting>the Annual Meeting of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="2985" to="2989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Musical Enculturation: How Young Listeners Construct Musical Knowledge through Perceptual Experience</title>
		<author>
			<persName><forename type="first">Erin</forename><forename type="middle">E</forename><surname>Hannon</surname></persName>
		</author>
		<idno type="DOI">10.1093/acprof:oso/9780195331059.003.0007</idno>
		<ptr target="https://doi.org/10.1093/acprof:oso/9780195331059.003.0007" />
	</analytic>
	<monogr>
		<title level="m">Neoconstructivism</title>
		<editor>
			<persName><forename type="first">Scott</forename><surname>Johnson</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="132" to="156" />
		</imprint>
	</monogr>
	<note>1st ed.</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Music Acquisition: Effects of Enculturation and Formal Training on Development</title>
		<author>
			<persName><forename type="first">Erin</forename><forename type="middle">E</forename><surname>Hannon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurel</forename><forename type="middle">J</forename><surname>Trainor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="466" to="472" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Signals, Sound, and Sensation</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Hartmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">AIP Series in Modern Acoustics and Signal Processing</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>AIP Press</publisher>
			<pubPlace>United States</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Transcription and Transnotation in Ethnomusicology</title>
		<author>
			<persName><forename type="first">Avigdor</forename><surname>Herzog</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the International Folk Music Council</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="100" to="101" />
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Humanities and Engineering Perspectives on Music Transcription</title>
		<author>
			<persName><forename type="first">Andre</forename><surname>Holzapfel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanouil</forename><surname>Benetos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Killick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Widdess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Scholarship in the Humanities</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="747" to="764" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Mantle</forename><surname>Hood</surname></persName>
		</author>
		<title level="m">The Ethnomusicologist</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>McGraw-Hill</publisher>
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Sweet Anticipation: Music and the Psychology of Expectation</title>
		<author>
			<persName><forename type="first">David</forename><surname>Huron</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The Vocal Generosity Effect: How Bad Can Your Singing Be?</title>
		<author>
			<persName><forename type="first">Sean</forename><surname>Hutchins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Roquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Peretz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Music Perception</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="147" to="159" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Cross-Cultural Work in Music Cognition: Challenges, Insights, and Recommendations</title>
		<author>
			<persName><forename type="first">Nori</forename><surname>Jacoby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Hellmuth Margulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Clayton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erin</forename><surname>Hannon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henkjan</forename><surname>Honing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Iversen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Robert Klein</surname></persName>
		</author>
		<idno type="DOI">10.1525/mp.2020.37.3.185</idno>
		<ptr target="https://doi.org/10.1525/mp.2020.37.3.185" />
	</analytic>
	<monogr>
		<title level="j">Music Perception</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="185" to="195" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The&quot; Objective&quot; and Subjective View in Music Transcription</title>
		<author>
			<persName><forename type="first">Nazir</forename><forename type="middle">A</forename><surname>Jairazbhoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ethnomusicology</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="263" to="273" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Global Notation as a Tool for Cross-Cultural and Comparative Music Analysis</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Killick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Analytical Approaches to World Music</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="235" to="279" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">CREPE: A Convolutional Representation for Pitch Estimation</title>
		<author>
			<persName><forename type="first">Jong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Wook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">Pablo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="161" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Embodied Cognition and the Magical Future of Interaction Design</title>
		<author>
			<persName><forename type="first">David</forename><surname>Kirsh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer-Human Interaction (TOCHI)</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The Encoding of Vowels and Temporal Speech Cues in the Auditory Cortex of Professional Musicians: An EEG Study</title>
		<author>
			<persName><forename type="first">Jürg</forename><surname>Kühnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Elmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lutz</forename><surname>Jäncke</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuropsychologia.2013.04.007</idno>
		<ptr target="https://doi.org/10.1016/j.neuropsychologia.2013.04.007" />
	</analytic>
	<monogr>
		<title level="j">Neuropsychologia</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1608" to="1618" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Emotion, Engagement and Meaning in Strong Experiences of Music Performance</title>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Lamont</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychology of Music</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="574" to="594" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">An Overview of Hierarchical Structure in Music</title>
		<author>
			<persName><forename type="first">Fred</forename><surname>Lerdahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ray</forename><surname>Jackendoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Music Perception</title>
		<imprint>
			<biblScope unit="page" from="229" to="252" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The Psychology of Music: Rhythm and Movement</title>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">J</forename><surname>Levitin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><forename type="middle">A</forename><surname>Grahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>London</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev-psych-122216-011740</idno>
		<ptr target="https://doi.org/10.1146/annurev-psych-122216-011740" />
	</analytic>
	<monogr>
		<title level="j">Annual Review of Psychology</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="51" to="75" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Phoneme-Informed Note Segmentation of Monophonic Vocal Music</title>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emir</forename><surname>Demirel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Polina</forename><surname>Proutskova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Dixon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on NLP for Music and Spoken Audio (NLP4MusA)</title>
		<meeting>the 2nd Workshop on NLP for Music and Spoken Audio (NLP4MusA)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="17" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The Musical Significance of Transcription (Comments on Hood, &quot;Musical Significance</title>
		<author>
			<persName><forename type="first">George</forename><surname>List</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ethnomusicology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="193" to="197" />
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The Reliability of Transcription</title>
	</analytic>
	<monogr>
		<title level="j">Ethnomusicology</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="353" to="377" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Margulis</surname></persName>
		</author>
		<author>
			<persName><surname>Hellmuth</surname></persName>
		</author>
		<title level="m">On Repeat: How Music Plays the Mind</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Computer-Aided Melody Note Transcription Using the Tony Software: Accuracy and Efficiency</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Mauch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Cannam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Bittner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Fazekas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajie</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Dixon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First International Conference on Technologies for Music Notation and Representation</title>
		<meeting><address><addrLine>TENOR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">pYIN: A Fundamental Frequency Estimator Using Probabilistic Threshold Distributions</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Mauch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Dixon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="659" to="663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Intonation in Unaccompanied Singing: Accuracy, Drift, and a Model of Reference Pitch Memory</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Mauch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Frieler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Dixon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">136</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="401" to="411" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">Vim</forename><surname>Meer</surname></persName>
		</author>
		<author>
			<persName><surname>Van Der</surname></persName>
		</author>
		<ptr target="http://thoughts4ideas.eu/praat-manual-for-musicologists" />
		<title level="m">Praat Manual for Musicologists</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Universality and Diversity in Human Song</title>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">A</forename><surname>Mehr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manvir</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dean</forename><surname>Knox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ketter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Pickens-Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nori</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alena</forename><forename type="middle">A</forename><surname>Jacoby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erin</forename><forename type="middle">J</forename><surname>Egner</surname></persName>
		</author>
		<author>
			<persName><surname>Hopkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">366</biblScope>
			<biblScope unit="issue">6468</biblScope>
			<biblScope unit="page">868</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Computational Music Analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Meredith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Springer International Publishing</publisher>
			<pubPlace>Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">DALI: A Large Dataset Of Synchronized Audio, Lyrics And Notes, Automatically Created Using Teacher-Student Machine Learning Paradigm</title>
		<author>
			<persName><surname>Meseguer-Brocal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alice</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffroy</forename><surname>Cohen-Hadria</surname></persName>
		</author>
		<author>
			<persName><surname>Peeters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">19th International Society for Music Information Retrieval Conference</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">Milton</forename><surname>Metfessel</surname></persName>
		</author>
		<title level="m">Phonophotography in Folk Music: American Negro Songs in New Notation</title>
		<meeting><address><addrLine>Chapel Hill</addrLine></address></meeting>
		<imprint>
			<publisher>University of North Carolina Press</publisher>
			<date type="published" when="1928">1928</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A Systematic Literature Review on Computational Musicology</title>
		<author>
			<persName><forename type="first">Bhavya</forename><surname>Mor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunita</forename><surname>Garhwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Archives of Computational Methods in Engineering</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="923" to="937" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Fundamentals of Music Processing: Audio, Analysis</title>
		<author>
			<persName><forename type="first">Meinard</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="https://autrimncpa.wordpress.com" />
	</analytic>
	<monogr>
		<title level="m">Music in Motion: The Automated Transcription for Indian Music (AUTRIM) Project by NCPA and UvA</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015. 2023</date>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
	<note>Music in Motion</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Nature Addresses Helicopter Research and Ethics Dumping</title>
		<author>
			<persName><forename type="first">Nature</forename><surname>Editors</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">606</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2022">2022. 7912</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Nettl</surname></persName>
		</author>
		<title level="m">The Study of Ethnomusicology: Thirty-Three Discussions</title>
		<imprint>
			<publisher>University of Illinois Press</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Agreement among Human and Automated Transcriptions of Global Songs</title>
		<author>
			<persName><forename type="first">Yuto</forename><surname>Ozaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Mcbride</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanouil</forename><surname>Benetos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joren</forename><surname>Peter Q Pfordresher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">T</forename><surname>Six</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Polina</forename><surname>Tierney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emi</forename><surname>Proutskova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haruka</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haruno</forename><surname>Kondo</surname></persName>
		</author>
		<author>
			<persName><surname>Fukatsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">22nd International Society for Music Information Retrieval Conference (ISMIR 2021)</title>
		<imprint>
			<publisher>International Society for Music Information Retrieval</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="500" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Globally, Songs and Instrumental Melodies Are Slower and Higher and Use More Stable Pitches than Speech: A Registered Report</title>
		<author>
			<persName><forename type="first">Yuto</forename><surname>Ozaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Tierney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">Q</forename><surname>Pfordresher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">M</forename><surname>Mcbride</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanouil</forename><surname>Benetos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Polina</forename><surname>Proutskova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gakuto</forename><surname>Chiba</surname></persName>
		</author>
		<idno type="DOI">10.1126/sciadv.adm9797</idno>
		<ptr target="https://doi.org/10.1126/sciadv.adm9797" />
	</analytic>
	<monogr>
		<title level="j">Science Advances</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page">9797</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Vocal Mistuning Reveals the Origin of Musical Scales</title>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">Q</forename><surname>Pfordresher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="52" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Imprecise Singing Is Widespread</title>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">Q</forename><surname>Pfordresher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kimberly</forename><forename type="middle">M</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Belyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Liotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2182" to="2190" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Investigating the Singing Voice: Quantitative and Qualitative Approaches to Studying Cross-Cultural Vocal Production</title>
		<author>
			<persName><forename type="first">Polina</forename><surname>Proutskova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
		<respStmt>
			<orgName>Goldsmiths University of London</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD Dissertation</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">The VocalNotes Dataset</title>
		<author>
			<persName><forename type="first">Polina</forename><surname>Proutskova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Mcbride</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuto</forename><surname>Ozaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gakuto</forename><surname>Chiba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoxin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Yue</surname></persName>
		</author>
		<ptr target="https://forms.gle/W86j2koBwpkfmnBc9" />
	</analytic>
	<monogr>
		<title level="m">International Society for Music Information Retrieval Conference, Late-Breaking/Demo</title>
		<meeting><address><addrLine>Milano</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">VocalNotes Cross-Cultural Song Transcription Methodology: Framework, Challenges and Lessons</title>
		<author>
			<persName><forename type="first">Polina</forename><surname>Proutskova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Velichkina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Mcbride</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gakuto</forename><surname>Chiba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miranda</forename><surname>Crowdus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Nikolaenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuto</forename><surname>Ozaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Analytical Approaches to World Music</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">How the Degree of Instrumental Practice in Music Increases Perceptual Sensitivity</title>
		<author>
			<persName><forename type="first">Alice</forename><surname>Proverbio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eleonora</forename><surname>Mado</surname></persName>
		</author>
		<author>
			<persName><surname>Bellini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain Research</title>
		<imprint>
			<biblScope unit="volume">1691</biblScope>
			<biblScope unit="page" from="15" to="25" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">N</forename><surname>Razumovskaya</surname></persName>
		</author>
		<title level="m">Традиционная Музыка Русского Поозерья (Traditional Music of Russian Poozer</title>
		<meeting><address><addrLine>St.-Petersburg</addrLine></address></meeting>
		<imprint>
			<publisher>Kompositor</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Individual Differences in Human Frequency-Following Response Predict Pitch Labeling Ability</title>
		<author>
			<persName><forename type="first">Katherine</forename><forename type="middle">S</forename><surname>Reis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Shannon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">P</forename><surname>Heald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">C</forename><surname>Veillette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Howard</forename><forename type="middle">C</forename><surname>Van Hedger</surname></persName>
		</author>
		<author>
			<persName><surname>Nusbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">14290</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Reliability Assessment of Singing Voice F0-Estimates Using Multiple Algorithms</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Rosenzweig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Scherbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meinard</forename><surname>Muller</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP39728.2021.9413372</idno>
		<ptr target="https://doi.org/10.1109/ICASSP39728.2021.9413372" />
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting><address><addrLine>Toronto, ON, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="261" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Erkomaishvili Dataset: A Curated Corpus of Traditional Georgian Vocal Music for Computational Musicology</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Rosenzweig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Scherbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Shugliashvili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meinard</forename><surname>Vlora Arifi-Müller</surname></persName>
		</author>
		<author>
			<persName><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the International Society for Music Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Modelling of Note Events for Singing Transcription</title>
		<author>
			<persName><forename type="first">Matti</forename><forename type="middle">P</forename><surname>Ryynänen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anssi</forename><forename type="middle">P</forename><surname>Klapuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA Tutorial and Research Workshop (ITRW) on Statistical and Perceptual Audio Processing</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Anti-Colonial Strategies in Cross-Cultural Music Science Research</title>
		<author>
			<persName><forename type="first">Sarah</forename><forename type="middle">A</forename><surname>Sauvé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wyatt</forename><surname>Schiefelbein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hideo</forename><surname>Daikoku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shantala</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvia</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Music Perception</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="277" to="292" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Comparative Musicology: The Science of the World&apos;s Music</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Savage</surname></persName>
		</author>
		<idno type="DOI">10.31234/osf.io/b36fm</idno>
		<ptr target="https://doi.org/10.31234/osf.io/b36fm" />
	</analytic>
	<monogr>
		<title level="j">PsyArXiv</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Statistical Universals Reveal the Structures and Functions of Human Music</title>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">E</forename><surname>Savage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emi</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">E</forename><surname>Currie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">29</biblScope>
			<biblScope unit="page" from="8987" to="8992" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Vocal Communication of Emotion: A Review of Research Paradigms</title>
		<author>
			<persName><forename type="first">Klaus</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="227" to="256" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Prescriptive and Descriptive Music-Writing</title>
		<author>
			<persName><forename type="first">Charles</forename><surname>Seeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Musical Quarterly</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="184" to="195" />
			<date type="published" when="1958">1958</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Roadmap for Music Information Research</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michela</forename><surname>Magas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanouil</forename><surname>Benetos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Magdalena</forename><surname>Chudy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Flexer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emilia</forename><forename type="middle">Gómez</forename><surname>Gutiérrez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabien</forename><surname>Gouyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herrera</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergi</forename><surname>Jordà</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Puig</forename></persName>
		</author>
		<ptr target="https://mires.eecs.qmul.ac.uk/files/MIRES_Roadmap_ver_1.0.0.pdf" />
	</analytic>
	<monogr>
		<title level="j">The MIReS Consortium</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Individual Differences in Music Performance</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">A</forename><surname>Sloboda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="397" to="403" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Forum on Transcription</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Stanyek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Twentieth-Century Music</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="101" to="161" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">The Estill Voice Model: Theory &amp; Translation</title>
		<author>
			<persName><forename type="first">Kimberly</forename><surname>Steinhauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><forename type="middle">M</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jo</forename><surname>Estill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Estill Voice International</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Validity in Music Information Research Experiments</title>
		<author>
			<persName><forename type="first">Bob</forename><forename type="middle">L T</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Flexer</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2301.01578</idno>
		<idno type="arXiv">arXiv:2301.01578</idno>
		<ptr target="https://doi.org/10.48550/ARXIV.2301.01578" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<author>
			<persName><forename type="first">Johan</forename><surname>Sundberg</surname></persName>
		</author>
		<title level="m">The Science of the Singing Voice</title>
		<imprint>
			<publisher>Northern Illinois University Press</publisher>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Top-down Modulation of Auditory Processing: Effects of Sound Context, Musical Expertise and Attentional Focus</title>
		<author>
			<persName><forename type="first">Mari</forename><surname>Tervaniemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Kruck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wouter</forename><surname>De Baene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Schröger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Alter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><forename type="middle">D</forename><surname>Friederici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1636" to="1642" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Analytical Ethnomusicology: How We Got Out of Analysis and How to Get Back In</title>
		<author>
			<persName><forename type="first">Leslie</forename><surname>Tilley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Springer Handbook of Systematic Musicology</title>
		<editor>
			<persName><forename type="first">Rolf</forename><surname>Bader</surname></persName>
		</editor>
		<meeting><address><addrLine>Germany; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>1st ed</note>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Principles of Voice Production</title>
		<author>
			<persName><surname>Titze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ingo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">W</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1148" to="1148" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">A Probabilistic Model for the Transcription of Single-Voice Melodies</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Viitaniemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anssi</forename><surname>Klapuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antti</forename><surname>Eronen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Finnish Signal Processing Symposium, FINSIG&apos;03</title>
		<meeting>the 2003 Finnish Signal Processing Symposium, FINSIG&apos;03<address><addrLine>Tampere, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="59" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">On the Preparation and Validation of a Large-Scale Dataset of Singing Transcription</title>
		<author>
			<persName><forename type="first">Jun-You</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jyh-Shing Roger</forename><surname>Jang</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP39728.2021.9414601</idno>
		<ptr target="https://doi.org/10.1109/ICASSP39728.2021.9414601" />
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting><address><addrLine>Toronto, ON, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="276" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">MusicYOLO: A Sight-Singing Onset/Offset Detection Framework Based on Object Detection Instead of Spectrum Frames</title>
		<author>
			<persName><forename type="first">Xianke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqing</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP43922.2022.9746684</idno>
		<ptr target="https://doi.org/10.1109/ICASSP43922.2022.9746684" />
	</analytic>
	<monogr>
		<title level="m">ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="396" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Zbikowski</surname></persName>
		</author>
		<author>
			<persName><surname>Michael</surname></persName>
		</author>
		<title level="m">Conceptualizing Music: Cognitive Structure, Theory, and Analysis</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
