<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Machine and human emotion classification diverge for naturalistic images</title>
				<funder ref="#_gAZMHkG">
					<orgName type="full">Australian Research Council&apos;s Discovery Projects funding scheme</orgName>
				</funder>
				<funder>
					<orgName type="full">Australian Government</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2024-06-18">18 June 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Amy</forename><surname>Dawel</surname></persName>
							<email>amy.dawel@anu.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Medicine and Psychology</orgName>
								<orgName type="institution">The Australian National University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Paige</forename><surname>Mewton</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Medicine and Psychology</orgName>
								<orgName type="institution">The Australian National University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tayla</forename><surname>Williams</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Medicine and Psychology</orgName>
								<orgName type="institution">The Australian National University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eva</forename><forename type="middle">G</forename><surname>Krumhuber</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Experimental Psychology</orgName>
								<orgName type="institution">University College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Medicine and Psychology</orgName>
								<orgName type="institution">The Australian National University</orgName>
								<address>
									<postCode>2600</postCode>
									<settlement>Canberra</settlement>
									<region>ACT</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Machine and human emotion classification diverge for naturalistic images</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-06-18">18 June 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">F027904FE7B2A87E0604136A479DFC70</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-10-22T02:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Facial expression</term>
					<term>emotion</term>
					<term>machine analysis</term>
					<term>artificial intelligence</term>
					<term>naturalistic</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We thank Patrice Ford, Julia Gillett, Alison Schofield, and other student members of the ANU Emotions and Faces Lab for their contributions to stimulus development.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Machine and human emotion classification diverge for naturalistic images</head><p>There have been numerous attempts to create software capable of interpreting human emotional expressions, hoping to foster human-like rapport in customer service, gaming, psychological therapy, and other human-oriented domains <ref type="bibr" target="#b11">(Lewinski et al., 2014;</ref><ref type="bibr" target="#b12">Littlewort et al., 2011;</ref><ref type="bibr" target="#b14">McDuff et al., 2016;</ref><ref type="bibr" target="#b19">Zeng et al., 2009)</ref>. Many of these machine classifiers are rooted in basic emotions theory (BET; <ref type="bibr" target="#b5">Ekman, 1992)</ref>, which posits clear prototypes for seven basic emotions: anger, contempt, disgust, fear, happiness, sadness, and surprise. However, naturalistic expressions are often complex, mixed, and subtle <ref type="bibr" target="#b2">(Cowen &amp; Keltner, 2020;</ref><ref type="bibr" target="#b15">Schmidt et al., 2006)</ref>, thereby challenging the compatibility of the BET approach with real-world emotional expressions.</p><p>Operationalized through morphological features known as "action units" (AUs; <ref type="bibr" target="#b6">Ekman et al., 2002)</ref>, BET prototypes represent emotional expressions as formulas. For instance, the "happy" formula combines AU12 "lip corner puller" with AU6 "cheek raiser". While BET-based machine classifiers excel at classifying intense posed expressions, they struggle with spontaneous expressions elicited in the lab (e.g., people viewing emotional movies or images) that may not exhibit the expected AU combinations <ref type="bibr" target="#b8">(Krumhuber, Küster, Namba, Shah, et al., 2021;</ref><ref type="bibr" target="#b9">Krumhuber, Küster, Namba, &amp; Skora, 2021;</ref><ref type="bibr" target="#b18">Yitzhak et al., 2018)</ref>. BET-based approaches may further diverge from human perception as naturalistic expressions often incorporate additional non-AU information such as tears, eye and head gaze cues, and facial coloring <ref type="bibr" target="#b7">(Kret, 2015;</ref><ref type="bibr" target="#b10">Küster et al., 2021;</ref><ref type="bibr" target="#b17">Thorstenson et al., 2021)</ref>.</p><p>Previous studies evaluating BET-based emotion classification often used controlled, labgenerated expression stimuli <ref type="bibr" target="#b3">(Dawel et al., 2022)</ref>, where expressions are artificially posed or induced by emotionally evocative stimuli rather than real-world interactions. Furthermore, database labels served as the "ground truth" for performance accuracy (e.g., <ref type="bibr" target="#b8">Krumhuber, Küster, Namba, Shah, et al., 2021;</ref><ref type="bibr" target="#b9">Krumhuber, Küster, Namba, &amp; Skora, 2021;</ref><ref type="bibr" target="#b18">Yitzhak et al., 2018)</ref>.</p><p>However, for developing software able to detect real-world social cues, naturalistic expressions should be used with human perception serving as the ground truth. <ref type="foot" target="#foot_0">1</ref> The present research aims to compare human versus machine emotion classification using a widely used commercial software (Affdex) to examine classification rates for a large set of naturalistic facial expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stimuli</head><p>The full stimulus set comprised 2,453 still image frames extracted from 824 YouTube clips from various sources such as news, reality TV, dramas, adverts, talk/game shows, and vlogs. Each stimulus showed a facial expression displayed by young White adults (N = 1,402 expressors). The expressions were selected via two routes. First, three human coders identified the clearest version of each expression in each clip. Second, Affdex likelihood scores were used to identify the image frames with the highest evidence that each emotion was present, above a minimum threshold. Supplement S1 explains the selection process and criteria in detail. As with real-world expressions, these faces appear with varied lighting and viewpoints, and likely include both posed and genuinely-felt expressions. Stimuli were retained in the study if Affdex produced a full set of emotion classification output for the target image frame. Affdex classification was performed on the entire video-clips as the software is optimized for dynamic stimulus classification. However, our analyses were based on the output for individual image frames, aligning with how we measured human classification for these frames. Given that Affdex uses only the face region whilst humans typically integrate contextual information in their emotion judgements <ref type="bibr" target="#b0">(Aviezer et al., 2008)</ref>, we cropped the images for the human observer study so that only the head/face remained visible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Machine Classification</head><p>Affdex outputs a score between 0 and 100 that "indicates the likelihood each &lt;basic&gt; emotion or facial expression to occur, as recognized by a human observer" (iMotions, 2018). Our main analyses used the Affdex default cut-off score of 50 to classify an emotion as present (score &gt;= 50) or absent (&lt; 50) for each image. If none of the seven emotion category scores for an image met the cut-off threshold, we labelled it as showing "no emotion" and excluded it from analyses.</p><p>If more than one emotion score for an image met the cut-off threshold, we labelled it as showing "mixed emotion" and also excluded it from analyses. We conducted supplementary analyses with cut-off scores of 30 and 70 and found that these produced similar results (see Supplement S2).</p><p>Note, Affdex also outputs likelihood scores from 0 to 100 for 21 AUs, which we used to calculate the prototypicality of facial expression stimuli <ref type="bibr" target="#b8">(Krumhuber, Küster, Namba, Shah, et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human Observers</head><p>Human observers were asked to label each image as showing one of the seven basic emotions, an "other emotion", or "no emotion". Each stimulus 2 was labelled by an average of 20.4 (range = 13 to 63) White adult human observers (total sample = 76 males, 169 females; aged 18-32 years, M = 27.4, SD = 3.6) 3 recruited from Prolific (www.prolific.com). The modal emotion labelling response among human observers served as the ground truth for each image. Stimuli were labelled as "no emotion" and excluded from the main analyses when the modal agreement was less than 50 percent. If an image had more than one modal response, we labelled it as "mixed emotion" and also excluded it from analyses. Note, we conducted supplementary analyses with minimum modal agreement rates at 30 and 70 percent and found that these produced similar results (Supplement S2).</p><p>Each observer completed one to three 30-minute testing sessions and renumerated £3.00 GBP per session. In each session, 200 randomly selected images were presented (image size on screen: 4.5 cm high x 4.3 cm wide) one at a time, with the nine labelling options below each image. Presentation and response times were unlimited. <ref type="foot" target="#foot_1">4</ref> If an observer recognised a person's labelled this stimulus twice, but all provided the same label both times.</p><p>3 Data from an additional nine participants were excluded because they labelled fewer than 50 stimuli and thus had insufficient exposure to the range of expressions.</p><p>face (e.g., because they followed the person on YouTube), they were asked to select a tenth option labelled "I know who this person is!". Trials selecting this option were excluded to ensure observer judgments were not influenced by prior knowledge of the expressor (0.5 % of all trials excluded). This research was approved by The Australian National University Human Research Ethics Committee (protocol 2015/305) and carried out as per the Declaration of Helsinki.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Of the original 2,453 images, 1,921 (78%) met the cut-off thresholds for Affdex (likelihood score &gt;= 50) or human classification (modal agreement &gt;= 50%) or both.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human-Machine Classification Agreement</head><p>Figure <ref type="figure">1</ref> presents human and Affdex classification rates for each emotion (see Supplement S3 for confusion matrices). Agreement was calculated as the percent classified as that emotion by both humans and Affdex out of the total stimuli per category identified by either source. Strikingly, agreement was below one-third for all emotions except happiness (67.4%), and below 10% for disgust (8.9%), anger (7.2%), sadness (4.9%), and contempt (0%).</p><p>Disagreements occurred on both sides, sometimes asymmetrically. For example, within the disagreed components human observers classified an additional 273 images as sad (89.8% of the total sad images), whereas Affdex detected only 16 additional sad expressions (5.3%).</p><p>Alternatively, Affdex detected 26 contempt expressions (83.9%), whereas humans classified only five stimuli as contempt (16.1%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig 1 Percentage of stimuli classified as each emotion by humans (left) and Affdex (right), with human-Affdex agreement indicated by circle overlap</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Action Units (AUs) for Disagreed-upon Classifications</head><p>Figure <ref type="figure" target="#fig_0">2</ref> shows the mean Affdex AU likelihood scores and prototype frequency for the disagreed classifications (i.e., the non-overlapping components of Figure <ref type="figure">1</ref>). Findings show that AUs which signal a particular emotion according to BET <ref type="bibr" target="#b6">(Ekman et al., 2002)</ref> were more likely to occur in the Affdex-only than human-only classifications, pointing towards Affdex's reliance on the presence of prototype-specific AUs. To evaluate the presence of emotion prototypes as predicted by BET, we used the method outlined by <ref type="bibr" target="#b8">Krumhuber, Küster, Namba, Shah, et al. (2021)</ref>. Briefly, we calculated a weighted prototypicality score for each stimulus by averaging the AU likelihood scores within a combination and multiplying this average by 1 if the AUs for a full prototype were present or 0.75 if the AUs were present for a major variant of that expression. Thus, a higher prototype score indicates greater evidence for the presence of an emotion prototype. Note, we were unable to perform this analysis for anger because Affdex does not provide scores for AU23, which BET posits as a key component of the anger prototype.</p><p>A 2 (classifier: Affdex-only, human-only) by 6 (emotion label) ANOVA on the prototype scores for the disagreed-upon stimuli revealed a significant main effect of classifier, F(1, 1144) = 664.34, MSE = 475, p &lt; .001, indicating that Affdex-only classified stimuli were overall more prototypical than human-only classified ones. However, this effect was further qualified by a significant interaction between classifier and emotion label, F(5, 1144) = 21.93, MSE = 475, p &lt; .001. Post hoc pairwise comparisons using Tukey's HSD method showed that stimuli classified by Affdex-only were significantly more prototypical than those classified by humans-only for all emotions (all ps &lt; .001) except for fear (p = .999).  <ref type="bibr">Ekman et al. (2002, p. 174)</ref>. Supplement S4 presents AU profiles for agreed versus Affdex-only classifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>The present study makes a novel contribution to the literature by testing a BET-based emotion classifier (Affdex) on naturalistic, in-the-wild expressions with ground truth defined by agreement amongst human observers. Our major finding was that Affdex emotion classification is strikingly divergent from that of humans for naturalistic expressions. This applied to all basic emotions except happiness, across three stimulus selection thresholds, and despite our naturalistic stimuli being pre-selected to show the clearest point of expression. Furthermore, neither human observers nor Affdex were forced into classifying any emotion as present (i.e., stimuli could be classified as showing "no emotion" or "other emotion"). Expressions classified by Affdex as showing a particular emotion were more likely to be prototypical, aligning with the software's reliance on BET prototypes. Overall, these results point to significant limitations in machine ability to label naturalistic expressions in a human-like manner, highlighting the need for improved algorithms that can better handle the complexity and variability of real-world emotional expressions.</p><p>The classification divergence in the present study points to an urgent need for psychological research into other commercial and open-source software, particularly AI-based models like ChatGPT-40. The ability of AI to interact with people in a human-like manner requires that AI is benchmarked against human performance, which psychology offers specialized expertise in assessing. Alternatively, where the aim is for AI to predict human affect and behavior, it may be more fruitful to focus on individual AUs (e.g., <ref type="bibr" target="#b13">McDuff et al., 2013)</ref> as there is only a weak correlation between full facial displays and felt emotion <ref type="bibr" target="#b4">(Durán &amp; Fernández-Dols, 2021;</ref><ref type="bibr" target="#b16">Tcherkassof &amp; Dupré, 2021)</ref>. The present study included unequal numbers of stimuli for each emotion. However, the observed frequencies track previous observations for real-life interactions <ref type="bibr" target="#b1">(Calvo et al., 2014)</ref>. Future work should also consider dynamic expressions and contextual information, to improve the ecological validity of machine classification. Nonetheless, the present study provides compelling evidence that an influential BET-based machine classifier performs poorly against human classification of emotions in naturalistic expressions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig 2</head><label>2</label><figDesc>Fig 2 Mean AU scores (from Affdex) for the disagreed-upon classifications (non-overlapping components of Figure 1)</figDesc><graphic coords="10,72.00,111.36,589.05,388.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="8,72.00,148.30,200.75,566.82" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Alternatively, if software aims to classify people's emotional experiences, the ground truth should capture the expressor's felt emotion, such as physiological responses and/or self-reported emotions.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>Human observers also rated how genuine they thought each expression was using the procedure byDawel   et al. (2017).</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>This research is supported by the <rs type="funder">Australian Government</rs> through the <rs type="funder">Australian Research Council's Discovery Projects funding scheme</rs> (Project No. <rs type="grantNumber">DP220101026</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_gAZMHkG">
					<idno type="grant-number">DP220101026</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Angry, disgusted, or afraid? Studies on the malleability of emotion perception</title>
		<author>
			<persName><forename type="first">H</forename><surname>Aviezer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Hassin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Grady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moscovitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bentin</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1467-9280.2008.02148.x</idno>
		<ptr target="https://doi.org/10.1111/j.1467-9280.2008.02148.x" />
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="724" to="732" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recognition of facial expressions of emotion is related to their frequency in everyday life</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Calvo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gutiérrez-García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fernández-Martín</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nummenmaa</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10919-014-0191-3</idno>
		<ptr target="https://doi.org/10.1007/s10919-014-0191-3" />
	</analytic>
	<monogr>
		<title level="j">Journal of Nonverbal Behavior</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="549" to="567" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">What the face displays: Mapping 28 emotions conveyed by naturalistic expression</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Cowen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Keltner</surname></persName>
		</author>
		<idno type="DOI">10.1037/amp0000488</idno>
		<ptr target="https://doi.org/10.1037/amp0000488" />
	</analytic>
	<monogr>
		<title level="j">American Psychologist</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="349" to="364" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A systematic survey of face stimuli used in psychological research 2000-2020</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dawel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Horsburgh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ford</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13428-021-01705-3</idno>
		<ptr target="https://doi.org/10.3758/s13428-021-01705-3" />
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1889" to="1901" />
			<date type="published" when="2022">2022</date>
			<pubPlace>Scopus</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Do emotions result in their predicted facial expressions? A meta-analysis of studies on the co-occurrence of expression and emotion</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">I</forename><surname>Durán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Fernández-Dols</surname></persName>
		</author>
		<idno type="DOI">10.1037/emo0001015</idno>
		<ptr target="https://doi.org/10.1037/emo0001015" />
	</analytic>
	<monogr>
		<title level="j">Emotion</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1550" to="1569" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An argument for basic emotions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition and Emotion</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3/4</biblScope>
			<biblScope unit="page" from="169" to="200" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Facial Action Coding System: Investigator&apos;s Guide. Research Nexus division of Network Information Research Corporation. iMotions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">V</forename><surname>Friesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Hager</surname></persName>
		</author>
		<ptr target="iMotions.help.imotions.com/hc/en-us/articles/115000512165-Facial-Expressions-Affdex-Methods-Guide" />
		<imprint>
			<date type="published" when="2002">2002. 2018</date>
		</imprint>
	</monogr>
	<note>Facial Expressions Affdex Methods Guide</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Emotional expressions beyond facial muscle actions. A call for studying autonomic signals and their impact on social perception</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Kret</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2015.00711</idno>
		<ptr target="https://doi.org/10.3389/fpsyg.2015.00711" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2015-05">2015. May</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Emotion recognition from posed and spontaneous dynamic expressions: Human observers versus machine analysis</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Krumhuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Küster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Namba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Calvo</surname></persName>
		</author>
		<idno type="DOI">10.1037/emo0000712</idno>
		<ptr target="https://doi.org/10.1037/emo0000712" />
	</analytic>
	<monogr>
		<title level="j">Emotion</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="447" to="451" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Human and machine validation of 14 databases of dynamic facial expressions</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Krumhuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Küster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Namba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Skora</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13428-020-01443-y</idno>
		<ptr target="https://doi.org/10.3758/s13428-020-01443-y" />
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="686" to="701" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">PDSTD -The Portsmouth Dynamic Spontaneous Tears Database</title>
		<author>
			<persName><forename type="first">D</forename><surname>Küster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Krumhuber</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13428-021-01752-w</idno>
		<ptr target="https://doi.org/10.3758/s13428-021-01752-w" />
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<date type="published" when="2021-11">2021. November</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automated facial coding: Validation of basic emotions and FACS AUs in FaceReader</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lewinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Den Uyl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Butler</surname></persName>
		</author>
		<idno type="DOI">10.1037/npe0000028</idno>
		<ptr target="https://doi.org/10.1037/npe0000028" />
	</analytic>
	<monogr>
		<title level="j">Psychology, and Economics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="227" to="236" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>Journal of Neuroscience</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The computer expression recognition toolbox (CERT). Face and Gesture</title>
		<author>
			<persName><forename type="first">G</forename><surname>Littlewort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Whitehill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fasel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Movellan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bartlett</surname></persName>
		</author>
		<idno type="DOI">10.1109/FG.2011.5771414</idno>
		<ptr target="https://doi.org/10.1109/FG.2011.5771414" />
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="298" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Do emotions in advertising drive sales ?</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mcduff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>El Kaliouby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kodra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Larguinat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Affectiva</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">AFFDEX SDK: A cross-platform real-time multi-face expression recognition toolkit</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mcduff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mahmoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mavadati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Amr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Turcot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>El Kaliouby</surname></persName>
		</author>
		<idno type="DOI">10.1145/2851581.2890247</idno>
		<ptr target="https://doi.org/10.1145/2851581.2890247" />
	</analytic>
	<monogr>
		<title level="m">Conference on Human Factors in Computing Systems -Proceedings, 07-12-May</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3723" to="3726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The role of structural facial asymmetry in asymmetry of peak facial expressions</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<idno type="DOI">10.1080/13576500600832758</idno>
		<ptr target="https://doi.org/10.1080/13576500600832758" />
	</analytic>
	<monogr>
		<title level="j">Laterality</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="540" to="561" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The emotion-facial expression link: Evidence from human and automatic expression recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tcherkassof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dupré</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00426-020-01448-4</idno>
		<ptr target="https://doi.org/10.1007/s00426-020-01448-4" />
	</analytic>
	<monogr>
		<title level="j">Psychological Research</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2954" to="2969" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The influence of facial blushing and paling on emotion perception and memory</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Thorstenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Pazda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Krumhuber</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11031-021-09910-5</idno>
		<ptr target="https://doi.org/10.1007/s11031-021-09910-5" />
	</analytic>
	<monogr>
		<title level="j">Motivation and Emotion</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="818" to="830" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neuropsychologia The contribution of facial dynamics to subtle expression recognition in typical viewers and developmental visual agnosia</title>
		<author>
			<persName><forename type="first">N</forename><surname>Yitzhak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gilaie-Dotan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Aviezer</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuropsychologia.2018.04.035</idno>
		<ptr target="https://doi.org/10.1016/j.neuropsychologia.2018.04.035" />
	</analytic>
	<monogr>
		<title level="j">Neuropsychologia</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="26" to="35" />
			<date type="published" when="2017">2018. October 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A survey of affect recognition methods: Audio, visual, and spontaneous expressions</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">I</forename><surname>Roisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2008.52</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2008.52" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="58" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Expression authenticity: The role of genuine and deliberate displays in emotion perception</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zloteanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Krumhuber</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2020.611248</idno>
		<ptr target="https://doi.org/10.3389/fpsyg.2020.611248" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">611248</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
