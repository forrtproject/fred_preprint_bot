<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Information Uncertainty Influences Learning Strategy from Sequentially Delayed Rewards</title>
				<funder>
					<orgName type="full">University of Maryland</orgName>
				</funder>
				<funder ref="#_mNAZnTD">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_YhaU4bh">
					<orgName type="full">National Institute of Mental Health</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sean</forename><forename type="middle">R</forename><surname>Maulhardt</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Alec</forename><surname>Solway</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Caroline</forename><forename type="middle">J</forename><surname>Charpentier</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Information Uncertainty Influences Learning Strategy from Sequentially Delayed Rewards</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4C85B72B8093BAA67C237ECF1F5F059C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-10-22T02:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>credit assignment</term>
					<term>reinforcement learning</term>
					<term>delayed reward</term>
					<term>temporal discounting</term>
					<term>information uncertainty</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>When receiving a reward after a sequence of multiple events, how do we determine which event caused the reward? This problem, known as temporal credit assignment, can be difficult for human solutions given a complex and uncertain environment. It's not clear whether people adjust their strategies to tackle this problem based on the uncertainty of the environment. To address this, we adapted a reward learning task that creates a temporal credit problem combining sequentially delayed rewards, intervening events, and varying uncertainty via the amount of information presented during feedback. Using computational modeling, two learning strategies were developed: eligibility trace whereby previously selected actions are updated as a function of the temporal sequence -and tabular update -whereby only systematically-related past actions (rather than unrelated intervening events) are updated. We hypothesized that reduced uncertainty would correlate with increased use of the tabular strategy, considering the model's capacity to incorporate additional feedback information. Our results supported this hypothesis. Both models effectively learned the task, and choices made by participants (N=142) were best explained by a hybrid model that combined both strategies. However, the tabular model outperformed under conditions of low uncertainty, as evidenced by more accurate predictions of participants' behavior and an increased tabular weight parameter. These findings provide new insights into the mechanisms implemented by humans to solve temporal credit assignment and how they adapt their strategy to the uncertainty and observability of the environment.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>The credit assignment (CA) problem poses an obstacle for various computational systems, wherein its solution allows the system to learn from environmental states. CA involves determining the causal contributions from various states to an outcome. Potential contributing states can be thought of as any abstraction of an environment, such as different ingredients for a dinner leading to a tasty meal. For instance, let's consider the scenario of receiving a compliment at dinner. To attribute credit accurately, one must recall past behaviors that contributed to the overall pleasantness of the meal. While a pinpointing comment about the specific ingredients that enhanced the dining experience can offer valuable insights, a general compliment on the entire dinner introduces additional uncertainty. Questions may arise, such as "Was the entire dinner enjoyable, or just certain parts of it?" or "Which actions should be replicated in future meals?"</p><p>To mitigate the challenges of CA, some individuals may opt for prepackaged dinners, which provide a predetermined recipe minimizing the need for complex evaluation of uncertain prospects. However, this approach comes at the cost of reduced flexibility and the potential loss of pleasantness from a unique and personalized recipe. Ultimately, people credit based on various state features, including time constraints, available resources, information given, task uncertainty, and (culinary) expertise. Examining certain experimental solutions offers simplifying assumptions at the expense of real-world validity. In this study, our experimental design mimics real-world decision-making processes in which reward uncertainty stresses a temporal CA problem. This approach provides insights into participants' strategies for overcoming such obstacles in real-world environments.</p><p>In Reinforcement Learning (RL), agents need to evaluate the state features responsible for producing specific outcomes, as they seek to approach better-than-expected outcomes and avoid outcomes that were less-than-expected <ref type="bibr" target="#b40">(Sutton &amp; Barto, 2018)</ref>. Additionally, RL offers a normative framework for addressing the intricacies of gaining reward in sequential environments, presenting an avenue for comparing human behavior to RL agents that overcome long-time horizon contingencies <ref type="bibr" target="#b4">(Daw et al., 2011;</ref><ref type="bibr" target="#b9">Gershman et al., 2014;</ref><ref type="bibr" target="#b26">Moran et al., 2019;</ref><ref type="bibr" target="#b44">Walsh &amp; Anderson, 2011)</ref>, and for developing AI agents that mimic human solutions in temporal CA <ref type="bibr" target="#b28">(Nguyen et al., 2023)</ref>. To handle changes in uncertainty, agents must adjust their manner of assigning credit through the implementation of different internal systems or modulating the components of such systems.</p><p>Uncertainty in CA manifests in two primary forms <ref type="bibr" target="#b0">(Agogino &amp; Tumer, 2004;</ref><ref type="bibr" target="#b24">Minsky, 1961;</ref><ref type="bibr" target="#b39">Sutton, 1984)</ref>. First, there is temporal CA, which deals with the sequential challenge of identifying causal relationships between a series of actions and their outcomes. This contrasts with and leads into structural CA, where the focus is on the parallel challenge of discerning the contributions of various competing components within an internal system. Research to date has mostly focused on structural CA, showing how learning the contingencies or the structure in a transition matrix can then lead to properly assigning delayed feedback to its source <ref type="bibr" target="#b11">(GlÃ¤scher et al., 2010;</ref><ref type="bibr" target="#b21">Lehmann et al., 2019;</ref><ref type="bibr" target="#b26">Moran et al., 2019;</ref><ref type="bibr" target="#b44">Walsh &amp; Anderson, 2011)</ref>. Furthermore, many recent RL experiments have uncovered that individuals learn by combining a strategy of naively adhering to the temporal sequence of rewards known as model-free (MF), and one of adopting an approach that considers the probabilistic transitions along the sequence to reward known as model-based (MB) <ref type="bibr" target="#b4">(Daw et al., 2011;</ref><ref type="bibr" target="#b9">Gershman et al., 2014;</ref><ref type="bibr" target="#b44">Walsh &amp; Anderson, 2011)</ref>. We refrain from utilizing these terms, as our task does not allow clean distinctions <ref type="bibr" target="#b4">(Daw et al., 2011;</ref><ref type="bibr" target="#b26">Moran et al., 2019)</ref>. However, several aspects of the MF and MB distinction remain relevant when dealing with temporal CA through delayed rewards and intervening random decisions that might disrupt the systematic relationship <ref type="bibr" target="#b18">(Kearns &amp; Singh, 2000)</ref>. A MB strategy can be more taxing as an additional layer of complexity is needed to weigh the correct states in a temporal sequence, whereas a MF strategy might be advantageous when the underlying state space is not immediately observable. Thus, various problems of temporal CA can impact the identifiability of the structural solution and pose a question of the variability in humanimplemented strategies.</p><p>Solutions that consider delayed feedback often entail embedding a temporal difference (TD) algorithm with an exponentially decaying eligibility trace but can either make use of an explicit transition matrix or not <ref type="bibr" target="#b4">(Daw et al., 2011;</ref><ref type="bibr" target="#b28">Nguyen et al., 2023;</ref><ref type="bibr" target="#b41">Sutton et al., 1999;</ref><ref type="bibr" target="#b44">Walsh &amp; Anderson, 2011)</ref>. If the temporal sequence of events contains intervening events which disrupt the pairs temporal continuity, then an agent may erroneously credit feedback if they naively follow the sequence of transitions. The unpredictable nature of intervening events and their outcomes can lead to confusion when attempting to both retain and inhibit updates for the stimuli along similar abstractions. Such as in the N-back task, the agent must retain intervening events and prevent inference from past trials when presented with test trials <ref type="bibr" target="#b17">(Kane et al., 2007)</ref>.</p><p>Routinely, independent tasks that measure the preference for delayed reward and workingmemory demands of delayed events have shown how excessive cognitive demand can increase preference for immediacy of reward <ref type="bibr" target="#b1">(Aranovich et al., 2016;</ref><ref type="bibr" target="#b42">Szuhany et al., 2018)</ref>. However, a unified task that gives the participants the agency to use additional information to reduce information uncertainty could provide further insights into the tradeoff between value and time in a realistic decision environment <ref type="bibr" target="#b37">(Solway et al., 2017)</ref>. <ref type="bibr" target="#b43">Tanaka et al. (2009)</ref> introduced a task that creates a temporal CA problem with the inherent dilemma of the dinner example. In a repeat decision task, immediate and delayed feedback was conjoined, that is, feedback was the summed reward from the current and threetrials back choice. This might be thought of as akin to receiving an entire dinner compliment, which provokes a partially observable reward function, rather than one for each separate course.</p><p>Their analysis showed that participants eschewed any type of structural approach and favored a simple learning solution of augmenting a TD model with an eligibility trace. The eligibility trace assigns credit based on the temporal sequence of previously experienced states, which embed both systematically and randomly related signals. Over time, randomly-related signals become more infrequent to a systematic signal and result in eventually identifying the true relationship.</p><p>In the current study, we aimed to address the problem of CA in sequentially delayed rewards and to characterize the strategies that an agent might implement under different degrees of uncertainty. We introduced an experimental manipulation on the degree of information uncertainty through two forms of reward feedback: conjoint, like in the study described above, and disjoint, where immediate and delayed outcomes were presented separately. To account for separated knowledge, we developed a tabulation method that is designed to partition the task based on the systematic relationship of the time-horizon. This tabulation model only credits systematically-related timings, uses a value function that augments time as a dimension, and utilizes this new value function to generate separate prediction errors for immediate and delayed rewards. Environmental obstacles, such as through intervening events, can have direct costs on temporal contiguity and lead to inefficiencies of the proposed computational solution <ref type="bibr" target="#b3">(Collins &amp; Frank, 2012;</ref><ref type="bibr" target="#b18">Kearns &amp; Singh, 2000;</ref><ref type="bibr" target="#b28">Nguyen et al., 2023)</ref>. Many computational models have mechanisms in place for handling uncertainty, such as through modulating free parameters or implementing stochastic policies. One proposed solution is to use an undirected and automated process, complemented with one that uses a directed and systematic solution <ref type="bibr" target="#b11">(GlÃ¤scher et al., 2010)</ref>; in our case, an eligibility trace and our proposed tabulation mechanism, respectively.</p><p>Here, we implemented each strategy (eligibility trace and tabular) as its own computational model, reflecting CA under different degrees of efficiency, and tested a hybrid computational model that combines both strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Predictions</head><p>Due to the dominance of the eligibility trace in past research, we seek to understand if additional information that reduces uncertainty will change the agent's CA strategy to a solution that can partition the environment around a task-relevant time horizon. Previously, the task's structure may have prompted the usage of an eligibility trace due to the uncertainty associated with conjoint feedback <ref type="bibr" target="#b43">(Tanaka et al., 2009)</ref>. Here, we hypothesize that the uncertainty reduction associated with additional information will promote the use of the tabular strategy over eligibility trace, and that we will observe this effect in two ways. First, the tabular model will explain participants' choices better in the disjoint condition, where additional information is provided, while the eligibility model will explain participants' choice better in the conjoint condition.</p><p>Second, the tabular weight will be higher in the disjoint than conjoint condition, while the eligibility weight will be higher in the conjoint than disjoint condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>163 participants were recruited from Prolific Academic (https://prolific.com) for an hour and a half long study over two sessions. Prolific inclusion criteria included: fluency in English, ages over 18, and no color blindness. Sessions were broken apart by two days to one week, but participants were allowed to complete the second session within that flexible interval. They were compensated a total of $30 for participating with a bonus dependent on their proportion of selecting the higher valued stimuli. Due to the possibility of external aid, participants were given instructions to not use any additional help and given an end-questionnaire asking if they had used external aid. Although all participants were paid, 13 participants were dropped for not completing the second stage, six were removed for admitting to using external aids, and two were dropped for duplicate stages. The resulting sample contains 142 total subjects (81 males, 60 females, 1 prefer not to say). Ages ranged from 18 to 63 (Mage = 26, SDage = 6.57) with employment statuses (50 full-time, 34 unemployed, 23 other, 22 part-time, 6 full-time nonpaid workers, and 7 missing). Most participants (n = 90) were from Europe <ref type="bibr">(33 Portugal,</ref><ref type="bibr">30 Poland,</ref><ref type="bibr">6</ref> Italy, 5 Hungary, 5 United Kingdom, 4 Greece, 4 Spain, and 3 other). The other subjects were predominately spread across North America (n = 22) and South Africa (n = 22); and lastly, a total of eight other subjects in the Middle East and Asia. Although all participants were fluent in English, first languages were predominately Portuguese (n = 34), Polish (n = 31), Spanish (n = 25), English (n = 10), Other (n = 22), and missing (n = 20).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Materials</head><p>The CA task was built with PsychoPy3 <ref type="bibr" target="#b31">(Pierce et al., 2022)</ref>. At the end of all materials, an exit survey was administered with questions assessing if they used external aids, the difficulty of the task on a scale of 0 (easy) to 100 (hard), <ref type="bibr">Mdifficulty = 60.56,</ref><ref type="bibr">SDdifficulty = 25.64,</ref><ref type="bibr">and two</ref> open-questions on noticing anything particular or the way they had learned the values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CA Task</head><p>Participants were given explicit instructions, leading questions, and diagrams of the task structure. The leading questions were aimed at helping participants understand and therefore were intentionally thought-provoking. In sum, participants answered four instruction questions to help solidify their understanding (Mcorrect = 56% SDcorrect = 19%). Upon answering one of the four instruction questions incorrectly, further instruction was provided. No participants were excluded based on instruction questions. At the end of the instructions, the final question asked if they felt they understood the task. Among the participants, 87% reported feeling comfortable and 13% feeling slight to moderate confusion. The specific object reward and whether the object had a delayed reward had to be learned throughout the task. Participants were told in the instructions that delayed objects always had a fixed delay of two trials ahead.</p><p>On each trial (Figure <ref type="figure">1A-B</ref>), participants were instructed to use the mouse to click one of the two objects displayed. They had a total of 15 seconds to make a choice, otherwise no selection was made. There was a total of 8 objects (4 associated with immediate and 4 with delayed feedback) with 336 trials presenting every unique pair of the 8 objects 12 repeated times.</p><p>Sixteen images were randomly assigned without replacement to one of the unique objects across the two sessions, resulting in sixteen different object stimuli. Upon selection, a green box surrounded the selected object, and the participant went to the feedback stage. The feedback was dependent on both the reward (starting rewards: -4, -1, 1, 4) and a fixed delay (0, 2). The reward changed over time with three fixed gaussian random walks, N(0, .25), which was then later rounded to a nearest integer (Figure <ref type="figure">1C</ref>). The random walk conditions were chosen due to the limitations of the online software and were randomized for each participant (28.17% of participants had the same random walk for both conditions). However, the sequence of decision pairs was totally random. Depending on choice from previous trials, participants could receive none, immediate, delayed, or a conjoint immediate and delayed reward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1</head><p>Experimental design and random walks Note. A. Example trial sequence of the conjoint condition. Participants choose between two objects, then receive feedback. Here, a ball of yarn offers an immediate reward of '1', while a mouse provides a delayed reward of '4' after two trials. B. The disjoint condition presented on the same sequence of events but with dissociable feedback. C. This illustrates the three fixed random walk reward value patterns for all eight stimuli across trials. Each stimulus is linked to either an immediate (solid lines) or a delayed (dashed lines) reward. The starting reward values are 4, 1, -1, or -4, and they gradually drift throughout the task. Participants encounter a random sequence of unique pairs (28 in total) within one of the random walk conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conditions</head><p>Participants started in either the disjoint (n = 74) or conjoint (n = 70) condition. The conjoint condition gave participants both rewards conjoint together, such as receiving a delayed '4' reward from two trials back and an immediate '1' reward from the current trial, which would then be displayed as 5 (Figure <ref type="figure">1A</ref>). On the other hand, the disjoint condition gave participants a dissociable reward. The feedback displayed two boxes titled 'immediate reward' and 'delayed reward'; and consequently, did not sum the reward together (Figure <ref type="figure">1B</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure</head><p>Each participant filled out a consent form that described the type of task they would receive compensation for. After each participant completed the CA task, they took an exit survey. Time taken, in minutes, for the CA task <ref type="bibr">(Disjoint: Mend = 25.27,</ref><ref type="bibr">SDend = 8.77,</ref><ref type="bibr">Conjoint: Mend = 25.32,</ref><ref type="bibr">SDend = 10.07</ref>) and surveys (Mend = 7.25, SDend = 3.85) were reasonable. Response rate for the CA task was very high (Mresponse = 99.5%). Per trial time was calculated after removing non-answered trials, for an average trial time of 1.59 seconds (SDtrialtime = 1.37). After completing both sessions, participants were given their bonus. Those who did not complete the second session were paid for the first session but did not receive the bonus payment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reinforcement Learning Models</head><p>To better illustrate the two learning models and their associated value functions, a graphical representation is provided in Figure <ref type="figure">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2</head><p>Graphical representation of task conditions, learning models, and value functions Note. A. Differences in feedback presentation based on the participant's condition and the outcome used to generate the prediction error for immediate feedback. B. Example sequence of two-alternative forced choice trials and the participant's selection (darker arrow) in the current trial (red), the previous trial (T-1), and trial-minus-two (T-2). The colors correspond to the tabular model, which updates the immediate choice (+1) and trial-minus-two choice (+4), each generating a prediction error to update the value function. C. Temporal sequence of assigning credit (shown as a blue heatmap). In this model, tabular skips assigning credit to the previous state (S2). The triple period signifies that credit assignment can extend beyond the three depicted states. Note that the extent to which past states are assigned credits in each model depends on the free parameter lambda: for eligibility, higher lambda values mean that credit extend further back in time (less decay), while for tabular, higher lambda values mean less discounting of the trialminus-two state specifically. D-E. Value functions for the tabular model (D), which involves separate, independent, updates for the immediate and delayed chosen options, and for the eligibility trace (E), which utilizes a single prediction error for updates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Eligibility Trace</head><p>The eligibility trace model (we use the abbreviated version 'elg' in different graphs) uses the Rescorla-Wagner learning rule (Î´) to calculate the difference between reward and expected value <ref type="bibr" target="#b32">(Rescorla &amp; Wagner, 1972)</ref>.</p><formula xml:id="formula_0">ğ›¿ğ›¿ ğ‘¡ğ‘¡ = ğ‘Ÿğ‘Ÿ ğ‘¡ğ‘¡ (ğ‘ğ‘) -ğ‘£ğ‘£ ğ‘¡ğ‘¡ (ğ‘ğ‘)<label>(1)</label></formula><p>At the current time point (t), this calculates the difference between the actual reward (r) and estimated value (v). Actions (a) are then updated with a replacing eligibility trace, such that the unchosen actions are discounted.</p><formula xml:id="formula_1">ğ‘’ğ‘’ğ‘’ğ‘’ ğ‘¡ğ‘¡ (ğ‘ğ‘ ğ‘–ğ‘– ) = ï¿½ 1 ğ‘–ğ‘–ğ‘–ğ‘–ğ‘ğ‘ ğ‘–ğ‘– = ğ‘ğ‘ ğ‘¡ğ‘¡ ğœ†ğœ† ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ ğ‘’ğ‘’ğ‘’ğ‘’ ğ‘¡ğ‘¡-1 (ğ‘ğ‘ ğ‘–ğ‘– ) ğ‘–ğ‘–ğ‘–ğ‘–ğ‘ğ‘ ğ‘–ğ‘– â‰  ğ‘ğ‘ ğ‘¡ğ‘¡<label>(2)</label></formula><p>The replacing eligibility trace (et) updates all options (i) using a free decay rate parameter (Î»elg), bounded between 0 and 1, for each action. The current selection is updated with a replacing eligibility trace of 1, so that the current selection has no decay <ref type="bibr" target="#b36">(Singh &amp; Sutton, 1996)</ref>. The value function is then updated for each action.</p><formula xml:id="formula_2">ğ‘£ğ‘£ ğ‘¡ğ‘¡+1 (ğ‘ğ‘ ğ‘–ğ‘– ) â† ğ‘£ğ‘£ ğ‘¡ğ‘¡ (ğ‘ğ‘ ğ‘–ğ‘– ) + ğ›¼ğ›¼ ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ ğ›¿ğ›¿ ğ‘¡ğ‘¡ ğ‘’ğ‘’ğ‘’ğ‘’ ğ‘¡ğ‘¡ (ğ‘ğ‘ ğ‘–ğ‘– )<label>(3)</label></formula><p>The learning rate (Î±) is a free parameter that determines the magnitude of the update from the RPE and eligibility trace. Thus, the temporal sequence is highly meaningful for the valuation of past actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tabular</head><p>The tabular based method (abbreviated 'tab') has an explicit representation of the temporal sequence <ref type="bibr" target="#b40">(Sutton &amp; Barto, 2018)</ref>.</p><formula xml:id="formula_3">ğ›¿ğ›¿ ğ‘¡ğ‘¡ (ğ‘‘ğ‘‘) = ğ‘Ÿğ‘Ÿ ğ‘¡ğ‘¡ (ğ‘ğ‘) -ğ‘„ğ‘„ ğ‘¡ğ‘¡ (ğ‘‘ğ‘‘, ğ‘ğ‘)<label>(4)</label></formula><p>Two RPEs are calculated, one for the immediate reward and the other for the outcome of the two-trials previous choice based on the represented delay (d). The Q-learning function considers both the delay and the action chosen.</p><formula xml:id="formula_4">ğ‘„ğ‘„ ğ‘¡ğ‘¡+1 (ğ‘‘ğ‘‘, ğ‘ğ‘) â† ï¿½ ğ‘„ğ‘„ ğ‘¡ğ‘¡ (ğ‘‘ğ‘‘, ğ‘ğ‘) + ğ›¼ğ›¼ ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ ğœ†ğœ† ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ ğ›¿ğ›¿ ğ‘¡ğ‘¡ (ğ‘‘ğ‘‘) ğ‘–ğ‘–ğ‘–ğ‘–ğ‘‘ğ‘‘ = 2 ğ‘„ğ‘„ ğ‘¡ğ‘¡ (ğ‘‘ğ‘‘, ğ‘ğ‘) + ğ›¼ğ›¼ ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ ğ›¿ğ›¿ ğ‘¡ğ‘¡ (ğ‘‘ğ‘‘) ğ‘–ğ‘–ğ‘–ğ‘–ğ‘‘ğ‘‘ = 0<label>(5)</label></formula><p>Both RPEs are used to update the immediate choice and the choice from two trials ago. Both models have their own parameters used in the two separate models. Note that ğœ†ğœ† ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ is only used when we consider the delayed feedback (d = 2), whereas, immediate is not discounted at all.</p><p>Because the instructions were explicit about not crediting the action chosen one trial ago, the tabular model skips updates for the one trial delay. On the other hand, the eligibility trace will bleed credit into the randomly related objects dependent on the temporal sequence in which they were experienced. These decay rate parameters are not symmetrical between models, as the eligibility strategy decays the trace of the temporal sequence and the tabular strategy only downweighs the update for the two-trial back choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decision Rule and Hybrid Model</head><p>Both models, eligibility (elg) and tabular (tab), are placed into a SoftMax function. For the independent models, there is a single strategy weight; whereas the hybrid model infuses these two strategies at decision time through two free parameters which reflect a weighing the strategy contribution at decision time, referred to as strategy weight (Î²elg and Î²tab). These betas are used to weigh the value function calculated for each single-strategy model.</p><formula xml:id="formula_5">ğœ‹ğœ‹ ğ‘¡ğ‘¡ (ğ‘ğ‘) = ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ ï¿½ğ‘§ğ‘§(ğ›½ğ›½ ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ ğ‘£ğ‘£ ğ‘¡ğ‘¡ (ğ‘¡ğ‘¡)+ğ›½ğ›½ ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ âˆ‘ğ‘„ğ‘„ ğ‘¡ğ‘¡ (ğ‘‘ğ‘‘,ğ‘¡ğ‘¡))ï¿½ âˆ‘ ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ ï¿½ğ‘§ğ‘§(ğ›½ğ›½ ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ ğ‘£ğ‘£ ğ‘¡ğ‘¡ (ğ‘¡ğ‘¡ ğ‘–ğ‘– )+ğ›½ğ›½ ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ âˆ‘ğ‘„ğ‘„ ğ‘¡ğ‘¡ (ğ‘‘ğ‘‘,ğ‘¡ğ‘¡ ğ‘–ğ‘– ))ï¿½ ğ‘›ğ‘› ğ‘–ğ‘–=0<label>(6)</label></formula><p>When considered alone, the hybrid model can be reduced to either the eligibility or tabular through a strategy weight of zero assigned to the other strategy. Finally, we incorporate for each model's value function a z-scored locally weighted smoothing function within 5-trials behind of current trial before SoftMax decision choice. By normalizing the data before feeding into the</p><p>SoftMax, the hybrid model can account for different scales of the random walks and reduce the noise of sub-models operating on different scaled rewards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter Recovery</head><p>To achieve proficient parameter recovery, we simulated 300 datasets using modest uncertainty to generate each of the 300 parameter sets. For each of model's learning rate and decay rate, a beta distribution was utilized (Î± = 1.25, Î² = 1.25), while the decision weights used a gamma distribution (Î± = 1.25, Î² = 1). The three generated parameters were held constant between the two independent models and feedback conditions. Additionally, for the hybrid model, these six generated parameters were held constant across both feedback conditions. Each of the generative models returned the simulated agent's choice, value functions, and probabilities. Afterwards, the model was optimized using an evolutionary strategy (R package 'DEoptim') on the simulated agent's choices to identify the correlation between generative and recovered parameters <ref type="bibr" target="#b27">(Mullen et al., 2011)</ref>. To avoid local minimums, we used a large smoothing value was set for the population size (750 for each model and 1500 for hybrid), a modest number of iterations (60 for each model and 120 for hybrid), and strategy #3 which provided a jitter to the inherited parameters from previous fit iteration. Additionally, priors were used corresponding to the generated parameters distributions with modest uncertainty and the zscored locally weighted smoothing function was used. Pearson's correlations between simulated and recovered parameters were then computed and transposed onto a scatterplot with a fitted line (Figure <ref type="figure" target="#fig_0">3</ref>). For eligibility-conjoint, alpha (learning rate), r(299) = .86, beta (decision weight), r(299) = .99, and lambda (decay rate), r(299) = .81. For eligibility-disjoint, alpha, r(299) = .84, beta, r(299) = .98, and lambda, r(299) = .83. For tabular-conjoint, alpha, r(299) = .33, beta, r(299) = .92, and lambda, r(299) = .61. For tabular-disjoint, alpha, r(299) = .64, beta, r(299)</p><p>= .85, and lambda, r(299) = .64. For hybrid-conjoint, alpha-eligibility, r(299) = .48, betaeligibility, r(299) = .9, and lambda-eligibility, r(299) = .5, alpha-tabular, r(299) = .33, betatabular, r(299) = .84, and lambda-tabular, r(299) = .63. For hybrid-disjoint, alpha-eligibility, r(299) = .59, beta-eligibility, r(299) = .89, and lambda-eligibility, r(299) = .6, alpha-tabular, r(299) = .32, beta-tabular, r(299) = .83, and lambda-tabular, r(299) = .54. Noteworthy, betatabular appears to be underfitting the trend, along with some under-weighing and over-weighing based on the reward condition, such as in beta-eligibility for the hybrid model (see Figure <ref type="figure" target="#fig_0">3</ref>). To test our hypothesis, we ran the following analyses to understand how our conditions (conjoint vs. disjoint) and stage group order (disjoint-to-conjoint vs. conjoint-to-disjoint) influenced participants' choice and learning behavior. Initially, we performed basic descriptive statistics (Table <ref type="table" target="#tab_0">1</ref>) and regressions on experimental and constructed variables. Next, we aimed to validate the predictions of our models through two multilevel logistic regressions: one focusing on condition and delayed learning (Figure <ref type="figure" target="#fig_1">4</ref>), and the other on how our RL models track participant choice behavior across combined conditions (Figure <ref type="figure" target="#fig_2">5</ref>). Third, we compared our three models to determine which best fit the data for each participant and reward condition (Table <ref type="table" target="#tab_2">2</ref>).</p><p>Lastly, we examined our independent and hybrid model parameters via correlations (Table <ref type="table" target="#tab_3">3</ref>) and t-tests (Table <ref type="table" target="#tab_4">4</ref>) and compared them in multilevel models looking at the interaction of reward condition and stage group order (Figure <ref type="figure">6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Posterior Predictive Checks: Model Validation Against Behavioral Data</head><p>To validate our eligibility and tabular models and understand their relation to behavioral data and the behavioral pattern they each capture, we defined behavioral measures that show whether participants were tracking delayed rewards correctly. Initially, we expected certain sequence patterns to reveal delayed reward learning through participants' stay or switch behavior, akin to the key behavioral signature of the two-step task <ref type="bibr" target="#b4">(Daw et al., 2011)</ref>. We selected sequences where participants chose a delayed option, and it reappeared as a potential choice three trials later. If participants are learning the contingencies properly, they should use feedback information two trials into the future to either stay or switch on the following trial, depending on the reward's valence, rather than relying on immediate or one-trial-forward feedback information (Figure <ref type="figure" target="#fig_1">4A</ref>). We quantified this signature using a multilevel logistic regression from the 'lme4' package in R:</p><formula xml:id="formula_6">ğ‘†ğ‘†ğ‘’ğ‘’ğ‘ğ‘ğ‘†ğ‘† ~ ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶ğ‘‘ğ‘‘ğ‘–ğ‘–ğ‘’ğ‘’ğ‘–ğ‘–ğ¶ğ¶ğ¶ğ¶ * ğ‘‡ğ‘‡ğ‘–ğ‘–ğ‘‡ğ‘‡ğ‘’ğ‘’ * ğ‘…ğ‘…ğ‘’ğ‘’ğ‘…ğ‘…ğ‘ğ‘ğ‘Ÿğ‘Ÿğ‘‘ğ‘‘ + (1 + ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶ğ‘‘ğ‘‘ğ‘–ğ‘–ğ‘’ğ‘’ğ‘–ğ‘–ğ¶ğ¶ğ¶ğ¶ * ğ‘‡ğ‘‡ğ‘–ğ‘–ğ‘‡ğ‘‡ğ‘’ğ‘’ * ğ‘…ğ‘…ğ‘’ğ‘’ğ‘…ğ‘…ğ‘ğ‘ğ‘Ÿğ‘Ÿğ‘‘ğ‘‘ | ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘’ğ‘’ğ‘†ğ‘†ğ‘’ğ‘’)<label>(7)</label></formula><p>We fit three logistic regressions from equation 9: one on the participants' data, and two on the data generated from eligibility and tabular models. Both eligibility and tabular used the participants' best fitting parameters to generate model-independent choice. The significance value of the interaction provides evidence claims on deviation from chance (.5) rather than comparisons between the three models (Figure <ref type="figure" target="#fig_1">4C</ref>); nevertheless, the interaction plot with confidence intervals (Figure <ref type="figure" target="#fig_1">4B</ref>) informs us of differences between models, as well as similarities and differences between each model predictions and participants' data.</p><p>Next, we examined whether participants were tracking the random-walk of rewards and how the models corresponded with our conditions. Similar to <ref type="bibr" target="#b43">Tanaka et al. (2009)</ref>, we initially focused on illustrating how participants were capable of tracking rewards in specific example pairs (Figure <ref type="figure" target="#fig_2">5A</ref>). Following this, we demonstrated our hybrid model's capability to replicate these choice trajectories (Figure <ref type="figure" target="#fig_2">5B</ref>). This analysis provides a foundational understanding of individual decision-making processes and the model's performance in isolated instances.</p><p>However, for a comprehensive understanding of overall trial-by-trial accuracy and decisionmaking patterns in our two models (tabular and eligibility) under both conditions (disjoint and conjoint), we expanded our analysis. This broader approach involved aggregating data across all random walks and pairs, aiming to identify overarching trends and interactions not apparent in individual examples. We quantified participants' overall propensity of choice by simulating parameters for our two models, using a beta distribution (Î± = 1, Î² = 2) for our learning rate, a gamma distribution (Î± = 3, Î² = 1) for the inverse temperature parameter, and a beta distribution (Î± = 2, Î² = 3) for the decay parameter. This maintained consistent parameters across the two models and kept a high inverse temperature parameter to reduce random noise. The two models then made selections on the same sequences seen by participants during the task. We fit a multilevel logistic regression (equation 10) predicting participants' choice on each trial from the choice probability predicted by each model separately, both interacting with condition (conjoint and disjoint):</p><formula xml:id="formula_7">ğ¶ğ¶â„ğ¶ğ¶ğ‘–ğ‘–ğ‘†ğ‘†ğ‘’ğ‘’ ~ ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶ğ‘‘ğ‘‘ğ‘–ğ‘–ğ‘’ğ‘’ğ‘–ğ‘–ğ¶ğ¶ğ¶ğ¶ * (ğ¸ğ¸ğ¸ğ¸ğ¸ğ¸ + ğ‘‡ğ‘‡ğ‘ğ‘ğ‘†ğ‘†) + (1 + ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶ğ‘‘ğ‘‘ğ‘–ğ‘–ğ‘’ğ‘’ğ‘–ğ‘–ğ¶ğ¶ğ¶ğ¶ * (ğ¸ğ¸ğ¸ğ¸ğ¸ğ¸ + ğ‘‡ğ‘‡ğ‘ğ‘ğ‘†ğ‘†) | ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘’ğ‘’ğ‘†ğ‘†ğ‘’ğ‘’) (8)</formula><p>This regression model allowed us to estimate and compare whether eligibility or tabular predictions were closer to participants' choices and whether that effect varied with the condition (Figure <ref type="figure" target="#fig_2">5C</ref>).</p><p>Each multilevel model used the 'mixed' function from the 'afex' package, to calculate pvalues using the Satterthwaite approximation for logistic regressions and Kenward-Roger approximation otherwise. These methods keep type I error rate from being inflated <ref type="bibr" target="#b23">(Luke, 2017)</ref>.</p><p>However, despite the superior performance, the first behavioral check (equation 9) was too computationally taxing to implement this method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Fitting Metrics</head><p>After quantifying behavioral signatures from participants' and model data, we quantified our model fits in each of our RL models. We fit each of the three models (eligibility, tabular, and hybrid) to participants' choice using the optimizer from R package 'DEoptim', separately for each condition. We provided four descriptive statistics of model fit per participant and condition (Table <ref type="table" target="#tab_2">2</ref>): the percentage of best-fitting model, the mean and standard deviation of negative loglikelihood, and pseudo-R2. Pseudo-RÂ² was calculated by subtracting one from the ratio of the fitted model's negative log-likelihood to that of the null (or random) model for each participant's condition. Note, the current log-likelihoods are penalized for extreme values using priors, for each learning rate and decay parameters a beta distribution was used (Î± = 1.25, Î² = 1.25) and for the inverse temperature parameter, a gamma distribution (Î± = 1.25, Î² = 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Computational Parameters Across Conditions</head><p>We first looked at the bivariate correlations between the parameters partitioned on condition (Table <ref type="table" target="#tab_3">3</ref>). Although these parameters are dependent on the model they are yoked to and violate independency assumptions of traditional statistics, we reasoned that comparing parameters across condition and stage group order (order of conditions) would still provide additional information to corroborate our hypothesis that tabular and eligibility weights should vary with information uncertainty and assess whether this effect was impacted by condition order. Additionally, exploring potential changes in decay and learning rate parameters may provide some insights into the cognitive mechanisms that drive the change in strategy -for example, increased or decrease reliance on a strategy could be accompanied by a modulation in either decay or learning rates. Checking for condition-driven changes in parameter values can help us dissociate between these mechanisms. Thus, we decided to rely on the parameters from our independent models since those were better recovered (Figure <ref type="figure" target="#fig_0">3</ref>), although the differences in parameters from the hybrid model were similar. Transitioning from more biased statistics, such as those from a simplified model to a less biased model, like those partitioning variance, provides an ease of interpretation when considering various experimental conditions. T-tests give quick information about the initial decoupling of effects but suffer from inherent simplifying assumptions (Table <ref type="table" target="#tab_4">4</ref>). Multilevel models were used to handle the within-subject variation between the two conditions and provide an estimate for each model parameter across condition (conjoint, disjoint) and stage group order (disjoint-to-conjoint, conjoint-to-disjoint), utilizing a participant-level random intercept.</p><formula xml:id="formula_8">ğ‘ƒğ‘ƒğ‘ğ‘ğ‘Ÿğ‘Ÿğ‘ğ‘ğ‘‡ğ‘‡ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ğ‘Ÿğ‘Ÿ ~ ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶ğ‘‘ğ‘‘ğ‘–ğ‘–ğ‘’ğ‘’ğ‘–ğ‘–ğ¶ğ¶ğ¶ğ¶ * ğ‘†ğ‘†ğ‘’ğ‘’ğ‘ğ‘ğ¸ğ¸ğ‘’ğ‘’ + (1 | ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘’ğ‘’ğ‘†ğ‘†ğ‘’ğ‘’)<label>(9)</label></formula><p>If one of the parameters, specifically learning rate for tabular, needed a more complex variance and correlation structures for the random effects, such as compound symmetry, the 'nlme' package was used instead of 'lme4'. This can often happen when there is a negative relationship in the parameter between the two conditions that the participant is measured in.</p><p>For each of the multilevel models, the first measure of interest was the intraclass correlation (ICC). Typically, the ICC is calculated from an unconditioned model which includes only a fixed intercept and random participant intercepts. This model helps to estimate the variance attributable to differences between participants, thereby assessing the reliability of measurements within the same experimental condition across different subjects. In terms of this experiment, this coefficient would represent the correlation or consistency between the subject's conditions. Next, we compared models with and without an interaction between condition and stage order to test whether adding an interaction improved model fit. To do so, a likelihood ratio test using the 'anova' function in R was used, which follows a chi-square distribution. Last, the final statistic test used the interaction in the 'mixed' function, which should caution the interpretation of main effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transparency and Openness</head><p>This study was approved by the University of Maryland College Park IRB (ID: 1155349-32). All code and cleaned data can be accessed to reproduce all statistics, tables, and figures at https://osf.io/rp56b/. This study was not preregistered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decisions Favor Optimality in The First Disjoint Stage</head><p>Initial behavioral analysis sought to uncover participants' choice behavior from decision metrics, specifically optimal choice, delayed optimal choice, and average outcome received per trial (see Table <ref type="table" target="#tab_0">1</ref> for means and standard deviations). Optimal choice is defined as the selection of the higher reward object. Delayed optimal choice subsets trials where a participant selected a delay, and the selection was the higher valued object. Average outcome is the selected reward of the choice regardless of delayed contingency. Noteworthy, a participant choosing the optimal option more often than chance (50%) reflects learning performance about the delayed contingencies and random walk of reward. Optimal choice and delayed optimal choice were placed into a one-sample t-test for each of the four combinations of reward condition and stage (see all means and standard deviations in Table <ref type="table" target="#tab_0">1</ref>), resulting in performance for all conditions being significantly above chance, Âµ = .5, p &lt; .001. To quantify how performance varied across conditions, each variable was then placed into a linear regression with an interaction term between reward condition (disjoint, conjoint) and stage (first, second). Residual and QQ plots were all reasonable and did not merit the use of nonparametric tests. For optimal choice, there was a significant main effect of reward condition, b = .1, SE = .018, t(280) = 5.32, p &lt; .001, Ï‰p 2 = .09, a significant effect of stage, b = .04, SE = .018, t(280) = 5.18, p = .04, Ï‰p 2 = .01, and a significant interaction, b = -.1, SE = .025, t(280) = -4.05, p &lt; .001, Ï‰p 2 = .05. For delayed optimal choice, there was a significant main effect of reward condition, b = .09, SE = .018, t(280) = 4.93, p &lt; .001, Ï‰p 2 = .08, a significant main effect of stage, b = .04, SE = .018, t(280) = 2.31, p = .02, Ï‰p 2 = .01, and a significant interaction, b = -.11, SE = .026, t(280) = -4.08, p &lt; .001, Ï‰p 2 = .05.</p><p>For average outcome, there was a significant main effect of reward condition, b = .36, SE = .15, t(280) = 2.47, p = .01, Ï‰p 2 = .02, no effect of stage, p = .23, and a significant interaction, b = -.42, SE = .21, t(280) = -2.00, p &lt; .05, Ï‰p 2 = .01. Consequently, the observed order effect of stage and reward condition appears to have predominantly influenced these behavioral markers, as starting in the disjoint condition gave the highest numbers for all variables (Table <ref type="table" target="#tab_0">1</ref>). Participants performed best starting in the disjoint condition, but they also performed in the conjoint comparably to those who ended in the disjoint condition. Thus, the group that started in disjoint and ended in conjoint performed overall better than the group that started in conjoint and ended in disjoint. Note. Means and standard deviations (SD) are shown for outcome received (Outcome), proportion of choosing the optimal option (Optimal), proportion of choosing optimally when selecting between an immediate and delayed option (OptiDelay), across the two reward conditions (Disjoint and Conjoint) and current stage (1 and 2). Note that disjoint-1 and conjoint-2 represent the same group of participants (N=74) while conjoint-1 and disjoint-2 represent another group (N=70).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tabular and Eligibility Best Map onto Disjoint and Conjoint Conditions, Respectively</head><p>The logistic regression (equation 9) examines whether the reward feedback conditions (Condition: conjoint, disjoint) influenced participants' decision to stay on their delay choice three trials in the future (Stay: 1 stay, 0 switch), while accounting for the reward valence (Reward: + positive feedback, -negative feedback, = null feedback) received from current, one, and two-trials forward (Time: 0F, 1F, 2F, respectively). For example, a participant might erroneously stay on their choice three trials in the future due to a positive immediate reward but should have correctly switched due to a negative reward two-trials in the future (Figure <ref type="figure" target="#fig_1">4A</ref>). We fit three logistic regressions from equation 7: one on the participants' data, and two on the data generated from eligibility and tabular models (Figure <ref type="figure" target="#fig_1">4B</ref>).</p><p>The three-way interaction (disjoint*+*2F) was significant for all three logistic regression models (Figure <ref type="figure" target="#fig_1">4C</ref>), odds-ratios for participant, b = 1.79, SE =1.  <ref type="figure" target="#fig_1">4B</ref> -red line), the interaction was such that in the conjoint condition, the reward valence on choice (probability of stay difference between positive versus negative rewards) was strongest for immediate (0F). However, in the disjoint condition, the effect of reward valence on choice was strongest for two-trials forward (2F). Although both models were able to capture this effect, within the disjoint condition for the signal state (2F), tabular overlapped within participants' 95% CI for both negative (going below chance) and positive valence. Furthermore, this overlap suggests that the tabular model is better at predicting participants' tendency to repeat rewarded delayed choices in the disjoint rather than conjoint condition. The second posterior predictive check analysis was set to reproduce the entire choice trajectories from the models. Specifically, we first showed participants' ability to track reward in some example pairs (Figure <ref type="figure" target="#fig_2">5A</ref>), as well as how our hybrid model mimics this choice trajectory (Figure <ref type="figure" target="#fig_2">5B</ref>). Next, to quantify the models' (eligibility and tabular) trial-by-trial predictive accuracy and compare it between conditions, we ran another multilevel logistic regression (equation 10), collapsing across all random walks and pairs. We were particularly interested in understanding how the predictions made by the tabular and eligibility models corresponded to disjoint and conjoint conditions, respectively. Specifically, we anticipated that the tabular model would predict participants' choices more accurately in the disjoint condition and the eligibility model would predict participants' choices more accurately in the conjoint condition. To test this, we employed the same task sequences observed by participants and utilized identically randomly sampled parameters to generate choice probabilities for both models (refer to Methods for further details), which were included as predictors of participants choice in the regression together with condition (equation 10, Figure <ref type="figure" target="#fig_2">5C</ref>). Using the 'mixed' function in 'afex', we found a positive interaction between tabular predictions and condition, odds-ratio, b = 1.48, F(1, 140.89) = 34.57, p &lt; .001, such that the predictions of the tabular model explained participants' choices better in the disjoint compared to the conjoint condition, as hypothesized. For the interaction between eligibility predictions and condition, we found a numeric reduction in the odds ratio, b = .92, F(1, 138.96) = 4.41, p = .037, also confirming our hypothesis that eligibility predicts participants' choices more accurately in conjoint than the disjoint condition.  <ref type="formula" target="#formula_0">1</ref>) illustrating the left choice to be the immediate with starting value 4. The grey dotted line tracks when the optimality of the reward shifts from left (1.00) to equivalent (0.50) and then to right (0.00). Thus, participants in the different conditions (disjoint, conjoint) and different stages (1, 2) should follow the optimality of reward with participant data on top. Further, lighter colors belong to the same group as darker colors. B. The same analysis was performed on choice data generated by the hybrid model, using the best-fitting participant parameters. C. Fixed effects from a logistic regression predicting choice on each trial from tabular predictions, eligibility predictions, and their interaction with condition (equation 10), aligning with our hypothesis that tabular predicts behavior better in disjoint than joint condition (significant disjoint * tabular interaction), while eligibility does not (absence of disjoint * eligibility interaction). Dots and associated numbers represent the odds ratio for each effect; horizontal error bars represent 95% confidence intervals. *** p &lt; .001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Fits</head><p>In both conditions, our hybrid model showed better fits to the participants' data compared to either single strategy model (Table <ref type="table" target="#tab_2">2</ref>), suggesting that participants' behavior on this task is a mixture of eligibility and tabular learning overall. While some participants were better fitted with an independent-model eligibility strategy, an independent-model tabular strategy was the least likely to be implemented across all conditions. Participants who started in the disjoint condition had the best fit of the tabular model (in terms of likelihood and pseudo-R 2 ), which aligns with our previous statistics (Table <ref type="table" target="#tab_0">1</ref>) and hypotheses. Additionally, those who ended in the disjoint condition had similar fits to those in the conjoint condition; however, we did not perform any inferential tests to substantiate that finding. Mean and SD represent the mean standard deviations of negative loglikelihoods across participants, respectively. Pseudo-R 2 represents the proportion of variance accounted in participants' choices relative to a random model with higher values representing better fit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reinforcement Learning Model Parameters Across Conditions</head><p>Examining the correlation between parameters, we found that some of the individual model parameters were correlated with one another (Table <ref type="table" target="#tab_3">3</ref>). Noteworthy, correlations amongst the parameters in different conditions (between conjoint and disjoint) were surprisingly low but some were significant. The strongest correlations appeared in a relationship in the strategy weights between eligibility and tabular; however, this did not hold across conditions. The strategy weight also represents the degree of decision stochasticity or reward sensitivity when selecting between two choices and thus would correlate between models. Other parameters seemed to tradeoff dependent on the relationship between learning rate, strategy weight, and decay weight. Next, each of the RL hybrid model parameters were used in a combination of various statistical tests to assess whether each parameter differs between strategies (eligibility vs. tabular) and between conditions (conjoint vs. disjoint) (Table <ref type="table" target="#tab_4">4</ref>), as well as whether these effects of condition interacted with phase order (i.e. which condition was completed first) (Figure <ref type="figure">6</ref>).</p><p>First, we found that the tabular beta parameter, but not the eligibility beta parameter, varied with condition. Specifically, the weight of tabular was larger in the disjoint relative to conjoint condition, consistent with our hypothesis of increased reliance on tabular in the disjoint condition. Interestingly, the eligibility decay rate also varied with condition with more decay of the trace in the conjoint condition. Finally, the learning rate in the tabular condition appeared to have larger updates of the prediction error in disjoint, but one should be mindful in interpreting this effect given the recovery rate of this parameter. All t-tests are shown in Table <ref type="table" target="#tab_4">4</ref>. Note. Two-tailed paired t-tests were computed for individual parameters extracted from each independent model (Elg: Eligibility; or Tab: Tabular): learning rate (Alpha), decision weight (Beta) and decay rate (Lambda), comparing conjoint versus disjoint condition. The differences in proportion of optimal choices overall (Optimal) and optimal choices of delayed option in mixed choice types (OptiDelay) are also shown. df, degrees of freedom associated with each t-test; t, tstatistic, with negative numbers representing a larger mean in disjoint than conjoint condition; p, p-value, * p &lt; .05 ** p &lt; .01 *** p &lt; .001, 95% CI, 95% confidence interval; d, effect size calculated as Cohen's d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Parameter Mixed Effects Regression</head><p>For each parameter, we ran a mixed-effects linear regression predicting the parameter value from condition, stage group order, and their interaction. For beta-tabular, an unconditional model allowed an estimate of the ICC, accounting for 39.26% of the total variation between participants. The best fitting model held without an interaction term, showing a marginal significant change in deviance compared to the model without an interaction, Ï‡ 2 (1) = 3.74, p = .053. The final noninteraction model showed a significant main effect of reward condition, b = .11, SE = .03, F(1, 141) = 16.31, p &lt; .001, a significant main effect of stage group order, b = .16, SE = .04, F(1, 140) = 12.95, p &lt; .001. The effect was such that beta-tabular was higher in the disjoint than joint condition, as predicted, but an additional effect of stage order as starting in disjoint led to greater effects than starting in conjoint (Figure <ref type="figure">6A</ref>). However, slope differences between stage order groups had only marginal effects. For beta-eligibility, an unconditional model allowed an estimate of the ICC, accounting for 36.38% of the total variation between participants. The best fitting model held without an interaction term, showing a non-significant change in deviance when comparing models with and without the interaction term, Ï‡ 2 (1) = .01, p = .92. The final model showed no effect of reward condition, p = .78, but a significant effect of stage order, odds-ratio: b = .21, SE = .07, F(1, 140) = 10.86, p &lt; .001. The main effect of stage order showed overall higher beta-eligibility in participants who started in the disjoint condition compared to those who started in the conjoint condition (Figure <ref type="figure">6B</ref>).</p><p>For lambda-tabular, an unconditional model allowed an estimate of the ICC, accounting for 6.15% of the total variation between participants. The best fitting model held without an interaction term, showing a marginal significant change in deviance compared to the model without an interaction, Ï‡ 2 (1) = 3.02, p = .08. There was no significant effect for reward condition, p = .89, but there was an effect of stage order, odds-ratio: b = .1, SE = .03, F(1, 140) = 8.07, p = .005 (Figure <ref type="figure">6C</ref>). In effect, there was an increase in weighing the two-trial update when moving to the second stage, regardless of the condition. For lambda-eligibility, an unconditional model allowed an estimate of the ICC, accounting for 13.68% of the total variation between participants. The best fitting model consisted of a non-interaction model as the change in model deviance was not significant, Ï‡ 2 (1) = .01, p = .92. The non-interaction model showed a significant effect of reward condition, odds-ratio: b = .11, SE = .03, F(1, 141) = 13.15, p &lt; .001, and a non-significant effect of stage order, p = .19 (Figure <ref type="figure">6D</ref>). Participants in the disjoint condition appeared to have lower rates of discounting delayed updates as compared to conjoint, indicating the eligibility trace decayed at a slower rate.</p><p>For alpha-tabular, the fit was based on a generalized least squares model with compound symmetry, as the within-subject correlation was negative, Ï = -.05. The negative correlation indicates that the within-subject effect appears to be inversely correlated, such that when a person has a higher learning rate, they are more likely to have a lower learning rate in the next stage or vice-versa. Maximum likelihood was used to compare models, which did not support adding the interaction term, likelihood ratio test: Ï‡ 2 (1) = .08, p = .78. The non-interaction model showed a significant effect of reward condition, b = .15, SE = .03, Ï‡ 2 (1) = 18.65, p &lt; .001, and a non-significant effect of stage order, p = .17 (Figure <ref type="figure">6E</ref>). Generally, the learning rate was higher for participants in the disjoint condition as compared to the conjoint condition. For alphaeligibility, an unconditional model allowed an estimate of the ICC, accounting for 29.93% of the total variation between participants. The best fitting model consisted of a non-interaction model as the change in model deviance was not significant, Ï‡ 2 (1) = .12, p = .73. The non-interaction model showed no significant effects of reward condition, p = .21, or stage order, p = .78 (Figure <ref type="figure">6F</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 6</head><p>Effect of condition and stage on hybrid model parameters Note. A mixed-effect linear regression model was applied to each RL parameter from their yoked individual model, predicting the parameter value from condition, stage order, and their interaction (equation 9). Specifically, this regression was performed on decision weight -Betafor both tabular (A) and eligibility (B), decay rate -Lambda -for tabular (C), eligibility (D), as well as the learning rate -Alpha -across tabular (E) and eligibility (F). The results display estimated marginal means, marked with arrows pointing towards the participants' final condition (Conjoint and Disjoint) and the order of stages, namely from conjoint stage 1 to disjoint stage 2 (C â†’ D, black) and from disjoint stage 1 to conjoint stage 2 (D â†’ C, grey). Shaded areas represent 95% confidence intervals. Additionally, individual lines for each participant are shown.</p><p>The top left corner of the results highlights the p-value associated with the main effect of the reward condition (C), the main effect of the stage (S), or their interaction (C * S).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>When presented with sequentially delayed rewards, the problem of credit assignment (CA) requires a person to engage in an intuitive or strategic solution <ref type="bibr" target="#b24">(Minsky, 1961)</ref>. This question of individual solutions was posed under a novel reward learning task under the guise of two differing structural strategies of temporal CA <ref type="bibr" target="#b43">(Tanaka et al., 2009;</ref><ref type="bibr" target="#b45">Walsh &amp; Anderson, 2014)</ref>. To achieve this objective, we manipulated the degree of information in feedback presentation which modulated uncertainty through a partially and fully observable reward function. The eligibility trace, a viable and versatile solution for unobservable environments, was contrasted to our tabular model which differentiated credit assignment on the dimension of time <ref type="bibr" target="#b43">(Tanaka et al., 2009;</ref><ref type="bibr" target="#b44">Walsh &amp; Anderson, 2011)</ref>. The eligibility trace updates with a single prediction error that decays signal and error credit towards the sequence of past actions.</p><p>Contrasted to that, the tabular model employs two distinct prediction errors along the immediate and two-trial back timing but collapses across the time dimension rather than appropriately differentiating actions. Predictively, the learning efficiency differ between the two strategies but either strategy can fully observe the reward function over time; and consequently, we implemented random reward walks to prevent full learning <ref type="bibr" target="#b43">(Tanaka et al., 2009)</ref>. We predicted that in the disjoint condition, which made immediate versus delayed reward information available, the tabular strategy would have greater utilization rates as to make use of that information, while in the conjoint condition, where no such detailed information was provided, eligibility trace may be defaulted to.</p><p>Our findings were consistent with this prediction across multiple analyses. First, the tabular model was found to capture clear patterns of behavior, specifically the tendency to repeat the choice of a delayed reward option in the disjoint versus joint condition, which the eligibility model did not capture. Second, predictions of the tabular model explained participants' choices better than eligibility model predictions in the disjoint condition, while the opposite was found in the conjoint condition. Third, parameters from the tabular and eligibility models provided more insights into the specific mechanisms deployed by participants to adapt to the change in uncertainty about delayed rewards. Regarding these mechanisms, the strategy weight of tabular was overall higher during the disjoint condition as compared to conjoint with an additional increase when starting in the disjoint condition. The decay rate only held differences between the stage order for each group, such that starting in the disjoint condition led to larger updates of the two-trial back option overall compared to starting in the conjoint condition. For eligibility, the effect of condition was only found in the decay rate, such that rates of CA were longer in the disjoint condition; whereas strategy weight remained constant across conditions was higher in the group of participants who started in the disjoint condition.</p><p>These effects are not without certain caveats for the parameter mechanisms. Despite the effect of condition on the tabular learning rate, we choose not to interpret this effect due to poor recovery of this parameter. We decided to interpret our inverse temperature parameters as strategy weights, but we note that they may also be confounded with an index of choice stochasticity. For reinforcement learning, these temperature parameters can further viewed under different paradigms, such as explore-exploit, stochastic-deterministic choice, or sensitivity to value differences <ref type="bibr" target="#b7">(Eckstein et al., 2022;</ref><ref type="bibr" target="#b22">Luce, 2005;</ref><ref type="bibr" target="#b40">Sutton &amp; Barto, 2018)</ref>. In our data, it is possible that the generally higher beta parameters in participants who started in the disjoint condition (compared to those who started in the conjoint condition), as shown by a main effect of stage order group on both beta tabular and beta eligibility, might reflect higher choice consistency (less stochasticity) in this group of participants. Generally, the order effects we observed for some of the parameters could reflect boredom with the task. That is, when starting in the conjoint condition, people were primed with more uncertainty that led to less effortful answers in the second stage. Comparatively, starting in the disjoint condition could result in better understanding of the task structure as to observe the distinction between immediate and delayed rewards.</p><p>The eligibility trace strategy weight did not substantially change between conditions, which may highlight its suitability across varying uncertainty. The eligibility trace mechanism has shown promise in a variety of tasks, but can become a suboptimal approach when considering an experimental task that contains randomly related events in the time horizon <ref type="bibr" target="#b4">(Daw et al., 2011;</ref><ref type="bibr" target="#b11">GlÃ¤scher et al., 2010;</ref><ref type="bibr" target="#b21">Lehmann et al., 2019;</ref><ref type="bibr" target="#b44">Walsh &amp; Anderson, 2011)</ref>.</p><p>Furthermore, the eligibility trace solution might have been warranted due to Tanaka et al.</p><p>(2009)'s partially observable feedback presentation and constant stimulus-outcome association.</p><p>Indeed, over time agents would fully observe the reward function rather than rely on cognitively dissociating the feedback. Often the case, cues help participants maximize reward over long time horizons rather than only focusing on options that are immediately reinforcing <ref type="bibr" target="#b13">(Gureckis &amp; Love, 2009;</ref><ref type="bibr" target="#b45">Walsh &amp; Anderson, 2014)</ref>. However, in the absence of cues, participants need to rely on other avenues of information. One such possibility is the reward signal itself <ref type="bibr" target="#b5">(Dayan, 2009)</ref>.</p><p>Human cognition can handle challenges such as sparse rewards, partially observable states, and long-term consequences, even with limited experience <ref type="bibr" target="#b4">(Daw et al., 2011;</ref><ref type="bibr" target="#b8">Gershman &amp; Daw, 2017;</ref><ref type="bibr" target="#b28">Nguyen et al., 2023)</ref>. However, as the complexity of these environments increases, our understanding of effective strategies to navigate uncertainty remains limited <ref type="bibr" target="#b8">(Gershman &amp; Daw, 2017)</ref>. Research to date has shown how learning the contingencies in a transition matrix, then leads to delayed feedback being properly credited to the necessary state <ref type="bibr" target="#b11">(GlÃ¤scher et al., 2010;</ref><ref type="bibr" target="#b21">Lehmann et al., 2019;</ref><ref type="bibr" target="#b26">Moran et al., 2019;</ref><ref type="bibr" target="#b44">Walsh &amp; Anderson, 2011)</ref>.</p><p>However, consider a multistep environment with fixed transitions and where the uncertainty is represented in a partially observable reward function. Solutions that consider delayed feedback often entail embedding a TD algorithm with an exponentially decaying eligibility trace but can either make use of an explicit transition matrix or not. The individual dilemma of determining the correct weight mixture of tracing and explicit transitions can be difficult; indeed, incorporating additional information can afford larger rewards at the cost of computation and efficiency <ref type="bibr" target="#b15">(Hastie et al., 2009;</ref><ref type="bibr" target="#b29">Niv et al., 2015;</ref><ref type="bibr" target="#b46">Watkins, 1989)</ref>. In other words, balancing opportunity costs requires an algorithmic tradeoff between resources and optimality, where prioritizing resources may lead to biased predictions about future rewards in an effort to minimize cognitive effort <ref type="bibr" target="#b19">(Kim et al., 2021;</ref><ref type="bibr" target="#b46">Watkins, 1989)</ref>. How individuals differently adapt their learning strategy may lead to accepting inefficiencies to maximize value. In line with this reasoning, complexity is not always a merited strategy and could lead to a reduction in accuracy when additional information is irrelevant <ref type="bibr" target="#b12">(Glaze et al., 2018)</ref>. Thus, information is vital when appropriating the correct amount of complexity to an environmental problem.</p><p>Another popular tradeoff has been termed between MF and MB learners. However, the MF and MB distinction is carefully tied to a probabilistic stage transitions that can tease apart the influence of these computational systems <ref type="bibr" target="#b4">(Daw et al., 2011)</ref> and their variation with uncertainty in the environment <ref type="bibr" target="#b20">(Lee et al., 2014)</ref>. Recent research has made significant progress in distinguishing between MF and MB approaches to CA, revealing a dynamic interplay between retrospective and prospective processes <ref type="bibr" target="#b6">(Deserno et al., 2021;</ref><ref type="bibr" target="#b9">Gershman et al., 2014;</ref><ref type="bibr">Moran et al., 2019</ref><ref type="bibr">Moran et al., , 2021;;</ref><ref type="bibr" target="#b34">Shahar et al., 2019</ref><ref type="bibr" target="#b33">Shahar et al., , 2021))</ref>. This emerging body of work highlights how information that updates the current model can lead to retrospectively reassigning credit based on revealed structural information. However, when the state-transitions are randomly related to one another like in this task or in real world events that are independent from one another, then the structural solution is not entirely obvious. Regarding this, <ref type="bibr" target="#b43">Tanaka et al.'s (2009)</ref> task is difficult to solve prospectively as intervening test items continue to disrupt working memory and displace the temporal contiguity between feedback and choice. Uncertainty is also represented in the reward function regarding the separation of feedback, where both sources of reward would be summed together. How participants are solving this task remains a conundrum, where our twocondition system -one with disjoint reward and one conjoint -helped demonstrate an increased propensity to engage a prospective system given disjoint feedback.</p><p>The current strategies implemented in our eligibility and tabular models are restricted to mechanisms involved in RL. However, taking models from other paradigms, such as those of memory and temporal contiguity might shed light into the diversity of strategies involved <ref type="bibr" target="#b35">(Shanks et al., 1989)</ref>. These models rely on episodic memory which is often encoded into a buffer system that can incorporate several chunks based on individual differences. Another common model type involves temporal discounting which is used to value different stimuli dependent on everyone's rate of discounting. However, these model types have predominantly been applied to hypothetical future rewards rather than learning value in an experiential format <ref type="bibr" target="#b2">(Bickel et al., 2011;</ref><ref type="bibr" target="#b16">Horan et al., 2017)</ref>. Future studies might look into modifying these models for preservation, working memory, or learning rate asymmetry to potentially capture more of the decision variance <ref type="bibr" target="#b3">(Collins &amp; Frank, 2012;</ref><ref type="bibr" target="#b30">Niv et al., 2005;</ref><ref type="bibr" target="#b38">Sugawara &amp; Katahira, 2021)</ref>. <ref type="bibr" target="#b3">Collins and Frank (2012)</ref> hybrid working memory and RL model, value modifications involving n-back models <ref type="bibr" target="#b14">(Harbison et al., 2011)</ref>, or a novel implementation of the successor representation <ref type="bibr" target="#b10">(Gershman et al., 2012)</ref> could be potential avenues for further discovery on the variety of individual strategies in overcoming CA in sequentially delayed rewards.</p><p>In summary, CA is a nontrivial problem and the mechanisms implemented in human solutions remain elusive. Despite the complex and demanding nature of the task, participants were able to overcome shifting rewards in a delayed repeat decision task with intervening events.</p><p>As such, we manipulated the manner of feedback to incentivize participants to increase prospective processes in hope of characterizing differential strategy use related to information uncertainty. To this, we found evidence of increased weight of a decision strategy that was more efficient when additional information was provided and dependent on the order participants experienced the task. Thus, participants appear to choose to utilize this information or forgo it dependent on the observability of uncertain states. We hope that further investigations will examine new avenues in this experimental design and computational modeling decomposition of individual credit assignment strategies, as well as individual differences in the implementations of these strategies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3</head><label>3</label><figDesc>Figure 3</figDesc><graphic coords="16,72.00,127.20,451.50,451.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4</head><label>4</label><figDesc>Figure 4</figDesc><graphic coords="24,72.00,403.17,468.00,263.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 Model</head><label>5</label><figDesc>Figure 5</figDesc><graphic coords="26,72.00,182.39,440.25,440.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="10,72.00,127.20,468.00,263.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="11,72.00,458.37,468.00,263.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Descriptive statistics for behavioral performance</figDesc><table><row><cell>Disjoint-1</cell><cell>Mean</cell><cell>SD</cell><cell>Disjoint-2</cell><cell>Mean</cell><cell>SD</cell></row><row><cell>Outcome</cell><cell>0.85</cell><cell>3.18</cell><cell>Outcome</cell><cell>0.61</cell><cell>3.25</cell></row><row><cell>Optimal</cell><cell>0.7</cell><cell>0.46</cell><cell>Optimal</cell><cell>0.65</cell><cell>0.48</cell></row><row><cell>OptiDelay</cell><cell>0.82</cell><cell>0.39</cell><cell>OptiDelay</cell><cell>0.78</cell><cell>0.41</cell></row><row><cell>Conjoint-1</cell><cell>Mean</cell><cell>SD</cell><cell>Conjoint -2</cell><cell>Mean</cell><cell>SD</cell></row><row><cell>Outcome</cell><cell>0.48</cell><cell>3.17</cell><cell>Outcome</cell><cell>0.66</cell><cell>3.12</cell></row><row><cell>Optimal</cell><cell>0.61</cell><cell>0.49</cell><cell>Optimal</cell><cell>0.65</cell><cell>0.48</cell></row><row><cell>OptiDelay</cell><cell>0.77</cell><cell>0.42</cell><cell>OptiDelay</cell><cell>0.8</cell><cell>0.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>.39, 2.3]; eligibility, b = 1.19, SE =1.08, Z = 2.33, p = .02, 95% CI [1.03, 1.37]; tabular, b = 1.38, SE =1.08, Z = 4.34, p &lt; .001, 95% CI [1.19, 1.59]. For participants' data (Figure</figDesc><table /><note><p>14, Z = 4.55, p &lt; .001, 95% CI [1</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>Model-fitting metrics Model-fitting metrics for each model across reward condition and stage. Percentages represent the proportion of individuals best fit by each model based on negative loglikelihoods.</figDesc><table><row><cell>Disjoint-1</cell><cell>Eligibility</cell><cell>Tabular</cell><cell>Hybrid</cell></row><row><cell>Percent</cell><cell>18.92%</cell><cell>2.7%</cell><cell>78.38%</cell></row><row><cell>Mean</cell><cell>177</cell><cell>175.76</cell><cell>165.26</cell></row><row><cell>SD</cell><cell>29.19</cell><cell>37.19</cell><cell>38.33</cell></row><row><cell>Pseudo-R 2</cell><cell>.24</cell><cell>.245</cell><cell>.29</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>Correlation between independent model parameters</figDesc><table><row><cell>Parameter</cell><cell>BetaTab</cell><cell>BetaElg</cell><cell cols="3">LambdaTab LambdaElg AlphaTab</cell><cell>AlphaElg</cell></row><row><cell>BetaTab</cell><cell>.45**</cell><cell>.82**</cell><cell>.21**</cell><cell>.27**</cell><cell>.04</cell><cell>-.24**</cell></row><row><cell>BetaElg</cell><cell>.81**</cell><cell>.37**</cell><cell>.12</cell><cell>.13</cell><cell>-.1</cell><cell>-.4**</cell></row><row><cell>LambdaTab</cell><cell>.11</cell><cell>.03</cell><cell>.06</cell><cell>.09</cell><cell>-.06</cell><cell>.02</cell></row><row><cell>LambdaElg</cell><cell>.26**</cell><cell>.07</cell><cell>.06</cell><cell>.18**</cell><cell>.06</cell><cell>.03</cell></row><row><cell>AlphaTab</cell><cell>-.33*</cell><cell>-.4**</cell><cell>.02</cell><cell>-.01*</cell><cell>-.04</cell><cell>.25**</cell></row><row><cell>AlphaElg</cell><cell>-.43**</cell><cell>-.44**</cell><cell>-.08</cell><cell>.02</cell><cell>.62**</cell><cell>.32**</cell></row><row><cell cols="7">Note. Diagonals are correlations between disjoint-conjoint, below are conjoint-conjoint</cell></row><row><cell cols="6">correlations and above are disjoint-disjoint correlations. * p &lt; .05, ** p &lt; .01</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc>Differences in model parameters by condition, for each strategy</figDesc><table><row><cell>Variable</cell><cell>df</cell><cell>t</cell><cell>p</cell><cell>95% CI</cell><cell>d</cell></row><row><cell>AlphaElg</cell><cell>141</cell><cell>1.26</cell><cell>.211</cell><cell>[-.02, .08]</cell><cell>.11</cell></row><row><cell>AlphaTab</cell><cell>141</cell><cell>-4.32</cell><cell>&lt;.001**</cell><cell>[-.21, -.08]</cell><cell>-.36</cell></row><row><cell>BetaElg</cell><cell>141</cell><cell>.28</cell><cell>.78</cell><cell>[-.08, .1]</cell><cell>.02</cell></row><row><cell>BetaTab</cell><cell>141</cell><cell>-4.04</cell><cell>&lt;.001***</cell><cell>[-.18, -.06]</cell><cell>-.34</cell></row><row><cell>LambdaElg</cell><cell>141</cell><cell>-3.63</cell><cell>&lt;.001**</cell><cell>[-.18, -.05]</cell><cell>-.3</cell></row><row><cell>LambdaTab</cell><cell>141</cell><cell>.14</cell><cell>.89</cell><cell>[-.06, .07]</cell><cell>.01</cell></row><row><cell>Optimal</cell><cell>141</cell><cell>-4.53</cell><cell>&lt;.001***</cell><cell>[-.06, -.03]</cell><cell>-.38</cell></row><row><cell>OptiDelay</cell><cell>141</cell><cell>-1.77</cell><cell>.08</cell><cell>[-.04, .00]</cell><cell>-.15</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was supported by the <rs type="funder">University of Maryland</rs> start-up funds to AS and CJC. CJC is also supported by a <rs type="funder">National Institute of Mental Health</rs> <rs type="grantNumber">R00</rs> award (<rs type="grantNumber">R00MH123669</rs>). A special thanks to <rs type="person">Catherine Hartley</rs> and <rs type="person">Gail Rosenbaum</rs> for their invaluable support and guidance during the initial development of this topic and methods.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_YhaU4bh">
					<idno type="grant-number">R00</idno>
				</org>
				<org type="funding" xml:id="_mNAZnTD">
					<idno type="grant-number">R00MH123669</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Unifying temporal and structural credit assignment problems</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Agogino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tumer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
		<respStmt>
			<orgName>Autonomous Agents and Multi-Agent Systems Conference</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The effect of cognitive challenge on delay discounting</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Aranovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mcclure</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fryer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Mathalon</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuroimage.2015.09.027</idno>
		<ptr target="https://doi.org/10.1016/j.neuroimage.2015.09.027" />
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="page" from="733" to="739" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Remember the Future: Working Memory Training Decreases Delay Discounting Among Stimulant Addicts</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">K</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Landes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Baxter</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.biopsych.2010.08.017</idno>
		<ptr target="https://doi.org/10.1016/j.biopsych.2010.08.017" />
	</analytic>
	<monogr>
		<title level="j">Biological Psychiatry</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="260" to="265" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">How much of reinforcement learning is working memory, not reinforcement learning? A behavioral, computational, and neurogenetic analysis: Working memory in reinforcement learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G E</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Frank</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1460-9568.2011.07980.x</idno>
		<ptr target="https://doi.org/10.1111/j.1460-9568.2011.07980.x" />
	</analytic>
	<monogr>
		<title level="j">European Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1024" to="1035" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Model-Based Influences on Humans&apos; Choices and Striatal Prediction Errors</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Daw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Seymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Dolan</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuron.2011.02.027</idno>
		<ptr target="https://doi.org/10.1016/j.neuron.2011.02.027" />
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1204" to="1215" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Prospective and retrospective temporal difference learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<idno type="DOI">10.1080/09548980902759086</idno>
		<ptr target="https://doi.org/10.1080/09548980902759086" />
	</analytic>
	<monogr>
		<title level="j">Network: Computation in Neural Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="46" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Dopamine enhances model-free credit assignment through boosting of retrospective model-based inference</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deserno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Michely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Dolan</surname></persName>
		</author>
		<idno type="DOI">10.7554/eLife.67778</idno>
		<ptr target="https://doi.org/10.7554/eLife.67778" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10" to="e67778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The interpretation of computational model parameters depends on the context. eLife</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Eckstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Master</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wilbrecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Collins</surname></persName>
		</author>
		<idno type="DOI">10.7554/eLife.75474</idno>
		<ptr target="https://doi.org/10.7554/eLife.75474" />
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="11" to="e75474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Reinforcement Learning and Episodic Memory in Humans and Animals: An Integrative Framework</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Daw</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev-psych-122414-033625</idno>
		<ptr target="https://doi.org/10.1146/annurev-psych-122414-033625" />
	</analytic>
	<monogr>
		<title level="j">Annual Review of Psychology</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="101" to="128" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Retrospective revaluation in sequential decision making: A tale of two systems</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Markman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Otto</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0030844</idno>
		<ptr target="https://doi.org/10.1037/a0030844" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="182" to="194" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The Successor Representation and Temporal Context</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Todd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Sederberg</surname></persName>
		</author>
		<idno type="DOI">10.1162/NECO_a_00282</idno>
		<ptr target="https://doi.org/10.1162/NECO_a_00282" />
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1553" to="1568" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<author>
			<persName><forename type="first">J</forename><surname>GlÃ¤scher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Daw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Doherty</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuron.2010.04.016</idno>
		<ptr target="https://doi.org/10.1016/j.neuron.2010.04.016" />
	</analytic>
	<monogr>
		<title level="m">States versus Rewards: Dissociable Neural Prediction Error Signals Underlying Model-Based and Model-Free Reinforcement Learning</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="585" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A bias-variance trade-off governs individual differences in on-line learning in an unpredictable environment</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Glaze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L S</forename><surname>Filipowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Kable</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">I</forename><surname>Gold</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41562-018-0297-4</idno>
		<ptr target="https://doi.org/10.1038/s41562-018-0297-4" />
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="213" to="224" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Short-term gains, long-term pains: How cues about state aid learning in dynamic environments</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Gureckis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Love</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cognition.2009.03.013</idno>
		<ptr target="https://doi.org/10.1016/j.cognition.2009.03.013" />
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="293" to="313" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Harbison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Atkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Dougherty</surname></persName>
		</author>
		<title level="m">N-back training task performance: Analysis and model</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<title level="m">The elements of statistical learning: Data mining, inference, and prediction</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Altered experiential, but not hypothetical, delay discounting in schizophrenia</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Horan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Green</surname></persName>
		</author>
		<idno type="DOI">10.1037/abn0000249</idno>
		<ptr target="https://doi.org/10.1037/abn0000249" />
	</analytic>
	<monogr>
		<title level="j">Journal of Abnormal Psychology</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="301" to="311" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Working memory, attention control, and the n-back task: A question of construct validity</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R A</forename><surname>Conway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Miura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J H</forename><surname>Colflesh</surname></persName>
		</author>
		<idno type="DOI">10.1037/0278-7393.33.3.615</idno>
		<ptr target="https://doi.org/10.1037/0278-7393.33.3.615" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="615" to="622" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Bias-Variance Error Bounds for Temporal Difference Updates</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Prefrontal solution to the bias-variance tradeoff during reinforcement learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.celrep.2021.110185</idno>
		<ptr target="https://doi.org/10.1016/j.celrep.2021.110185" />
	</analytic>
	<monogr>
		<title level="j">Cell Reports</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page">110185</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shimojo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Doherty</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuron.2013.11.028</idno>
		<ptr target="https://doi.org/10.1016/j.neuron.2013.11.028" />
	</analytic>
	<monogr>
		<title level="m">Neural Computations Underlying Arbitration between Model-Based and Model-free Learning</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="687" to="699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">One-shot learning and behavioral eligibility traces in sequential decision making</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Liakoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Herzog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gerstner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Preuschoff</surname></persName>
		</author>
		<idno type="DOI">10.7554/eLife.47463</idno>
		<ptr target="https://doi.org/10.7554/eLife.47463" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8" to="e47463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Individual choice behavior: A theoretical analysis</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Luce</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>Courier Corporation</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Evaluating significance in linear mixed-effects models in R</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Luke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="1494" to="1502" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Steps toward Artificial Intelligence</title>
		<author>
			<persName><forename type="first">M</forename><surname>Minsky</surname></persName>
		</author>
		<idno type="DOI">10.1109/JRPROC.1961.287775</idno>
		<ptr target="https://doi.org/10.1109/JRPROC.1961.287775" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IRE</title>
		<meeting>the IRE</meeting>
		<imprint>
			<date type="published" when="1961">1961</date>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="8" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Human subjects exploit a cognitive map for credit assignment</title>
		<author>
			<persName><forename type="first">R</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Dolan</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.2016884118</idno>
		<ptr target="https://doi.org/10.1073/pnas.2016884118" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2021">2021. 2016884118</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Retrospective model-based inference guides model-free credit assignment</title>
		<author>
			<persName><forename type="first">R</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Keramati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Dolan</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41467-019-08662-8</idno>
		<ptr target="https://doi.org/10.1038/s41467-019-08662-8" />
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">DEoptim: An R Package for Global Optimization by Differential Evolution</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Mullen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ardia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Gil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Windover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cline</surname></persName>
		</author>
		<idno type="DOI">10.18637/jss.v040.i06</idno>
		<ptr target="https://doi.org/10.18637/jss.v040.i06" />
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gonzalez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.08171</idno>
		<ptr target="http://arxiv.org/abs/2307.08171" />
		<title level="m">Credit Assignment: Challenges and Opportunities in Developing Human-like AI Agents</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Reinforcement Learning in Multidimensional Environments Relies on Attention Mechanisms</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Niv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Leong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radulescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="DOI">10.1523/JNEUROSCI.2978-14.2015</idno>
		<ptr target="https://doi.org/10.1523/JNEUROSCI.2978-14.2015" />
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page" from="8145" to="8157" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dopamine, uncertainty and TD learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Niv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Duff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<idno type="DOI">10.1186/1744-9081-1-6</idno>
		<ptr target="https://doi.org/10.1186/1744-9081-1-6" />
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Functions</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Building Experiments in PsychoPy (Version 3) [Computer software</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hirst</surname></persName>
		</author>
		<author>
			<persName><surname>Macaskill</surname></persName>
		</author>
		<author>
			<persName><surname>Michael</surname></persName>
		</author>
		<ptr target="https://uk.sagepub.com/en-gb/eur/building-experiments-in-psychopy/book273700" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A theory of Pavlovian conditioning: Variations in the effectiveness of reinforcement and nonreinforcement</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Rescorla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Classical Conditioning II</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Appleton-Century-Crofts</publisher>
			<date type="published" when="1972">1972</date>
			<biblScope unit="page" from="64" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Assigning the right credit to the wrong action: Compulsivity in the general population is associated with augmented outcomeirrelevant value-based learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">U</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moutoussis</surname></persName>
		</author>
		<author>
			<persName><surname>Nspn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bullmore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fonagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moutoussis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Neufeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Romero-Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Clair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>VÃ©rtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Whitaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Dolan</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41398-021-01642-x</idno>
		<ptr target="https://doi.org/10.1038/s41398-021-01642-x" />
	</analytic>
	<monogr>
		<title level="j">Translational Psychiatry</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">564</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>NSPN (funded) staff</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Credit assignment to state-independent task representations and its relationship with model-based decision making</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">U</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Kievit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moutoussis</surname></persName>
		</author>
		<author>
			<persName><surname>Nspn Consortium</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Dolan</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1821647116</idno>
		<ptr target="https://doi.org/10.1073/pnas.1821647116" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Proceedings of the National Academy of Sciences</publisher>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="15871" to="15876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Temporal contiguity and the judgement of causality by human subjects</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Shanks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Pearson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dickinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Quarterly Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="139" to="159" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Reinforcement learning with replacing eligibility traces</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="123" to="158" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Simulating future value in intertemporal choice</title>
		<author>
			<persName><forename type="first">A</forename><surname>Solway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lohrenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Montague</surname></persName>
		</author>
		<idno type="DOI">10.1038/srep43119</idno>
		<ptr target="https://doi.org/10.1038/srep43119" />
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">43119</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dissociation between asymmetric value updating and perseverance in human reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sugawara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Katahira</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41598-020-80593-7</idno>
		<ptr target="https://doi.org/10.1038/s41598-020-80593-7" />
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3574</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Temporal credit assignment in reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts Amherst</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<title level="m">Reinforcement learning: An introduction</title>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Second edition</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0004-3702(99)00052-1</idno>
		<ptr target="https://doi.org/10.1016/S0004-3702(99)00052-1" />
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="181" to="211" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The impact of depressed mood, working memory capacity, and priming on delay discounting</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Szuhany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mackenzie</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Otto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Behavior Therapy and Experimental Psychiatry</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="37" to="41" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Serotonin Affects Association of Aversive Outcomes to Past Actions</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shishida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Schweighofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Okamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yamawaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Doya</surname></persName>
		</author>
		<idno type="DOI">10.1523/JNEUROSCI.2799-09.2009</idno>
		<ptr target="https://doi.org/10.1523/JNEUROSCI.2799-09.2009" />
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">50</biblScope>
			<biblScope unit="page" from="15669" to="15674" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning from delayed feedback: Neural responses in temporal credit assignment</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13415-011-0027-0</idno>
		<ptr target="https://doi.org/10.3758/s13415-011-0027-0" />
	</analytic>
	<monogr>
		<title level="j">Cognitive, Affective, &amp; Behavioral Neuroscience</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="131" to="143" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Navigating complex decision spaces: Problems and paradigms in sequential choice</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0033455</idno>
		<ptr target="https://doi.org/10.1037/a0033455" />
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="466" to="486" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Learning from delayed rewards</title>
		<author>
			<persName><forename type="first">C</forename><surname>Watkins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
