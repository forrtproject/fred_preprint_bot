<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AI Should Develop Human Empathy, Not Replace It</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-10-02">2 October 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Ethan</forename><surname>Landes</surname></persName>
							<email>e.landes@kent.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Kent</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jim</forename><forename type="middle">A C</forename><surname>Everett</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Kent</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">AI Should Develop Human Empathy, Not Replace It</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-10-02">2 October 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">CEA256837B75B545119C05F9EA30ACE6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-22T20:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Artificial Moral Enhancement</term>
					<term>Empathy Training</term>
					<term>Virtue Ethics</term>
					<term>Artificial Empathy</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Empathizing with another is perhaps the most human emotion of all and a long shot from the cold calculations of vectorized tokens that drive the inner workings of contemporary LLMs. This chapter looks beyond the questions of what to make of AI-generated "empathy" and instead asks whether AI can be used to develop our capacities for genuine human empathy. Empathy is not a static trait, instead capable of growth and development, and this paper explores whether AI can and, more importantly, should be used to increase one's empathy. Nothing in principle stands in the way of AI improving our empathy, but the possibility raises unanswered questions about whether such an approach would be effective or would backfire in unexpected ways, such as encouraging the commodification of empathy as a technological tool that can make companies money, rather than a fundamental part of the human experience.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>3 Empathy -the ability to understand and share the feelings of another person -is in many ways at the heart of our social lives. Our feelings of empathy are thought to help us connect with others, build trust, and form meaningful relationships <ref type="bibr">(Decety &amp; Ickes, 2009;</ref><ref type="bibr" target="#b24">Waal, 2008)</ref>. It is empathy that allows us to respond with care and understanding, and it is empathy that can often reduce conflict and foster cooperation <ref type="bibr" target="#b23">(Vanman, 2016)</ref>. Empathy is clearly fundamental. But in an increasingly AI-mediated world where people not only interact with each other through technologies like smartphones and computers but also interact more and more with non-human "agents" powered by LLMs (large language models), how should we think about empathy? Here, in contrast to discussions that have tended to dominate the literature, we distinguish three sorts of empathy-involving relationships between humans and AI.</p><p>First, we might empathize with AI, where AI is the object of human empathy. Insofar as this is a genuine mental state of a human involving the (perceived or imagined) mental states of something else, empathy directed towards AI is genuine empathy. One might worry empathy is misplaced when directed towards AI and that our limited empathetic resources should instead be directed towards humans. However, as anyone who has cried at the end of the movies Iron Giant, Big Hero Six, or Terminator 2 knows, not only can our feelings towards robots be very real, but we regularly empathize with things like fictional entities that lack any genuine mental life to empathize with. One might also worry that the amount of anthropomorphization required to empathize with AI is fundamentally damaging to human users, elevating the risk of alienation, false beliefs, gaslighting, or even psychosis <ref type="bibr" target="#b7">(Kleinman, 2025;</ref><ref type="bibr" target="#b15">Ã˜stergaard, 2023)</ref>, but the mental state of empathy is nonetheless occurring. Therefore, even if, normatively, one believes AI is not the sort of thing that we should empathize with, descriptively, AI can certainly be the target of empathy.</p><p>Second, AI may "empathize" with us, where a human is the object of the empathy <ref type="bibr" target="#b6">(Inzlicht et al., 2024)</ref>. This is fake empathy, as contemporary forms of AI neither understand nor share our feelings <ref type="bibr" target="#b16">(Perry, 2023;</ref><ref type="bibr" target="#b20">Shteynberg et al., 2024)</ref>. When an LLM chatbot tells us they are sorry for our situation, their output is merely the result of their underlying transformers recursively predicting the next word in a sequence based on statistical representations of their training data.</p><p>Even if these outputs are interpreted as warm and moving <ref type="bibr" target="#b0">(Ayers et al., 2023;</ref><ref type="bibr" target="#b25">Wenger et al., 2025)</ref>, they are unreal pastiches of the genuine empathy felt by humans <ref type="bibr" target="#b22">(Vallor, 2024)</ref>.. While much discussion in the recent literature has focused on the distinction between real and fake empathy and the potential for AI as a provider of empathy, there is a further third possible relationship between humans, empathy, and AI: AI may help users develop their own capacity for empathy. Empathy can be developed, enhanced, and trained <ref type="bibr" target="#b8">(Lam et al., 2011;</ref><ref type="bibr" target="#b21">Teding van Berkhout &amp; Malouff, 2016;</ref><ref type="bibr" target="#b26">Wu et al., 2024)</ref>, and AI might be able to aid in this process. Rather than a specific moment, thought, or emotion directed at a specific person or thing, here, the involvement of AI serves to develop the moral motivations, skills, or sensitivities of the human user. While the AI's outputs are phony, the resulting empathy is not. If used correctly, AI could, at least in theory, inspire the human user to improve their moral motivations and develop moral insight by inspiring moral growth in users <ref type="bibr" target="#b9">(Landes et al., 2025)</ref>. AI could serve as a sort of moral scaffolding, inspiring empathy-developing reflection and thought on behalf of the users, resulting in genuine empathy directed at things other than AI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Could AI be effective at developing our empathy?</head><p>Even if empathy can be trained, undirected LLM use is extraordinarily unlikely to improve empathy in users. Moral growth from LLM use likely requires directed and effortful engagement <ref type="bibr" target="#b9">(Landes et al., 2025)</ref>, and LLMs appear to decrease the amount of effort users spend on tasks <ref type="bibr" target="#b11">(Lee et al., 2025)</ref>.</p><p>Rather than giving people access to AI and hoping it improves their empathy, a more effective approach is likely to be one in which AI is used as a tool in part of procedures known to improve empathy. For example, human-based simulations have been found to successfully improve empathy, where trainees either take the role of the person being empathized with <ref type="bibr" target="#b10">(Larti et al., 2018)</ref> or practice interacting with the target of empathy as themselves <ref type="bibr" target="#b5">(Gholamzadeh et al., 2018)</ref>. LLMs may be able to augment in-person training by playing either the subject or object of empathy in text-based role-playing exercises with the trainee, with hidden prompts to hit certain beats or emphasize certain topics. Similarly, other empathy training protocols augment lectures and forms of online training with discussion boards or writing tasks <ref type="bibr" target="#b13">(Mueller et al., 2018;</ref><ref type="bibr" target="#b19">Shapiro et al., 2006)</ref>. Here, LLMs may be incorporated to encourage further reflection on discussion board posts, generating follow-up questions that tie together aspects of participants' posts and key themes preselected by the course designer. This could potentially fruitfully be combined with other approaches, such as simulation approaches, that employ reflective cycles.</p><p>Deploying LLMs in these, or even more sophisticated ways, is well within the scope of contemporary technology. The growing sophistication of LLMs -particularly increases in context windows (the LLM counterpart to human working memory) -mean that LLMs can be prompted to follow sophisticated training regimens. These include personalizing responses to users' demographics, carrying forward information from earlier interactions with users', and following procedures with specific checkpoints or triggers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Would AI be effective at developing our empathy?</head><p>There are reasons to be optimistic that AI could be used as a tool to enhance empathy. Many empathy-increasing practices include steps that work with the primarily text-based medium of contemporary AIs. Moreover, the current body of research has found that AI, if anything, is more capable than humans at producing moving and apparently empathetic text-based responses <ref type="bibr" target="#b0">(Ayers et al., 2023;</ref><ref type="bibr" target="#b25">Wenger et al., 2025)</ref>. To the extent that empathy training works to improve someone's ability to communicate compassion or perspective-taking via language, we could be optimistic about the role of LLMs as trainers.</p><p>There are also, however, reasons to be pessimistic about AI's ability to be a tool for improving human empathy. First, People trust AI less than human decision-makers in a phenomenon known as "algorithm aversion" <ref type="bibr" target="#b3">(Dietvorst et al., 2015)</ref>. Even if AI can generate more apparently moving empathetic statements when people do not know the source, when people know they are generated by AI (which, ethically, they must), they show resistance <ref type="bibr" target="#b25">(Wenger et al., 2025)</ref>. Therefore, humans using AI-driven empathy training tools may, regardless of the quality of AI outputs, fail to engage with the training to the extent they would with other humans. That said, in some limited cases, people can trust AI more than humans <ref type="bibr" target="#b12">(Logg et al., 2019)</ref>, and in studies of algorithmic aversion in the moral domain trust in AI is not zero, just comparatively less than humans (e.g., <ref type="bibr" target="#b14">Myers &amp; Everett, 2025)</ref>. Therefore, even if algorithmic aversion prevents engagement with AI-driven training, it will likely not completely eliminate it.</p><p>LLMs may additionally struggle to be an effective tool of developing human empathy because of the primarily text-based medium of modern LLMs. Text-based exchanges with LLMs lack depth and miss important elements like facial expressions and vocal tone that are central to human interactions <ref type="bibr" target="#b1">(Crockett, 2025;</ref><ref type="bibr" target="#b16">Perry, 2023)</ref>, and developing empathy toward others through AI conversations seems unlikely to foster the real-world embodied skills that empathy can depend on. It misses key elements like reading facial expressions, sensing hesitation, and offering gestures like a smile or a gentle touch -crucial parts of genuine human connection. This does not mean AI-based training is likely to be completely ineffective, as text can still be an effective medium for expressing and exchanging empathy, and AI models are likely to become increasingly multimodal as technology advances.</p><p>Finally, AI use, regardless of the format or medium, may degrade human empathy. Building on ideas from virtue ethics, we might think that empathy is a skill that must be practiced, a form of connective or emotional labour that ties us together. If AI steps in too often as a support system, even to encourage reflection, it is plausible to imagine that we become less proficient at spontaneously practicing empathy ourselves in the same way that other kinds of deskilling occur when relying too much on AI <ref type="bibr" target="#b18">(Sambasivan &amp; Veeraraghavan, 2022)</ref>. Similarly, if people grow accustomed to using an LLM that always listens and responds without judgment, they might lose patience for the messiness and complexity of real human emotions, which require effort, vulnerability, and compromise. These are real possibilities, and we simply do not know yet what the long-term effects of AI-mediated empathy training would be.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Should AI be used to develop empathy?</head><p>Overall, there are reasons to be optimistic that modern LLMs could successfully be employed as part of larger procedures to improve the empathy of human trainees. It is a separate question of whether they should be used to improve the empathy of users. We take the answer not only to be a qualified yes but that it is a better lens through which to focus on AI-related empathy than is typically considered.</p><p>As discussed in Section 1, discussion of the role of empathy in human-AI relationships has typically focused on whether AI can or should replace human empathizers. Endorsing the use of AI-produced empathy requires thinking that while it may be phony, AI "empathy" is better than nothingit is an ersatz substitute but an improvement, nonetheless. For example, because human empathizers tire out and are not good at empathizing with marginalized groups, for use cases where empathy fatigue or marginalization are a risk, AI empathizing could be an improvement on the state of affairs we have now <ref type="bibr" target="#b6">(Inzlicht et al., 2024)</ref>. Taking this strategy, the possibilities for AI are nearly endless. AI empathizers could free up the emotional workload of doctors, augment HR departments, or provide companionship to lonely people <ref type="bibr" target="#b17">(Pugh, 2024)</ref>.</p><p>The ethical problem with focusing on AI "empathy" is in how it cheapens empathy. As something produced by AI, "empathy" is no longer a fundamentally human virtue and way of connecting with others. Empathy is reduced to nothing more than a means to an end -a design problem to solve. Because many people are lonely, AI can keep them company. Because doctors are not always the best at bedside manner, AI can replace them to better comfort humans.</p><p>Because training HR caseworkers is expensive and time-consuming, AI can take over some of the workload. Through this lens, empathy is a way of mollifying customers, creating the perception of friendship, or removing the need for genuine human-to-human connections in trying times. Empathy is a commodity to be purchased from tech companies, one API credit at a time.</p><p>The pressures towards cheapening empathy will doubtless only grow stronger as AI becomes more sophisticated and AI companies offer new "solutions" related to empathy. This can be fought by shifting focus away from replacing human empathy with AI "empathy" towards instead using AI to develop human empathy, thereby retaining empathy's value and its fundamental humanity. If AI can be used to successfully increase a person's empathy, the resulting growth in empathy would be genuine, even if the means to arrive at that empathy were fake. The origin of one's empathy-related skills and motivations do not change whether or not we genuinely empathize in the same way the $5 in our pocket is still $5, regardless of whether it ended up there as part of our monthly salary, we found it on the street, or we stole it in a violent robbery <ref type="bibr" target="#b9">(Landes et al., 2025)</ref>. Used as a tool for our moral development, AI doesn't limit us or cut us off from others and limit our moral growth (see <ref type="bibr" target="#b22">Vallor, 2024)</ref>. Instead, AI helps us develop our own virtues and flourish as a community of moral and social agents. We thereby preserve our autonomy in the face of AI <ref type="bibr" target="#b4">(Floridi &amp; Cowls, 2019)</ref> because we are not yielding ground to AI but using it as scaffolding.</p><p>Even if using AI as a tool seems less fraught than using AI to replace humans, it is not without potential downsides. Replacing humans with AI in empathy training again risks alienating humans in the training because humans will know (and ethically should know) that they are interacting with an AI that is not displaying genuine empathy and is not genuinely deserving of the human's empathy, potentially causing existential concerns. Relatedly, AI being used as a tool -even a tool for growth -could encourage a superficial or performative focus on merely saying the right things rather than forging genuine emotional connections with others. Moreover, given the cultural bias in LLMs' training data <ref type="bibr">(Tao et al., 2024)</ref>, teaching empathy through AI might subtly impose biased or narrow versions of what "good" empathy looks like, marginalizing other cultural and moral perspectives.</p><p>Perhaps most critically, though, we must consider the question of how AI could improve empathy against the broader backdrop: one in which AI companies use their money and influence to change politics to align with their own economic interests, where the pursuit of "efficiency" leads to job displacement and economic precarity, and where the labour of training AI models is often outsourced to underpaid and exploited workers in the global South. Perhaps we are willing to ignore this broader context and accept these consequences as a reasonable cost for the greater good if AI could meaningfully and strongly enhance our empathy and make a real difference in the world. There nonetheless remains a deeper risk for how widespread use of this technology could change the way we understand ourselves and others. To even entertain the idea that AI could be used to train or enhance empathy is to risk opening a discursive space in which, by talking about whether it is possible, we implicitly assume that it is desirable in the first place.</p><p>By moving to the idea that we should rely on AI for either providing or enhancing empathy, we risk empathy becoming another product or service: something that could be delivered via</p></div>		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>subscription or branded as a self-help solution rather than an integral, shared part of the human experience.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Comparing Physician and Artificial Intelligence Chatbot Responses to Patient Questions Posted to a Public Social Media Forum</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Ayers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Poliak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Leas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Faix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Longhurst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hogarth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.1001/jamainternmed.2023.1838</idno>
		<ptr target="https://doi.org/10.1001/jamainternmed.2023.1838" />
	</analytic>
	<monogr>
		<title level="j">JAMA Internal Medicine</title>
		<imprint>
			<biblScope unit="volume">183</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="589" to="596" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<author>
			<persName><forename type="first">M</forename><surname>Crockett</surname></persName>
		</author>
		<ptr target="https://www.theguardian.com/commentisfree/2025/feb/28/ai-empathy-humans" />
	</analytic>
	<monogr>
		<title level="m">AI is &apos;beating&apos; humans at empathy and creativity</title>
		<imprint>
			<date type="published" when="2025-02-27">2025, February 27</date>
		</imprint>
	</monogr>
	<note>But these games are rigged</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The social neuroscience of empathy</title>
		<editor>Decety, J., &amp; Ickes, W. J.</editor>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Algorithm aversion: People erroneously avoid algorithms after seeing them err</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Dietvorst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Simmons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Massey</surname></persName>
		</author>
		<idno type="DOI">10.1037/xge0000033</idno>
		<ptr target="https://doi.org/10.1037/xge0000033" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology. General</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="114" to="126" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Unified Framework of Five Principles for AI in Society</title>
		<author>
			<persName><forename type="first">L</forename><surname>Floridi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cowls</surname></persName>
		</author>
		<idno type="DOI">10.1162/99608f92.8cd550d1</idno>
		<ptr target="https://doi.org/10.1162/99608f92.8cd550d1" />
	</analytic>
	<monogr>
		<title level="j">Harvard Data Science Review</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The effects of empathy skills training on nursing students&apos; empathy and attitudes toward elderly people</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gholamzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Khastavaneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Khademian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghadakpour</surname></persName>
		</author>
		<idno type="DOI">10.1186/s12909-018-1297-9</idno>
		<ptr target="https://doi.org/10.1186/s12909-018-1297-9" />
	</analytic>
	<monogr>
		<title level="j">BMC Medical Education</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">198</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">In praise of empathic AI</title>
		<author>
			<persName><forename type="first">M</forename><surname>Inzlicht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Cameron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>D'cruz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bloom</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2023.12.003</idno>
		<ptr target="https://doi.org/10.1016/j.tics.2023.12.003" />
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="89" to="91" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Microsoft boss troubled by rise in reports of &apos;AI psychosis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Kleinman</surname></persName>
		</author>
		<ptr target="https://www.bbc.com/news/articles/c24zdel5j18o" />
	</analytic>
	<monogr>
		<title level="j">BBC News</title>
		<imprint>
			<date type="published" when="2025-08-20">2025, August 20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Empathy training: Methods, evaluation practices, and validity</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C M</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kolomitro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Alamparambil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Multidisciplinary Evaluation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="162" to="200" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rage against the authority machines: How to design artificial moral advisors for moral enhancement</title>
		<author>
			<persName><forename type="first">E</forename><surname>Landes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Voinea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Uszkai</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00146-024-02135-3</idno>
		<ptr target="https://doi.org/10.1007/s00146-024-02135-3" />
	</analytic>
	<monogr>
		<title level="j">AI &amp; SOCIETY</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2237" to="2248" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The effects of an empathy role-playing program for 11 operating room nursing students in Iran</title>
		<author>
			<persName><forename type="first">N</forename><surname>Larti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ashouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aarabi</surname></persName>
		</author>
		<idno type="DOI">10.3352/jeehp.2018.15.29</idno>
		<ptr target="https://doi.org/10.3352/jeehp.2018.15.29" />
	</analytic>
	<monogr>
		<title level="j">Journal of Educational Evaluation for Health Professions</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The Impact of Generative AI on Critical Thinking: Self-Reported Reductions in Cognitive Effort and Confidence Effects From a Survey of Knowledge Workers</title>
		<author>
			<persName><forename type="first">H.-P</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tankelevitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Drosos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rintel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Banks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI Conference on Human Factors in Computing Systems</title>
		<meeting><address><addrLine>CHI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2025">2025. 2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Algorithm appreciation: People prefer algorithmic to human judgment</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Logg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Minson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Moore</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.obhdp.2018.12.005</idno>
		<ptr target="https://doi.org/10.1016/j.obhdp.2018.12.005" />
	</analytic>
	<monogr>
		<title level="j">Organizational Behavior and Human Decision Processes</title>
		<imprint>
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="page" from="90" to="103" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An Online Intervention Increases Empathy, Resilience, and Work Engagement Among Physical Therapy Students</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">"</forename><surname>De Heer</surname></persName>
		</author>
		<author>
			<persName><surname>Dirk</surname></persName>
		</author>
		<ptr target="https://www.jstor.org/stable/48722085" />
	</analytic>
	<monogr>
		<title level="j">Journal of Allied Health</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="196" to="203" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">People expect artificial moral advisors to be more utilitarian and distrust utilitarian moral advisors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A C</forename><surname>Everett</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cognition.2024.106028</idno>
		<ptr target="https://doi.org/10.1016/j.cognition.2024.106028" />
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">256</biblScope>
			<biblScope unit="page">106028</biblScope>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Will Generative Artificial Intelligence Chatbots Generate Delusions in Individuals Prone to Psychosis?</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Ã˜stergaard</surname></persName>
		</author>
		<idno type="DOI">10.1093/schbul/sbad128</idno>
		<ptr target="https://doi.org/10.1093/schbul/sbad128" />
	</analytic>
	<monogr>
		<title level="j">Schizophrenia Bulletin</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1418" to="1419" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">AI will never convey the essence of human empathy</title>
		<author>
			<persName><forename type="first">A</forename><surname>Perry</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41562-023-01675-w</idno>
		<ptr target="https://doi.org/10.1038/s41562-023-01675-w" />
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1808" to="1809" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Pugh</surname></persName>
		</author>
		<title level="m">The Last Human Job: The Work of Connecting in a Disconnected World</title>
		<imprint>
			<publisher>Princeton University Press</publisher>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The deskilling of domain expertise in AI development</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sambasivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Veeraraghavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2022 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Point-of-view writing: A method for increasing medical students&apos; empathy, identification and expression of emotion, and insight</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Boker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lie</surname></persName>
		</author>
		<idno type="DOI">10.1080/13576280500534776</idno>
		<ptr target="https://doi.org/10.1080/13576280500534776" />
	</analytic>
	<monogr>
		<title level="j">Education for Health: Change in Learning &amp; Practice</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="96" to="105" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Does it matter if empathic AI has no empathy</title>
		<author>
			<persName><forename type="first">G</forename><surname>Shteynberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Halpern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sadovnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Garthoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Montemayor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Olson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Hulsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fairweather</surname></persName>
		</author>
		<idno type="DOI">10.1038/s42256-024-00841-7</idno>
		<ptr target="https://doi.org/10.1038/s42256-024-00841-7" />
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="496" to="497" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The efficacy of empathy training: A metaanalysis of randomized controlled trials</title>
		<author>
			<persName><forename type="first">E</forename><surname>Teding Van Berkhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Malouff</surname></persName>
		</author>
		<idno type="DOI">10.1037/cou0000093</idno>
		<ptr target="https://doi.org/10.1037/cou0000093" />
	</analytic>
	<monogr>
		<title level="j">Journal of Counseling Psychology</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="41" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The AI mirror: How to reclaim our humanity in an age of machine thinking</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vallor</surname></persName>
		</author>
		<idno type="DOI">10.1093/oso/9780197759066.001.0001</idno>
		<ptr target="https://doi.org/10.1093/oso/9780197759066.001.0001" />
		<imprint>
			<date type="published" when="2024">2024</date>
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The role of empathy in intergroup relations</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Vanman</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.copsyc.2016.06.007</idno>
		<ptr target="https://doi.org/10.1016/j.copsyc.2016.06.007" />
	</analytic>
	<monogr>
		<title level="j">Current Opinion in Psychology</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="59" to="63" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Putting the Altruism Back into Altruism: The Evolution of Empathy</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">B M</forename><surname>Waal</surname></persName>
		</author>
		<author>
			<persName><surname>De</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev.psych.59.103006.093625</idno>
		<ptr target="https://doi.org/10.1146/annurev.psych.59.103006.093625" />
	</analytic>
	<monogr>
		<title level="j">Annual Review of Psychology</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="279" to="300" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The AI Empathy Choice Paradox: People Prefer Human Empathy Despite Rating AI Empathy Higher</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Wenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cameron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Inzlicht</surname></persName>
		</author>
		<ptr target="https://osf.io/ghw2v_v1" />
	</analytic>
	<monogr>
		<title level="j">OSF</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Categories of training to improve empathy: A systematic review and meta-analysis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-Z</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.1037/bul0000453</idno>
		<ptr target="https://doi.org/10.1037/bul0000453" />
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">150</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1237" to="1260" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
