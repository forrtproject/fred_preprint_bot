<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantic content outperforms speech prosody in predicting affective experience in naturalistic settings</title>
				<funder ref="#_hME8fph">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder>
					<orgName type="full">Stanford HAI Google Cloud Credit Award</orgName>
				</funder>
				<funder ref="#_h7Dx4Rg">
					<orgName type="full">Swiss National Science Foundation</orgName>
					<orgName type="abbreviated">SNSF</orgName>
				</funder>
				<funder>
					<orgName type="full">Leibniz Institute for Psychology (ZPID)</orgName>
				</funder>
				<funder>
					<orgName type="full">German Academic Scholarship Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Timo</forename><forename type="middle">K</forename><surname>Koch</surname></persName>
							<email>timo.koch@unisg.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Behavioral Science and Technology</orgName>
								<orgName type="institution">University of St. Gallen</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">Ludwig-Maximilians-Universität München</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Communication</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gabriella</forename><forename type="middle">M</forename><surname>Harari</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Communication</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ramona</forename><surname>Schoedel</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">Ludwig-Maximilians-Universität München</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Charlotte Fresenius Hochschule</orgName>
								<orgName type="institution">University of Psychology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Samuel</forename><forename type="middle">D</forename><surname>Gosling</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zachariah</forename><surname>Marrero</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Florian</forename><surname>Bemmann</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">Media Informatics Group</orgName>
								<orgName type="institution">Ludwig-Maximilians-Universität München</orgName>
								<address>
									<addrLine>Torstrasse 25</addrLine>
									<postCode>9000</postCode>
									<settlement>St. Gallen</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Markus</forename><surname>Bühner</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">Ludwig-Maximilians-Universität München</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Clemens</forename><surname>Stachl</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Behavioral Science and Technology</orgName>
								<orgName type="institution">University of St. Gallen</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peter</forename><surname>Ehrich</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dominik</forename><surname>Heinrich</surname></persName>
						</author>
						<title level="a" type="main">Semantic content outperforms speech prosody in predicting affective experience in naturalistic settings</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FF8B6223DD5293A6DC833DE65FE65D3A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-22T20:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Affect</term>
					<term>Speech</term>
					<term>Prosody</term>
					<term>Voice</term>
					<term>Machine Learning Word count: 5788</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Humans possess the remarkable ability to recognize affective states (e.g., emotions) from speech prosody (i.e., voice acoustics). Algorithms are now trained to do the same task at scale and increasingly deployed commercially. These algorithms have typically been trained on speech data based on enacted (e.g., by professional actors) or observed (e.g., externally rated) affect expressions collected in controlled lab settings. However, these algorithms are deployed to recognize subjective affect experiences from speech in real-world settings. This discrepancy between the controlled, often idealized speech data used for algorithm training and the naturalistic speech encountered during real-world deployment raises questions about whether algorithms can reliably detect subjective affective experiences in everyday settings.</p><p>Here, we investigate whether experienced affect can be predicted from naturalistic speech samples collected via smartphones. In two field studies (experimental Study 1: N = 409; observational Study 2: N = 687), we collected 25,403 speech samples from participants along with their self-reported affective experiences. Machine learning analyses suggest that prosody reveals only limited affective information (r md = .17) and is outperformed by semantic content (r md = .33) captured by word embeddings from a large language model. Our findings challenge the generalizability of prosody-based emotion recognition technologies to naturalistic settings and underscore the importance of incorporating semantic content in the algorithmic recognition of subjective affective experiences from speech.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic content outperforms speech prosody in predicting affective experience in naturalistic settings</head><p>The prosody of a voice, such as acoustic tone, pitch, and rhythm, serves as a primary means for conveying emotional information <ref type="bibr" target="#b25">(Kraus, 2017;</ref><ref type="bibr" target="#b42">Ponsot, Burred, Belin, &amp; Aucouturier, 2018;</ref><ref type="bibr" target="#b47">Scherer, 2003)</ref>. Decades of research investigated how speech prosody varies with affect (Larrouy-Maestri, Poeppel, &amp; Pell, 2024), finding, for example, that higher pitch and faster speech rates are associated with excitement or stress <ref type="bibr" target="#b2">(Banse &amp; Scherer, 1996)</ref>. More recently, algorithms trained to automatically recognize affect and affective disorders from speech prosody offer promising potential applications in the domains of health care, human-machine interaction, education, and business <ref type="bibr" target="#b21">(Hildebrand et al., 2020;</ref><ref type="bibr" target="#b37">Milling, Pokorny, Bartl-Pokorny, &amp; Schuller, 2022;</ref><ref type="bibr" target="#b57">Vlahos, 2019)</ref>. The widespread adoption of voice assistants like Amazon's Alexa and Apple's Siri has also fueled commercial interest in developing algorithms that can detect affect in everyday life. These algorithms aim to quantify at scale how patients, customers, and employees feel in a given moment, often to provide personalized recommendations, feedback, and services (e.g., <ref type="bibr" target="#b35">Matz &amp; Netzer, 2017;</ref><ref type="bibr" target="#b53">Seiferth et al., 2023)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithmic recognition of affective expression in the lab versus experience in the wild</head><p>The prosody of a voice, such as acoustic tone, pitch, and rhythm, serves as a primary means for conveying emotional information <ref type="bibr" target="#b25">(Kraus, 2017;</ref><ref type="bibr" target="#b42">Ponsot, Burred, Belin, &amp; Aucouturier, 2018;</ref><ref type="bibr" target="#b47">Scherer, 2003)</ref>. Decades of research investigated how speech prosody varies with affect (Larrouy-Maestri, Poeppel, &amp; Pell, 2024), finding, for example, that higher pitch and faster speech rates are associated with excitement or stress <ref type="bibr" target="#b2">(Banse &amp; Scherer, 1996)</ref>. More recently, algorithms trained to automatically recognize affect and affective disorders from speech prosody offer promising potential applications in the domains of health care, human-machine interaction, education, and business <ref type="bibr" target="#b21">(Hildebrand et al., 2020;</ref><ref type="bibr" target="#b37">Milling, Pokorny, Bartl-Pokorny, &amp; Schuller, 2022;</ref><ref type="bibr" target="#b57">Vlahos, 2019)</ref>. The widespread adoption of voice assistants like Amazon's Alexa and Apple's Siri has also fueled commercial interest in developing algorithms that can detect affect in everyday life. These algorithms aim to quantify at scale how patients, customers, and employees feel in a given moment, often to provide personalized recommendations, feedback, and services (e.g., <ref type="bibr" target="#b35">Matz &amp; Netzer, 2017;</ref><ref type="bibr" target="#b53">Seiferth et al., 2023)</ref>.</p><p>Existing affect recognition algorithms are predominantly trained on enacted emotional speech (from professional actors in lab settings, <ref type="bibr" target="#b4">Bänziger, Mortillaro, &amp; Scherer, 2012;</ref><ref type="bibr" target="#b50">Schuller, 2018;</ref><ref type="bibr" target="#b59">Vogt, André, &amp; Wagner, 2008)</ref> or on labeled speech samples (such as TV clips, <ref type="bibr" target="#b18">Grimm, Kroschel, &amp; Narayanan, 2008)</ref>. Using such training data generally results in high performance for the algorithmic recognition of, for example, affective arousal (r max = .81) and valence (r max = .68) in controlled settings <ref type="bibr" target="#b62">(Weninger, Eyben, Schuller, Mortillaro, &amp; Scherer, 2013)</ref>. However, these data, which are used to train the algorithms, differ in two important respects from the data that are actually processed by the algorithms when they are applied in the real world.</p><p>First, the data used to train such algorithms consists of actors' portrayals of affective states or raters' labels of existing voice samples that both rely on folk theories of how affect is expressed; for instance, how does a sad person sound when they talk? <ref type="bibr" target="#b3">(Batliner et al., 2011;</ref><ref type="bibr" target="#b50">Schuller, 2018;</ref><ref type="bibr" target="#b63">Wilting, Krahmer, &amp; Swerts, 2006)</ref>. However, those prototypical affective expressions are not necessarily reflective of people's subjective affective experience; that is, their genuine subjective affective state. For example, someone might feel nervous about giving a presentation, but choose to express themselves by talking calmly and confidently. Investigating affective expression is invaluable for understanding emotional communication <ref type="bibr" target="#b12">(Ekman &amp; Friesen, 1969)</ref>, but the main purpose of such algorithms is to detect people's subjective affective experience. This is important for both researchers and commercial entities who want to gain a deeper understanding of how individuals, such as patients or customers, genuinely feel, as opposed to merely observing how they outwardly express themselves.</p><p>The second way the data used to train current algorithms differ from instances in which they will be used is whether speech data has been produced and collected in laboratory or real-world settings. In controlled lab settings, actors enact predefined affective states or desired emotions are elicited in participants and their speech is recorded, allowing for the investigation of affective speech in standardized conditions using controlled stimuli (e.g., <ref type="bibr" target="#b10">Cowen, Laukka, Elfenbein, Liu, &amp; Keltner, 2019)</ref>. However, speech data collected in lab settings limit ecological validity. Algorithms trained on such data may perform poorly in real-world situations, where conditions differ significantly from controlled environments. For example, lab recordings are made in isolated rooms without background noise, unlike the complex auditory environments of everyday life (e.g., noisy cafés or restaurants). As a result, these algorithms might fail to accurately recognize affect in naturalistic settings, leading to unreliable results and misinterpretations in practical applications.</p><p>The focus on affective expression (versus affective experience) and on lab-based (versus real world) speech samples that characterized past research raises concerns about the generalizability of the performance of existing algorithms. Specifically, the speech data on which these algorithms are being trained do not match the speech data encountered in the settings in which they are deployed. This raises the question: Are algorithms capable of accurately detecting subjective affective experience in everyday life, and with comparable degrees of accuracy as they have shown for affective expression in the lab?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prosody versus semantics for affect recognition from speech</head><p>In addition to prosody, the semantic content (i.e., meaning of the words) of speech plays a central role in conveying affective information. Prosody and semantics interact dynamically, working together to transmit affective cues through speech <ref type="bibr" target="#b5">(Ben-David, Multani, Shakuf, Rudzicz, &amp; van Lieshout, 2016)</ref>. Prior research suggests that prosody often takes precedence over semantics in how humans perceive affective states <ref type="bibr" target="#b5">(Ben-David, Multani, Shakuf, Rudzicz, &amp; van Lieshout, 2016;</ref><ref type="bibr" target="#b31">Lin Yi, Ding Hongwei, &amp; Zhang Yang, 2020)</ref>. In the same fashion, prior work indicates that algorithms might also prioritize prosodic cues over semantics for the recognition of affect from speech <ref type="bibr" target="#b13">(El Ayadi, Kamel, &amp; Karray, 2011;</ref><ref type="bibr" target="#b41">Polzehl, Schmitt, Metze, &amp; Wagner, 2011;</ref><ref type="bibr" target="#b51">Schuller, Rigoll, &amp; Lang, 2004)</ref>.</p><p>However, as noted above, these findings are based on algorithms trained to recognize affect expression (not experience) in controlled lab settings. Consequently, what remains unknown is whether algorithms rely more on prosodic features or semantic content when recognizing affective experience from spontaneous speech in real-world settings.</p><p>In the present work, we address the existing gaps in algorithmic affect recognition from speech, namely (1) the generalization of algorithms' prediction performance from affective expression in the lab to affective experience in the real world, and (2) the predictive power of prosody versus semantics for affect inferences from speech. To investigate the algorithmic recognition of affective experience in everyday life, we conducted two large-scale field studies in two different countries: a field experiment with German-speaking participants in Germany (Study 1) and an observational field study with English-speaking participants in the United States (Study 2). In both studies, we used smartphone applications to collect experience sampling reports and speech samples via the built-in microphone as people went about their daily lives. Using this approach, we collected a total of 25,403 speech samples, paired with self-reported affective experience from 1,096 participants.</p><p>The first study was a field experiment. We manipulated what people said in speech samples collected in naturalistic settings to conservatively test the predictive power of voice prosody for algorithmic recognition of affective experience, while controlling for semantic content. Participants were instructed to read out pre-tested scripted sentences with varying emotional sentiment (i.e., positive, negative, and neutral) while recording their voice. From the extracted prosodic features we then predicted self-reported affective states on the dimensions of arousal and valence using machine learning. In the second study, we collected spontaneous speech samples in naturalistic settings to investigate the predictive power of voice prosody and semantic content for algorithmic recognition of affective experience.</p><p>Participants were prompted to talk spontaneously about their current situation, thoughts, and feelings while recording their voice. Just as in Study 1, we subsequently predicted self-reported affective states on the dimensions of arousal as well as contentment and sadness (for emotional valence) from voice prosody to investigate the predictive power of prosody in spontaneous speech. Additionally, we used the semantic content captured by word embeddings from those speech samples to predict momentary affective states and compare the models' performance to those trained solely on prosody.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Data collection</head><p>The present work consists of two studies leveraging machine learning, each based on a separate dataset. The workflow of data collection, processing, and predictive modeling is described in detail in the following section and illustrated in Figure <ref type="figure" target="#fig_0">1</ref>. Study 1: Scripted speech. Data collection for Study 1 was part of a large panel study (from May 15 until November 14, 2020) using the PhoneStudy research app <ref type="bibr" target="#b49">(Schoedel &amp; Oldemeier, 2020)</ref>. Data collection was approved by the ethics committee of the psychology department at LMU Munich and all procedures adhered to the General Data Protection Regulation (GDPR). We recruited a quota sample of N = 850 participants representative of the German population regarding age, gender, education, income, confession, and relationship status. Participants had to be between 18 and 65 years old, fluent in German, and in the possession of an Android smartphone (Android 5 or higher) for private usage as a sole user. The study also comprised two two-week experience sampling phases (July 27, 2020, to <ref type="bibr">August 9, 2020;</ref><ref type="bibr">September 21, 2020</ref><ref type="bibr">, to October 4, 2020)</ref> during which participants received two to four short questionnaires per day on their personal smartphone as part of an experience sampling procedure. Here, self-reported momentary affective valence and arousal were assessed in two separate items on six-point Likert scales: Valence ranging from "very unpleasant" (1) to "very pleasant" (6) and arousal ranging from "very inactive" (1) to "very activated" (6). The experience sampling procedure also covered other psychological properties (i.e., sleep quality, stress, situational characteristics) that had been used in other research (große Deters <ref type="bibr" target="#b19">&amp; Schoedel, 2024;</ref><ref type="bibr" target="#b48">Schoedel et al., 2023)</ref>.</p><p>The last experience sampling questionnaire of each day included an additional instruction, where participants were asked to read out a series of predefined emotional sentences while making an audio recording of their voice. The sentences presented to the participants are based on a set of 54 validated German neutral and affective sentences <ref type="bibr" target="#b11">(Defren et al., 2018)</ref> and differ in their emotional content: positive (e.g., "My team won yesterday."), negative (e.g., "Nobody is interested in my life."), and neutral (e.g., "The plate is on the round table."). In each measurement instance, participants were instructed to read out three positive, neutral, and negative sentences. The order of the categories was randomized per experience sampling instance. For each emotional content category, three sentences were randomly drawn (with replacement) from the respective sets of sentences in the database. The experimental manipulation of these emotional semantic categories allowed us to control for the content spoken by our participants and at the same time enabled us to conduct a privacy-friendly study. The audio recording was started by the participants via a button on the screen. Participants could stop the recording manually after a minimum of four seconds. Alternatively, the recording was stopped automatically after twelve seconds.</p><p>We chose these lower-and upper-time thresholds because this is the minimum and maximum time required to read out the three sentences per condition, determined by reading the sentences extremely fast and extremely slow and recording the times. Once the audio record had been completed, we used the widely adopted OpenSMILE open-source algorithm <ref type="bibr" target="#b15">(Eyben, Wöllmer, &amp; Schuller, 2010)</ref> to automatically extract acoustic features directly on the participant's device. Here, we used the extended Geneva Minimalistic Acoustic Parameters Set (eGeMAPS) that comprises 88 acoustic features <ref type="bibr" target="#b15">(Eyben, Wöllmer, &amp; Schuller, 2010)</ref> and the 2016 Interspeech Computational Paralinguistic Challenge set of 6,737 acoustic features <ref type="bibr" target="#b52">(Schuller et al., 2016)</ref>. Those feature sets have been used in a range of prior studies on affect recognition from speech (van Rijn &amp; Larrouy-Maestri, 2023; <ref type="bibr" target="#b61">Weidman et al., 2020)</ref>. We used the parsimonious eGeMAPS feature set for our main analyses. After feature extraction, the voice records were automatically deleted and only the extracted voice features were transmitted to our servers. As a result, there were three sets of voice features per experience sampling instance (one per sentiment condition).</p><p>We collected 11187 audio logs with respective affect self-reports from 577 participants.</p><p>Participants made on average 19.39 (SD = 12.37) voice records. We excluded data from 158 participants with less than ten voice samples in total and the data from eight participants who had no variance in all their valence and arousal scores across all their observations. Finally, we excluded 163 voice logs from 22 participants because the respective voice indicators suggested that no human voice was recorded (mean voicing score &lt; 0.5, voiced segments per second = 0, mean voiced segment length = 0). This left us with a final data set of 9941 voice samples with corresponding acoustic features from 3378 experience sampling instances for valence and arousal from 409 participants (49.48% female, Age M = 42.97 years).</p><p>In the final sample, there were on average 24.31 (SD = 10.54) voice logs with 8.26 (SD = 3.53) corresponding affect self-reports per participant. Overall self-reported valence was positive (M = 4.70, SD = 1.04) and overall arousal was slightly geared towards activation (M = 3.67, SD = 1.34). The distribution of valence and arousal ratings is provided in the online supplements.</p><p>Study 2: Spontaneous speech. Data collection for Study 2 was part of the UT1000 Project at the University of Texas at Austin in the United States in fall 2018 <ref type="bibr" target="#b65">(Wu et al., 2021)</ref>. During a three-week self-tracking assignment using their smartphones, students from a Synchronous Massive Online Course (SMOC) in introductory psychology received four short experience sampling questionnaires per day where they could also make speech records at the end. Here, self-reported arousal (assessed on a five-point Likert scale ranging from "low energy" (1) to "high energy" (5)), contentment, and sadness were assessed in separate items on four-point Likert scales (ranging from "not at all" (1) to "very much" (4)) among other psychological properties as part of an experience sampling procedure <ref type="bibr" target="#b65">(Wu et al., 2021)</ref>. In Study 2, we captured emotional valence on two items (contentment and sadness) instead of one as done in Study 1. According to the affect grid, contentment and sadness have a comparable low level of arousal and an opposing emotional valence <ref type="bibr" target="#b43">(Posner, Russell, &amp; Peterson, 2005;</ref><ref type="bibr" target="#b45">Russell, 1980)</ref>.</p><p>For the audio records, participants received the following instruction: "Please record a short audio clip in which you describe the situation you are in, and your current thoughts and feelings. Collect about 10 seconds of environmental sound after the description." The responses to this prompt are analyzed in the present study. Any parts of the record that did not contain speech were cut out before further analysis since the focus of this work is on affect in human speech. The collected speech samples had also been used in another research project that describes the data collection procedure in more detail <ref type="bibr" target="#b33">(Marrero, Gosling, Pennebaker, &amp; Harari, 2022)</ref>.</p><p>In total, we collected 23482 audio logs with corresponding affect self-reports from 980 participants. Participants made on average 23.96 (SD = 18.40) speech records with corresponding affect self-reports. We followed the identical procedure to filter the data as in Study 1: First, we excluded the data from 281 participants with less than ten audio records in total and from another participant who had no variance in all their self-reports across all their experience samples. To ensure comparability of the two studies with regard to the length of speech samples, we removed 6,871 speech transcripts that contained less than 15 words and were less than four seconds long which is equivalent to the length of the sentences that had been read out in Study 1. Acoustic features indicated that human voice had been recorded in all of the remaining speech samples.</p><p>This procedure left us with a final data set of 15462 speech samples with an average length of 52.37 words (SD = 25.49) and duration of 34.33 seconds (SD = 13.13) with corresponding experience-sampled self-reports on momentary affective experience from 687 participants (62.49% female, Age M = 18.58 years). In the final sample, there were on average 22.51 (SD = 14.10) speech samples with corresponding affect self-reports per participant.</p><p>Overall participants reported balanced experienced contentment (M = 1.68, SD = 0.86) and low sadness (M = 0.48, SD = 0.75). Overall arousal was balanced out (M = 1.97, SD = 0.95). The distribution of self-reported arousal, contentment, and sadness scores is provided in the repository.</p><p>Replicating the approach from Study 1, we extracted the extended Geneva Minimalistic Acoustic Parameters Set (eGeMAPS) and the 2016 Interspeech Computational Paralinguistic Challenge set from the collected audio files using the OpenSMILE algorithm <ref type="bibr" target="#b14">(Eyben et al., 2016;</ref><ref type="bibr" target="#b15">Eyben, Wöllmer, &amp; Schuller, 2010;</ref><ref type="bibr" target="#b52">Schuller et al., 2016)</ref>. In contrast to the on-device feature-extraction approach in Study 1, those features were extracted from the raw recorded audio files after data collection in Study 2.</p><p>We transcribed all raw audio records using the Google Speech-to-text API (version 1).</p><p>Automatic speech-to-text technology has been shown to be well-suited for transcription tasks in psychological research <ref type="bibr" target="#b39">(Pfeifer, Chilton, Grilli, &amp; Mehl, 2024)</ref>. The Google API also assigns a sentiment score (M = 0.03, SD = 0.30) within the interval of [-1; 1] to each speech transcript. Then, we extracted state-of-the-art word embeddings from speech transcripts using the text R package <ref type="bibr" target="#b23">(Kjell, Giorgi, &amp; Schwartz, 2021)</ref>. Word embeddings are vector representations of words in a high-dimensional space, which capture their contextualized meaning and relationships with other words. Specifically, we used the 1,024 dimensions from the second to last layer (layer 23) from the large language model "RoBERTa large" as features as recommended in prior work <ref type="bibr" target="#b32">(Liu et al., 2019;</ref><ref type="bibr" target="#b34">Matero, Hung, &amp; Schwartz, 2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Machine learning</head><p>In both studies, we trained linear Elastic Net regularized regression models <ref type="bibr" target="#b66">(Zou &amp; Hastie, 2005)</ref> and non-linear tree-based Random Forest models <ref type="bibr" target="#b7">(Breiman, 2001;</ref><ref type="bibr" target="#b64">Wright &amp; Ziegler, 2017)</ref>, and a baseline model on the extracted features for the prediction of self-reported affective experience. The baseline model predicted the respective mean values for affective experience of the respective training set for all cases in a test set. We evaluated model performance using a five times repeated ten-fold cross-validation scheme <ref type="bibr" target="#b6">(Bischl, Mersmann, Trautmann, &amp; Weihs, 2012)</ref> and used blocking of participants in the resampling procedure to ensure that for each train/test set pair the given participant is either in the training set or in the test set. In the results section, we report on the average (median) model performance across those 50 models. Before predictive modeling, we replaced extreme outliers (mean +/-four times SD) with missing values that were imputed as part of the resampling procedure.</p><p>In our main analyses, we predicted self-reported affective states (Study 1: arousal and valence; Study 2: arousal, contentment, sadness) from prosodic features (eGeMAPS feature set). For Study 2, we also used semantic content, captured by word embeddings, as predictor.</p><p>We conducted a number of supplementary analyses that can be found in the repository: As done in prior research <ref type="bibr" target="#b61">(Weidman et al., 2020)</ref>, we predicted affective states from the extensive 2016 Interspeech Computational Paralinguistic Challenge set <ref type="bibr" target="#b52">(Schuller et al., 2016)</ref> to investigate whether a larger prosodic feature set would affect prediction performance. Moreover, in line with prior work <ref type="bibr" target="#b55">(Sun, Schwartz, Son, Kern, &amp; Vazire, 2020;</ref><ref type="bibr" target="#b61">Weidman et al., 2020)</ref>, we predicted momentary fluctuations in affective experience from each individual's baseline (defined as their median response) to explore if those would be also predictable from speech cues. Specifically for Study 1, we predicted self-reported affective states from prosodic features separately for each sentence condition (positive, neutral, negative) to assess whether the sentiment of the read-out sentences would affect prediction performance. Here, we also conducted F -tests to analyze whether models' prediction errors were significantly different across sentence conditions. For Study 2, we predicted self-reported affective states from a combination of prosodic and semantic features to investigate whether combining those features improved overall performance.</p><p>Our prediction models were evaluated based on how accurate new (unseen) samples can be predicted. Throughout this manuscript, we report on the Spearman correlation (ρ) of true and predicted scores and the coefficient of determination (R 2 ) as measures of model fit.</p><p>In the repository, we provide the mean absolute error (MAE) and the root mean squared error (RMSE) as additional performance measures for prediction models. In our main analyses, we carried out variance-corrected (one-sided) t-tests comparing the R 2 measures of prediction models with those of the baseline models <ref type="bibr" target="#b38">(Nadeau &amp; Bengio, 2003)</ref> to determine whether prosodic and semantic features predicted affective experience beyond chance (alpha = 0.05). We adjusted for multiple comparisons (n = 16) via Holm correction.</p><p>Our quality checks were data driven, based on voice features (i.e., humans marked as absent when mean voicing score &lt; 0.5, voiced segments per second = 0, mean voiced segment length = 0). We evaluated the quality of the resulting speech data by training machine learning models on a task they should be able to accomplish if the data are of high quality. Specifically, we trained the models with gender as the dependent variable because past work indicates that speaker gender can be reliably predicted from voice cues <ref type="bibr" target="#b26">(Kwasny &amp; Hemmerling, 2021)</ref>. Those models predicted speaker gender from prosody with very high accuracy (Study 1: Accuracy md = 91.52%; Study 2: Accuracy md = 95.80%), indicating good data quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prediction of affective states from prosodic cues in scripted and spontaneous speech</head><p>We found differences in how well machine learning models predicted self-reported momentary affective states from prosodic features derived from scripted and spontaneous speech samples collected via smartphones, as depicted in Figure <ref type="figure" target="#fig_1">2</ref>. When participants read out scripted content (Study 1), the prediction of momentary affective arousal from voice prosody was low on average (r md = 0.17, r sd = 0.08), yet significantly better than the baseline model (i.e., a model that constantly predicts the mean from the respective training set). The prediction of momentary affective valence from voice prosody was on average even lower than for arousal and was not significantly better than chance (r md = 0.03, r sd = 0.07).</p><p>We analyzed whether the experimentally altered sentiment (positive/ neutral/ negative) of the scripted sentences had an effect on affect predictions from prosody. We found no significant differences in prediction errors across the three sentence conditions for arousal (F (2,49702) = 0.01, p = .500) and valence (F (2,49702) = 0.70, p = .993) predictions suggesting that sentences' sentiment did not influence affect predictions from prosodic features.</p><p>Models trained on prosodic features from participants talking spontaneously about their current situation, thoughts, and feelings (Study 2) also yielded a low average prediction performance overall. Still, the prediction of momentary arousal (r md = 0.12, r sd = 0.04) and contentment (r md = 0.15, r sd = 0.05) were significantly better than chance. Predictions of sadness were not better than the baseline (r md = 0.03, r sd = 0.05). To provide insights into which prosodic features could be associated with affective states, we include an overview of the importance of prosodic features in Elastic Net models and report on the correlations of voice features with momentary affective experience in the repository.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prediction of affective states from semantic cues in spontaneous speech</head><p>Machine learning models trained on semantic content in the form of word embeddings extracted from participants' spontaneous speech (Study 2) yielded good performance results overall that were significantly better than chance, outperforming models trained on voice prosody (see Figure <ref type="figure" target="#fig_1">2</ref>. Specifically, we found semantic content to be, on average, more predictive of the momentary experience of arousal (r md = 0.31, r sd = 0.04), contentment (r md = 0.33, r sd = 0.03), and sadness (r md = 0.22, r sd = 0.03) than voice prosody. Models trained on both prosodic features and word embeddings showed a similar prediction performance to those trained on word embeddings alone, suggesting that the predictions were primarily driven by semantic information. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>Affect-recognition algorithms developed in research and commercial tools are mainly trained on prosody extracted from enacted or labeled speech samples collected in controlled lab settings. Yet, these algorithms are deployed to detect people's subjective affective experience in naturalistic settings. In the present work, we investigated whether subjective momentary affective states are predictable from prosodic and semantic features in real-world speech samples collected with smartphones. In contrast to prior work, which was based on affective expressions from enacted speech data from the lab, our findings suggest that prosody reveals only limited information about affective experience and is outperformed by semantic content in the real world.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prosody reveals limited affective information in scripted and spontaneous speech</head><p>The first major finding to emerge from this work is that prosody provided limited information about affective experience in naturalistic settings. Machine learning models trained on prosodic features achieved a lower prediction performance for affective experience (up to r md = .17) compared to prior work on the algorithmic prediction of affective expression in controlled lab settings (up to r max = .81, e.g., <ref type="bibr" target="#b62">Weninger, Eyben, Schuller, Mortillaro, &amp; Scherer, 2013)</ref>, which might have also been susceptible to overfitting to the training data <ref type="bibr" target="#b0">(Aishwarya, Kaur, &amp; Seemakurthy, 2024)</ref>. This result supports the notion that recognizing real-life affective experience from voice cues algorithmically is inherently more challenging than detecting affective expression in the lab <ref type="bibr" target="#b59">(Vogt, André, &amp; Wagner, 2008)</ref>.</p><p>Nevertheless, past research predicting affective experience from speech in the wild reported similar prediction performance <ref type="bibr" target="#b9">(Carlier et al., 2022;</ref><ref type="bibr" target="#b55">Sun, Schwartz, Son, Kern, &amp; Vazire, 2020;</ref><ref type="bibr" target="#b61">Weidman et al., 2020)</ref>. Moreover, the prediction of arousal from prosody was successful across prediction models whereas the prediction of valence yielded significant predictions only for contentment, but not for overall valence and sadness. This finding is in line with prior work showing that valence is more challenging to infer algorithmically than is arousal due to its subjective nature <ref type="bibr" target="#b54">(Sridhar &amp; Busso, 2022)</ref>. Thus, our findings raise questions about the generalizability of the high prediction performance from past research (e.g., <ref type="bibr" target="#b62">Weninger, Eyben, Schuller, Mortillaro, &amp; Scherer, 2013)</ref>, which focus on recognizing affective expression from speech data collected in lab settings. Specifically, it seems that the performance obtained from lab-based studies may not extend to recognition of affective experience from speech prosody in everyday life.</p><p>Another reason why prediction performance is lower in real-world data sets could be that they contain fewer instances of extreme affective experience than those used in lab studies focusing on enacted or labeled speech. Consequently, our predictions targeted "normal" everyday affective states with only a few cases of extreme affect experience, which are inherently less intense than the short-lived emotions that have been investigated in prior work <ref type="bibr" target="#b47">(Scherer, 2003)</ref>, and thus present a greater challenge for algorithmic recognition <ref type="bibr" target="#b59">(Vogt, André, &amp; Wagner, 2008)</ref>.</p><p>To test whether a broader set of prosodic features would improve prediction performance, we reran predictions using the 2016 Interspeech Computational Paralinguistic Challenge set of 6,737 prosodic features (versus 88 features in the eGeMAPS set used in the main analyses) <ref type="bibr" target="#b14">(Eyben et al., 2016;</ref><ref type="bibr" target="#b52">Schuller et al., 2016)</ref>. In line with prior research, the larger voice feature set did not yield better predictions of momentary affective experience <ref type="bibr" target="#b61">(Weidman et al., 2020)</ref>, suggesting that the limited predictive power of prosody was not due to the parsimonious size of the prosodic feature set (see repository for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic content outperforms prosody in predicting subjective affective experience</head><p>The second major finding to emerge from this work is that semantic content captured by word embeddings from a large language model outperformed prosody in predicting affective experience in spontaneous speech. This insight suggests that speech content may be more informative than prosodic cues for algorithms recognizing experienced affect in everyday life. It contrasts prior research suggesting that speech prosody is more relevant than semantics for affect inferences by humans <ref type="bibr" target="#b5">(Ben-David, Multani, Shakuf, Rudzicz, &amp; van Lieshout, 2016;</ref><ref type="bibr" target="#b31">Lin Yi, Ding Hongwei, &amp; Zhang Yang, 2020)</ref> and algorithms trained on lab data <ref type="bibr" target="#b13">(El Ayadi, Kamel, &amp; Karray, 2011;</ref><ref type="bibr" target="#b41">Polzehl, Schmitt, Metze, &amp; Wagner, 2011;</ref><ref type="bibr" target="#b51">Schuller, Rigoll, &amp; Lang, 2004)</ref>. However, our results align with primary research that found semantic content to be more predictive than prosodic cues in algorithmically predicting momentary subjective affective experience <ref type="bibr" target="#b9">(Carlier et al., 2022;</ref><ref type="bibr" target="#b55">Sun, Schwartz, Son, Kern, &amp; Vazire, 2020;</ref><ref type="bibr" target="#b61">Weidman et al., 2020)</ref>. Consequently, to enhance prediction accuracy, speech-based emotion recognition systems deployed in real-world settings should integrate semantic content into their models rather than relying solely on prosodic features.</p><p>We propose that the discovered superiority of semantic content over prosody in affect recognition from speech may be due to the structured nature of text, which current algorithms, like the large language model employed in this work, can more effectively process compared to the complexities of prosodic cues involving subtle nuances such as intonation and emphasis on specific words. However, emerging speech foundation models, such as Whisper, Wav2Vec, and HuBERT, which are trained on vast datasets using self-supervised learning, are poised to perform better in recognizing affective states from prosody than current approaches relying on standardized acoustic feature sets and supervised machine learning as used in the present work <ref type="bibr" target="#b1">(Baevski, Zhou, Mohamed, &amp; Auli, 2020;</ref><ref type="bibr" target="#b17">Goron, Asai, Rut, &amp; Dinov, 2024;</ref><ref type="bibr" target="#b22">Hsu et al., 2021)</ref>. These powerful models capture complex vocal patterns through unsupervised feature extraction and deeper representations of prosody improving the accuracy in affect recognition <ref type="bibr" target="#b60">(Wagner et al., 2023)</ref>. Moreover, participants in our study were prompted to speak spontaneously about their current situations, thoughts, and feelings, which can be expected to reveal some affective information. Further research is needed to examine how the themes people talk about (e.g., explicitly describing how one feels, which is likely to carry a rich affective signal, versus describing the physical environment, which may contain minimal affective content) impact affect recognition performance.</p><p>Finally, we replicated analyses from past work that investigated predictions of fluctuations in affective states from speech prosody and semantic content. The results were consistent with prior work, indicating that the prediction performance for within-person fluctuations in affective states is lower than the performance of models predicting between-person differences <ref type="bibr" target="#b55">(Sun, Schwartz, Son, Kern, &amp; Vazire, 2020;</ref><ref type="bibr" target="#b61">Weidman et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>The findings of this work are limited in three ways. First, there are some differences between the two studies that might affect the comparability of results: In particular, the studies differed in terms of the scales used to assess affective experience (see methods section for details), the sampling strategy (representative quota sample versus undergraduate cohort), language and culture (Germany versus USA), and smartphone platforms (Android only versus Android and iOS). We sought to minimize the effects of these differences by, where possible, using consistent methodologies and statistical approaches across studies.</p><p>Nonetheless, the findings might be considered transferable but not directly comparable between studies.</p><p>Second, both data sets were collected in WEIRD (Western, Educated, Industrialized, Rich, and Democratic) countries <ref type="bibr" target="#b20">(Henrich, Heine, &amp; Norenzayan, 2010)</ref>. Plenty of prior research has pointed to cultural differences in the experience and expression of affect <ref type="bibr" target="#b30">(Lim, 2016)</ref>, including cultural variations in prosodic affect markers in the voice <ref type="bibr" target="#b8">(Brooks et al., 2023;</ref><ref type="bibr" target="#b29">Laukka &amp; Elfenbein, 2021;</ref><ref type="bibr">Sauter, Eisner, Ekman, &amp; Scott, 2010;</ref><ref type="bibr" target="#b56">van Rijn &amp; Larrouy-Maestri, 2023)</ref>. Therefore, the generalizability of the current work should be considered as limited to this cultural area. Future studies should seek to broaden participant inclusion to diverse cultural contexts, especially non-Western countries <ref type="bibr" target="#b40">(Phan, Modersitzki, Gloystein, &amp; Müller, 2023)</ref>.</p><p>Third, in contrast to prior work using passive speech sensing, for example via the Electronically Activated Recorder (EAR) <ref type="bibr" target="#b36">(Mehl, 2017;</ref><ref type="bibr" target="#b55">Sun, Schwartz, Son, Kern, &amp; Vazire, 2020;</ref><ref type="bibr" target="#b61">Weidman et al., 2020)</ref>, participants had to actively log their speech via their smartphone in the present work. As a consequence, participants might not have spoken as naturally as they would have in a naturalistic conversation. Further, they might have made the audio records only in selected situations that were suitable for making an audio record, for example when they were alone in a quiet place.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this work, we investigated whether machine learning algorithms can recognize subjective affective experience from naturalistic speech samples collected via smartphones in everyday life. Extracted prosodic voice parameters provided only limited affective information and were outperformed by semantic content as captured in word embeddings.</p><p>Our findings challenge whether the optimistic prediction results from prior research on the recognition of affective expression (e.g., enacted and labeled speech) from lab settings generalize to the recognition of subjective affective experience from everyday speech. Moreover, our results highlight important design implications, suggesting that speech-based emotion recognition systems intended for real-world applications should integrate semantic content into their models. The ability to detect how people genuinely feel, as opposed to merely observing what they outwardly express, is crucial to gaining a deeper understanding of human emotions and guiding the development of technologies that are better placed to serve and respond to our emotional needs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1 . Flow diagram illustrating data collection, processing, and predictive modeling in Study 1 and Study 2.</figDesc><graphic coords="9,72.00,222.90,468.00,249.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2 . Box and whisker plot of out-of-sample prediction performance measures from five times repeated ten-fold cross-validation for each affect measure. Feature sets (either prosodic or semantic features) for model training are shown in parentheses. Names of models that are significantly better than chance are displayed in boldface. Pearson correlation is not available for baseline models because it predicts a constant value, for which correlation measures are not defined.</figDesc><graphic coords="19,72.01,111.44,467.99,422.09" type="bitmap" /></figure>
		</body>
		<back>

			<div type="funding">
<div><p>for their support with the technical implementation of the on-device voice feature extraction for Study 1, the <rs type="funder">Leibniz Institute for Psychology (ZPID)</rs> for the funding for data collection for Study 1, and <rs type="person">Sumer Vaid</rs> for the technical support with the setup of the computational environment for the analyses of Study 2. This project was supported by the <rs type="funder">Swiss National Science Foundation (SNSF)</rs> under project number <rs type="grantNumber">215303</rs>, the <rs type="funder">National Science Foundation (NSF)</rs> under project number <rs type="grantNumber">1758835</rs>, a <rs type="funder">Stanford HAI Google Cloud Credit Award</rs>, and a scholarship of the <rs type="funder">German Academic Scholarship Foundation</rs> awarded to the first author.</p><p>The authors made the following contributions.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_h7Dx4Rg">
					<idno type="grant-number">215303</idno>
				</org>
				<org type="funding" xml:id="_hME8fph">
					<idno type="grant-number">1758835</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data, materials, and code availability</head><p>All data processing and statistical analyses in this work were performed with the statistical software R version 4.0.4 (R Core Team, 2021). For machine learning, we used the mlr3 framework <ref type="bibr" target="#b27">(Lang et al., 2019)</ref>. Specifically, we used the glmnet <ref type="bibr" target="#b16">(Friedman, Hastie, &amp; Tibshirani, 2010)</ref> and ranger <ref type="bibr" target="#b64">(Wright &amp; Ziegler, 2017)</ref> packages to fit machine learning models. We provide the R code, figures, and results in the project's repository on the Open Science Framework (<ref type="url" target="https://osf.io/a5db8/">https://osf.io/a5db8/</ref>). Raw data of the voice samples may contain personally identifiable information and, therefore, cannot be shared publicly. We preregistered Study 1 as a transparent account of our work <ref type="bibr" target="#b24">(Koch &amp; Schoedel, 2021)</ref> and extended the analytical approach to the data from Study 2.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A computationally efficient speech emotion recognition system employing machine learning classifiers and ensemble learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Aishwarya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Seemakurthy</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10772-024-10095-8</idno>
		<ptr target="https://doi.org/10.1007/s10772-024-10095-8" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Speech Technology</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="239" to="254" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2006.11477</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2006.11477" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Acoustic profiles in vocal emotion expression</title>
		<author>
			<persName><forename type="first">R</forename><surname>Banse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
		<idno type="DOI">10.1037/0022-3514.70.3.614</idno>
		<ptr target="https://doi.org/10.1037/0022-3514.70.3.614" />
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="614" to="636" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The Automatic Recognition of Emotions in Speech</title>
		<author>
			<persName><forename type="first">A</forename><surname>Batliner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Seppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Steidl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Devillers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vidrascu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-15184-2_6</idno>
		<ptr target="https://doi.org/10.1007/978-3-642-15184-2_6" />
	</analytic>
	<monogr>
		<title level="m">Cognitive Technologies</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="71" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Introducing the Geneva Multimodal expression corpus for experimental research on emotion perception</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bänziger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mortillaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0025827</idno>
		<ptr target="https://doi.org/10.1037/a0025827" />
	</analytic>
	<monogr>
		<title level="j">Emotion</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1161" to="1179" />
			<date type="published" when="2012">2012</date>
			<pubPlace>Washington, D.C.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Prosody and Semantics Are Separate but Not Separable Channels in the Perception of Emotional Speech: Test for Rating of Emotions in Speech</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Multani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Shakuf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rudzicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H H M</forename><surname>Van Lieshout</surname></persName>
		</author>
		<idno type="DOI">10.1044/2015_JSLHR-H-14-0323</idno>
		<ptr target="https://doi.org/10.1044/2015_JSLHR-H-14-0323" />
	</analytic>
	<monogr>
		<title level="j">Journal of Speech, Language, and Hearing Research</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="72" to="89" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Resampling Methods for Meta-Model Validation with Recommendations for Evolutionary Computation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bischl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mersmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Trautmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Weihs</surname></persName>
		</author>
		<idno type="DOI">10.1162/EVCO_a_00069</idno>
		<ptr target="https://doi.org/10.1162/EVCO_a_00069" />
	</analytic>
	<monogr>
		<title level="j">Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="249" to="275" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep learning reveals what vocal bursts express in different cultures</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tzirakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Baird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Opara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename><surname>Cowen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename></persName>
		</author>
		<idno type="DOI">10.1038/s41562-022-01489-2</idno>
		<ptr target="https://doi.org/10.1038/s41562-022-01489-2" />
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="240" to="250" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">In Search of State and Trait Emotion Markers in Mobile-Sensed Language: Field Study</title>
		<author>
			<persName><forename type="first">C</forename><surname>Carlier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Niemeijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mestdagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bauwens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vanbrabant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Geurts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename><surname>Kuppens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
		<idno type="DOI">10.2196/31724</idno>
		<ptr target="https://doi.org/10.2196/31724" />
	</analytic>
	<monogr>
		<title level="j">JMIR Mental Health</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">31724</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The primacy of categories in the recognition of 12 emotions in speech prosody across two cultures</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Cowen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laukka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Elfenbein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Keltner</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41562-019-0533-6</idno>
		<ptr target="https://doi.org/10.1038/s41562-019-0533-6" />
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="369" to="382" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Emotional Speech Perception: A set of semantically validated German neutral and emotionally affective sentences</title>
		<author>
			<persName><forename type="first">S</forename><surname>Defren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>De Brito Castilho Wesseling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Shakuf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lachmann</surname></persName>
		</author>
		<idno type="DOI">10.21437/SpeechProsody.2018-145</idno>
		<ptr target="https://doi.org/10.21437/SpeechProsody.2018-145" />
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Speech Prosody</title>
		<imprint>
			<publisher>ISCA</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="714" to="718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The Repertoire of Nonverbal Behavior: Categories, Origins, Usage, and Coding</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">V</forename><surname>Friesen</surname></persName>
		</author>
		<idno type="DOI">10.1515/semi.1969.1.1.49</idno>
		<ptr target="https://doi.org/10.1515/semi.1969.1.1.49" />
	</analytic>
	<monogr>
		<title level="j">Semiotica</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="98" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Survey on speech emotion recognition: Features, classification schemes, and databases</title>
		<author>
			<persName><forename type="first">M</forename><surname>El Ayadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Kamel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Karray</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2010.09.020</idno>
		<ptr target="https://doi.org/10.1016/j.patcog.2010.09.020" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="572" to="587" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The Geneva Minimalistic Acoustic Parameter Set (GeMAPS) for Voice Research and Affective Computing</title>
		<author>
			<persName><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Andre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename></persName>
		</author>
		<idno type="DOI">10.1109/TAFFC.2015.2457417</idno>
		<ptr target="https://doi.org/10.1109/TAFFC.2015.2457417" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="190" to="202" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Opensmile: The munich versatile and fast open-source audio feature extractor</title>
		<author>
			<persName><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wöllmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<idno type="DOI">10.1145/1873951.1874246</idno>
		<ptr target="https://doi.org/10.1145/1873951.1874246" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Multimedia -MM &apos;10</title>
		<meeting>the International Conference on Multimedia -MM &apos;10<address><addrLine>Firenze, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1459">2010. 1459</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Regularization paths for generalized linear models via coordinate descent</title>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving Domain Generalization in Speech Emotion Recognition with Whisper</title>
		<author>
			<persName><forename type="first">E</forename><surname>Goron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dinov</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP48485.2024.10446997</idno>
		<ptr target="https://doi.org/10.1109/ICASSP48485.2024.10446997" />
	</analytic>
	<monogr>
		<title level="m">ICASSP 2024 -2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="11631" to="11635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The Vera am Mittag German audio-visual emotional speech database</title>
		<author>
			<persName><forename type="first">M</forename><surname>Grimm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kroschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICME.2008.4607572</idno>
		<ptr target="https://doi.org/10.1109/ICME.2008.4607572" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo</title>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="865" to="868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Keep on scrolling? Using intensive longitudinal smartphone sensing data to assess how everyday smartphone usage behaviors are related to well-being</title>
		<author>
			<persName><forename type="first">F</forename><surname>Große Deters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schoedel</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.chb.2023.107977</idno>
		<ptr target="https://doi.org/10.1016/j.chb.2023.107977" />
	</analytic>
	<monogr>
		<title level="j">Computers in Human Behavior</title>
		<imprint>
			<biblScope unit="volume">150</biblScope>
			<biblScope unit="page">107977</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The weirdest people in the world?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Henrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Heine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Norenzayan</surname></persName>
		</author>
		<idno type="DOI">10.1017/S0140525X0999152X</idno>
		<ptr target="https://doi.org/10.1017/S0140525X0999152X" />
	</analytic>
	<monogr>
		<title level="j">The Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="83" to="135" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Voice analytics in business research: Conceptual foundations, acoustic feature extraction, and applications</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hildebrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Efthymiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Busquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Hampton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Novak</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jbusres.2020.09.020</idno>
		<ptr target="https://doi.org/10.1016/j.jbusres.2020.09.020" />
	</analytic>
	<monogr>
		<title level="j">Journal of Business Research</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page" from="364" to="374" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units</title>
		<author>
			<persName><forename type="first">W.-N</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bolte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lakhotia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2106.07447</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2106.07447" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Text: An R-package for Analyzing and Visualizing Human Language Using Natural Language Processing and Deep Learning</title>
		<author>
			<persName><forename type="first">O</forename><surname>Kjell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Giorgi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Schwartz</surname></persName>
		</author>
		<idno type="DOI">10.31234/osf.io/293kt</idno>
		<ptr target="https://doi.org/10.31234/osf.io/293kt" />
	</analytic>
	<monogr>
		<title level="j">PsyArXiv</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Predicting Affective States from Acoustic Voice Cues Collected with Smartphones</title>
		<author>
			<persName><forename type="first">T</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schoedel</surname></persName>
		</author>
		<idno type="DOI">10.23668/psycharchives.4454</idno>
		<ptr target="https://doi.org/10.23668/psycharchives.4454" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Voice-only communication enhances empathic accuracy</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Kraus</surname></persName>
		</author>
		<idno type="DOI">10.1037/amp0000147</idno>
		<ptr target="https://doi.org/10.1037/amp0000147" />
	</analytic>
	<monogr>
		<title level="j">The American Psychologist</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="644" to="654" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gender and Age Estimation Methods Based on Speech Using Deep Neural Networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kwasny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hemmerling</surname></persName>
		</author>
		<idno type="DOI">10.3390/s21144785</idno>
		<ptr target="https://doi.org/10.3390/s21144785" />
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page">4785</biblScope>
			<date type="published" when="2021">2021</date>
			<pubPlace>Basel, Switzerland</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mlr3: A modern object-oriented machine learning framework in R</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schratz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pfisterer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Coors</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename><surname>Bischl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
		<idno type="DOI">10.21105/joss.01903</idno>
		<ptr target="https://doi.org/10.21105/joss.01903" />
	</analytic>
	<monogr>
		<title level="j">Journal of Open Source Software</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">44</biblScope>
			<date type="published" when="1903">2019. 1903</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The Sound of Emotional Prosody: Nearly 3 Decades of Research and Future Directions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Larrouy-Maestri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Poeppel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Pell</surname></persName>
		</author>
		<idno type="DOI">10.1177/17456916231217722</idno>
		<ptr target="https://doi.org/10.1177/17456916231217722" />
	</analytic>
	<monogr>
		<title level="j">Perspectives on Psychological Science</title>
		<imprint>
			<biblScope unit="page">17456916231217722</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cross-Cultural Emotion Recognition and In-Group Advantage in Vocal Expression: A Meta-Analysis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Laukka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Elfenbein</surname></persName>
		</author>
		<idno type="DOI">10.1177/1754073919897295</idno>
		<ptr target="https://doi.org/10.1177/1754073919897295" />
	</analytic>
	<monogr>
		<title level="j">Emotion Review</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="11" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cultural differences in emotion: Differences in emotional arousal level between the East and the West</title>
		<author>
			<persName><forename type="first">N</forename><surname>Lim</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.imr.2016.03.004</idno>
		<ptr target="https://doi.org/10.1016/j.imr.2016.03.004" />
	</analytic>
	<monogr>
		<title level="j">Integrative Medicine Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="105" to="109" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Prosody Dominates Over Semantics in Emotion Word Processing: Evidence From Cross-Channel and Cross-Modal Stroop Effects</title>
		<author>
			<persName><forename type="first">Lin</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ding</forename><surname>Hongwei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhang</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1044/2020_JSLHR-19-00258</idno>
		<ptr target="https://doi.org/10.1044/2020_JSLHR-19-00258" />
	</analytic>
	<monogr>
		<title level="j">Journal of Speech, Language, and Hearing Research</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="896" to="912" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1907.11692</idno>
		<ptr target="https://doi.org/10.48550/arXiv.1907.11692" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Evaluating voice samples as a potential source of information about personality</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">N K</forename><surname>Marrero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Gosling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Pennebaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Harari</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.actpsy.2022.103740</idno>
		<ptr target="https://doi.org/10.1016/j.actpsy.2022.103740" />
	</analytic>
	<monogr>
		<title level="j">Acta Psychologica</title>
		<imprint>
			<biblScope unit="volume">230</biblScope>
			<biblScope unit="page">103740</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Evaluating Contextual Embeddings and their Extraction Layers for Depression Assessment</title>
		<author>
			<persName><forename type="first">M</forename><surname>Matero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Schwartz</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.wassa-1.9</idno>
		<ptr target="https://doi.org/10.18653/v1/2022.wassa-1" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Workshop on Computational Approaches to Subjectivity, Sentiment &amp; Social Media Analysis</title>
		<meeting>the 12th Workshop on Computational Approaches to Subjectivity, Sentiment &amp; Social Media Analysis<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="89" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Using Big Data as a window into consumers&apos; psychology</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Matz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Netzer</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cobeha.2017.05.009</idno>
		<ptr target="https://doi.org/10.1016/j.cobeha.2017.05.009" />
	</analytic>
	<monogr>
		<title level="j">Current Opinion in Behavioral Sciences</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="7" to="12" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The Electronically Activated Recorder (EAR): A Method for the Naturalistic Observation of Daily Social Behavior</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Mehl</surname></persName>
		</author>
		<idno type="DOI">10.1177/0963721416680611</idno>
		<ptr target="https://doi.org/10.1177/0963721416680611" />
	</analytic>
	<monogr>
		<title level="j">Current Directions in Psychological Science</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="184" to="190" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Is Speech the New Blood? Recent Progress in AI-Based Disease Detection From Audio in a Nutshell</title>
		<author>
			<persName><forename type="first">M</forename><surname>Milling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pokorny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bartl-Pokorny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<idno type="DOI">10.3389/fdgth.2022.886615</idno>
		<ptr target="https://doi.org/10.3389/fdgth.2022.886615" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Digital Health</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">886615</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Inference for the Generalization Error</title>
		<author>
			<persName><forename type="first">C</forename><surname>Nadeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1024068626366</idno>
		<ptr target="https://doi.org/10.1023/A:1024068626366" />
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="239" to="281" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">How ready is speech-to-text for psychological language research? Evaluating the validity of AI-generated English transcripts for analyzing free-spoken responses in younger and older adults</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Pfeifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Chilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Grilli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Mehl</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13428-024-02440-1</idno>
		<ptr target="https://doi.org/10.3758/s13428-024-02440-1" />
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Mobile Sensing around the Globe: Considerations for Cross-Cultural Research</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Modersitzki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Gloystein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Müller</surname></persName>
		</author>
		<idno type="DOI">10.1017/9781009099813.009</idno>
		<ptr target="https://doi.org/10.1017/9781009099813.009" />
	</analytic>
	<monogr>
		<title level="m">Technology and Measurement around the Globe</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Tay</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Woo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Behrend</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="176" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Anger recognition in speech using acoustic and linguistic cues</title>
		<author>
			<persName><forename type="first">T</forename><surname>Polzehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wagner</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.specom.2011.05.002</idno>
		<ptr target="https://doi.org/10.1016/j.specom.2011.05.002" />
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1198" to="1209" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cracking the social code of speech prosody using reverse correlation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ponsot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Burred</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Belin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-J</forename><surname>Aucouturier</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1716090115</idno>
		<ptr target="https://doi.org/10.1073/pnas.1716090115" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="3972" to="3977" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The circumplex model of affect: An integrative approach to affective neuroscience, cognitive development, and psychopathology</title>
		<author>
			<persName><forename type="first">J</forename><surname>Posner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Peterson</surname></persName>
		</author>
		<idno type="DOI">10.1017/S0954579405050340</idno>
		<ptr target="https://doi.org/10.1017/S0954579405050340" />
	</analytic>
	<monogr>
		<title level="j">Development and Psychopathology</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="715" to="734" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">R: A Language and Environment for Statistical Computing</title>
		<author>
			<persName><forename type="first">Team</forename><surname>Core</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>R Foundation for Statistical Computing</publisher>
			<pubPlace>Vienna, Austria</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A Circumplex Model of Affect</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Russell</surname></persName>
		</author>
		<idno type="DOI">10.1037/h0077714</idno>
		<ptr target="https://doi.org/10.1037/h0077714" />
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1161" to="1178" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Cross-cultural recognition of basic emotions through nonverbal emotional vocalizations</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Sauter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Eisner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename></persName>
		</author>
		<idno type="DOI">10.1073/pnas.0908239106</idno>
		<ptr target="https://doi.org/10.1073/pnas.0908239106" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2408" to="2412" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Vocal communication of emotion: A review of research paradigms</title>
		<author>
			<persName><forename type="first">K</forename><surname>Scherer</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0167-6393(02)00084-5</idno>
		<ptr target="https://doi.org/10.1016/S0167-6393(02)00084-5" />
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="227" to="256" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Snapshots of daily life: Situations investigated through the lens of smartphone sensing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Schoedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kunz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bemmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bühner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sust</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Basic Protocol: Smartphone Sensing Panel Study</title>
		<author>
			<persName><forename type="first">R</forename><surname>Schoedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Oldemeier</surname></persName>
		</author>
		<idno type="DOI">10.23668/psycharchives.2901</idno>
		<ptr target="https://doi.org/10.23668/psycharchives.2901" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Speech emotion recognition: Two decades in a nutshell, benchmarks, and ongoing trends</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<idno type="DOI">10.1145/3129340</idno>
		<ptr target="https://doi.org/10.1145/3129340" />
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="90" to="99" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Speech emotion recognition combining acoustic features and linguistic information in a hybrid support vector machine-belief network architecture</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rigoll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lang</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2004.1326051</idno>
		<ptr target="https://doi.org/10.1109/ICASSP.2004.1326051" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">577</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Steidl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Batliner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hirschberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Burgoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Baird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename><surname>Evanini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2016-129</idno>
		<ptr target="https://doi.org/10.21437/Interspeech.2016-129" />
		<title level="m">The INTERSPEECH 2016 Computational Paralinguistics Challenge: Deception, Sincerity and Language</title>
		<imprint>
			<date type="published" when="2005">2016. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">How to e-mental health: A guideline for researchers and practitioners using digital technology in the context of mental health</title>
		<author>
			<persName><forename type="first">C</forename><surname>Seiferth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Aas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Brandhorst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Carlbring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Conzelmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename><surname>Löchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<idno type="DOI">10.1038/s44220-023-00085-1</idno>
		<ptr target="https://doi.org/10.1038/s44220-023-00085-1" />
	</analytic>
	<monogr>
		<title level="j">Nature Mental Health</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="542" to="554" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Unsupervised Personalization of an Emotion Recognition System: The Unique Properties of the Externalization of Valence in Speech</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
		<idno type="DOI">10.1109/TAFFC.2022.3187336</idno>
		<ptr target="https://doi.org/10.1109/TAFFC.2022.3187336" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1959" to="1972" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">The language of well-being: Tracking fluctuations in emotion experience through everyday speech</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vazire</surname></persName>
		</author>
		<idno type="DOI">10.1037/pspp0000244</idno>
		<ptr target="https://doi.org/10.1037/pspp0000244" />
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="364" to="387" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Modelling individual and cross-cultural variation in the mapping of emotions to speech prosody</title>
		<author>
			<persName><forename type="first">P</forename><surname>Van Rijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Larrouy-Maestri</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41562-022-01505-5</idno>
		<ptr target="https://doi.org/10.1038/s41562-022-01505-5" />
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="386" to="396" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Talk to Me: How Voice Computing Will Transform the Way We Live, Work, and Think</title>
		<author>
			<persName><forename type="first">J</forename><surname>Vlahos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Eamon</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Books</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Automatic Recognition of Emotions from Speech: A Review of the Literature and Recommendations for Practical Realisation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Vogt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wagner</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-85099-1_7</idno>
		<ptr target="https://doi.org/10.1007/978-3-540-85099-1_7" />
	</analytic>
	<monogr>
		<title level="m">Affect and Emotion in Human-Computer Interaction</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peter</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Beale</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">4868</biblScope>
			<biblScope unit="page" from="75" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Dawn of the Transformer Era in Speech Emotion Recognition: Closing the Valence Gap</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Triantafyllopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wierstorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Burkhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Schuller</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2023.3263585</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2023.3263585" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="10745" to="10759" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Not) hearing happiness: Predicting fluctuations in happy mood from acoustic cues using machine learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Weidman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vazire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Quoidbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Ungar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Dunn</surname></persName>
		</author>
		<idno type="DOI">10.1037/emo0000571</idno>
		<ptr target="https://doi.org/10.1037/emo0000571" />
	</analytic>
	<monogr>
		<title level="j">Emotion</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="642" to="658" />
			<date type="published" when="2020">2020</date>
			<pubPlace>Washington, D.C.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">On the Acoustics of Emotion in Audio: What Speech, Music, and Sound have in Common</title>
		<author>
			<persName><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mortillaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2013.00292</idno>
		<ptr target="https://doi.org/10.3389/fpsyg.2013.00292" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Real vs. Acted emotional speech</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wilting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Krahmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G J</forename><surname>Swerts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Spoken Language Processing (Interspeech</title>
		<meeting>the International Conference on Spoken Language Processing (Interspeech</meeting>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ziegler</surname></persName>
		</author>
		<idno type="DOI">10.18637/jss.v077.i01</idno>
		<ptr target="https://doi.org/10.18637/jss.v077.i01" />
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Multi-modal data collection for measuring health, behavior, and living environment of large-scale participant cohorts</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bastami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Maestre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Thomaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Julien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename><surname>Nagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename></persName>
		</author>
		<idno type="DOI">10.1093/gigascience/giab044</idno>
		<ptr target="https://doi.org/10.1093/gigascience/giab044" />
	</analytic>
	<monogr>
		<title level="j">GigaScience</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Regularization and variable selection via the elastic net</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1467-9868.2005.00503.x</idno>
		<ptr target="https://doi.org/10.1111/j.1467-9868.2005.00503" />
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="301" to="320" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
