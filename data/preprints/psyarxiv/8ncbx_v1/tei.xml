<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Meta-Research: Does Scientific Productivity Increase the Publication of Positive Results? Examining Research Groups&apos; Scientific Productivity and Positive Results in a German Clinical Psychology Sample</title>
				<funder>
					<orgName type="full">Berlin University Alliance</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Louis</forename><surname>Schiekiera</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Clinical Psychological Intervention</orgName>
								<orgName type="institution">Freie Universität Berlin</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><roleName>Clinical</roleName><forename type="first">Helen</forename><surname>Niemeyer</surname></persName>
							<email>helen.niemeyer@fu-berlin.de</email>
							<affiliation key="aff0">
								<orgName type="department">Clinical Psychological Intervention</orgName>
								<orgName type="institution">Freie Universität Berlin</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Psychological Intervention</orgName>
								<orgName type="department" key="dep2">Department of Education and Psychology</orgName>
								<orgName type="institution">Freie Universität Berlin</orgName>
								<address>
									<addrLine>Schloßstraße 1</addrLine>
									<postCode>12163</postCode>
									<settlement>Berlin, Email</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Meta-Research: Does Scientific Productivity Increase the Publication of Positive Results? Examining Research Groups&apos; Scientific Productivity and Positive Results in a German Clinical Psychology Sample</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CF426AFAE956C06728CFB497B035946C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-10-22T02:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>positive results</term>
					<term>negative results</term>
					<term>scientific productivity</term>
					<term>clinical psychology</term>
					<term>publication bias</term>
					<term>open science</term>
					<term>meta-research</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Background: The overrepresentation of positive results in psychology is often attributed in part to publication bias. However, the impact of research group output on the prevalence of positive results has not yet been investigated. The present study examines whether German clinical psychology research groups with high versus low publication outputs differ in the prevalence of positive outcomes in their publications. Methods: Scientific productivity was defined as the ratio of quantitative-empirical publications to the number of academic staff per chair. We analyzed publications authored by clinical psychology researchers at German universities from 2013 to 2022, sourced from PubMed and OpenAlex. After excluding meta-analyses, reviews, and nonempirical studies, 2,280 empirical studies from 99 research groups were identified. We then randomly sampled and coded 300 papers, evenly split between the highest and lowest output quartiles, and examined the first hypothesis. Results: There was no statistically significant difference between the highest and the lowest output quartiles, with both reporting approximately 90% positive results. Higher group paper counts were not associated with more positive results.</p><p>However, results with partial support were significantly more prevalent in the highest output quartile than in the lowest output quartile. Conclusion: Our results suggest a general excess of positive results in clinical psychology. Contrary to our hypothesis, German clinical psychology research groups with high and low publication outputs do not differ in the prevalence of positive outcomes in their publications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RUNNING HEAD: POSITIVE RESULTS AND SCIENTIFIC PRODUCTIVITY IN GERMAN CLINICAL PSYCHOLOGY</head><p>Positive results, defined as findings that fully or partially support a tested hypothesis, are typically contrasted with negative results, which do not support (or which contrast with) the hypothesis <ref type="bibr" target="#b11">(Fanelli, 2012)</ref>. In the quantitative sciences, the criterion for determining whether a hypothesis is supported is often the statistical significance of an effect <ref type="bibr" target="#b15">(Hubbard, 2015)</ref>.</p><p>Consequently, positive and negative results are typically distinguished by their statistical significance. Since <ref type="bibr">Fanelli (2010a;</ref><ref type="bibr">2012)</ref> observed that the extent of positive results is particularly pronounced in psychology and psychiatry studies compared to other disciplines of science, many studies have investigated positive results in these disciplines. Evidence of high rates of positive results in psychology dates back to the middle of the last century, when <ref type="bibr" target="#b30">Sterling (1959)</ref> found that 97% of studies using significance tests in psychology journals reported positive results. Sterling's observations, which he and his colleagues later replicated <ref type="bibr" target="#b31">(Sterling et al., 1995)</ref>, are in line with more recent studies examining the statistical significance of the first reported hypothesis, which revealed a proportion of positive results ranging from 91-97% in psychology <ref type="bibr">(Fanelli, 2010a</ref><ref type="bibr" target="#b11">(Fanelli, , 2012;;</ref><ref type="bibr" target="#b27">Scheel et al, 2021;</ref><ref type="bibr">Open Science Collaboration, 2015)</ref>. However, studies investigating all hypotheses in original studies have found lower rates of positive results, such as 66% <ref type="bibr" target="#b35">(Toth et al., 2021)</ref> and 67% (van den <ref type="bibr">Akker et al., 2023)</ref>.</p><p>The high prevalence of positive results has been attributed to several factors, including publication bias <ref type="bibr" target="#b11">(Fanelli, 2012;</ref><ref type="bibr" target="#b27">Scheel et al., 2021)</ref>, questionable research practices <ref type="bibr" target="#b27">(Scheel et al., 2021)</ref>, and robust statistical power <ref type="bibr" target="#b24">(Monsarrat &amp; Vergnes, 2018;</ref><ref type="bibr" target="#b32">Sterne et al., 2005)</ref>, although the latter cannot fully explain the high positive rates in the field of psychology given the often small sample sizes in psychological research <ref type="bibr" target="#b26">(Schäfer &amp; Schwarz, 2019;</ref><ref type="bibr" target="#b33">Szucs &amp; Ioannidis, 2017)</ref>.</p><p>Large effects and significant results in underpowered studies suggest that factors such as publication bias and questionable research practices may likewise contribute to these findings <ref type="bibr" target="#b27">(Scheel et al., 2021)</ref>.</p><p>Commonly proposed methods to reduce the overrepresentation of positive results in psychology include registered reports (RRs) and preregistration. <ref type="bibr" target="#b27">Scheel et al. (2021)</ref> found that the proportion of positive results in standard psychology reports lay at 96%, compared to 44% in RRs, while <ref type="bibr">van den Akker et al. (2023)</ref> did not detect a significant difference between preregistered and non-preregistered studies in terms of the proportion of positive results.</p><p>In a study analyzing over 4,600 papers from all scientific disciplines published from 1990 to 2007, <ref type="bibr" target="#b11">Fanelli (2012)</ref> observed that negative results are disappearing throughout the sciences, and especially the social sciences. This overrepresentation of positive results has often been framed as a symptom of the transformation of the scientific culture into a 'publish-or-perish' culture, in which increasing competition and pressures to publish distort the cumulative scientific knowledge production <ref type="bibr" target="#b36">(Van Dalen et al., 2012;</ref><ref type="bibr" target="#b34">Tian et al., 2016)</ref>. In another study, <ref type="bibr" target="#b10">Fanelli (2010b)</ref> demonstrated that rates of positive results were correlated with research &amp; development (R&amp;D) expenditures per capita in US federal states, which Fanelli described as indicators of "competitive and productive" academic environments (p. 1). This account suggests that the pressure to publish is primarily determined by macro-level factors (broader political, cultural, and technological<ref type="foot" target="#foot_0">1</ref> processes), but it neglects the fact that pressures to publish might vary in terms of factors on the micro (individual processes) or meso (organizational processes) level as well. From a sociological perspective on science, the non-publication of negative results can also be understood as the outcome of an evaluation process <ref type="bibr" target="#b21">(Lamont, 2012;</ref><ref type="bibr" target="#b17">Kjellberg et al., 2013)</ref>. Such an evaluation process is a complex social interaction involving multiple actors, such as co-authors, and their negotiations <ref type="bibr">(Lamont, 2009;</ref><ref type="bibr" target="#b20">Krüger &amp; Reinhart, 2017)</ref>, and it is shaped by the social environment, including the organizational context of the respective research group <ref type="bibr" target="#b14">(Friedland &amp; Alford, 1991)</ref>.</p><p>While some experimental studies have investigated individual researchers' evaluations of the publishability of study results as a function of their direction <ref type="bibr" target="#b0">(Atkinson et al., 1982;</ref><ref type="bibr" target="#b1">Augusteijn et al., 2023;</ref><ref type="bibr" target="#b5">Chopra et al., 2022;</ref><ref type="bibr" target="#b7">Elson et al., 2020;</ref><ref type="bibr" target="#b8">Epstein, 1990;</ref><ref type="bibr" target="#b22">Mahoney, 1977)</ref>, metascience studies on rates of positive results and publication bias have overlooked meso-level factors.</p><p>Therefore, the present study investigates the relevance of a research group's scientific productivity for rates of positive results in clinical psychology. We define scientific productivity as the ratio of the number of quantitative empirical publications within a specific time period to the number of academic staff per research chair.</p><p>We propose that positive results might be more prevalent in research groups with a high publication output for two reasons, one social and the other statistical: First, we assume that on the meso level, the pressure to publish might be more intense in research chairs with high scientific productivity due, for example, to greater expectations of continuous productivity or higher performance orientation, which might in turn result in higher numbers of positive findings as these might be considered to have a higher chance of acceptance for publication <ref type="bibr" target="#b13">(Franco et al., 2014)</ref>.</p><p>Second, we suggest that significant results might be more prevalent in research groups with high scientific productivity due to the discussed associations of research funding with sample size <ref type="bibr" target="#b2">(Billingham et al., 2013;</ref><ref type="bibr">Button et al., 2013)</ref> and with publication output <ref type="bibr" target="#b16">(Jacob &amp; Lefgren, 2011)</ref>.</p><p>In short, research chairs with higher scientific productivity might receive more funding, which may in turn enable larger sample sizes and reduce the possibility of underpowered studies leading to type-II errors <ref type="bibr" target="#b29">(Shreffler &amp; Huecker, 2023)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Present Study</head><p>The present study aimed first, to examine the extent to which an excess of positive results, as typically identified in psychology, can also be replicated in clinical psychology and second, to assess the relationship between the proportion of positive results and scientific productivity. We chose to focus on German clinical psychology for two key reasons: First, a comprehensive survey of all published papers in this distinct sample is feasible, allowing us to determine each research chair's publication output over a specific time span. This ability to track the full empirical research output facilitates relative comparisons between different research chairs based on higher or lower publication counts per chair. Second, even though multiple studies have investigated positive results in psychology, it remains unknown whether rates of positive results are likewise exceptionally high in clinical psychology, a sub-discipline that has a strong impact on health systems and society <ref type="bibr" target="#b6">(Doran &amp; Kinchin, 2017;</ref><ref type="bibr" target="#b19">Knapp &amp; Wong, 2020;</ref><ref type="bibr" target="#b23">McDaid et al., 2019)</ref>.</p><p>Our primary hypothesis (H1) was that research groups with lower scientific productivity would report a lower prevalence of positive results than their counterparts with a higher publication output. To test this hypothesis, we collected all empirical studies published by clinical psychology researchers affiliated with German universities between 2013 and 2022. Excluding non-empirical papers, this resulted in a dataset of 2,280 studies. We used a random sample of 300 papers, equally split between the bottom (Q1) and top (Q4) quartiles of scientific productivity, thus taking into account the number of researchers in a research chair, and categorized each paper as reporting positive (full or partial support) or negative results (no support) based on the first reported hypothesis, following the methodology used by <ref type="bibr">Fanelli (2010a;</ref><ref type="bibr">2012)</ref> and <ref type="bibr">Scheel and colleagues (2021)</ref>.</p><p>Furthermore, we tested a secondary hypothesis (H2) by employing logistic regression to explore whether a predictive relationship exists between the publication count of a research group and the likelihood of positive outcomes, i.e. without taking into account the number of researchers in a research chair. This hypothesis was introduced a priori because our measure of scientific productivity might penalize research groups with a higher number of researchers and benefit research groups with a lower number of researchers, depending on whether a high number of early or senior career researchers are affiliated with a respective group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Open Practices</head><p>Our study was pre-registered on the Open Science Framework (OSF). All data were sourced from published papers. However, since we categorize research chairs according to their scientific productivity, which might be interpreted as a sensitive measure of academic success or failure, we do not publicly disclose our data. Nevertheless, all other aspects of the study methodology, including data collection and analysis procedures, are fully published to ensure transparency and reproducibility of the findings. All R analysis scripts are published on the project's OSF page.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sample of Publications to Estimate Scientific Productivity</head><p>To estimate the scientific productivity of research chairs, we collected all quantitativeempirical original publications first-authored by clinical psychology researchers affiliated with German universities from 2013 to 2022. The full procedure is depicted in Figure <ref type="figure" target="#fig_0">1</ref>. Exclusions from this collection included meta-analyses, reviews, editorials, comments, corrigenda, errata, letters, and qualitative studies. 2 2 The first five data-sampling steps presented are identical to the data collection of the main dataset in <ref type="bibr">Schiekiera et al. (2024)</ref>, which is presented in the appendix. Since the data collection and definition plays a major role in the computations for the outcome scientific productivity for this article, it is presented in the main text here and in the appendix in <ref type="bibr">Schiekiera et al. (2024)</ref>. First, we compiled a list of all state university working groups (n = 99) which focused on clinical psychology, psychotherapy, or related fields in Germany, excluding laboratories in hospitals. The chairs are located at 52 different universities and cover all 16 German federal states.</p><p>Second, we gathered all names, email addresses, and academic level (predoctoral, postdoctoral, or professorship) of employed researchers from the working group homepages, resulting in a list comprising 1,128 researcher names. Of these, 24 researchers were listed on more than one laboratory or chair web page. We extracted study metadata from (a) PubMed and (b) OpenAlex using the R packages rentrez (Winter, 2017) and openalexR <ref type="bibr">(Priem et al., 2023)</ref>, respectively. A for loop was used to iterate over the researchers' names (n = 1,128) for both OpenAlex and PubMed. The exact search terms for OpenAlex and PubMed can be found in the Appendix. This procedure resulted in a total of 32,181 articles for PubMed and 19,662 articles for OpenAlex.</p><p>Third, the extracted OpenAlex and PubMed data were cleaned using a script. Initially, studies authored by researchers who were not on our predefined list were excluded (PM: n = 9,663;</p><p>OpenAlex: n = 8,827), along with duplicate articles (PM: n = 2490; OpenAlex: n = 5,570).</p><p>Notably, 55% (17,540) of the PubMed articles were attributed to being authored by just five researchers, consisting of two postdocs and three PhD candidates. This anomaly arose because PubMed has a built-in spelling correction function, which mistakenly included researchers with similar names in the corpus. Investigating this discrepancy, one of the present authors (LS) conducted a manual review of the total number of articles authored by these five researchers within our sample, which revealed that only 56 articles were actually first-authored by these researchers. This led to the exclusion of the erroneously included 17,484 entries from our dataset, which we categorized as 'name confusion'. In the case of OpenAlex, additional cleaning steps were performed to exclude data points that did not represent journal articles according to OpenAlex (n = 6,209), or which contained the terms 'systematic review <ref type="bibr">', 'meta-analysis', 'comment', 'corrigendum', 'erratum' or 'correction'</ref> in the title section (OA: n = 972). This step was not necessary for PubMed as these article types could already be excluded through specifications in the search terms.</p><p>In the fourth step of our analysis, LS manually screened both OpenAlex and PubMed study and journal titles using an Excel spreadsheet and excluded all non-psychological studies from the corpus. Non-psychological studies were broadly defined as those not reporting the investigation of any association with mental processes, emotions, or behaviors in humans. This criterion led to the removal of 347 articles from the PubMed dataset and 938 articles from the OpenAlex dataset that did not belong to our field of study. For the PubMed dataset, we identified and excluded 134 instances of name confusion (where researchers with similar or common names were incorrectly included) and 62 articles classified under the wrong article types (e.g., review articles instead of original research). The exclusions for the OpenAlex dataset were more extensive: We found 117 instances of name confusion, 717 articles under incorrect article types, and additionally identified and removed 84 duplicates.</p><p>In the fifth step, data from OpenAlex and PubMed were merged, with overlapping entries being excluded based on duplicate DOIs (n = 1,487). Standardized metadata were extracted using DOIs via the rcrossref library in R <ref type="bibr" target="#b5">(Chamberlain et al., 2022)</ref>. Crossref indicated that 33 of these entries did not represent journal articles, leading to their exclusion. Subsequently, abstracts were gathered using EndNote 20 (Clarivate, 2023). However, 178 entries lacked identifiable abstracts and were thus removed from the corpus.</p><p>The resulting dataset, comprising 2,540 entries, was imported into Covidence (Covidence, 2023), which identified and removed an additional 71 duplicates based on the available metadata. Following this step, the 'Title and Abstract Screening' sample in Covidence consisted of 2,469 entries.</p><p>In the fifth step, two researchers (JD, LS) manually annotated a total of n = 2,469 studies.</p><p>During the title and abstract screening phase of our study, n = 307 abstracts were excluded for not being quantitative or empirical in nature, resulting in a set of n = 2,162 papers.</p><p>In the sixth step, we matched these papers with a total of 1,152 authors (including duplications due to associations with more than one research group). An automated matching process using an R-script revealed that the first authors of 4 papers were not listed in our sample and were thus excluded. Additionally, 573 researchers were found to have no quantitative-empirical studies listed after matching.</p><p>Consequently, in the seventh step, we counterchecked each researcher without publications listed in our corpus against their respective institutions' homepages. One subject was removed from the dataset because the countercheck revealed they were an institute secretary and not a researcher. For 35 of the 573 researchers without publications, a total of 113 papers were added to the dataset. For the remaining researchers, no publications were identified.</p><p>In the eighth step, to further verify the accuracy of our sample, we randomly sampled 101 researchers and checked whether all papers in our dataset were also listed on their publication list web pages. We found that 20 papers had been missed in our initial approach and thus added them to the dataset. Of the 425 papers checked during this step, 7 were excluded due to 'name confusion' and a further 3 were removed as they were posters or review articles.</p><p>Our final dataset for the computation of scientific productivity comprised n = 2,280 research papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Annotation</head><p>Papers were annotated utilizing the procedure suggested by <ref type="bibr">Fanelli (2010a;</ref><ref type="bibr">2012)</ref> and <ref type="bibr">Scheel and colleagues (2021)</ref> in their studies on rates of positive results: "By examining the abstract and/or full-text, it was determined whether the authors of each paper had concluded to have found a positive (full or partial) or negative (null or negative) support. If more than one hypothesis was being tested, only the first one to appear in the text was considered. We excluded meeting abstracts and papers that either did not test a hypothesis or for which we lacked sufficient information to determine the outcome" <ref type="bibr">(Fanelli, 2010a, p. 8)</ref>. The criterion of 'insufficient information in the paper' is defined similarly to <ref type="bibr">Scheel et al.'s (2021)</ref> study. It includes cases in which the abstract is unclear and the full text of the paper is unavailable for review <ref type="bibr" target="#b27">(Scheel et al., 2021;</ref><ref type="bibr">Fanelli, 2010a)</ref>. Additionally, this category includes instances in which, even if the full text is accessible, both the abstract and the full text are unclear with respect to the hypotheses tested and/or the conclusions drawn. This implies that each paper in the sample provides a single data point for the primary dependent variable result type: whether the first hypothesis was fully supported, partially supported, or not supported at all. We chose to sample 150 papers each from Quartile 1 (Q1) and Quartile 4 (Q4) of scientific productivity, a numerical index computed as the ratio of the number of quantitative-empirical publications of a research chair (group paper count) to the number of academic staff per chair (number of researchers). This approach was taken to be consistent with the sample sizes used by <ref type="bibr">Fanelli (2010a;</ref><ref type="bibr">2012)</ref> and <ref type="bibr">Scheel and colleagues (2021)</ref>, in order to estimate the rate of support for the respective first mentioned hypothesis. We randomly sampled a total of 354 papers, of which 54 were excluded from further analysis for the following reasons: 21 were exploratory, 12 were descriptive, 11 lacked a clear hypothesis, 5 were conference abstracts, 2 were not available in full text, 1 was a meta-analysis, 1 had no quantitative hypothesis, and 1 was not an original study. After the first coding round (n = 300), additional resampling was necessary: 44 papers for the second round, 8 for the third, and 2 for the fourth. The final sampled dataset comprised 150 papers each for Q1 and Q4. From the included studies, JD and LS annotated the result type: JD rated 45%, LS rated 45%, and both JD and LS rated the remaining 10%. Of the 30 studies rated by both, the raters agreed in 27 cases (agreement = 90%; κ = .800).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Period</head><p>Data collection was conducted between March and June 2023 and data annotation was completed in February 2024. Data analysis was conducted from March to April 2024.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistical Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Variables</head><p>The unit of analysis in our study is a journal article, with each article contributing one data point to our dataset. The categories 'full support' and 'partial support' of the dependent variable result type are combined into a single category, 'support,' resulting in a binary dependent variable ('support' = 1 vs. 'no support' = 0). The main independent variable is scientific productivity, which is defined as the ratio of the number of quantitative-empirical publications of a research chair (group paper count) to the number of academic staff per chair (number of researchers).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preregistered Analyses</head><p>All analyses were performed using R (R Core Team, 2023). In the present study, we tested two hypotheses:</p><p>• H1. Primary hypothesis (scientific productivity): The rate of positive results in lowoutput research chairs in German clinical psychology is lower than that in high-output research chairs.</p><p>• H2. Secondary hypothesis (per group paper count): Higher publication counts of research chairs are associated with higher rates of positive results.</p><p>For the primary hypothesis (H1), a one-sided proportion test with an alpha level of 5% was conducted to investigate whether the rate of positive results in low-output research chairs (Q1) is lower than that in high-output research chairs (Q4; scientific productivity).</p><p>Some clinical psychology research groups consist of only one or two persons. Computing the ratio of quantitative-empirical publications to the number of academic staff per chair as an index of scientific productivity might favor these one-or two-person groups and disadvantage larger groups, especially those with a higher number of doctoral students who have only recently joined. Therefore, as part of a sensitivity analysis, we additionally tested whether the rate of positive results (full or partial support) from low-output research chairs in clinical psychology is statistically lower than that in high-output research chairs when excluding (a) all one-person research groups and (b) all one-person and two-person research groups. For the primary hypothesis and the sensitivity analysis, we used the built-in prop.test R function to compare rates of positive results between low-output and high-output research chairs <ref type="bibr" target="#b27">(Scheel et al., 2021)</ref>.</p><p>For the secondary hypothesis (H2), we further tested whether higher publication counts of research chairs (group paper count) are associated with higher rates of positive results, using logistic regression with an alpha level of 5%. The outcome was binary ('support' = 1 vs. 'no support' = 0) and the predictor was metric (no. of papers). Thus, we hypothesized a positive relationship between outcome and predictor. <ref type="bibr" target="#b11">Fanelli (2012)</ref> used this method to predict the result as a function of publication year. For this analysis, we used the built-in glm R function for logistic regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exploratory Analyses</head><p>In addition to testing the two hypotheses, we conducted the following exploratory analyses to provide a broader context for our findings: First, we compared the rates of (a) full support ('full support' = 1 vs. 'no support OR partial support' = 0) and (b) partial support (partial support' = 1 vs.</p><p>'no support OR full support' = 0) between low-output and high-output research chairs to investigate whether rates of positive results differ as a function of scientific productivity when using different operationalizations of positive results than the categorization 'full support OR partial support' = 1 vs. 'no support' = 0 suggested in the literature <ref type="bibr">(Fanelli, 2010a;</ref><ref type="bibr" target="#b27">Scheel et al., 2021)</ref>. Second, we also compared the rates of positive results in our sample with those reported by <ref type="bibr">Fanelli (2010a)</ref> and <ref type="bibr" target="#b27">Scheel et al. (2021)</ref> in order to place our findings in the context of previous research. Third, we extended our logistic regression model by including additional variables such as the individual paper count of researchers, publication year, and the number of researchers in a group in order to explore their potential associations with rates of positive results. Lastly, we analyzed reporting patterns between the top and bottom quartiles of scientific productivity, focusing on the use of the term "significant" and the explicit mention of hypotheses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Descriptive Statistics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Researchers</head><p>The qualification levels among the n = 1,151 researchers in our study were distributed as follows: The majority (712, 61.86%) were predoctoral researchers and PhD students, followed by postdocs <ref type="bibr">(321, 27.89%), and professors (118, 10.25%)</ref>. The mean number of first-authored quantitative-empirical original works published per researcher between 2013 and 2022 lay at 2.07 (SD = 3.70). Mean first authorship counts differed by qualification level: Predoctoral researchers and PhD students had a mean of 0.61 (SD = 1.12) of first-authored papers, postdocs a mean of 4.01 (SD = 4.61), and professors a mean of 5.60 (SD = 5.75).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Research groups</head><p>On average, research groups consisted of 11.63 researchers per group (SD = 9.15, Median = 10, Range = [1, 55]). The mean number of first authorships of quantitative-empirical original works per group published between 2013 and 2022 lay at 24.03 (SD = 21.59, Median = 18, Range = [0, 153]). Across the groups, the mean scientific productivity (defined as the ratio of quantitativeempirical publications to the number of academic staff per chair) was 2.36 (SD = 1.79, Median = 1.89, Range = [0, 10.2]). To compare the bottom quartile (Q1) and the top quartile (Q4) of scientific productivity from the n = 99 research chairs, we assigned 25 research chairs to the bottom quartile and 24 research chairs to the top quartile, and 25 each to Q2 and Q3. Table <ref type="table" target="#tab_0">1</ref> and Figure <ref type="figure" target="#fig_1">2</ref> show the distribution of number of publications and number of researchers as well as the different quartiles of scientific productivity of the research chairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Full publication dataset</head><p>The publication years of the 2,280 articles in our full dataset ranged from 2013 to 2022, with a mean publication year of 2018.29 (SD = 2.99). 2,183 (95.75%) were written in English, while 97 (4.25%) were written in German. The most frequent journals in the corpus, with their respective counts, were Frontiers in Psychology (n = 86), PLOS ONE (n = 80), Psychiatry Research (n = 54), Behaviour Research and Therapy (n = 52), and Journal of Behavior Therapy and Experimental Psychiatry (n = 49). In the full publication dataset, the distribution of scientific productivity quartiles was as follows: Researchers from Q1 accounted for 13.29% of all publications (n = 303), researchers from Q2 for 22.11% (n = 504), researchers from Q3 for 34.12%</p><p>(n = 778), and researchers from Q4 for 30.48% (n = 695). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hypotheses</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H1: Primary Hypothesis -Scientific Productivity</head><p>Among the sampled papers, 135 out of 150 from Q1 (low scientific productivity) and 134 out of 150 from Q4 (high scientific productivity) reported full or partial support for the first mentioned hypothesis. This equals a positive result rate of 90.00% for Q1 papers (95% confidence interval (95% CI = [84.04, 94.29]) and 89.33% for Q4 (95% CI = [83.25, 93.78]; see Fig. <ref type="figure">3</ref>). The observed difference of -0.67% was not statistically significant according to the preregistered onesided proportions test (χ 2 (1) = 0.00, p = .500). The overall rate of positive results was 89.67%.</p><p>Consequently, the hypothesis that the rate of positive results in the top quartile is lower than that in the bottom quartile (H1) was rejected. Sensitivity analyses revealed that even after excluding all papers from research chairs consisting solely of one person (n = 5 papers from Q4, none from Q1) and all one-and two-person research chairs (n = 6 papers from Q4, none from Q1), the observed differences of -1.03% and -1.11% were not statistically significant according to two preregistered one-sided sensitivity</p><p>proportions tests (no one-person research groups: χ2(1) = 0.01, p = .540; no one-and two-person research groups: χ2(1) = 0.01, p = .548).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H2: Secondary Hypothesis -Group Paper Count</head><p>The logistic regression model testing the association between higher publication counts of research chairs and higher rates of positive results showed very weak explanatory power (Tjur's R 2 = .00). The effect of group paper count on positive results was not statistically significant (beta = 0.00, 95% CI [-0.00, 0.02], p = .356). Therefore, the hypothesis that higher publication counts of research chairs are associated with higher rates of positive results (H2) was rejected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 3</head><p>Rates of positive results for high-output and low-output research chairs. Error bars show 95% confidence intervals around the observed rate of positive results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exploratory results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Full and partial support</head><p>Ninety-four out of 150 (62.67%) papers from Q1 and 78 out of 150 (52.00%) papers from Q4 reported full support for the first mentioned hypothesis. In contrast, 41 out of 150 (27.33%) papers from Q1 and 56 out of 150 (37.33%) papers from Q4 reported partial support for the first mentioned hypothesis. The observed differences of -10.67% for full support and +10% for partial support were statistically significant according to two non-preregistered one-sided proportions tests (full support: χ 2 (1) = 3.07, p = .040; partial support: χ 2 (1) = 2.99, p = .042).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rates of positive results in psychology papers in other studies using Fanelli's method</head><p>We compared the rate of positive results found in our analyses with those reported by <ref type="bibr">Fanelli (2010a)</ref> and <ref type="bibr" target="#b27">Scheel et al. (2021)</ref>, adopting their approaches to estimate the rate of positive results in psychology and psychiatry papers. The difference in rates of positive results between our sample and those reported by Fanelli (89.67% -91.49% = -1.82%) was not statistically significant (χ 2 (1) = 1.84, p = .667). However, the difference between our sample and the standard psychology papers<ref type="foot" target="#foot_1">3</ref> (excluding registered reports) examined by Scheel and colleagues (89.67% -96.05% = -6.38%) was statistically significant (χ 2 (1) = 4.66, p = .031).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Logistic regression associations</head><p>Furthermore, we introduced three further variables into our logistic regression model from H2: the paper count of an individual, the publication year, and the number of researchers in a group. The logistic model likewise showed very weak explanatory power here (Tjur's R 2 = .01).</p><p>All regression coefficients were not statistically significant (group paper count: beta = 0.00, 95%</p><p>CI [-0.00, 0.02], p = .313; individual paper count: beta = 0.00, 95% CI [-0.06, 0.05], p = .702;</p><p>publication year: beta = -0.11, 95% CI [-0.24, 0.03], p = .125; number or researchers: beta = 0.00, 95% CI [-0.03,0.03], p = .884).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reporting differences between Q1 and Q4</head><p>Of all n = 288 papers written in English (n = 12 in German), 38.00% in Q1 and 26.67% in Q4 used the pattern "significant*" when describing the results of the first mentioned hypothesis; this difference between the two quartiles was statistically significant (χ 2 (1) = 3.90, p = .048).</p><p>Furthermore, 36.67% of English-language papers in Q1 explicitly mentioned the hypothesis (pattern "hypothes*"), compared to 39.33% in Q4; this difference was not statistically significant (χ 2 (1) = 0.13, p = .721). A common way of introducing hypotheses was through the textual pattern "hypothesize/hypothesise," found in n = 88 (30.56%) of all papers written in English. Only n = 3</p><p>(1.04%) of all papers written in English used the pattern "test* the hypothes*", which Fanelli (2010) and <ref type="bibr" target="#b27">Scheel et al. (2021)</ref> used for their sampling of standard quantitative studies in psychology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>In the present study, we assessed the percentage of clinical psychology articles that supported the first hypothesis and found that overall, the proportion of positive results was high, with approximately 9 in 10 studies (89.67%) reporting positive results. Contrary to our primary hypothesis, we did not find a notable discrepancy between research chairs with high and low research output (89.33% vs. 90.00%). This suggests that the high rate of positive results is pervasive, and is not significantly influenced by the level of research productivity.</p><p>Our findings differ from those of <ref type="bibr" target="#b10">Fanelli (2010b)</ref>, who reported that pressures to publish, measured at the federal state level in the US, increased scientists' bias toward positive results.</p><p>Using the scientific productivity of research groups as a meso-level proxy of academic productivity, we did not confirm this pattern. Further analyses revealed that there were also no significant differences after excluding all publications from one-person chairs and from one-and two-person chairs. Moreover, higher publication counts of research chairs were not associated with higher rates of positive results in a logistic regression model. Our preregistered analyses suggest that the high proportion of positive results in clinical psychology cannot be differentially explained by scientific productivity or group paper count.</p><p>However, our exploratory analyses suggested differences between high and low research outputs in terms of the extent of reporting full support and partial support for the first reported hypothesis: Surprisingly, publications from research chairs with low research output more frequently reported full support than did publications from research chairs with high research output. Conversely, publications from research chairs with low research output less frequently reported partial support compared to publications from research chairs with high research output.</p><p>This pattern might be partly explained by differences in reporting style, with the bottom quartile mentioning the pattern "significant*" more often than the top quartile. Contrary to our expectation, researchers in groups with low scientific productivity more often presented their findings as fully rather than partially positive. We can only speculate about the reasons for this. According to expected utility theory, individuals with higher resources are less attracted by prospective gains but also less discouraged by possible loss compared to individuals with lower resources <ref type="bibr">(Mongin, 1997;</ref><ref type="bibr" target="#b25">Passarelli &amp; Del Ponte, 2020)</ref>. Thus, researchers in groups with low productivity may perceive the non-publication of a paper due to negative results as a greater loss compared to researchers in highly productive groups, who already have a steady stream of publications and might therefore be more willing to report partially positive, mixed, or negative results.</p><p>Furthermore, the proportion of positive results in our sample of clinical psychology studies (89.67%) was significantly lower than the proportion of 96.05% reported by Scheel and colleagues (2021) but did not differ significantly from the proportion of 91.5% reported by <ref type="bibr">Fanelli (2010a)</ref>.</p><p>It is challenging to find a clear explanation for why our observations of positive results are similar to those of <ref type="bibr">Fanelli (2010a)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Strengths and limitations</head><p>A strength of the current study is that we included all empirical and quantitative original studies from the clinical psychology sample, and not merely those that employed the phrase 'test* the hypothes*' <ref type="bibr">(Fanelli, 2010a;</ref><ref type="bibr" target="#b27">Scheel et al., 2021)</ref>. By not restricting our dataset to studies that used specific phrasing to describe their hypothesis-testing, we minimized the potential bias that might be introduced by selectively sampling only those studies explicitly mentioning this pattern, since only n = 3 (1.04%) of all papers written in English used the pattern "test* the hypothes*" in our sample. However, the inclusion of all papers comes with challenges in terms of coding hypotheses, as determining what constitutes a hypothesis can be difficult when the language used is ambiguous.</p><p>While the present study investigated the rates of positive results in the top and bottom quartile of scientific productivity, we did not analyze those in the second and third quartile of this variable. This approach may limit our understanding of the overall distribution and variability of positive results across the spectrum of scientific productivity. By focusing on the extremes, we the investigation of all hypotheses in a study <ref type="bibr">(van den Akker et al., 2023)</ref> or the distinction between solely positive results and mixed or negative results <ref type="bibr">(Schiekiera et al., 2024)</ref>. Moreover, our definition of scientific productivity may disproportionately benefit research chairs with a small number of researchers while penalizing those with a larger number of researchers. To mitigate this, however, we also controlled for the raw group and individual paper count.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>This study examined the proportion of positive results within research groups in German clinical psychology, analyzing the association between scientific productivity and positive outcomes. Contrary to our hypothesis, the degree of scientific productivity did not differentially explain high rates of positive outcomes. Furthermore, our study showed that partially supported hypotheses were more prevalent in the highest output quartile than in the lowest, a difference that may be influenced by variations in reporting styles between different scientific productivity levels.</p><p>These findings underscore the need for further research to explore the complex dynamics of evaluative processes that lead to the non-publication of negative results, considering factors at the macro, meso, and micro levels.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1</head><label>1</label><figDesc>Figure 1 Data acquisition procedure for the final corpus of all quantitative-empirical original studies firstauthored by clinical psychology researchers affiliated with German universities from 2013 to 2022</figDesc><graphic coords="7,72.00,146.09,394.20,452.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2</head><label>2</label><figDesc>Figure 2 Number of publications and number of researchers per research group and scientific productivity for n = 99 German clinical psychology research chairs</figDesc><graphic coords="15,107.45,193.49,367.50,259.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>may have missed patterns that might occur in the middle quartiles, which could provide a more nuanced understanding of the factors influencing positive results in clinical psychology.Our research focused exclusively on German clinical psychology, limiting the ability to generalize our findings to other geographical areas and different fields of psychology or related disciplines. This specificity may obscure broader trends, rendering it challenging to determine whether the observed patterns are unique to German research culture, clinical psychology as a whole, or specifically to German clinical psychology. It would not have been feasible to additionally measure the relative scientific productivity of clinical psychology in other countries as comprehensively as for German clinical psychology, given the extensive data collection, cleaning and coding steps in this analysis.The operational definitions of 'positive results' and 'scientific productivity' might have influenced the interpretation of the data. Alternative definitions of 'positive results' could include</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Descriptive statistics for the four quartiles of scientific productivity for n = 99 German clinical Note. Group Paper Count = Number of publications per group between 2013 and 2022.</figDesc><table><row><cell>psychology research chairs</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Quartile Metric</cell><cell>M</cell><cell>SD</cell><cell>Median</cell><cell>Range</cell></row><row><cell>Q1 Scientific Productivity</cell><cell>0.90</cell><cell>0.41</cell><cell>1.00</cell><cell>[0, 1.25]</cell></row><row><cell>Group Paper Count</cell><cell>12.40</cell><cell>14.70</cell><cell>7.00</cell><cell>[0, 65]</cell></row><row><cell cols="2">Number of Researchers 11.96</cell><cell>12.38</cell><cell>8.00</cell><cell>[1, 55]</cell></row><row><cell>Q2 Scientific Productivity</cell><cell>1.62</cell><cell>0.18</cell><cell>1.67</cell><cell>[1.29, 1.89]</cell></row><row><cell>Group Paper Count</cell><cell>21.28</cell><cell>12.87</cell><cell>17.00</cell><cell>[5, 57]</cell></row><row><cell cols="2">Number of Researchers 13.24</cell><cell>8.06</cell><cell>12.00</cell><cell>[3, 34]</cell></row><row><cell>Q3 Scientific Productivity</cell><cell>2.30</cell><cell>0.25</cell><cell>2.27</cell><cell>[2, 2.67]</cell></row><row><cell>Group Paper Count</cell><cell>31.56</cell><cell>19.32</cell><cell>27.00</cell><cell>[4, 66]</cell></row><row><cell cols="2">Number of Researchers 13.64</cell><cell>8.22</cell><cell>11.00</cell><cell>[2, 31]</cell></row><row><cell>Q4 Scientific Productivity</cell><cell>4.73</cell><cell>2.16</cell><cell>3.94</cell><cell>[2.79, 10.20]</cell></row><row><cell>Group Paper Count</cell><cell>31.17</cell><cell>30.47</cell><cell>29.00</cell><cell>[5, 153]</cell></row><row><cell cols="2">Number of Researchers 7.50</cell><cell>5.79</cell><cell>7.50</cell><cell>[1, 24]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>but lower than those ofScheel and colleagues (2021), because the latter study represents a conceptual replication of the former, with everything held constant except for the publication year.<ref type="bibr" target="#b27">Scheel (2021)</ref> analyzed a more recent dataset (2013 to 2018), while Fanelli (2010a) did not place any restriction on year of publication. Neither study focused exclusively on clinical psychology.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The creation and development of technical bibliometric infrastructures (e.g., Scholar, Web of Science, etc.) that allow for a more accurate measurement of scientific publication activity is also likely to have increased the pressure to publish.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>Scheel and colleagues (2021)  use the term "standard psychology papers" to contrast them with registered reports, meaning that standard papers are non-registered report studies.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments: Thanks to the <rs type="funder">Berlin University Alliance</rs> for funding this project. Special thanks to <rs type="person">Kristina Eichel</rs> and <rs type="person">Jonathan Diederichs</rs> for their help in this research project. Special thanks as well to <rs type="person">Sarah Mannion</rs> for correcting the manuscript.</p></div>
<div><head>Funding:</head><p>The project was funded by the <rs type="funder">Berlin University Alliance</rs> (312_OpenCall_3).</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Author Contributions:</head><p>• Conceptualization: L.J. Schiekiera, H. Niemeyer </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Statistical significance, reviewer evaluations, and the scientific process: Is there a (statistically) significant relationship?</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Furlong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Wampold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Counseling Psychology</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">189</biblScope>
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Quality assessment of scientific manuscripts in peer review and education</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Augusteijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wicherts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sijtsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Van Assen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An audit of sample sizes for pilot and feasibility trials being undertaken in the United Kingdom registered in the United Kingdom Clinical Research Network database</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Billingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Whitehead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Julious</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC medical research methodology</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Button</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Ioannidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mokrysz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Nosek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Flint</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Robinson</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Power failure: why small sample size undermines the reliability of neuroscience</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Munafò</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature reviews neuroscience</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="365" to="376" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rcrossref: Client for various crossref apis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Boettiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Haaland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stegmann</surname></persName>
		</author>
		<ptr target="https://CRAN.R-project.org/package=rcrossref" />
	</analytic>
	<monogr>
		<title level="s">CESifo Working Paper No. 9776. Clarivate</title>
		<imprint>
			<date type="published" when="2022">2022. 2022. 2024</date>
		</imprint>
	</monogr>
	<note>The null result penalty. EndNote 20. www.support.clarivate.com/Endnote/ Covidence. (2023). Covidence review software. www.covidence.org</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A review of the economic impact of mental illness</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Doran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kinchin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Australian Health Review</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="48" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Metascience on peer review: Testing the effects of a study&apos;s originality and statistical significance in a field experiment</title>
		<author>
			<persName><forename type="first">M</forename><surname>Elson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Utz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Methods and Practices in Psychological Science</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="65" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Confirmational response bias among social work journals</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Epstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technology, &amp; Human Values</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="38" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
	<note>Science</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Positive&quot; results increase down the hierarchy of the sciences</title>
		<author>
			<persName><forename type="first">D</forename><surname>Fanelli</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0010068</idno>
		<ptr target="https://doi.org/10.1371/journal.pone.0010068" />
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">10068</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>a)</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Do pressures to publish increase scientists&apos; bias? An empirical support from US States Data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Fanelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">10271</biblScope>
			<date type="published" when="2010">2010b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Negative results are disappearing from most disciplines and countries</title>
		<author>
			<persName><forename type="first">D</forename><surname>Fanelli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title/>
		<idno type="DOI">10.1007/s11192-011-0494-7</idno>
		<ptr target="https://doi.org/10.1007/s11192-011-0494-7" />
	</analytic>
	<monogr>
		<title level="j">Scientometrics</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="891" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Publication bias in the social sciences: Unlocking the file drawer</title>
		<author>
			<persName><forename type="first">A</forename><surname>Franco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Malhotra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Simonovits</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.1255484</idno>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">345</biblScope>
			<biblScope unit="issue">6203</biblScope>
			<biblScope unit="page" from="1502" to="1505" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bringing Society Back In: Symbols, Practices and Institutional Contradictions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Alford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The New Institutionalism in Organizational Analysis</title>
		<editor>
			<persName><forename type="first">W</forename><surname>Powell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Dimaggio</surname></persName>
		</editor>
		<imprint>
			<publisher>University Of Chicago Press</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="232" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Corrupt research: The case for reconceptualizing empirical management and social science</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hubbard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Sage Publications</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The impact of research grant funding on scientific productivity</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lefgren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of public economics</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="issue">9-10</biblScope>
			<biblScope unit="page" from="1168" to="1177" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Kjellberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mallard</surname></persName>
		</author>
		<title level="m">Valuation Studies? Our Collective Two Cents</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title/>
		<idno type="DOI">10.3384/vs.2001-5992.131111</idno>
		<ptr target="https://doi.org/10.3384/vs.2001-5992.131111" />
	</analytic>
	<monogr>
		<title level="j">Valuation Studies</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Economics and mental health: the current scenario</title>
		<author>
			<persName><forename type="first">M</forename><surname>Knapp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">World Psychiatry</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="14" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Theories of Valuation-Building Blocks for Conceptualizing Valuation Between Practice and Structure</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Krüger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reinhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Historical Social Research. Lamont</title>
		<imprint>
			<date type="published" when="2009">2017. 2009</date>
			<publisher>Harvard University Press</publisher>
		</imprint>
	</monogr>
	<note>How professors think: Inside the curious world of academic judgment</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Toward a Comparative Sociology of Valuation and Evaluation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lamont</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev-soc-070308-120022</idno>
		<ptr target="https://doi.org/10.1146/annurev-soc-070308-120022" />
	</analytic>
	<monogr>
		<title level="j">Annual Review of Sociology</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="201" to="221" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Publication prejudices: An experimental study of confirmatory bias in the peer review system</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive therapy and research</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="161" to="175" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The economic case for the prevention of mental illness</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mcdaid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wahlbeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual review of public health</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="373" to="389" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The intriguing evolution of effect sizes in biomedical research over time: Smaller but more often statistically significant</title>
		<author>
			<persName><forename type="first">P</forename><surname>Monsarrat</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open Science Collaboration</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">J.-N</forename><surname>Vergnes</surname></persName>
			<affiliation>
				<orgName type="collaboration">Open Science Collaboration</orgName>
			</affiliation>
		</author>
	</analytic>
	<monogr>
		<title level="j">gix121. Mongin, Philippe</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4716</biblScope>
			<date type="published" when="1997">2018. 1997. 2015</date>
		</imprint>
	</monogr>
	<note>Science</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">OpenAlex: A fully-open index of scholarly works, authors, venues, institutions, and concepts</title>
		<author>
			<persName><forename type="first">F</forename><surname>Passarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Del Ponte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Piwowar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
		<ptr target="&lt;https://www.R-project.org/&gt;" />
	</analytic>
	<monogr>
		<title level="m">_R: A Language and Environment for Statistical Computing_. R Foundation for Statistical Computing</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2020. 2018. 2022. 2023</date>
		</imprint>
	</monogr>
	<note>Oxford Research Encyclopedia of Politics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The meaningfulness of effect sizes in psychological research: Differences between sub-disciplines and the impact of potential biases</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in psychology</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">442717</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An excess of positive results: Comparing the standard psychology literature with registered reports</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Scheel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Schijen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lakens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Methods and Practices in Psychological Science</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">25152459211007467</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Classifying positive results in clinical psychology using natural language processing</title>
		<author>
			<persName><forename type="first">L</forename><surname>Schiekiera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Diederichs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Niemeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Zeitschrift für Psychologie: Topical Issue Natural Language Processing in Psychology</title>
		<imprint/>
	</monogr>
	<note>in press. Hogrefe</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Type I and Type II errors and statistical power</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shreffler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Huecker</surname></persName>
		</author>
		<idno type="PMID">32491462</idno>
	</analytic>
	<monogr>
		<title level="m">StatPearls. StatPearls Publishing</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Publication decisions and their possible effects on inferences drawn from tests of significance-or vice versa</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Sterling</surname></persName>
		</author>
		<idno type="DOI">10.1080/01621459.1959.10501497</idno>
		<ptr target="https://doi.org/10.1080/01621459.1959.10501497" />
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">285</biblScope>
			<biblScope unit="page" from="30" to="34" />
			<date type="published" when="1959">1959</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Publication decisions revisited: The effect of the outcome of statistical tests on the decision to publish and vice versa</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Sterling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Weinkam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The american statistician</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="108" to="112" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Sterne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Egger</surname></persName>
		</author>
		<title level="m">The funnel plot. Publication bias in metaanalysis: Prevention, assessment and adjustments</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="73" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">When null hypothesis significance testing is unsuitable for research: A reassessment</title>
		<author>
			<persName><forename type="first">D</forename><surname>Szucs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P A</forename><surname>Ioannidis</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnhum.2017.00390</idno>
		<ptr target="https://doi.org/10.3389/fnhum.2017.00390" />
	</analytic>
	<monogr>
		<title level="j">Front Hum Neurosci</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">390</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Perish or publish in China: Pressures on young Chinese scholars to publish in internationally indexed journals</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Publications</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Study preregistration: An evaluation of a method for transparent reporting</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Toth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Banks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mellor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>O'boyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dehaven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bochantin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Borns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Business and Psychology</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="553" to="571" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Intended and unintended consequences of a publish-or-perish culture: A worldwide survey</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Van Dalen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Henkens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1282" to="1293" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">R</forename><surname>Van Den Akker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Van Assen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elsherif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Preregistration in practice: A comparison of preregistered and nonpreregistered studies in psychology</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wicherts</surname></persName>
			<affiliation>
				<orgName type="collaboration">D. J.</orgName>
			</affiliation>
		</author>
		<ptr target="https://osf.io/preprints/metaarxiv/fhdbs/Winter" />
	</analytic>
	<monogr>
		<title level="m">rentrez: An R package for the NCBI eUtils API</title>
		<imprint>
			<date type="published" when="2017">2023. 2017</date>
		</imprint>
	</monogr>
	<note>No. e3179v2. PeerJ Preprints</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
