<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Combined eye tracking and electroencephalography during referential selection in dyadic interaction</title>
				<funder ref="#_7wUmHJt">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_3eABEDU">
					<orgName type="full">VolkswagenStiftung (Volkswagen Foundation)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Ingmar</forename><surname>Brilmayer</surname></persName>
							<email>ingmar.brilmayer@uni-koeln.de</email>
						</author>
						<author>
							<persName><forename type="first">Philip</forename><surname>Georgis</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Petra</forename><forename type="middle">B</forename><surname>Schumacher</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute for German Language and Literature I</orgName>
								<orgName type="department" key="dep2">Linguistics</orgName>
								<orgName type="institution">University of Cologne</orgName>
								<address>
									<addrLine>Albertus-Magnus-Platz</addrLine>
									<postCode>50923</postCode>
									<settlement>Cologne</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Institute for German Language and Literature I</orgName>
								<orgName type="department" key="dep2">Linguistics</orgName>
								<orgName type="institution">University of Cologne</orgName>
								<address>
									<addrLine>Albertus-Magnus-Platz</addrLine>
									<postCode>50923</postCode>
									<settlement>Cologne</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Institute for German Language and Literature I</orgName>
								<orgName type="department" key="dep2">Linguistics</orgName>
								<orgName type="institution">University of Cologne</orgName>
								<address>
									<addrLine>Albertus-Magnus-Platz</addrLine>
									<postCode>50923</postCode>
									<settlement>Cologne</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Combined eye tracking and electroencephalography during referential selection in dyadic interaction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F4271F8E7B32DFEB97F8FC47FE7AF236</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-22T20:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Understanding language in real-world interaction requires methods that integrate auditory, visual, and neural signals. We present a proof-of-concept study combining mobile electroencephalography (EEG) and eye tracking to investigate referential selection during dyadic communication. Using a simplified version of the director task, pairs of participants engaged in naturalistic object-movement instructions while EEG and gaze data were recorded and synchronized via LabStreamingLayer. This approach allowed us to calculate overlapcorrected, regression-based event-related potentials (rERPs) time-locked to spoken nouns and to participants' fixations, focusing on the N400/P300 (300-500 ms) and later (500-800 ms) time windows. Our results show that gaze behavior and fixation-related potentials provide crucial information for interpreting language-related ERPs: targets competing with occluded referents elicited stronger P300 effects, suggesting higher attentional demands, while fixation timing systematically modulated neural responses. Contrary to predictions, competitors received little visual attention, indicating that participants prioritized direct task-relevant information. These findings highlight the potential of multimodal data integration for understanding attentional and predictive mechanisms in real-world communication.</p><p>Importantly, we demonstrate that established analytic techniques, such as artifact subspace reconstruction, independent component analysis, and linear deconvolution for ERP calculation, are applicable to noisy, naturalistic data sets. This study thus provides a methodological framework for linking gaze and neural activity during interactive language processing beyond laboratory constraints. All preprocessing and analysis scripts, together with example data sets, are openly available on OSF (<ref type="url" target="https://osf.io/5ds4z">https://osf.io/5ds4z</ref>; a modified and optimized Python implementation is also available on github:</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction: Multimodal Language</head><p>Language in communication is a complex system of multimodal signals, in which the entirety of a person's body -from speech and breathing to facial expressions, gestures, and other bodily movements -can be a potential source of information <ref type="bibr" target="#b29">(Goodwin, 2003;</ref><ref type="bibr" target="#b66">Mondada, 2016)</ref>. In recent years, there has been a notable trend in linguistics to acknowledge this complexity by moving towards a multimodal view on human language <ref type="bibr" target="#b7">(Benetti et al., 2023;</ref><ref type="bibr" target="#b36">Hasson et al., 2018;</ref><ref type="bibr" target="#b41">Holler &amp; Levinson, 2019;</ref><ref type="bibr" target="#b59">Levinson &amp; Holler, 2014;</ref><ref type="bibr" target="#b65">Mazzini et al., 2023;</ref><ref type="bibr" target="#b66">Mondada, 2016;</ref><ref type="bibr" target="#b79">Perniss, 2018;</ref><ref type="bibr" target="#b102">Vigliocco et al., 2014)</ref>. From this perspective, language in face-to-face interaction is anchored in the immediate environment in which it occurs by referring to persons or objects available in a room with which people (want to) interact.</p><p>Beside speech, listeners and speakers make use of information from the visual domain, for instance to establish reference (e.g., <ref type="bibr" target="#b14">Debreslioska et al., 2013;</ref><ref type="bibr" target="#b31">Graham &amp; Argyle, 1975;</ref><ref type="bibr" target="#b34">Hanna &amp; Brennan, 2007;</ref><ref type="bibr" target="#b48">Kita &amp; Özyürek, 2003;</ref><ref type="bibr" target="#b94">So et al., 2009;</ref><ref type="bibr" target="#b95">Somashekarappa et al., 2020)</ref>.</p><p>Studies of language in real-world interaction have in the majority targeted the time-frequency domain, as for instance studies on entrainment, cross-frequency coupling or inter-brain synchrony <ref type="bibr" target="#b17">(Dikker et al., 2021;</ref><ref type="bibr" target="#b20">Drijvers &amp; Holler, 2022;</ref><ref type="bibr" target="#b78">Pérez et al., 2017)</ref>. Linguistic event-related potential studies on language in interaction have so far been limited to virtual reality <ref type="bibr" target="#b43">(Huizeling et al., 2023;</ref><ref type="bibr" target="#b97">Tromp et al., 2018;</ref><ref type="bibr" target="#b111">Zappa et al., 2019)</ref>, probably due to the wide range of difficulties connected with real-world EEG studies, from technical setup, experimental control and bad data quality, to labor-intensive annotation and complex analyses. For instance, it can be very difficult to identify enough occurrences of comparable linguistic events across participants, if they are allowed to speak freely, making an eventrelated potential study a challenging endeavor.</p><p>Still, a growing body of evidence from controlled studies suggests that linguistic (i.e. speech) information and information from the visual channel (e.g. gestures) are simultaneously and immediately processed and integrated by the human brain (e.g. <ref type="bibr" target="#b8">Biau et al., 2016;</ref><ref type="bibr" target="#b42">Hubbard et al., 2009;</ref><ref type="bibr" target="#b74">Özyürek, 2021;</ref><ref type="bibr" target="#b77">Peeters et al., 2017;</ref><ref type="bibr" target="#b100">van Wassenhove et al., 2005;</ref><ref type="bibr" target="#b107">Willems et al., 2008;</ref><ref type="bibr" target="#b110">Wu &amp; Coulson, 2005;</ref><ref type="bibr" target="#b112">Zhang et al., 2021)</ref>. Using functional magnetic resonance imaging (fMRI), <ref type="bibr" target="#b42">Hubbard et al., (2009)</ref> for instance showed that gestures modulate activity in the auditory cortex related to the processing of auditory language, providing evidence for the tight coupling of speech and gesture in the brain. ERP studies have repeatedly shown that it is especially the N400 and P300 which are sensitive to multimodal integration (e.g. <ref type="bibr" target="#b75">Özyürek et al., 2007;</ref><ref type="bibr" target="#b76">Peeters et al., 2015;</ref><ref type="bibr" target="#b107">Willems et al., 2008;</ref><ref type="bibr" target="#b112">Zhang et al., 2021)</ref>. In two EEG studies, <ref type="bibr" target="#b112">Zhang et al. (2021)</ref>, for example, were able to show that multimodal cues modulate the N400 event-related potential (ERP) component, usually observable when the meaning of a word does not fit its context in the widest sense (see <ref type="bibr" target="#b58">Kutas &amp; Federmeier, 2011)</ref>. Zhang and colleagues found interactions in the N400 time window (300-500 ms after word onset) between prosodic accentuation and several types of gestural and facial cues. That is, the probabilistic and predictive integration of incoming linguistic meaning (i.e. word meaning), as reflected in the N400 component <ref type="bibr" target="#b9">(Bornkessel-Schlesewsky &amp; Schlesewsky, 2019)</ref>, extends beyond the auditory speech signal to include visual cues that are weighted differently depending on their contexts of use <ref type="bibr" target="#b112">(Zhang et al., 2021)</ref>. <ref type="bibr" target="#b76">Peeters et al. (2015)</ref> further demonstrated that the integration of speech and pointing gestures affects the P300 time range, indicating the importance of domain general, attention related mechanisms involved in the integration of visual and auditory cues during language comprehension. There are many more examples in the literature that highlight the crucial role of the N400/P300 time range in the processing and integration of information across different information sources, for instance in natural reading (e.g. <ref type="bibr" target="#b2">Antúnez et al., 2022;</ref><ref type="bibr" target="#b19">Dimigen et al., 2012;</ref><ref type="bibr" target="#b60">Li et al., 2024;</ref><ref type="bibr" target="#b70">Niefind &amp; Dimigen, 2016)</ref>, or the the integration of written and spoken language with visual input (e.g. <ref type="bibr" target="#b40">Hirschfeld et al., 2011;</ref><ref type="bibr" target="#b51">Knoeferle et al., 2011;</ref><ref type="bibr" target="#b61">Liu et al., 2011;</ref><ref type="bibr" target="#b91">Sitnikova et al., 2008;</ref><ref type="bibr" target="#b96">Staudte et al., 2021)</ref>.</p><p>In the present manuscript, we demonstrate that it is possible to record and analyze multimodal EEG data (visual, auditory) acquired during real-world interaction and offer an approach of how to integrate gaze and EEG data during data analysis and interpretation (all scripts used in this study are available online: <ref type="url" target="https://osf.io/5ds4z/">https://osf.io/5ds4z/</ref>; also see the optimized</p><p>Python implementation with much shorter running time on github:</p><p><ref type="url" target="https://github.com/XlinCLab/DGAME">https://github.com/XlinCLab/DGAME</ref>). Using time-resolved regression <ref type="bibr" target="#b18">(Dimigen &amp; Ehinger, 2021;</ref><ref type="bibr">Smith &amp; Kutas, 2015a</ref><ref type="bibr" target="#b93">, 2015b)</ref>, we calculated language-and fixation-related event-related potentials from 128-channel EEG data recorded with mobile recording devices from freely-moving participants. Focusing on a traditional time window in neurolinguistic research, the N400/P300 time-window (300-500 ms after word onset), and a second, later time window (500-800 ms; see section 2 for motivation), we show how gaze data and fixation-related potentials can be informative about language-related potentials, and how the interpretation of the latter can benefit from the analysis of the former. For this purpose, we report behavioral (eye gaze, fixations) and event-related potential (ERP) results from a realworld study using a simplified version of the director task <ref type="bibr" target="#b46">(Keysar et al., 2000)</ref> as a proof-ofconcept. Before we elaborate on the present study, we want to discuss three main difficulties related with recording and analyzing multimodal data from real-world environments, and how they can be overcome to gain a benefit from multimodal data recordings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Multimodal Data Recording in Real World Environments: Challenges</head><p>Recording data from multiple modalities, such as EEG, eye tracking, and audio, while offering insights into the interaction of processes from different modalities <ref type="bibr" target="#b36">(Hasson et al., 2018;</ref><ref type="bibr" target="#b41">Holler &amp; Levinson, 2019;</ref><ref type="bibr" target="#b79">Perniss, 2018)</ref>, introduces significant challenges in data acquisition, synchronization and analysis <ref type="bibr" target="#b33">(Gregori et al., 2023)</ref>. Here, we want to briefly discuss three of these challenges: data synchronization, noisy EEG data and overlapping event responses.</p><p>First, each modality is recorded with its own temporal resolution. For instance, EEG systems often sample at high rates (e.g., 500-1000 Hz) to capture rapid fluctuations in neural activity, eye trackers usually operate from 120 Hz to over 1000 Hz depending on the device, while audio recordings usually use high sampling rates (44.1 kHz or more) to capture the fine structure of acoustic signals. The mismatch in these rates can lead to difficulties in aligning data streams for meaningful analysis. Second, recording EEG data from freely moving participants introduces strong non-neural artifacts related to participants' movements (muscle artifacts and moving recording devices; cf. <ref type="bibr" target="#b30">Gorjan et al., 2022)</ref>. And, third, freely acting participants perform many tasks beside the processing of experimental stimuli, i.e. they process visual and auditory information, they plan and produce linguistic utterances, fixations, saccades and other movements. This creates a heavy overlap of the event-related responses of all these different events (cf. <ref type="bibr" target="#b18">Dimigen &amp; Ehinger, 2021;</ref><ref type="bibr" target="#b22">Ehinger &amp; Dimigen, 2019;</ref><ref type="bibr">Smith &amp; Kutas, 2015a</ref><ref type="bibr" target="#b93">, 2015b)</ref>. While in traditional laboratory experiments, this can be effectively controlled, in real-world experiments, it has to be accounted for during analyses.</p><p>In the following, we discuss our approach to address these three issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.1">Data Synchronization: LabStreamingLayer</head><p>LabStreamingLayer (LSL, <ref type="bibr" target="#b55">Kothe et al., 2024)</ref> is a software framework and protocol designed for real-time data streaming and synchronization in research and experimental environments. LSL facilitates the seamless communication and integration of various data sources and devices, over LAN or WiFi. The LSL framework includes libraries and tools for different programming languages (e.g., C++, Python, MATLAB), allowing researchers to create applications that generate, transmit, or receive data streams. The protocol ensures that data streams are synchronized, which is crucial for accurate analysis and interpretation.</p><p>LSL assigns each data sample generated by a source (e.g., a sensor or device) a unique timestamp that reflects the moment of its creation. This timestamp is recorded in the metadata accompanying the data sample. LSL employs a clock synchronization protocol to ensure that the timestamps of data samples are aligned across different devices and sources. This involves maintaining a shared understanding of time across all involved components, i.e. in a network of data sources, one clock is designated as the "master" clock (in our case the recording computer). This clock serves as the reference for synchronization. Other clocks (in our case the clocks of the recording devices, i.e. audio, video, EEG, ET), adjust their timing to match the master clock by periodically requesting synchronization updates from the master clock. This provides the necessary adjustments to bring the clocks into alignment. These adjustments are then applied to the timestamps of data samples generated by recording devices, allowing the precise analysis of simultaneously recorded data streams in time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.2">Noisy EEG data: ASR and ICA</head><p>In naturalistic research settings, EEG recordings are subject to a variety of artifacts from muscle activity, movement, and environmental noise, making it challenging to extract clean neural signals <ref type="bibr" target="#b30">(Gorjan et al., 2022)</ref>. Our approach uses a combination of Independent Component Analysis (ICA) and Artifact Subspace Reconstruction (ASR) <ref type="bibr">(Delorme &amp; Makeig, 2004;</ref><ref type="bibr" target="#b53">Kothe &amp; Jung, 2016;</ref><ref type="bibr" target="#b69">Mullen et al., 2015;</ref><ref type="bibr" target="#b54">Kothe et al., 2019)</ref>. ICA decomposes the complex EEG signal into a set of statistically independent components, allowing the isolation and removal of those components that represent stationary, non-neural artifacts <ref type="bibr">(Delorme &amp; Makeig, 2004)</ref>. ASR, by contrast, is well-suited to detect and correct for transient, high-amplitude disturbances in the data. It works by reconstructing the underlying brain activity from a subspace defined by the cleaner segments of the recording (see <ref type="bibr" target="#b53">Kothe &amp; Jung, 2016)</ref>. Together, ICA and ASR form a powerful, complementary framework that can effectively clean noisy EEG data, even when collected in uncontrolled, real-world environments (see <ref type="bibr" target="#b32">Gramann, 2024</ref> for a very recent review on mobile EEG data).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.3">Event overlap</head><p>Accounting for the overlap of event-related responses in EEG data (and other time series data) can be achieved using (non-)linear deconvolution (e.g. <ref type="bibr" target="#b18">Dimigen &amp; Ehinger, 2021;</ref><ref type="bibr">Smith &amp; Kutas, 2015a</ref><ref type="bibr" target="#b93">, 2015b)</ref>. In <ref type="bibr">Smith and Kutas (2015ab)</ref>, the deconvolution technique within the rERP framework is introduced as a solution to the problem of overlapping eventrelated potentials (ERPs) in continuous EEG recordings. Traditional ERP methods average epochs time-locked to events, assuming either that responses do not overlap, or they control for the overlap via stimulus or experiment design. Yet, in real-world experimental paradigms, events occur in rapid succession, and their corresponding neural responses mix together, very likely distorting the observed ERP waveform of interest. The deconvolution approach treats the continuous EEG signal as a (linear) combination of overlapping responses. By formulating a general linear model (GLM), each event is represented by a predictor that spans a defined time window, effectively "tagging" the EEG data with the timing and expected shape of the ERP. The design matrix constructed from these predictors allows for the simultaneous estimation of regression coefficients for each event type. These coefficients can then be directly interpreted or be used to reconstruct the original, overlap-corrected ERPs.</p><p>The regression-based deconvolution method (rERP) thus disentangles the overlapping neural responses, enabling more accurate estimation of the true underlying ERP waveform for an event of interest. It also permits the inclusion of additional covariates and interactions, enhancing the capacity to model complex, real-world data where events are not neatly separated into categories. The technique has been implemented in several statistical software environments (e.g. Matlab and Julia: <ref type="bibr" target="#b22">Ehinger &amp; Dimigen, 2019;</ref><ref type="bibr">Python: Sassenhagen, 2019)</ref>,</p><p>providing researchers with tools to analyze continuous EEG data under more naturalistic conditions where event overlap is unavoidable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.4">Benefits: Gaze and fixation-related potentials as cues to language processing</head><p>The present study should be understood as a proof-of-concept. We want to demonstrate that considering the aspects described above allows us to tap into the neural integration of spoken language with visual information during real-world interaction. For instance, when participants listen to an utterance, such as "Move the candle to the left, please" while viewing a real-world visual scene, eye-tracking data can inform us whether their gaze is directed toward the target object (candle) or not at the moment the word 'candle' is uttered. This allows us to investigate how visual attention or, more generally, visual information modulates neural responses to language in real time. Furthermore, we can relate fixation-related potentials to language-related potentials within and across experimental variables. For instance, fixation-related potentials show a reliable positive component for gazes at target objects, as compared to distractor objects (e.g. <ref type="bibr" target="#b10">Brouwer et al., 2013;</ref><ref type="bibr" target="#b44">Kamienkowski et al., 2012)</ref>. In case of referential ambiguities, as in the present study (see section 2), fixationrelated potentials can thus be informative cues as to whether participants treated an object as a target or 'just' as a referential, but task-irrelevant competitor. Moreover, studies on natural reading demonstrated that semantically meaningful parafoveal preview can reduce the N400 component when fixating a word during reading <ref type="bibr" target="#b2">(Antúnez et al., 2022;</ref><ref type="bibr" target="#b15">Degno &amp; Liversedge, 2020;</ref><ref type="bibr" target="#b19">Dimigen et al., 2012;</ref><ref type="bibr" target="#b60">Li et al., 2024;</ref><ref type="bibr" target="#b70">Niefind &amp; Dimigen, 2016)</ref>, revealing that tracking the eyes can also provide important information about the availability of predictive cues during language comprehension. Likewise, studies that use a combination of visual stimuli and spoken or written language, such as the visual world paradigm or sentence-picture verification tasks <ref type="bibr" target="#b40">(Hirschfeld et al., 2011;</ref><ref type="bibr" target="#b51">Knoeferle et al., 2011;</ref><ref type="bibr" target="#b61">Liu et al., 2011;</ref><ref type="bibr" target="#b91">Sitnikova et al., 2008;</ref><ref type="bibr" target="#b96">Staudte et al., 2021)</ref>, suggest that the degree of similarity between visual and linguistic input is strongly reflected in the N400 time range. Interestingly, not only visual cues influence linguistic processing: research has also demonstrated that linguistic cues influence visual processing <ref type="bibr" target="#b40">(Hirschfeld et al, 2011;</ref><ref type="bibr" target="#b39">Henderson &amp; Hayes, 2017)</ref>. In the present study, in which participants engage with complex visual scenes while processing spoken language, combined EEG and eye tracking recordings can thus inform us on how the brain's electrophysiological responses in the N400 time range are shaped by the (un-)availability of visual and auditory linguistic cues that can be used for predicting upcoming sensory input. In the following section, we will describe the study design that we used for our proof-of-concept experiment.</p><p>seated on opposite sides of a vertical array (shelf) in which objects are placed, for instance two boxes and two candles. One participant (the director) gives instructions to the other participant (the matcher) to move objects in the shelf. However, some of the objects are hidden from the view of the director. For instance, when one of the two candles is hidden from the director's view (i.e., it is in the participant's privileged ground, PG) and the director asks the participant to move the candle, a referential conflict arises: from the perspective of the director, the candle is uniquely identifiable, but from the perspective of the matcher, the referential expression is ambiguous, unless the matcher considers the perspective of the director.</p><p>We chose this task for several reasons. First, it is well suited to examine how gaze behavior and language-related event-related potentials are integrated during referent selection in a naturalistic experimental setting, since each instruction must be followed by the visual identification of the target object, before it can be moved. Second, the director task can be designed to be repetitive enough to be used in an event-related potential study, which usually requires a large number of comparable trials to produce reliable results. Finally, there is a rich literature on gaze behavior in the director's task (e.g. <ref type="bibr" target="#b5">Barr, 2008;</ref><ref type="bibr">Epley, Keysar, et al., 2004;</ref><ref type="bibr" target="#b25">Epley, Morewedge, et al., 2004;</ref><ref type="bibr" target="#b27">Ferguson &amp; Breheny, 2012;</ref><ref type="bibr" target="#b35">Hanna et al., 2003;</ref><ref type="bibr" target="#b38">Heller et al., 2008</ref>; J. J. <ref type="bibr" target="#b103">Wang et al., 2019)</ref>, and there also exist two event-related potential studies <ref type="bibr" target="#b80">(Richter et al., 2020;</ref><ref type="bibr" target="#b90">Sikos et al., 2019)</ref> that used computerized versions of the director task to which we can compare our results.</p><p>In the years following the first eye tracking study of the director task <ref type="bibr" target="#b46">(Keysar et al., 2000)</ref> the literature was dominated by a debate about the time point in processing at which participants consider the perspective of others, early <ref type="bibr" target="#b38">(Heller et al., 2008;</ref><ref type="bibr" target="#b35">Hanna et al., 2003;</ref><ref type="bibr" target="#b27">Ferguson &amp; Breheny, 2012;</ref><ref type="bibr">Epley et al., 2004)</ref> or late <ref type="bibr" target="#b46">(Keysar et al., 2000;</ref><ref type="bibr" target="#b5">Barr, 2008;</ref><ref type="bibr" target="#b103">Wang et al., 2019;</ref><ref type="bibr" target="#b6">Barr, 2016)</ref>. Today, it is general consensus that whether and at which time in processing participants consider the perspective of others depends on a variety of factors, including (linguistic) context (e.g., <ref type="bibr" target="#b38">Heller et al., 2008;</ref><ref type="bibr" target="#b11">Brown-Schmidt et al., 2008)</ref>, task <ref type="bibr" target="#b26">(Ferguson et al., 2015;</ref><ref type="bibr" target="#b84">Ryskin et al., 2020)</ref>, culture <ref type="bibr" target="#b103">(Wang et al., 2019;</ref><ref type="bibr" target="#b108">Wu et al., 2013;</ref><ref type="bibr" target="#b109">Wu &amp; Keysar, 2007)</ref> and the availability of cognitive resources <ref type="bibr" target="#b12">(Cane et al., 2017;</ref><ref type="bibr">Epley et al., 2004)</ref>. <ref type="bibr" target="#b35">Hanna et al. (2003)</ref>, for instance, demonstrated in two experiments that addressees are unable to ignore visually salient objects in privileged ground if they match a speaker's description and assign reference as early as possible once a potential referent is identifiable. Recently, a probabilistic model of reference resolution has been developed assuming that the knowledge of the self and the (assumed) knowledge of the other in dyadic interaction are assigned a weight and are then combined into a probabilistic communicative model by both speaker and addressee <ref type="bibr" target="#b37">(Heller &amp; Brown-Schmidt, 2023;</ref><ref type="bibr" target="#b67">Mozuraitis et al., 2016</ref><ref type="bibr" target="#b68">Mozuraitis et al., , 2018;;</ref><ref type="bibr" target="#b84">Ryskin et al., 2020)</ref>. This approach highlights that the other's knowledge can only be inferred based on weighted cues from different available sources. In the director task, one of the rather salient and informative cues for the director's knowledge and intended reference is visual: whether a potential referent of a referential expression is occluded or not. Gazes at privileged ground competitors thus occur because they are informative when it comes to inferring the knowledge of the director -participants are collecting evidence for or against a referential choice.</p><p>To the best of our knowledge, there exist only two event-related potential studies using versions of the director task. The first study by <ref type="bibr" target="#b90">Sikos et al. (2019)</ref> utilized temporary referential ambiguities and the Nref ERP component, a sustained negativity interpreted as a marker of referential ambiguity <ref type="bibr" target="#b99">(Van Berkum et al., 2007)</ref>. On a computer screen, participants saw displays of cards showing animals with accessories, e.g. a brontosaurus with boots and a brontosaurus with a purse, such that when participants were asked to "move the brontosaurus with boots", "brontosaurus" was referentially ambiguous until the word "boots". They found a graded Nref effect that has the highest amplitude when the referential competitor is in common ground, followed by trials with no competitor and trials with a competitor in privileged ground. They argue that the reduction of the Nref in trials with privileged ground competitors shows that participants do not consider privileged ground competitors as potential referents in a way as they do for common ground competitors. In other words, they consider the perspective of the other while determining referential candidates. In the ERP study by <ref type="bibr" target="#b80">Richter et al. (2020)</ref>, participants were presented with 4x4 grids of objects on a computer screen. Referential ambiguities were created using size contrasted object triplets (e.g. small, medium, large candle) of which either the large or small object was in privileged ground (hidden from the director). When the director now asks to "move the small candle" while the small candle is in privileged ground of the addressee, a referential ambiguity arises: the small candle in privileged ground and the medium candle in common ground are potential referents of the utterance from the speaker's perspective. In two separate studies, <ref type="bibr" target="#b80">Richter et al. (2020)</ref> recorded eye tracking and EEG data using the same experiment, creating comparable data sets of two modalities. While the behavioral results and the results of the eye tracking study replicate previous results (longer reaction times and an increased number of gazes at privileged ground competitors in pair trials), the ERP results are different from <ref type="bibr" target="#b90">Sikos et al. (2019)</ref>. Across conditions, Richter et al. ( <ref type="formula">2020</ref>) found a broadly distributed, positive ERP component with posterior maximum, that was more pronounced (more positive) for trials with privileged ground competitor as compared to trials without, which is the opposite of the negative effect in <ref type="bibr" target="#b90">Sikos et al. (2019)</ref>. <ref type="bibr" target="#b80">Richter et al. (2020)</ref> interpret this late positive effect as reflecting difficulties associated with the integration of privileged ground information.</p><p>In the present study, we used a version of the director task modified in way to make it very simple for participants and to reduce the amount of variance in our data. We used a 4x4 shelf on a table as visual display and placed six objects in it (Figure <ref type="figure" target="#fig_4">1</ref>). While there were two singleton, i.e. unique objects (e.g. one cream jar, one tube), four objects were pairs of identical objects (e.g. two candles, two roses), with one of each pair hidden from the director's view. We chose this very simple design, because the matcher would only need one piece of information to resolve the referential ambiguity: Is an object occluded from the director's view, or not? By this, we wanted to minimize eye movements related to the and one with a purse), thus possibly fostering gazes at the competitor object, in order to collect evidence for or against a referential choice. Here, no comparison was necessary.</p><p>Participants could either gaze at the competitor, see that it is hidden and then switch to the target, or they could gaze at the target, see that it is not hidden, and move it. Effectively, this makes the target the most informative object in the present study, because it provides the necessary visual information (hidden or not) and because it is the actual target object.</p><p>Accordingly, we expected participants to pay little attention to the hidden competitors, signaling no difficulties in taking the perspective of the other into consideration.</p><p>Since we recorded EEG and gaze data of the matcher, we are able to track where participants' gaze was directed, while they heard a target word and by considering it as a predictor in our rERP calculation. Likewise, we could also include the progress of linguistic information in the calculation of the fixation-related potentials, in order to examine differences between trials, in which participants were hearing or had already heard the target word while fixating the target object, and such trials, in which they were not. Here, we focus on the N400/P300 time range (300-500 ms) and included an additional later time window (500-800 ms) to account for later effects that we expected based on the very long effect latencies in <ref type="bibr" target="#b80">Richter et al. (2020)</ref> and <ref type="bibr" target="#b90">Sikos et al. (2019)</ref>.</p><p>We assume that if participants collected sufficient visual information about the target object (e.g. its position in the shelf) before they have linguistic evidence for the identity of the upcoming target (target fixations before noun), this possibly facilitates target selection during and after linguistic input (e.g. noun onset) in a top-down manner. We assume that this (predictive) information is reflected in a reduction in N400 amplitude and/ or an increase in P300 amplitude for the event-related potentials time locked to noun onset. In trials in which participants made the majority of target gazes during the noun, visual target information and linguistic target information processing occur simultaneously, with visual and auditory linguistic input providing bottom up information at the same time. The relative absence of a priori visual information about the target object (e.g. its position) probably leads to relative difficulties in target detection, as compared to the cases, in which participants already have information about the target object before noun onset. This should be reflected in a more negative going ERP between 300-500 ms, e.g. an increase of the N400, and a decrease of the P300 in the ERP time locked to noun onset. Finally, in trials in which target gazes occur later than the offset of the critical noun, participants already have complete linguistic evidence regarding the identity of the target stimulus, before they start actively searching for the object in the shelf. That is, it should mirror the cases, where visual information is available before linguistic information. Again, we expect this to be reflected in a further decrease in P300 amplitude (difficulties in target selection and task execution) and an increase of the N400 amplitude (integration difficulties since no target has been visually identified). For the fixation-related potentials, we expect the effects to be similar, but reversed: fixations to the target object before noun onset occur without the presence of linguistic information, thus they are uninformed fixations with regard to the task-related status of an object. That is, it is still unclear if it will become the target or not, while fixations to the target object after the noun has been fully uttered can be considered informed fixations, with the task-related knowledge about the objects already provided. Thus, N400 and P300 amplitude should be reversed as compared to language-relate event-related potentials. Fixation-related potentials of fixations that occur during the noun, on the other hand, should behave similar to language-related ERPs, since in these cases, the relative information status of both linguistic and visual input is similar in both cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Materials and Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Participants</head><p>Thirty-three native speakers of German (17 female, mean age: 25.48, range: 20-32) participated in the current study after written informed consent. The EEG recordings of 7 participants had to be excluded from analysis due to technical problems with the recording equipment or human error (7 participants; 4 female). Consequently, we also excluded these participants from the analysis of the eye tracking data (final sample: 26 subjects, 14 female; mean age: 24.94, range: 20-32). Participants either received course credits or monetary compensation for their participation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Experimental Stimuli and Procedure</head><p>In the experiment, a participant and a confederate were seated face-to-face on opposite sides of a table. The participant's (the matcher) task was to move objects around a 4x4 shelf placed in the middle of the table according to the instructions of the confederate (the director). Four of the sixteen compartments of the shelf were occluded from the view of the confederate. In the shelf, six objects were placed, two were singleton items (singleton), the other four were two pairs of identical objects (pair). In two of the hidden compartments, one object from each pair of identical objects was placed. The other two hidden compartments remained empty. In German, gender is marked on the determiner preceding a noun and can be used as a cue for referential predictions, when a set of potential referents with different grammatical gender exists. In order to avoid these predictions, all objects used in the current experiment had two-</p><p>syllable, feminine names (singleton objects: die Pflanze, 'plant'; die Spritze, 'syringe'; die Flasche, 'bottle'; die Dose, 'can/tin'; object pairs: die Kerze, 'candle'; die Blume, 'flower';</p><p>die Vase, 'vase'; die Tasse, 'cup'). The objects were grouped into two sets of two critical pairs and two singleton objects. The grouping of objects into sets was counterbalanced across participants. In the experiment, all participants saw all objects across four experimental blocks. In Block 1 and Block 2, the objects from set A were used, in Block 3 and Block 4 the objects from set B. In the experimental pauses after Block 1 and Block 3, the objects in the shelf and the hidden compartments were randomly scrambled by our lab assistants. The directors were instructed to move every object in the shelf ten times within one experimental block, to write down the respective target position in the shelf, not to move the same object twice in a row, and to use 'left' and 'right' from the matcher's perspective. Matchers were instructed to move objects only one compartment at a time into any direction and to remember that they can see more than the director.</p><p>The experimental procedure was very simple. After setup or redecoration, the lab assistants left the room and the director started with the instructions. After each object was moved ten times, a block ended. The experiment lasted between 35-60 minutes. Including setup, an experimental session lasted between 2-3 hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Data Recording</head><p>All data were recorded and synchronized with the other data streams via labstreaminglayer (LSL; <ref type="url" target="https://github.com/sccn/labstreaminglayer">https://github.com/sccn/labstreaminglayer</ref>) using the LabRecorder app for LSL (<ref type="url" target="https://github.com/labstreaminglayer/App-LabRecorder">https://github.com/labstreaminglayer/App-LabRecorder</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Audio Data</head><p>We recorded the voice of the confederate using a headset microphone (AKG C 520). The recording was digitized with a sampling rate of 22050 Hz (Mono) using an external audio interface (Focusrite Scarlett 2i2 3rd Gen).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Eye tracking data</head><p>We recorded participants' gaze using pupil core eye tracking glasses (Pupil Labs) with a monocular setup. The pupil core headset records pupil data at 120 Hz nominal sampling rate and a second ("world") camera records the scene from a first person perspective with a sampling rate of 30 Hz. We used single marker calibration as implemented in the recording software PupilCapture. Angular accuracy and precision were kept below 1.5° and 0.15°, respectively. The data were recorded to hard drive and synchronized with the other data streams via LSL. For tracking the positions of the objects in the shelf throughout the recording, we attached four AprilTags <ref type="bibr" target="#b56">(Krogius et al., 2019;</ref><ref type="bibr" target="#b73">Olson, 2011;</ref><ref type="bibr" target="#b104">Wang &amp; Olson, 2016)</ref>, with a working principle similar to QR-codes, to the shelf, one in each corner. Using the surface tracking plugin provided by Pupil Labs, we defined surfaces (areas of interest) offline (here the 16 compartments of the 4x4 shelf). An algorithm then calculates the distances between the tags and the surfaces for each video frame, making it possible to correct for changes in viewpoint (e.g., by head or body movements) and to identify the surfaces' location throughout a recording. Thus, we are able to track at which of the 16 surfaces a participant is gazing at a given point in time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EEG Data</head><p>EEG Data were recorded using a CGX-mobile-128 System (Cognionics, San Diego, CA, USA) with 128 AgCl active electrodes relative to a left mastoid reference at a sampling rate of 500 Hz. The right mastoid served as ground. Four of the 128 channels were never recorded ('FTT10h', 'FTT9h', 'FFT10h', 'FFT9h'), because they had to be removed in order to fit the eye tracking glasses. Channel impedance was kept below 20 kΩ at the beginning of the experiment.</p><p>The EEG was digitized and synchronized with the other streams via LSL using the LSL-relay implemented in the CGX-Recording software. Each experimental block was recorded to a separate file (2 patterns and 2 object sets = 4 files), since in the time between blocks, participants were moving freely and did not perform any task-related actions. In addition, we thought of this as a security measure, so that in case of data loss (e.g. due to connectivity issues), we would not lose the data of an entire experimental session.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Data Preprocessing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Audio Data and Annotations</head><p>The recorded audio data was imported into Matlab using the xdf import functions, where it was normalized and exported to .wav. The resulting audio files were then subjected to automatic speech recognition and automatic segmentation <ref type="bibr" target="#b88">(Schiel, 1999</ref><ref type="bibr" target="#b89">(Schiel, , 2015) )</ref> via the BAS web interface <ref type="bibr" target="#b47">(Kisler et al., 2017)</ref>. Afterwards, we manually controlled the output and applied corrections where necessary. Manual corrections mostly involved misplaced fricative onsets and unclear pronunciations. We then manually annotated the audio recordings for experimental conditions. During the experiment, we also asked the confederates to write down the current position of the objects in the shelf. This information was then manually transferred into lists and combined with the annotations using custom scripts. This enabled us to automatically match the gaze data with the current object positions.</p><p>Although we did not systematically analyze the linguistic constructions used by the director, we would like to briefly note that instructions for all participants always included a full noun phrase consisting of a determiner (die, 'the.FEM') and a noun (e.g. Und dann die Tasse nach oben, bitte!, 'And then the cup up, please.').</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Eye Tracking Data</head><p>The preprocessing of the eye tracking data was carried out in R using the library eyetrackingR <ref type="bibr" target="#b28">(Forbes et al., 2025)</ref>. Any data with a pupil-detection confidence &lt;.60 were excluded from analysis and non-AOI gazes were not treated as missing. As there is no software or package available that can deal with the kind of data we recorded (e.g., our data is not trial based but continuous gaze data), large parts of the scripts we used for preprocessing are custom scripts needed to prepare the data for analysis (e.g., combine the gaze data with the linguistic annotations, create trials).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EEG Data</head><p>The EEG recordings of each block were imported into EEGLAB <ref type="bibr">(Delorme &amp; Makeig, 2004)</ref> using the import functions for .xdf files of the MobiLab plugin <ref type="bibr" target="#b72">(Ojeda et al., 2014)</ref>. Before any preprocessing steps, we created an event structure for each block containing all words spoken by the director and all fixations of a participant. The data was resampled to 250 Hz.</p><p>Afterwards. the data were concatenated and preprocessed as follows. We first filtered the data with a 2Hz FIR filter (pop_firfiltnew, cutoff frequency (-6dB): 1 Hz) to approach stationarity for later ICA decomposition. Afterwards, we rejected noisy channels in a semi-automatic fashion. First, we rejected channels selected by visual inspection of the raw EEG data. Then we used the pop_clean_rawdata() function <ref type="bibr" target="#b54">(Kothe et al., 2019)</ref> to automatically reject noisy channels, followed by automatic rejection using the kurtosis method of the pop_rejectchan() function with a threshold of 2. Afterwards, we applied a 100 Hz low-pass filter to the data and then used the function cleanLineNoise to remove line noise artifacts at 50 Hz. Then, we used artifact subspace reconstruction (ASR; <ref type="bibr" target="#b69">Mullen et al., 2015;</ref><ref type="bibr" target="#b53">Kothe &amp; Jung, 2016)</ref> as implemented in the clean_rawdata plugin for EEGLAB <ref type="bibr" target="#b54">(Kothe et al., 2019)</ref> to identify and correct noisy segments in the recording.</p><p>After ASR, we interpolated missing channels (mean = 26, range: 17-46) and re-referenced the data to average reference under reconstruction of the original reference. After re-referencing, the original reference and interpolated channels were removed again in order to avoid rank deficiency. Then, we used the AMICA algorithm to decompose the data into independent components (1 model, 2000 iterations, 10 times rejection of unlikely data). The resulting weights and sphere of the model were then transferred to a 0.6 Hz low-pass and 20 Hz highpass filtered version (pop_firfiltnew, cutoff frequency (-6dB): 0.3 Hz, 44.3 Hz) of the data with 250 Hz sampling rate. We then used the ICLabel plugin for EEGLAB to classify components and subsequently rejected all components not labeled "brain" (mean: 43, range:</p><p>14-62).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.5">Data analysis and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gaze data</head><p>We examined three measures of the gaze data: to investigate the time-course of target selection, (i) proportion of gazes at the target object and the competing object in a pair (the former being in common ground, the latter in privileged ground); (ii) gaze proportions at competitor objects relative to all gazes at other non-target objects; and (iii) the mean time point at which participants gazed at the target object for each trial, which we used as a predictor for the ERP analysis. This measure summarizes all gazes to the target, collapsing the first gaze at the target, the last gaze at the target and gazes in between into a single value.</p><p>While this has the disadvantage of being rather imprecise, as for instance compared to the first gaze at a target object, it provides a measure for the processing load associated with target identification throughout an entire trial and beyond the single word level. This measure is thus comparable to the total reading time in eye tracking research on reading (e.g. <ref type="bibr" target="#b49">Kliegl et al., 2004</ref><ref type="bibr" target="#b50">Kliegl et al., , 2006;;</ref><ref type="bibr" target="#b57">Kuperman et al., 2018)</ref>.</p><p>We contrasted the target of a pair (which conflicts with an object in privileged ground; hence "pair condition") with the target of a singleton ("singleton condition"). We used a clusterbased permutation analysis ( <ref type="bibr" target="#b64">(Maris &amp; Oostenveld, 2007)</ref>) to test for significant differences in gaze proportions between the pair and the singleton condition for the target AOI (cf. <ref type="bibr" target="#b80">Richter et al., 2020)</ref>. We calculated a paired, two-sided t-test for 100 ms time bins from 0 to 3500 milliseconds after critical noun onset. For all time bins with a significance threshold of 0.05 the procedure was then repeated 4000 times with randomly shuffled (not sequentially ordered), i.e. permuted versions of the binned data. This produces a distribution that is expected at chance level. These results are then compared to the original model using the sequentially ordered time series. Through this procedure, we can obtain a p-value that tells us how likely we would find a given effect when the distribution was random, i.e., if there was no effect of our experimental variable.</p><p>Since in the singleton condition, the privileged ground competitor was absent, we could not compare gazes at the privileged ground competitors between conditions in the same way.</p><p>Instead, we compared the gaze proportions at the competitor object in the pair condition with the gaze proportions averaged over all other, non-target objects: the two singleton objects and the two objects from the other pair (one in privileged ground, one in common ground).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[Figure 2 here]</head><p>Figure <ref type="figure" target="#fig_5">2</ref> shows the gaze proportions of the different objects in the shelf by condition. The figure reveals a higher proportion of gazes at the target object in the pair (solid) than in the singleton (dotted) condition. The cluster based permutation analysis of the target-AOI revealed a significant effect in a cluster between 1100-1900 ms after critical noun onset (p = 0.009). In a post-hoc analysis of the goal-AOI, although numerically present, the difference between conditions did not reach statistical significance (p &gt; 0.1). Since there were no privileged ground competitors in the singleton condition, we compared the gaze proportions of privileged ground competitors in the pair condition with the mean gaze proportions of all other, non-target objects (Figure <ref type="figure" target="#fig_6">3</ref>): the two singleton objects and the two objects from the other pair (one in privileged ground, one in common ground). As Figure <ref type="figure" target="#fig_6">3</ref> demonstrates, the gaze proportions associated with the hidden competitor is numerically lower than the mean gaze proportions associated with all other non-target objects in the shelf. According to the results of the permutation analysis, this difference is even statistically significant in clusters between 1400-3400 ms (p = .001) after stimulus onset.</p><p>[Figure <ref type="figure" target="#fig_6">3</ref> here]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EEG data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>First-order statistical analysis: rERP calculation</head><p>In order to account for the overlap between brain responses to the stimuli of interest (critical nouns) and other stimuli (determiners, words following or preceding the noun, fixations), we calculated a general linear regression model with a linear time basis function from -500 to 1500 milliseconds after event onset for 4 different event types: determiner preceding the noun, the critical noun, words preceding the determiner (max. 2), intervening between determiner and noun, and following the noun (max. 2), as well as fixations within -1500 to 3500 ms after noun onset. We were interested in the event-related potentials following critical nouns and fixations, the other events were only included to account for the overlap. For the determiners, we included the experimental condition (pair, singleton) as a categorical predictor and added trial as a covariate without interaction. For critical nouns, we additionally added a continuous predictor mean target gaze time (see section Gaze Data), for which we assumed an interaction with condition. For all other words before the determiner, or before or after the noun, we only calculated an intercept model to account for the overlap.</p><p>The following list contains the formulae used to compute the overlap corrected ERPs for all four event types described above. We used the resulting beta estimates to reconstruct the ERPs of critical nouns and fixations including the conditional means of the covariates (nouns: trial, fixations: saccadic amplitude).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results: Language-related ERPs</head><p>[Figure <ref type="figure" target="#fig_3">4</ref> here] at the posterior and central ROI. Finally, there is a later positivity at left-frontal and centrofrontal ROIs between ~600-850 ms for the pair condition. All other differences between conditions seem rather small and will not be considered.</p><p>Figure <ref type="figure" target="#fig_8">5</ref> shows the ERPs to critical nouns in the pair (left) and singleton (right) condition, grouped by ROI (saggital axis only) and the mean time point relative to noun onset at which participants fixated the target object (grouped for plotting purposes only; before noun: -1000ms to 0ms; during noun 0ms to 471 ms; after_noun: &gt; 471 ms). Overall, there are considerable differences between conditions. While the pair condition (red, left) shows the strongest effects at occipital channels before mean noun offset (more negative for later mean target fixation time), the singleton condition shows more sustained effects from ~125 ms to ~625 ms with reversed polarity (more positive for later mean target fixation times), but similar topographic distribution (occipital ROI). In addition, in the singleton condition shows a clear negative effect over central ROIs (more negative with later mean target fixation times) that is basically absent for the pair condition. The posterior and occipital positivity and (pre-)frontal negativity found for the pair condition at ~500 ms is also affected by the mean target fixation time: with later fixations, the positivity is stronger, but it does not allow a differentiation between fixation times during and after the critical nouns.</p><p>To summarize the results, we found that the pair condition elicited a distinct pattern of ERP effects, including a sustained frontal and posterior positivity peaking around 500 ms, a frontal negativity over right-hemispheric sites, and early occipital negativities, all largely absent or reduced in the singleton condition. Additionally, fixation timing modulated these effects differently across conditions: in the pair condition, later fixations amplified a posterior positivity, whereas in the singleton condition, later fixations led to a more sustained occipital positivity and central negativity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results: Fixation-related ERPs</head><p>Figure <ref type="figure" target="#fig_9">6</ref> shows the event-related potentials relative to target fixation onset by condition. The large positive deflection at 0 ms is probably the lambda response <ref type="bibr" target="#b81">(Ries et al., 2018)</ref>. It usually occurs 80-120 ms after fixation onset with stationary eye tracking equipment. As of today, we have not identified the source of this latency shift, but we assume that the source is the dispersion-based fixation detection algorithm assuming a dispersion of less than 1.2 degrees visual angle for at least 80 ms, the exact time of the latency shift. In combination with the limited accuracy of the mobile eye tracker (~1.8 degrees) this probably induced this shift. We are currently working on improving our pipeline to also account for head movements, in order to be able to move to a saccade-based detection algorithm as proposed by <ref type="bibr" target="#b23">(Engbert &amp; Kliegl, 2003)</ref>. Except for a small positivity (~400-650 ms) for the pair conditions, there is almost no difference between conditions. However, if we turn to Figure <ref type="figure" target="#fig_10">7</ref>, which shows the fixation-related ERPs by fixation time at selected channels, we can see clear differences between the two conditions. First, the effect of fixation time starts very early for the pair condition at channel Fpz (0-250 ms), with a more pronounced positivity for earlier fixations (before &gt; during &gt; after noun). It is followed by and partly overlaps with two positive effects (~200-700 ms and ~740-1000 ms) which are strongest over Cz (with the same direction (before &gt; during &gt; after noun). At channel Oz, there is a series of negative effects visible (before &gt; during &gt; after) which is less clear than the effects at channels Fz and Cz. For the singleton condition, on the other, the effects are different. At channel Fz there is only a rather late positivity for early fixations starting at ~500-1000 ms after fixation onset (before &gt; during &gt; after). It is preceded by a positivity over channel Cz between ~250 and 500 ms (before &gt; during &gt; after), which is similar to the effect found for the pair condition, but shorter (~250-500 ms). This positivity is followed by a later positive effect from ~800-1000 ms after fixation onset (after &gt; during &gt; before noun). At channel Oz, an early positivity is visible for the singleton condition (~100-375 ms) which is stronger for earlier fixations (before &gt; during &gt; after noun). It is followed by a later negativity (~500-800 ms) with a similar gradient (before &gt; during &gt; after noun).</p><p>Overall, the fixation-related potentials differed markedly by condition, with the pair condition showing earlier and more sustained effects of fixation timing across frontal and central sites, while the singleton condition exhibited later and more transient effects with distinct temporal and topographic profiles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Second-order statistical analysis: permutation analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results: Language-related ERPs</head><p>For the second-order statistical analysis of the language-related ERPs, we summarized the ERP data into time bins of 100 ms, in order to account for auto-correlation. We then calculated a linear regression model for each of the resulting time bins using the following formula data~laterality*saggitality*condition*mean_target_fixation*baseline, were baseline (mean predicted microvolt -250 ms to 0 ms relative to noun onset) was added to the model formula to apply robust baseline correction <ref type="bibr" target="#b0">(Alday, 2019)</ref>, and saggitality and laterality were categorical topographic predictors (laterality: left, central, right; saggitality: prefrontal, frontal, central, posterior, occipital). Afterwards we used a time bin-based permutation approach to test for significant effects. For this purpose, we shuffled the predicted microvolt values within each time bin to decouple the dependent and independent variables, thereby simulating the null hypothesis, under which the predictors of our model and the dependent variable (predicted microvolt) are not related. Then we calculated linear models for 2000 permuted versions of each time bin and compared their absolute t values |t| to the original, unshuffled models' |t| values. From this comparison, we created a probability distribution for each time bin, making it possible to compute how likely |t| is larger under the assumption that the null hypothesis is true, with smaller p indicating lower probability that the null hypothesis is true and higher probability that the effect of the unshuffled data is a true effect. Figure <ref type="figure" target="#fig_11">8</ref> shows a selection of significant effects in the time bins within the N400/P300 (black rectangle) and late positivity (blue rectangle) time range (see supplementary materials at <ref type="url" target="https://osf.io/5ds4z/">https://osf.io/5ds4z/</ref> for a full graphical and text summary of the permutation results and the results of the linear models per time bin). We see that the effect of mean target fixation significantly interacts with condition and with the topographical factors in both time windows, but with a focus on the earlier P300/N400 time window.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[FIGURE 9 AND FIGURE 10 HERE]</head><p>The effect plots of the time-range from 0-800 ms following noun onset (Figures <ref type="figure" target="#fig_12">9</ref> and <ref type="figure" target="#fig_13">10</ref>) make the time and topographic distribution of the significant effects a bit clearer. It becomes obvious from the effect plots in Figure <ref type="figure" target="#fig_12">9</ref>, that the effect of mean target fixation for the pair condition is strongest within the time-range of the N400/P300 (300-500 ms), visible as a positivity over central occipital channels. It is reversed over frontal and prefrontal channels and followed by a negativitiy strongest over right-central, frontal and prefrontal channels.</p><p>Although the earlier onset and more sustained nature of the negativity may suggest two different underlying effects, we cannot decide on basis of the present data whether the negativity and positivity are different effects or share an underlying dipole. Therefor, we will not discuss them separately. Note, that the very early effects directly after noun onset are very much attenuated as compared to the ERPs, because the baseline predictor canceled them out.</p><p>For the singleton condition (Figure <ref type="figure" target="#fig_13">10</ref>), the picture is very similar, although the occipital positivity starts earlier (200-300 ms) and is less pronounced (hence the main effect of condition visible in the ERPs). The same is true for the centro-frontal negativity, which starts very early for the singleton condition (0-100 ms) and is rather left-lateralized and stronger over the central ROI, as compared to the pair condition. In a similar way as the positivity, it is less pronounced than for the pair condition, explaining the main effect of condition visible over these ROIs and time-windows in the ERPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results: Fixation-related ERPs [FIGURE 11 HERE]</head><p>The statistical procedure for the fixation-related ERPs was identical to the one described for the nouns above. The model formula included the same topographic factors, a factor for condition (pair/singleton), a continuous predictor for the trial_time relative to noun onset at which a fixation occurred, and a continuous predictor baseline (-250 to 0 ms relative to fixation onset). For the second order analysis, we only focused on the reconstructed rERPs for target objects, dropping all other fixation-related potentials. Figure <ref type="figure" target="#fig_14">11</ref> summarizes the significant effects in the N400/P300 (300-500 ms) and late positivity time window (500-800 ms). Other than for the nouns, we can see that trial time significantly interacts with condition and the topographical factors in all time bins, without a clear focus on the earlier (N400/P300) or later (late positivity) time window. Figure <ref type="figure" target="#fig_15">12</ref> and Figure <ref type="figure" target="#fig_6">13</ref> summarize the corresponding effects by ROI.</p><p>[FIGURE <ref type="figure" target="#fig_15">12</ref> AND FIGURE <ref type="figure" target="#fig_6">13 HERE]</ref> 3.1.6 Summary</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gaze Data</head><p>The analysis of the gaze proportion revealed a significant increase in gazes at the target object in the presence of a referential competitor in privileged ground (pair condition).</p><p>Competitors, on the other hand, received significantly less gazes than all the other non-target objects in the visual display. The numeric increase in gazes at the goal compartment for the pair condition did not reach significance. We assume that the absence of gazes to the competitor objects can be explained via attentional mechanisms: As we discussed in the introduction, the target objects are the most informative object with regard to solving the experimental task. First of all, they fit the meaning of the linguistic utterance (e.g., being a vase), second, they provide all information necessary for resolving the referential ambiguity (hidden or not), and, most importantly, they are the target object that is supposed to be moved. Given that the hidden competitors remained in place for an entire experimental block (10-15 minutes), while the target objects changed position every few trials, participants probably acquired some knowledge about where the target objects are not located, thus paying little attention to the hidden objects. However, why the target objects in the pair condition received more gazes than the target objects in the singleton condition is hard to explain without additional data. Fortunately, the fixation-related potentials can provide clarification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fixation-related potentials</head><p>In the present study, the amplitude of the fixation-related potentials time-locked to target fixations was not directly influenced by condition in the time windows of interest (300-500, 500-800 ms). Both, targets in the singleton and pair condition elicited very similar ERPs with only minor differences. Still, we have reason to assume that participants treated the "pair targets" different, as compared to the "singleton targets", since differences in the fixation related potentials become apparent, when we consider the time in a trial at which a fixation occurred: when more linguistic evidence was available that the object currently gazed at is the actual target object (fixations during or after the noun), fixations to targets in the pair condition elicited an early occipital positivity (200-300 ms), followed by a sustained positivity with a similar gradient response. This early positivity was absent in singleton trials, which only showed a sustained positivity at occipital channels. Both effects, the early positivity for pair trials and the sustained positivity for both conditions are mirrored over central, frontal and prefrontal electrodes. We assume, that the amount of available linguistic information is responsible for the graded effects of fixation time. Fixations that occurred during (partial information available) or after the noun (all information available) elicited more positive going fixation-related potentials over posterior and occipital channels, and vice versa over frontal and prefrontal channels, because the visual information provided by the target object matches the linguistic information provided by the word. The fact that the effect is stronger in the pair, along with the pair specific early positivity, strengthens our assumption that targets in the pair attract attention and are, in a certain sense, treated as "targets among targets".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language-related potentials</head><p>The language-related potentials time-locked to noun onset revealed a positivity for the pair condition in the posterior and occipital ROIs. In central, frontal and prefrontal ROIs the amplitude of this effect reversed, while its temporal and spatial distribution remained rather constant. The statistical analysis confirmed that this effect is significant in the N400/P300</p><p>(300-500 ms) and in the later time window (500-800 ms) on which we focused our analysis.</p><p>We assume that this positive deflection is a P300 related to attentional reorientation following the detection of a linguistic target. It is reduced for targets in the singleton condition, since they do not have referential competitors and are thus easier to identify by virtue of our experimental design. Moreover, target words for which the majority of fixations to the target object occurred before or during hearing a target noun, i.e. trials in which participants either seemed to have made rather accurate predictions of the upcoming object (most gazes before noun) or have collected enough (partial) linguistic information to make a prediction (most gazes during noun), show a decreased positivity in both conditions, although this effect is less strong and starts earlier in the singleton condition. Overall, this likely demonstrates the P300 in this case reflects the degree of attentional reorientation that is necessary to solve the experimental task: to identify the target object. When the target object's location is already known (before noun), the P300 is maximally reduced, whereas in trials in which participants still have to actively search for the object after or during hearing the noun, target detection is more difficult and requires more attentional resources, because participants have to initiate a visual search, hence shift from a perceptive to an action-based attentional state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">General Discussion</head><p>We presented results from an interactive, real-world study using the director task paradigm that used mobile EEG and eye tracking equipment to simultaneously record eye tracking and EEG data of participants engaged in the director task. With as little instructions for the directors as needed, we tried to create an experimental situation that is as natural as possible given the experimental task and its repetitive nature. We discussed and demonstrated how the difficulties connected with event-related potentials in real-world EEG studies can be overcome using analysis tools suited for this kind of data, and how multimodal data recordings (EEG, ET) can be reciprocally informative. We showed that the temporal distribution and availability of visual (gaze) and speech information have an effect on both fixation-related and language-related event-related potentials. Together with the behavioral gaze data, the multimodal data recording provides us with a relatively complete picture of the state of participants' visual and auditory attention, allowing us to draw conclusions for one modality which are informed by the other. Since in real-world interaction, visual and auditory information are virtually always present simultaneously, our results highlight the importance of analyzing them together when dealing with data acquired from real-world, interactive experiments. In the following, we discuss our results against the literature and provide some suggestions of how to explore real-world communicative interaction further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Gaze distributions as cognitive offloading</head><p>In our experiment, participants showed an increase in the gaze proportions associated with the target object in trials with a referential competitor in common ground (pair condition).</p><p>This outcome somehow contrasts with our initial prediction that competitors would be attended as informative cues for perspective-taking. Instead, the data suggest that in our simplified design, participants adapted their strategy by prioritizing direct information from targets and disregarding competitors, which provided no additional task-relevant information once their occlusion status was known. The results also differ from the previous ERP studies <ref type="bibr" target="#b90">(Sikos et al., 2019;</ref><ref type="bibr" target="#b80">Richter et al., 2020)</ref>, who both found a reduction in target gazes for referents with a privileged ground competitor. Still, they are in line with other studies demonstrating that the strength of the interference effect of referential competitors depends on a variety of factors, including (linguistic) context (e.g., <ref type="bibr" target="#b38">Heller et al., 2008;</ref><ref type="bibr" target="#b11">Brown-Schmidt et al., 2008)</ref>, task <ref type="bibr" target="#b26">(Ferguson et al., 2015;</ref><ref type="bibr" target="#b84">Ryskin et al., 2020)</ref>, culture <ref type="bibr" target="#b109">(Wu &amp; Keysar, 2007;</ref><ref type="bibr" target="#b108">Wu et al., 2013;</ref><ref type="bibr" target="#b103">Wang et al., 2019)</ref>, and the availability of cognitive resources <ref type="bibr">(Epley et al., 2004a;</ref><ref type="bibr" target="#b12">Cane et al., 2017)</ref>. Clearly, our very simple experimental design and the fact that competitors remained in their compartment for the entire 10-15 minutes of a block made the referential conflict very explicit. Participants likely learned certain aspects of the placement of objects, so that "ignoring" the referential competitors was possible without increasing the effort of solving the task. Moreover, our use of identical competitors also likely reduced the number of gazes to competitor objects further, since there was no information gain from gazing at the competitors, as compared to gazing at the target objects. As we discussed above at several points, the target objects in the pair condition were the most salient and most informative stimuli in terms of task resolution, since they provide all necessary information for task resolution and they are the target object that is supposed to be moved. This makes gazing at the competitor object (of which participants know where it is) uneconomical. This is different from <ref type="bibr" target="#b80">Richter et al. (2020)</ref> or <ref type="bibr" target="#b90">Sikos et al. (2019)</ref>, where participants had to compare sizes or accessories of referential candidates in ever-changing, randomized visual displays to reach a decision. We argue that this is a behavior called cognitive offloading <ref type="bibr" target="#b82">(Risko &amp; Gilbert, 2016)</ref>: participants gaze at the stimulus display to gather the information that is necessary for task performance, since the alternative, memorizing the array, is cognitively very demanding. While this likely happened in <ref type="bibr" target="#b80">Richter et al. (2020)</ref> and <ref type="bibr" target="#b90">Sikos et al. (2019)</ref>, for the reasons outline above, this behavior was not necessary and even infelicitous in the present study, since learning certain aspects of the visual display was connected with less effort in the present study.</p><p>Further evidence for our claim that targets in the pair condition require more attentional resources in terms of reorientation comes from the effects in the fixation-related potentials.</p><p>Previous research on fixation-related potentials has identified a P300 effect for fixationrelated potentials, that distinguishes between fixations on singleton target stimuli within a display of distractors and fixations on the distractors <ref type="bibr" target="#b10">(Brouwer et al., 2013;</ref><ref type="bibr" target="#b44">Kamienkowski et al., 2012)</ref>. This receives strong evidence also from intracranial recordings, showing that the brain dissociates between target and distractor stimuli at the single-neuron level <ref type="bibr" target="#b106">(Wang et al., 2018)</ref>. The present fixation-related potentials show very early positive responses following target fixations in the pair condition, given that (partial) linguistic information has already been provided. That is, fixations that occurred during or after the target noun has been uttered, elicit a stronger early positivity effect in the pair condition, but not in the singleton condition. This provides indirect evidence that, compared to targets in the singleton condition, participants seemed to attribute greater attentional resources to the targets in the pair condition. This finding in the fixation-related potentials helps us in the interpretation of the gaze behavior and the language related event-related potentials, as we can make assumptions about participants' attentional states that are informed by data acquired from the participants themselves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The N400-P300 complex and multimodal integration</head><p>In the language-related potentials, we found a P300 following nouns, with higher amplitude in the pair condition. It is thus in line with the ERPs in <ref type="bibr" target="#b80">Richter et al. (2020)</ref> and <ref type="bibr" target="#b90">Sikos et al. (2019)</ref>, who also found an increased positivity for referents with common ground competitors. However, given that we found the reversed effect in the gaze proportions for targets, as compared to their study, this is surprising. The finding therefore suggests that P300 amplitude is not straightforwardly tied to the number of target fixations, but rather reflects the detection of referential conflicts that require attentional reorientation. Given the nature of the P300 as a decision related brain potential that seems to be at the intersection of cognition and behavior <ref type="bibr">(Aston-Jones &amp; Cohen, 2005;</ref><ref type="bibr" target="#b4">Aston-Jones &amp; Waterhouse, 2016;</ref><ref type="bibr" target="#b13">Chennu et al., 2009;</ref><ref type="bibr" target="#b63">Makeig et al., 2004;</ref><ref type="bibr" target="#b71">Nieuwenhuis et al., 2005;</ref><ref type="bibr" target="#b98">Twomey et al., 2015;</ref><ref type="bibr" target="#b101">Verleger et al., 2005)</ref>, the reduction in amplitude for the singleton targets becomes readily explainable: the target object can be understood similar to the repeated presentation of a stimulus in a controlled experiment (e.g. a standard): the singleton targets can be understood as the unsurprising, predicted and easily identifiable "standard" and the pair as the surprising, harder-to-detect deviant, requiring attentional reorientation and more effort (i.e. more gazes)</p><p>to be identified and selected correctly. As a consequence, they elicit the stronger P300</p><p>response. This interpretation is further strengthened by the effect of mean target fixation time for both conditions: the later participants fixated the target, i.e. the longer they needed to identify it, the larger the amplitude of the P300 in both conditions. The fact that e.g. <ref type="bibr" target="#b80">Richter et al. (2020)</ref> report fewer gazes at the target in their conflict condition is, from our perspective, explainable by cognitive offloading <ref type="bibr" target="#b82">(Risko and Gilbert, 2016)</ref>: in Richter et al.</p><p>(2020), participants compare sizes of distractors and targets, in order to arrive at a decision.</p><p>By contrast, in our design, participants solve the more difficult task by making more gazes to the actual target, since the distractor is uninformative and its position known. In both cases, participants use their gaze to collect the evidence needed to reach a decision, i.e. they employ the same action with different strategies. That is, what seems as a differences between our studies, is actually none.</p><p>However, at least since <ref type="bibr" target="#b83">Roehm et al. (2007)</ref> we know that the P300 and N400 response overlap during language comprehension. As a consequence, it is difficult to decide, whether a given effect in the P300/N400 time range is a real P300 effect without N400, a real N400 effect without P300, or a mixture of both, an N400 effect and a P300 effect. Although we acknowledge this problem, and although we acknowledge that solving it would be very helpful, yet, given that we cannot entirely solve it as of today, we do not think that this discussion is very fruitful, especially given that what we are gazing at are averaged responses that do not reflect the underlying processes, but rather 'peaky' responses of different latency, which we summed and averaged to a sustained positivity (cf. <ref type="bibr" target="#b1">Aliko et al., 2023</ref> for an interesting analysis and discussion on the topic). However, regardless of this discussion on component overlap, we argue that it is nonetheless interesting that, just as in controlled laboratory studies on multimodal integration, in our interactive, real-world study, we find the N400/P300 time window to be highly sensitive to the integration of visual and auditory cues during language comprehension -for fixation-related and language-related potentials. In both cases, we found significant increases in the N400/P300 range (and the following time window) for the time at which fixations to the target object occurred. The fixation-related potentials during natural reading mentioned in the introduction behave in a very similar way:</p><p>semantically congruent parafoveal preview gradually reduces P300/N400 amplitude of the following fixation on a target word <ref type="bibr" target="#b2">(Antúnez et al., 2022;</ref><ref type="bibr" target="#b19">Dimigen et al., 2012;</ref><ref type="bibr" target="#b60">Li et al., 2024;</ref><ref type="bibr" target="#b70">Niefind &amp; Dimigen, 2016)</ref>. Thus, our results, together with previous results on multimodal integration, which all report effects of multimodal integration in the N400 time window (e.g. <ref type="bibr" target="#b75">Özyürek et al., 2007;</ref><ref type="bibr" target="#b76">Peeters et al., 2015;</ref><ref type="bibr" target="#b107">Willems et al., 2008;</ref><ref type="bibr" target="#b112">Zhang et al., 2021)</ref>, strongly underline the adequacy of <ref type="bibr">Kutas and Federmeier's (2011: 22</ref>) description of the N400 as "[a] region [that] is more accurately described as reflecting the activity in a multimodal long-term memory system that is induced by a given input stimulus during a delimited time window", and highlights the need for a multimodal view on language and the brain, as put forward by <ref type="bibr" target="#b36">(Hasson et al., 2018)</ref>. Clearly, and this becomes obvious from our and previous results, participants use multimodal information for reaching referential decisions, and the integration of this information seems to be reflected in the N400-P300 time window. With the right analytic tools, we think that we can tap into this time window during real-time language comprehension in real-world interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Conclusion, open questions and outlook</head><p>With the current study we demonstrated that the real-time dynamics of language in interaction can be successfully investigated using mobile recording equipment. We demonstrated that even under naturalistic conditions, with freely-moving participants and large amounts of experimental noise, it is possible to calculate event-related potentials from noisy EEG data that are comparable to previous, controlled studies, with effects of multimodal integration taking place in the N400/P300 time window, as has been repeatedly demonstrated in controlled studies. Overall, our results required us to revise several of our original assumptions. For instance, although we predicted that competitors would be attended as informative cues, participants in fact ignored them; although we predicted that pair targets would be easier, the data revealed stronger P300 effects indicating greater attentional demand. These deviations from our predictions are not contradictions but highlight how realworld data can challenge and refine theoretical expectations. We want to stress at this point, that it is not necessary for this purpose and, given the multimodal nature of our stimuli, not really adequate, to offer isolated linguistic explanations for our results. As pointed out by <ref type="bibr" target="#b36">Hasson et al. (2018)</ref>, we showed that it is possible to explain the present results with reference to domain general processes related to attention and memory (learning of distractor positions in shelf, cognitive offloading, attention) and prediction (pre-noun gaze at target objects, semantic prediction), alone. At least for this kind of linguistic input (very simple, reduced utterances with noun phrases and without verbs), it is not necessary to draw back to language specific brain functions that should or should not be reflected in the N400 time window.</p><p>However, having spent considerable time examining the dataset before writing this manuscript, we want to stress that the number of questions the data raise is far higher than the number of answers they provide. Here is a non-exhaustive list of questions we did not discuss in the present manuscript so far, since admittedly, we do not have satisfying answers to them.</p><p>First, it would be very interesting to further subdivide the data into groups based on individual gazing strategies. However, a much higher number of participants would be needed for such an analysis. Second, future research should further examine the role of the director and their interaction with the participant, for instance how joint attention (participants following the gaze of the director) is used by participants to make inferences about the upcoming object. Here, we were only able to track the face of the director in a subset of participants and trials, making it difficult to analyze these gazes. However, we observed that on average participants followed a very clear gazing strategy: gaze at the director's face, gaze at the target object, gaze at the goal (Figure <ref type="figure" target="#fig_15">12</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[FIGURE 12 HERE]</head><p>Taking this information into account would be very helpful for data interpretation, e.g. how long participants gaze at the director's face before noun onset, or whether participants follow the gaze of the director to the target object, while the director is planning their instructions (cf. <ref type="bibr" target="#b21">Egurtzegi et al., 2022;</ref><ref type="bibr" target="#b86">Sauppe, 2016</ref><ref type="bibr" target="#b87">Sauppe, , 2017))</ref>, and whether and to which degree they use this visual cue provided by the director. Related to this, third, where do the language-related ERP effects before noun onset come from? How could there be a difference between conditions when the noun phrases used in the instructions are ambiguous until the noun?</p><p>There is likely some source of information which we have not considered thus far. The gaze direction of the director would, as mentioned above, be a potential candidate, but intonation patterns might also allow the participants to infer the next object prior to noun onset. Fourth, can we predict gaze behavior based on the language related ERPs using naturalistic data?</p><p>This list could be continued endlessly and the data open countless opportunities for further investigations.</p><p>Overall, the present study demonstrates that it is possible to collect EEG and eye tracking data simultaneously in interactive, real-world settings. Our study shows the potential of such real-world interactions with multimodal data recordings: they enable us to study the coordination of multimodal processes, such as gaze control, language comprehension and production, as well as discourse and social phenomena during natural interaction situations.</p><p>Future research must not only work out the details of this interplay, but must also establish new standards in the preprocessing, analysis and interpretation of such multimodal data sets by means of innovation, replication and scientific exchange.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declarations</head><p>Figures            </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><figDesc>comparison of objects for decision making: In Richter et al.'s (2020) study, for instance, participants had to compare sizes of objects to resolve the referential ambiguity, in Sikos et al.'s (2019) study, participants had to compare accessories (e.g. a Brontosaurus with boots</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1</head><figDesc>determiner preceding critical noun: µV~condition+trial 2 critical noun: µV ~ condition*mean_target_gaze_time+trial, 3 preceding, intervening and following words: µV ~ 1 4 fixations within -1.5 to 3.5 seconds around critical nouns: µV ~ condition*fixatedObject*trial_time+spl(saccadicAmplitude)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4</head><label>4</label><figDesc>Figure4shows the overlap corrected, reconstructed ERPs by condition and ROI. These can be interpreted just as traditional grand-averages. The most pronounced patterns include a positivity for the pair condition (~250-500 ms) spanning left and central prefrontal, frontal and central ROIs, followed by a later positivity for the pair condition with posterior maximum peaking at ~500 ms after stimulus onset. The frontal negativity clearly visible for the pair condition at ~500 ms over central-and right-prefrontal ROIs that extends to the frontal ROI over right-hemispheric channels is very similar in timing and morphology as compared to the posterior-occipital positivity and probably reflects a common underlying dipole. Moreover, a negativity for the pair condition is visible between 0 and ~ 400 ms at left-, right-and central-occipital channels. Over right-hemispheric channels, it is also visible</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Experimental setup in the current study.</figDesc><graphic coords="60,72.00,127.50,451.45,389.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Gaze proportions by condition(solid: pair, dashed: singleton)  and AOI (color). The black bar marks significant differences between conditions. Abbreviations: target: the target object; comp: competitor; fillerA/B: singleton, non-target objects; otherComp: the competitor of the nontarget pair; otherTarget: the non-hidden target of the other pair; goal: the goal compartment, i.e. the intended end point of the target object in the shelf.</figDesc><graphic coords="61,77.15,113.40,433.40,394.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3</head><label>3</label><figDesc>Figure 3 Gaze proportions to the hidden competitor (blue) and the mean over all other objects in the shelf (red) in the pair condition. Numerically, the gaze proportions to the hidden competitor are lower at all time points from -3.5 to 3.5 seconds after noun onset. Statistically significant differences are marked by black bars.</figDesc><graphic coords="62,111.35,120.30,392.75,341.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Overlap-corrected, reconstructed ERPs time-locked to noun onset by condition and region-of-interest. The dotted line marks the median noun duration.</figDesc><graphic coords="63,94.55,99.90,428.90,346.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Overlap-corrected, reconstructed ERPs ime-locked to noun onset by condition, saggital regions-of-interest and mean target fixation time. The dotted line marks the median noun duration.</figDesc><graphic coords="64,90.05,99.90,433.40,457.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Fixation-related potentials from -200 to 1000 ms after fixation onset by condition and region-of-interest.</figDesc><graphic coords="65,86.45,119.40,424.35,341.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Fixation-related potentials of fixations to targets from -200 to 1000 ms after fixation by condition and fixation time at selected channels.</figDesc><graphic coords="66,84.05,111.90,428.90,390.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>Figure8: Significant terms as revealed by the permutation analysis for the language-related event-related potentials.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Effects of mean target fixation time in 100 ms time bins from 0-800 ms after noun onset for the pair condition. Error bars represent 83 % confidence intervals, corresponding roughly to 0.05 significance threshold. The continuous predictor was grouped for plotting purposes.</figDesc><graphic coords="68,90.05,99.90,433.40,350.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Effects of mean target fixation time in 100 ms time bins from 0-800 ms after noun onset for the singleton condition. Error bars represent 83 % confidence intervals, corresponding roughly to 0.05 significance threshold. The continuous predictor was grouped for plotting purposes.</figDesc><graphic coords="69,72.00,105.90,451.45,355.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Significant terms as revealed by the permutation analysis for the fixation-related potentials.</figDesc><graphic coords="70,76.80,115.65,442.40,214.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Effects of target fixation time in 100 ms time bins from 0-800 ms after fixation onset for the pair condition. Error bars represent 83 % confidence intervals, corresponding roughly to 0.05 significance threshold. The continuous predictor was grouped for plotting purposes.</figDesc><graphic coords="71,87.25,105.90,451.45,355.55" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>The present study2.1 The TaskIn the present study, we designed a simplified real-world version of the director task<ref type="bibr" target="#b46">(Keysar et al., 2000)</ref> to demonstrate the validity of our approach. In this task, two participants are</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We want to thank <rs type="person">Claudia Kilter</rs>, <rs type="person">Brita Rietdorf</rs>, <rs type="person">Leonie Latza</rs>, <rs type="person">Lisa Lubomierski</rs> and <rs type="person">Robert Voigt</rs> for their help in recruiting participants, data collection, data annotation and analysis.</p></div>
			</div>
			<div type="funding">
<div><head>Funding</head><p>The research presented in this manuscript has been funded by the <rs type="funder">VolkswagenStiftung (Volkswagen Foundation)</rs> in the <rs type="projectName">Momentum</rs> project "<rs type="projectName">Communication electrified -towards a natural investigation of real-time language processing</rs>" of the third author.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_3eABEDU">
					<orgName type="project" subtype="full">Momentum</orgName>
				</org>
				<org type="funded-project" xml:id="_7wUmHJt">
					<orgName type="project" subtype="full">Communication electrified -towards a natural investigation of real-time language processing</orgName>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p><ref type="url" target="https://github.com/XlinCLab/DGAME">https://github.com/XlinCLab/DGAME</ref>).</p></div>
			</div>


			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Availability of data and materials</head><p>Due to data protection rules, the data cannot be made publicly available, but documented code for the analyses in this article is available at OSF (<ref type="url" target="https://osf.io/5ds4z">https://osf.io/5ds4z</ref>) together with two complete example data sets. Upon personal request, all data sets can be made available to other researchers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Code availability</head><p>All code used for the preprocessing, analysis and plotting of the present data is publicly available at OSF: <ref type="url" target="https://osf.io/5ds4z">https://osf.io/5ds4z</ref> (there is also a modified and optimized Python implementation available on github: <ref type="url" target="https://github.com/XlinCLab/DGAME">https://github.com/XlinCLab/DGAME</ref>).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflicts of interest/Competing interests</head><p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics approval</head><p>The study involved human participants. It was reviewed and approved by the Ethikkommission der Deutschen Gesellschaft für Sprachwissenschaft (Ethics Committee of the German Linguistic Society).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consent to participate</head><p>The participants provided their written informed consent to participate in this study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consent for publication</head><p>The participants provided their written informed consent to their data being used in a publication. All participants were given the possibility to chose what kind of data (EEG, ET, video, audio) they allow to be published for what kind of purpose (e.g. as part of group results in a journal, individual video/audio/EEG/ET in a journal, at a conference, with the scientific community etc.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Authors' contributions</head><p>Ingmar Brilmayer: design of the experiment, code design and writing code, data collection, data analysis, writing; Philip Georgis: coding, writing; Petra B.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Schumacher: writing</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">How much baseline correction do we need in ERP research? Extended GLM model can replace baseline correction while lifting its limits</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Alday</surname></persName>
		</author>
		<idno type="DOI">10.1111/psyp.13451</idno>
		<ptr target="https://doi.org/10.1111/psyp.13451" />
	</analytic>
	<monogr>
		<title level="j">Psychophysiology</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The entire brain, more or less, is at work: &apos;Language regions&apos; are artefacts of averaging</title>
		<author>
			<persName><forename type="first">S</forename><surname>Aliko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Small</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">I</forename><surname>Skipper</surname></persName>
		</author>
		<idno type="DOI">10.1101/2023.09.01.555886</idno>
		<ptr target="https://doi.org/10.1101/2023.09.01.555886" />
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
			<biblScope unit="page">555886</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic parafoveal processing in natural reading: Insight from fixation-related potentials &amp; eye movements</title>
		<author>
			<persName><forename type="first">M</forename><surname>Antúnez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Milligan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Hernández-Cabrera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Barber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Schotter</surname></persName>
		</author>
		<idno type="DOI">10.1111/psyp.13986</idno>
		<ptr target="https://doi.org/10.1111/psyp.13986" />
	</analytic>
	<monogr>
		<title level="j">Psychophysiology</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">13986</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adaptive gain and the role of the locus coeruleusnorepinephrine system in optimal performance</title>
		<author>
			<persName><forename type="first">G</forename><surname>Aston-Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="DOI">10.1002/cne.20723</idno>
		<ptr target="https://doi.org/10.1002/cne.20723" />
	</analytic>
	<monogr>
		<title level="j">Journal of Comparative Neurology</title>
		<imprint>
			<biblScope unit="volume">493</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="99" to="110" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Locus coeruleus: From global projection system to adaptive regulation of behavior</title>
		<author>
			<persName><forename type="first">G</forename><surname>Aston-Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Waterhouse</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.brainres.2016.03.001</idno>
		<ptr target="https://doi.org/10.1016/j.brainres.2016.03.001" />
	</analytic>
	<monogr>
		<title level="j">Brain Research</title>
		<imprint>
			<biblScope unit="volume">1645</biblScope>
			<biblScope unit="page" from="75" to="78" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pragmatic expectations and linguistic evidence: Listeners anticipate but do not integrate common ground</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Barr</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cognition.2008.07.005</idno>
		<ptr target="https://doi.org/10.1016/j.cognition.2008.07.005" />
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>Article 1</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Visual world studies of conversational perspective taking: Similar findings, diverging interpretations</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Barr</surname></persName>
		</author>
		<idno type="DOI">10.1075/aicr.93.10bar/html</idno>
		<ptr target="https://www.degruyterbrill.com/document/doi/10.1075/aicr.93.10bar/html" />
	</analytic>
	<monogr>
		<title level="m">Visually Situated Language Comprehension</title>
		<imprint>
			<publisher>John Benjamins Publishing Company</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="261" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multimodal processing in face-to-face interactions: A bridging link between psycholinguistics and sensory neuroscience</title>
		<author>
			<persName><forename type="first">S</forename><surname>Benetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pavani</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnhum.2023.1108354</idno>
		<ptr target="https://doi.org/10.3389/fnhum.2023.1108354" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Human Neuroscience</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hand gestures as visual prosody: BOLD responses to audio-visual alignment are modulated by the communicative nature of the stimuli</title>
		<author>
			<persName><forename type="first">E</forename><surname>Biau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Morís Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Holle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Avila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soto-Faraco</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuroimage.2016.02.018</idno>
		<ptr target="https://doi.org/10.1016/j.neuroimage.2016.02.018" />
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="page" from="129" to="137" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Toward a Neurobiologically Plausible Model of Language-Related, Negative Event-Related Potentials</title>
		<author>
			<persName><forename type="first">I</forename><surname>Bornkessel-Schlesewsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schlesewsky</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2019.00298</idno>
		<ptr target="https://doi.org/10.3389/fpsyg.2019.00298" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distinguishing between target and nontarget fixations in a visual search task using fixation-related potentials</title>
		<author>
			<persName><forename type="first">A.-M</forename><surname>Brouwer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Reuderink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A J</forename><surname>Van Gerven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B F</forename><surname>Van Erp</surname></persName>
		</author>
		<idno type="DOI">10.1167/13.3.17</idno>
		<ptr target="https://doi.org/10.1167/13.3.17" />
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Addressees distinguish shared from private information when interpreting questions during interactive conversation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Brown-Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gunlogson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Tanenhaus</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cognition.2007.11.005</idno>
		<ptr target="https://doi.org/10.1016/j.cognition.2007.11.005" />
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1122" to="1134" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Using perspective to resolve reference: The impact of cognitive load and motivation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Cane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Ferguson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Apperly</surname></persName>
		</author>
		<idno type="DOI">10.1037/xlm0000345</idno>
		<ptr target="https://doi.org/10.1037/xlm0000345" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attention Increases the Temporal Precision of Conscious Perception: Verifying the Neural-ST2 Model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chennu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Craston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wyble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pcbi.1000576</idno>
		<ptr target="https://doi.org/10.1371/journal.pcbi.1000576" />
	</analytic>
	<monogr>
		<title level="j">PLOS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">1000576</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gestural Viewpoint Signals Referent Accessibility</title>
		<author>
			<persName><forename type="first">S</forename><surname>Debreslioska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Özyürek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gullberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perniss</surname></persName>
		</author>
		<idno type="DOI">10.1080/0163853X.2013.824286</idno>
		<ptr target="https://doi.org/10.1080/0163853X.2013.824286" />
	</analytic>
	<monogr>
		<title level="j">Discourse Processes</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="431" to="456" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Eye Movements and Fixation-Related Potentials in Reading: A Review</title>
		<author>
			<persName><forename type="first">F</forename><surname>Degno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Liversedge</surname></persName>
		</author>
		<idno type="DOI">10.3390/vision4010011</idno>
		<ptr target="https://doi.org/10.3390/vision4010011" />
	</analytic>
	<monogr>
		<title level="j">Vision</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">EEGLAB: An open source toolbox for analysis of singletrial EEG dynamics including independent component analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Delorme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Makeig</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jneumeth.2003.10.009</idno>
		<ptr target="https://doi.org/10.1016/j.jneumeth.2003.10.009" />
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience Methods</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="21" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Crowdsourcing neuroscience: Inter-brain coupling during faceto-face interactions outside the laboratory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dikker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Michalareas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Oostrik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Serafimaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Kahraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Struiksma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Poeppel</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuroimage.2020.117436</idno>
		<ptr target="https://doi.org/10.1016/j.neuroimage.2020.117436" />
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">227</biblScope>
			<biblScope unit="page">117436</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Regression-based analysis of combined EEG and eyetracking data: Theory and applications</title>
		<author>
			<persName><forename type="first">O</forename><surname>Dimigen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">V</forename><surname>Ehinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="3" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Trans-saccadic parafoveal preview benefits in fluent reading: A study with fixation-related brain potentials</title>
		<author>
			<persName><forename type="first">O</forename><surname>Dimigen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kliegl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sommer</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuroimage.2012.04.006</idno>
		<ptr target="https://doi.org/10.1016/j.neuroimage.2012.04.006" />
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="381" to="393" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Face-to-face spatial orientation fine-tunes the brain for neurocognitive processing in conversation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Drijvers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Holler</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.isci.2022.105413</idno>
		<ptr target="https://doi.org/10.1016/j.isci.2022.105413" />
	</analytic>
	<monogr>
		<title level="j">iScience</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cross-linguistic differences in case marking shape neural power dynamics and gaze behavior during sentence planning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Egurtzegi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Blasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Bornkessel-Schlesewsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sauppe</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.bandl.2022.105127</idno>
		<ptr target="https://doi.org/10.1016/j.bandl.2022.105127" />
	</analytic>
	<monogr>
		<title level="j">Brain and Language</title>
		<imprint>
			<biblScope unit="volume">230</biblScope>
			<biblScope unit="page">105127</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unfold: An integrated toolbox for overlap correction, non-linear modeling, and regression-based EEG analysis</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">V</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Dimigen</surname></persName>
		</author>
		<idno type="DOI">10.7717/peerj.7838</idno>
		<ptr target="https://doi.org/10.7717/peerj.7838" />
	</analytic>
	<monogr>
		<title level="j">PeerJ</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">7838</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Microsaccades uncover the orientation of covert attention</title>
		<author>
			<persName><forename type="first">R</forename><surname>Engbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kliegl</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0042-6989(03)00084-1</idno>
		<ptr target="https://doi.org/10.1016/S0042-6989(03)00084-1" />
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1035" to="1045" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Perspective Taking as Egocentric Anchoring and Adjustment</title>
		<author>
			<persName><forename type="first">N</forename><surname>Epley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Keysar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Boven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gilovich</surname></persName>
		</author>
		<idno type="DOI">10.1037/0022-3514.87.3.327</idno>
		<ptr target="https://doi.org/10.1037/0022-3514.87.3.327" />
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Perspective taking in children and adults: Equivalent egocentrism but differential correction</title>
		<author>
			<persName><forename type="first">N</forename><surname>Epley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Morewedge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Keysar</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jesp.2004.02.002</idno>
		<ptr target="https://doi.org/10.1016/j.jesp.2004.02.002" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Social Psychology</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="760" to="768" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Task constraints distinguish perspective inferences from perspective use during discourse interpretation in a false belief task</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Ferguson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Apperly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bindemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cane</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cognition.2015.02.010</idno>
		<ptr target="https://doi.org/10.1016/j.cognition.2015.02.010" />
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="50" to="70" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Listeners&apos; eyes reveal spontaneous sensitivity to others&apos; perspectives</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Ferguson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Breheny</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jesp.2011.08.007</idno>
		<ptr target="https://doi.org/10.1016/j.jesp.2011.08.007" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Social Psychology</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ferguson</surname></persName>
		</author>
		<ptr target="http://www.eyetracking-r.com/" />
		<title level="m">eyetrackingR</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pointing as Situated Practice</title>
		<author>
			<persName><forename type="first">C</forename><surname>Goodwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pointing</title>
		<imprint>
			<publisher>Psychology Press</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Removal of movementinduced EEG artifacts: Current state of the art and guidelines</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gorjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gramann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>De Pauw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Marusic</surname></persName>
		</author>
		<idno type="DOI">10.1088/1741-2552/ac542c</idno>
		<ptr target="https://doi.org/10.1088/1741-2552/ac542c" />
	</analytic>
	<monogr>
		<title level="j">Journal of Neural Engineering</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">11004</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A cross-cultural study of the communication of extraverbal meaning by gestures</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Argyle</surname></persName>
		</author>
		<idno type="DOI">10.1080/00207597508247319</idno>
		<ptr target="https://doi.org/10.1080/00207597508247319" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Psychology</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="67" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mobile EEG for neurourbanism research -What could possibly go wrong? A critical review with guidelines</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gramann</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jenvp.2024.102308</idno>
		<ptr target="https://doi.org/10.1016/j.jenvp.2024.102308" />
	</analytic>
	<monogr>
		<title level="j">Journal of Environmental Psychology</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page">102308</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A Roadmap for Technological Innovation in Multimodal Communication Research</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gregori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Amici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Brilmayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ćwiek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fritzsche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Henlein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Herbort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kügler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lemanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liebal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lücking</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pouw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prieto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Rohrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Sánchez-Ramón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schulte-Rüther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Von Eiff</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-35748-0_30</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-35748-0_30" />
	</analytic>
	<monogr>
		<title level="m">Digital Human Modeling and Applications in Health, Safety, Ergonomics and Risk Management</title>
		<editor>
			<persName><forename type="first">V</forename><forename type="middle">G</forename><surname>Duffy</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer Nature Switzerland</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">14029</biblScope>
			<biblScope unit="page" from="402" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Speakers&apos; eye gaze disambiguates referring expressions early during face-to-face conversation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Brennan</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jml.2007.01.008</idno>
		<ptr target="https://doi.org/10.1016/j.jml.2007.01.008" />
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="596" to="615" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The effects of common ground and perspective on domains of referential interpretation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Tanenhaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Trueswell</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0749-596X(03)00022-6</idno>
		<ptr target="https://doi.org/10.1016/S0749-596X(03)00022-6" />
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Grounding the neurobiology of language in first principles: The necessity of non-language-centric explanations for language comprehension</title>
		<author>
			<persName><forename type="first">U</forename><surname>Hasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Egidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Willems</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cognition.2018.06.018</idno>
		<ptr target="https://doi.org/10.1016/j.cognition.2018.06.018" />
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">180</biblScope>
			<biblScope unit="page" from="135" to="157" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The Multiple Perspectives Theory of Mental States in Communication</title>
		<author>
			<persName><forename type="first">D</forename><surname>Heller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brown-Schmidt</surname></persName>
		</author>
		<idno type="DOI">10.1111/cogs.13322</idno>
		<ptr target="https://doi.org/10.1111/cogs.13322" />
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The role of perspective in identifying domains of reference</title>
		<author>
			<persName><forename type="first">D</forename><surname>Heller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grodner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Tanenhaus</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cognition.2008.04.008</idno>
		<ptr target="https://doi.org/10.1016/j.cognition.2008.04.008" />
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Meaning-based guidance of attention in scenes as revealed by meaning maps</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Hayes</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41562-017-0208-0</idno>
		<ptr target="https://doi.org/10.1038/s41562-017-0208-0" />
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="743" to="747" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Effects of language comprehension on visual processing -MEG dissociates early perceptual and late N400 effects</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hirschfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zwitserlood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dobel</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.bandl.2010.07.002</idno>
		<ptr target="https://doi.org/10.1016/j.bandl.2010.07.002" />
	</analytic>
	<monogr>
		<title level="j">Brain and Language</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="96" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multimodal Language Processing in Human Communication</title>
		<author>
			<persName><forename type="first">J</forename><surname>Holler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Levinson</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2019.05.006</idno>
		<ptr target="https://doi.org/10.1016/j.tics.2019.05.006" />
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="639" to="652" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Giving speech a hand: Gesture modulates activity in auditory cortex during speech perception</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dapretto</surname></persName>
		</author>
		<idno type="DOI">10.1002/hbm.20565</idno>
		<ptr target="https://doi.org/10.1002/hbm.20565" />
	</analytic>
	<monogr>
		<title level="j">Human Brain Mapping</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1028" to="1037" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Combining EEG and 3D-eyetracking to study the prediction of upcoming speech in naturalistic virtual environments: A proof of principle</title>
		<author>
			<persName><forename type="first">E</forename><surname>Huizeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Alday</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Peeters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hagoort</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuropsychologia.2023.108730</idno>
		<ptr target="https://doi.org/10.1016/j.neuropsychologia.2023.108730" />
	</analytic>
	<monogr>
		<title level="j">Neuropsychologia</title>
		<imprint>
			<biblScope unit="volume">191</biblScope>
			<biblScope unit="page">108730</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Fixation-related potentials in visual search: A combined EEG and eye tracking study</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Kamienkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Ison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Q</forename><surname>Quiroga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sigman</surname></persName>
		</author>
		<idno type="DOI">10.1167/12.7</idno>
		<ptr target="https://doi.org/10.1167/12.7" />
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Auditory cortical delta-entrainment interacts with oscillatory power in multiple fronto-parietal networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Keitel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A A</forename><surname>Ince</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kayser</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuroimage.2016.11.062</idno>
		<ptr target="https://doi.org/10.1016/j.neuroimage.2016.11.062" />
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">147</biblScope>
			<biblScope unit="page" from="32" to="42" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Taking Perspective in Conversation: The Role of Mutual Knowledge in Comprehension</title>
		<author>
			<persName><forename type="first">B</forename><surname>Keysar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Balin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Brauner</surname></persName>
		</author>
		<idno type="DOI">10.1111/1467-9280.00211</idno>
		<ptr target="https://doi.org/10.1111/1467-9280.00211" />
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Multilingual processing of speech via web services</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kisler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Reichel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schiel</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.csl.2017.01.005</idno>
		<ptr target="https://doi.org/10.1016/j.csl.2017.01.005" />
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="326" to="347" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">What does cross-linguistic variation in semantic coordination of speech and gesture reveal?: Evidence for an interface representation of spatial thinking and speaking</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Özyürek</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0749-596X(02)00505-3</idno>
		<ptr target="https://doi.org/10.1016/S0749-596X(02)00505-3" />
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="16" to="32" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Length, frequency, and predictability effects of words on eye movements in reading</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kliegl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellen</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName><surname>Rolfs</surname></persName>
		</author>
		<author>
			<persName><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Engbert</surname></persName>
		</author>
		<idno type="DOI">10.1080/09541440340000213</idno>
		<ptr target="https://doi.org/10.1080/09541440340000213" />
	</analytic>
	<monogr>
		<title level="j">European Journal of Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="262" to="284" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Tracking the mind during reading: The influence of past, present, and future words on fixation durations</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kliegl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nuthmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Engbert</surname></persName>
		</author>
		<idno type="DOI">10.1037/0096-3445.135.1.12</idno>
		<ptr target="https://doi.org/10.1037/0096-3445.135.1.12" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">135</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="12" to="35" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Comprehending how visual context influences incremental sentence processing: Insights from ERPs and picture-sentence verification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Knoeferle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Urbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kutas</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1469-8986.2010.01080.x</idno>
		<ptr target="https://doi.org/10.1111/j.1469-8986.2010.01080.x" />
	</analytic>
	<monogr>
		<title level="j">Psychophysiology</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="495" to="506" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Neural Entrainment Determines the Words We Hear</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kösem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Bosker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Takashima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hagoort</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cub.2018.07.023</idno>
		<ptr target="https://doi.org/10.1016/j.cub.2018.07.023" />
	</analytic>
	<monogr>
		<title level="j">Current Biology</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Artifact removal techniques with signal reconstruction (United States Patent No</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A E</forename><surname>Kothe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-P</forename><surname>Jung</surname></persName>
		</author>
		<idno>US20160113587</idno>
		<ptr target="https://patents.google.com/patent/US20160113587A1/en" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Kothe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Miyakoshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Delorme</surname></persName>
		</author>
		<ptr target="https://github.com/sccn/clean_rawdata" />
		<title level="m">Clean_rawdata (Version 2.7)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Computer software</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">The Lab Streaming Layer for Synchronized Multimodal Recording</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kothe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Shirazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Stenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Medine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Boulay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Grivich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mullen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Delorme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Makeig</surname></persName>
		</author>
		<idno type="DOI">10.1101/2024.02.13.580071</idno>
		<idno>02.13.580071</idno>
		<ptr target="https://doi.org/10.1101/2024.02.13.580071" />
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Flexible Layouts for Fiducial Tags</title>
		<author>
			<persName><forename type="first">M</forename><surname>Krogius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Haggenmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Olson</surname></persName>
		</author>
		<idno type="DOI">10.1109/IROS40897.2019.8967787</idno>
		<ptr target="https://doi.org/10.1109/IROS40897.2019.8967787" />
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="1898" to="1903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Contributions of reader-and textlevel characteristics to eye-movement patterns during passage reading</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kuperman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Matsuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Van Dyke</surname></persName>
		</author>
		<idno type="DOI">10.1037/xlm0000547</idno>
		<ptr target="https://doi.org/10.1037/xlm0000547" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology. Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1687" to="1713" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Thirty Years and Counting: Finding Meaning in the N400 Component of the Event-Related Brain Potential (ERP)</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kutas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Federmeier</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev.psych.093008.131123</idno>
		<ptr target="https://doi.org/10.1146/annurev.psych.093008.131123" />
	</analytic>
	<monogr>
		<title level="j">Annual Review of Psychology</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="621" to="647" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The origin of human multi-modal communication</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Levinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Holler</surname></persName>
		</author>
		<idno type="DOI">10.1098/rstb.2013.0302</idno>
		<ptr target="https://royalsocietypublishing.org/doi/full/10.1098/rstb.2013.0302" />
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society B: Biological Sciences</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Parafoveal and foveal N400 effects in natural reading: A timeline of semantic processing from fixationrelated potentials</title>
		<author>
			<persName><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kornrumpf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sommer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Dimigen</surname></persName>
		</author>
		<idno type="DOI">10.1111/psyp.14524</idno>
		<ptr target="https://doi.org/10.1111/psyp.14524" />
	</analytic>
	<monogr>
		<title level="j">Psychophysiology</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">14524</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">The influence of matching degrees of synchronous auditory and visual information in videos of real-world events on cognitive integration: An event-related potential study</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuroscience.2011.08.009</idno>
		<ptr target="https://doi.org/10.1016/j.neuroscience.2011.08.009" />
	</analytic>
	<monogr>
		<title level="j">Neuroscience</title>
		<imprint>
			<biblScope unit="volume">194</biblScope>
			<biblScope unit="page" from="19" to="26" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Delta and theta neural entrainment during phonological and semantic processing in speech perception</title>
		<author>
			<persName><forename type="first">G</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Y</forename></persName>
		</author>
		<idno type="DOI">10.1101/556837</idno>
		<ptr target="https://doi.org/10.1101/556837" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">556837</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Electroencephalographic Brain Dynamics Following Manually Responded Visual Targets</title>
		<author>
			<persName><forename type="first">S</forename><surname>Makeig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Delorme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Westerfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-P</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Townsend</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Courchesne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pbio.0020176</idno>
		<ptr target="https://doi.org/10.1371/journal.pbio.0020176" />
	</analytic>
	<monogr>
		<title level="j">PLOS Biology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">176</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Nonparametric statistical testing of EEG-and MEG-data</title>
		<author>
			<persName><forename type="first">E</forename><surname>Maris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Oostenveld</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jneumeth.2007.03.024</idno>
		<ptr target="https://doi.org/10.1016/j.jneumeth.2007.03.024" />
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience Methods</title>
		<imprint>
			<biblScope unit="volume">164</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="177" to="190" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Studying naturalistic human communication using dual-EEG and audio-visual recordings</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mazzini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Holler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Drijvers</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.xpro.2023.102370</idno>
		<ptr target="https://doi.org/10.1016/j.xpro.2023.102370" />
	</analytic>
	<monogr>
		<title level="j">STAR Protocols</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">102370</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Challenges of multimodality: Language and the body in social interaction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mondada</surname></persName>
		</author>
		<idno type="DOI">10.1111/josl.1_12177</idno>
		<ptr target="https://doi.org/10.1111/josl.1_12177" />
	</analytic>
	<monogr>
		<title level="j">Journal of Sociolinguistics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="336" to="366" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Combining Multiple Perspectives in Language Production: A Probabilistic Model</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mozuraitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stevenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Heller</surname></persName>
		</author>
		<ptr target="https://escholarship.org/uc/item/0q95n4qq" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Cognitive Science Society</title>
		<meeting>the Annual Meeting of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">38</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Modeling Reference Production as the Probabilistic Combination of Multiple Perspectives</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mozuraitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stevenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Heller</surname></persName>
		</author>
		<idno type="DOI">10.1111/cogs.12582</idno>
		<ptr target="https://doi.org/10.1111/cogs.12582" />
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Real-time neuroimaging and cognitive monitoring using wearable dry EEG</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Mullen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A E</forename><surname>Kothe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ojeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kerth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Makeig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-P</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cauwenberghs</surname></persName>
		</author>
		<idno type="DOI">10.1109/TBME.2015.2481482</idno>
		<ptr target="https://doi.org/10.1109/TBME.2015.2481482" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2553" to="2567" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Dissociating parafoveal preview benefit and parafoveaon-fovea effects during reading: A combined eye tracking and EEG study</title>
		<author>
			<persName><forename type="first">F</forename><surname>Niefind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Dimigen</surname></persName>
		</author>
		<idno type="DOI">10.1111/psyp.12765</idno>
		<ptr target="https://doi.org/10.1111/psyp.12765" />
	</analytic>
	<monogr>
		<title level="j">Psychophysiology</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1784" to="1798" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Decision making, the P3, and the locus coeruleus-Norepinephrine system</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nieuwenhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Aston-Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="DOI">10.1037/0033-2909.131.4.510</idno>
		<ptr target="https://doi.org/10.1037/0033-2909.131.4.510" />
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">131</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="510" to="532" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">MoBILAB: An open source toolbox for analysis and visualization of mobile brain/body imaging data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ojeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bigdely-Shamlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Makeig</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnhum.2014.00121</idno>
		<ptr target="https://doi.org/10.3389/fnhum.2014.00121" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Human Neuroscience</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">AprilTag: A robust and flexible visual fiducial system</title>
		<author>
			<persName><forename type="first">E</forename><surname>Olson</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICRA.2011.5979561</idno>
		<ptr target="https://doi.org/10.1109/ICRA.2011.5979561" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="3400" to="3407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Considering the Nature of Multimodal Language from a Crosslinguistic Perspective</title>
		<author>
			<persName><forename type="first">A</forename><surname>Özyürek</surname></persName>
		</author>
		<idno type="DOI">10.5334/joc.165</idno>
		<ptr target="https://doi.org/10.5334/joc.165" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
	<note>Article 1</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">On-line Integration of Semantic Information from Speech and Gesture: Insights from Event-related Brain Potentials</title>
		<author>
			<persName><forename type="first">A</forename><surname>Özyürek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Willems</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hagoort</surname></persName>
		</author>
		<idno type="DOI">10.1162/jocn.2007.19.4.605</idno>
		<ptr target="https://doi.org/10.1162/jocn.2007.19.4.605" />
	</analytic>
	<monogr>
		<title level="j">Journal of Cognitive Neuroscience</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Electrophysiological and kinematic correlates of communicative intent in the planning and production of pointing gestures and speech</title>
		<author>
			<persName><forename type="first">D</forename><surname>Peeters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Holler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hagoort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Özyürek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cognitive Neuroscience</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Article 12</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Linking language to the visual world: Neural correlates of comprehending verbal reference to objects through pointing and visual cues</title>
		<author>
			<persName><forename type="first">D</forename><surname>Peeters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Snijders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hagoort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Özyürek</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuropsychologia.2016.12.004</idno>
		<ptr target="https://doi.org/10.1016/j.neuropsychologia.2016.12.004" />
	</analytic>
	<monogr>
		<title level="j">Neuropsychologia</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="21" to="29" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Brain-to-brain entrainment: EEG interbrain synchronization while speaking and listening</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Carreiras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Duñabeitia</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41598-017-04464-4</idno>
		<ptr target="https://doi.org/10.1038/s41598-017-04464-4" />
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Article 1</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Why We Should Study Multimodal Language</title>
		<author>
			<persName><forename type="first">P</forename><surname>Perniss</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2018.01109</idno>
		<ptr target="https://doi.org/10.3389/fpsyg.2018.01109" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Common Ground Information Affects Reference Resolution: Evidence From Behavioral Data, ERPs, and Eye-Tracking</title>
		<author>
			<persName><forename type="first">M</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Höhle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Wartenburger</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2020.565651</idno>
		<ptr target="https://doi.org/10.3389/fpsyg.2020.565651" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">The fixation-related lambda response: Effects of saccade magnitude, spatial frequency, and ocular artifact removal</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Ries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Slayback</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Touryan</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ijpsycho.2018.09.004</idno>
		<ptr target="https://doi.org/10.1016/j.ijpsycho.2018.09.004" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Psychophysiology</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Cognitive Offloading</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>Risko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Gilbert</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2016.07.002</idno>
		<ptr target="https://doi.org/10.1016/j.tics.2016.07.002" />
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">To Predict or Not to Predict: Influences of Task and Strategy on the Processing of Semantic Relations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Roehm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Bornkessel-Schlesewsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rösler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schlesewsky</surname></persName>
		</author>
		<idno type="DOI">10.1162/jocn.2007.19.8.1259</idno>
		<ptr target="https://doi.org/10.1162/jocn.2007.19.8.1259" />
	</analytic>
	<monogr>
		<title level="j">Journal of Cognitive Neuroscience</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Probabilistic weighting of perspectives in dyadic communication</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ryskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stevenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Heller</surname></persName>
		</author>
		<ptr target="https://escholarship.org/uc/item/35m9s3fp" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Cognitive Science Society</title>
		<meeting>the Annual Meeting of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">42</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">How to analyse electrophysiological responses to naturalistic language with time-resolved multiple regression</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sassenhagen</surname></persName>
		</author>
		<idno type="DOI">10.1080/23273798.2018.1502458</idno>
		<ptr target="https://doi.org/10.1080/23273798.2018.1502458" />
	</analytic>
	<monogr>
		<title level="j">Language, Cognition and Neuroscience</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="474" to="490" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Verbal Semantics Drives Early Anticipatory Eye Movements during the Comprehension of Verb-Initial Sentences</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sauppe</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2016.00095</idno>
		<ptr target="https://doi.org/10.3389/fpsyg.2016.00095" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Word Order and Voice Influence the Timing of Verb Planning in German Sentence Production</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sauppe</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2017.01648</idno>
		<ptr target="https://doi.org/10.3389/fpsyg.2017.01648" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Automatic Phonetic Transcription of Non-Prompted Speech</title>
		<author>
			<persName><forename type="first">F</forename><surname>Schiel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ICPhS</title>
		<meeting>of the ICPhS</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="607" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">A Statistical Model for Predicting Pronounciation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Schiel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ICPhS</title>
		<meeting>of the ICPhS</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">195</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">What do you know? ERP evidence for immediate use of common ground during online reference resolution</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sikos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Tomlinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Heins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Grodner</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cognition.2018.10.013</idno>
		<ptr target="https://doi.org/10.1016/j.cognition.2018.10.013" />
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">182</biblScope>
			<biblScope unit="page" from="275" to="285" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Two Neurocognitive Mechanisms of Semantic Integration during the Comprehension of Visual Real-world Events</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sitnikova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Holcomb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Kiyonaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Kuperberg</surname></persName>
		</author>
		<idno type="DOI">10.1162/jocn.2008.20143</idno>
		<ptr target="https://doi.org/10.1162/jocn.2008.20143" />
	</analytic>
	<monogr>
		<title level="j">Journal of Cognitive Neuroscience</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2037" to="2057" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Regression-based estimation of ERP waveforms: I. The rERP framework</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kutas</surname></persName>
		</author>
		<idno type="DOI">10.1111/psyp.12317</idno>
		<ptr target="https://doi.org/10.1111/psyp.12317" />
	</analytic>
	<monogr>
		<title level="j">Psychophysiology</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="168" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Regression-based estimation of ERP waveforms: II. Nonlinear effects, overlap correction, and practical considerations</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kutas</surname></persName>
		</author>
		<idno type="DOI">10.1111/psyp.12320</idno>
		<ptr target="https://doi.org/10.1111/psyp.12320" />
	</analytic>
	<monogr>
		<title level="j">Psychophysiology</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="169" to="181" />
			<date type="published" when="2015">2015b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Using the Hands to Identify Who Does What to Whom: Gesture and Speech Go Hand-in-Hand</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goldin-Meadow</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1551-6709.2008.01006.x</idno>
		<ptr target="https://doi.org/10.1111/j.1551-6709.2008.01006.x" />
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="115" to="125" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">An Annotation Approach for Social and Referential Gaze in Dialogue</title>
		<author>
			<persName><forename type="first">V</forename><surname>Somashekarappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Howes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sayeed</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2020.lrec-1" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth Language Resources and Evaluation Conference</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Calzolari</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Béchet</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Blache</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Choukri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Cieri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Declerck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Goggi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Isahara</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Maegaard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Mariani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Mazo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Moreno</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Odijk</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">&amp;</forename><forename type="middle">S</forename><surname>Piperidis</surname></persName>
		</editor>
		<meeting>the Twelfth Language Resources and Evaluation Conference</meeting>
		<imprint>
			<publisher>European Language Resources Association</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="759" to="765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Graded expectations in visually situated comprehension: Costs and benefits as indexed by the N400</title>
		<author>
			<persName><forename type="first">M</forename><surname>Staudte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ankener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Drenhaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Crocker</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13423-020-01827-3</idno>
		<ptr target="https://doi.org/10.3758/s13423-020-01827-3" />
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="624" to="631" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">The combined use of virtual reality and EEG to study language processing in naturalistic environments</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tromp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Peeters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hagoort</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13428-017-0911-9</idno>
		<ptr target="https://doi.org/10.3758/s13428-017-0911-9" />
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="862" to="869" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">The classic P300 encodes a build-to-threshold decision variable</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Twomey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Connell</surname></persName>
		</author>
		<idno type="DOI">10.1111/ejn.12936</idno>
		<ptr target="https://doi.org/10.1111/ejn.12936" />
	</analytic>
	<monogr>
		<title level="j">European Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1636" to="1643" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Establishing reference in language comprehension: An electrophysiological perspective</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J A</forename><surname>Van Berkum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Koornneef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Otten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Nieuwland</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.brainres.2006.06.091</idno>
		<ptr target="https://doi.org/10.1016/j.brainres.2006.06.091" />
	</analytic>
	<monogr>
		<title level="j">Brain Research</title>
		<imprint>
			<biblScope unit="volume">1146</biblScope>
			<biblScope unit="page" from="158" to="171" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Visual speech speeds up the neural processing of auditory speech</title>
		<author>
			<persName><forename type="first">V</forename><surname>Van Wassenhove</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Poeppel</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.0408949102</idno>
		<ptr target="https://doi.org/10.1073/pnas.0408949102" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1181" to="1186" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Evidence for an Integrative Role of P3b in Linking Reaction to Perception</title>
		<author>
			<persName><forename type="first">R</forename><surname>Verleger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jaśkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wascher</surname></persName>
		</author>
		<idno type="DOI">10.1027/0269-8803.19.3.165</idno>
		<ptr target="https://doi.org/10.1027/0269-8803.19.3.165" />
	</analytic>
	<monogr>
		<title level="j">Journal of Psychophysiology</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="165" to="181" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Language as a multimodal phenomenon: Implications for language learning, processing and evolution</title>
		<author>
			<persName><forename type="first">G</forename><surname>Vigliocco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perniss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vinson</surname></persName>
		</author>
		<idno type="DOI">10.1098/rstb.2013.0292</idno>
		<ptr target="https://doi.org/10.1098/rstb.2013.0292" />
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society B: Biological Sciences</title>
		<imprint>
			<biblScope unit="volume">369</biblScope>
			<biblScope unit="issue">1651</biblScope>
			<biblScope unit="page">20130292</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Perspective-taking across cultures: Shared biases in Taiwanese and British adults</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Frisson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Apperly</surname></persName>
		</author>
		<idno type="DOI">10.1098/rsos.190540</idno>
		<ptr target="https://doi.org/10.1098/rsos.190540" />
	</analytic>
	<monogr>
		<title level="j">Royal Society Open Science</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">AprilTag 2: Efficient and robust fiducial detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Olson</surname></persName>
		</author>
		<idno type="DOI">10.1109/IROS.2016.7759617</idno>
		<ptr target="https://doi.org/10.1109/IROS.2016.7759617" />
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="4193" to="4198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">A scoping review of the use of lab streaming layer framework in virtual and augmented reality research</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Boulay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Barmaki</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10055-023-00799-8</idno>
		<ptr target="https://doi.org/10.1007/s10055-023-00799-8" />
	</analytic>
	<monogr>
		<title level="j">Virtual Reality</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2195" to="2210" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Encoding of Target Detection during Visual Search by Single Neurons in the Human Brain</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Mamelak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Adolphs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Rutishauser</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cub.2018.04.092</idno>
		<ptr target="https://doi.org/10.1016/j.cub.2018.04.092" />
	</analytic>
	<monogr>
		<title level="j">Current Biology</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="2058" to="2069" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Seeing and Hearing Meaning: ERP and fMRI Evidence of Word versus Picture Integration into a Sentence Context</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Willems</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Özyürek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hagoort</surname></persName>
		</author>
		<idno type="DOI">10.1162/jocn.2008.20085</idno>
		<ptr target="https://doi.org/10.1162/jocn.2008.20085" />
	</analytic>
	<monogr>
		<title level="j">Journal of Cognitive Neuroscience</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1235" to="1249" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">How culture influences perspective taking: Differences in correction, not integration</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Gann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Keysar</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnhum.2013.00822</idno>
		<ptr target="https://doi.org/10.3389/fnhum.2013.00822" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Human Neuroscience</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">The Effect of Culture on Perspective Taking</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Keysar</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1467-9280.2007.01946.x</idno>
		<ptr target="https://doi.org/10.1111/j.1467-9280.2007.01946.x" />
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="600" to="606" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Meaningful gestures: Electrophysiological indices of iconic gesture comprehension</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Coulson</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1469-8986.2005.00356.x</idno>
		<ptr target="https://doi.org/10.1111/j.1469-8986.2005.00356.x" />
	</analytic>
	<monogr>
		<title level="j">Psychophysiology</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="654" to="667" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Motor resonance during linguistic processing as shown by EEG in a naturalistic VR environment</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bolger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Pergandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mallet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-S</forename><surname>Dubarry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mestre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Frenck-Mestre</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.bandc.2019.05.003</idno>
		<ptr target="https://doi.org/10.1016/j.bandc.2019.05.003" />
	</analytic>
	<monogr>
		<title level="j">Brain and Cognition</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="page" from="44" to="57" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">More than words: Word predictability, prosody, gesture and mouth movements in natural language comprehension</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Frassinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tuomainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">I</forename><surname>Skipper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Vigliocco</surname></persName>
		</author>
		<idno type="DOI">10.1098/rspb.2021.0500</idno>
		<ptr target="https://doi.org/10.1098/rspb.2021.0500" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the Royal Society B: Biological Sciences</title>
		<imprint>
			<biblScope unit="volume">288</biblScope>
			<biblScope unit="page">20210500</biblScope>
			<date type="published" when="1955">2021. 1955</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
