<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Weakly-supervised Autism Severity Assessment in Long Videos</title>
				<funder ref="#_UUNg65X">
					<orgName type="full">European Union</orgName>
				</funder>
				<funder ref="#_EY6JpfC">
					<orgName type="full">French government</orgName>
				</funder>
				<funder ref="#_Ggjf5ku">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Abid</forename><surname>Ali</surname></persName>
							<email>abid.ali@inria.fr</email>
						</author>
						<author>
							<persName><forename type="first">Mahmoud</forename><surname>Ali</surname></persName>
							<email>mahmoud.ali@inria.fr</email>
						</author>
						<author>
							<persName><forename type="first">Camilla</forename><surname>Barbini</surname></persName>
							<email>camilla.barbini@hpu.lenval.com</email>
						</author>
						<author>
							<persName><forename type="first">Séverine</forename><surname>Dubuisson</surname></persName>
							<email>severine.dubuisson@lis-lab.fr</email>
						</author>
						<author>
							<persName><forename type="first">Jean-Marc</forename><surname>Odobez</surname></persName>
							<email>jean-marc.odobez@idiap.ch</email>
						</author>
						<author>
							<persName><forename type="first">Francois</forename><surname>Bremond</surname></persName>
							<email>francois.bremond@inria.fr</email>
						</author>
						<author>
							<persName><forename type="first">Susanne</forename><forename type="middle">Th</forename><surname>Ümmler</surname></persName>
							<email>susanne.thummler@hpu.lenval.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">STARS Team</orgName>
								<orgName type="institution" key="instit2">INRIA</orgName>
								<address>
									<settlement>Sophia Antipolis Valbonne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">STARS Team</orgName>
								<orgName type="institution" key="instit2">INRIA</orgName>
								<address>
									<settlement>Sophia Antipolis Valbonne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">CoBTek</orgName>
								<orgName type="institution" key="instit2">Université Côte d&apos;Azur Nice</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<address>
									<settlement>Marseille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Idiap Research Institute Martigny</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution" key="instit1">STARS Team</orgName>
								<orgName type="institution" key="instit2">INRIA</orgName>
								<address>
									<settlement>Sophia Antipolis Valbonne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution" key="instit1">CoBTek</orgName>
								<orgName type="institution" key="instit2">Université Côte d&apos;Azur Nice</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Weakly-supervised Autism Severity Assessment in Long Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">190795743FF2377D09C9727DB1FC0275</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-10-22T06:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>autism</term>
					<term>weakly-supervised</term>
					<term>ASD</term>
					<term>computervision</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Autism Spectrum Disorder (ASD) is a diverse collection of neurobiological conditions marked by challenges in social communication and reciprocal interactions, as well as repetitive and stereotypical behaviors. Atypical behavior patterns in a long, untrimmed video can serve as biomarkers for children with ASD. In this paper, we propose a video-based weakly-supervised method that takes spatio-temporal features of long videos to learn typical and atypical behaviors for autism detection. On top of that, we propose a shallow TCN-MLP network, which is designed to further categorize the severity score. We evaluate our method on actual evaluation videos of children with autism collected and annotated (for severity score) by clinical professionals. Experimental results demonstrate the effectiveness of behaviors biomarkers that could help clinicians in autism spectrum analysis.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Autism Spectrum Disorder (ASD) is a diverse collection of neurobiological conditions marked by challenges in social communication and reciprocal interactions, as well as repetitive and stereotypical behaviors. ASD typically manifests in early childhood and significantly impacts the lives of affected children and their families, with no established cure currently available. Although ASD is linked to a variety of factors, including genetics, biology, and environmental influences, the exact causes remain unidentified in many patients <ref type="bibr" target="#b0">[1]</ref>. Additionally, the incidence of ASD is increasing. According to the World Health Organization (WHO), 1 in 100 children has ASD <ref type="bibr" target="#b1">[2]</ref>. This figure is an average derived from multiple studies, which report a wide range of prevalence rates. According to data from the Autism and Developmental Disabilities Monitoring (ADDM) network in 2016, the current prevalence of autism spectrum disorder is one in every 54 children <ref type="bibr" target="#b2">[3]</ref>. Furthermore, the rate of ASD in middle-and low-income countries remains undetermined.</p><p>In a clinical setting, autism is identified through an interactive session where a skilled healthcare professional evaluates specific behavioral characteristics using both verbal and nonverbal tasks. The literature generally agrees that early detection, coupled with ongoing intervention, is crucial to optimize therapeutic outcomes. Therefore, taking advantage of brain neuroplasticity during early childhood, prompt diagnosis of ASD, and suggesting comprehensive behavioral interventions can lead to improved long-term results. Nonetheless, diagnosing ASD remains a complex task. Key factors involve specialized knowledge and specific diagnostic instruments that rely on interpreting child behavior, conducting parent interviews, long-term monitoring and symptom examination, and manual analysis. These assessments are time-consuming and clinically require arduous processes. Moreover, human evaluations can be subjective and vary widely. Effective treatment necessitates prompt diagnosis, yet accurate evaluations are typically not made until age 5, which is considered late for intervention <ref type="bibr" target="#b3">[4]</ref>. There is a need for a more appropriate and accessible initial diagnosis to enhance the accuracy of ASD detection.</p><p>Throughout the years, researchers have proposed several methods for ASD detection <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b9">[10]</ref>. Many of these methods focused mainly on a single module such as either repetitive gesture analysis (skeleton-based or appearance-based) or facial or eye-gaze patterns. However, a single module do not provide detailed insight into autistic behavior traits such as emotion exchanges, social-communication difficulties, atomic stereotypes, unusual or unbalanced movements, etc., which together form a crucial part of the diagnosis <ref type="bibr" target="#b10">[11]</ref> process.</p><p>Recent studies indicate that children with autism often display unique biomarkers of gestures, facial and emotional expressions, and behavioral activities. Utilizing these biomarkers can aid in identifying a distinct distribution of features, thereby enhancing the evaluation of autism.</p><p>Distinctive behavior biomarkers in children with autism may encompass stimming or repetitive movements such as flapping, rocking, specific atomic hand gestures such as playing with hair, mouth and nose, etc., and limited gestures coupled with challenges in interpreting others' gestures. They may also exhibit unusual or unbalanced movements and impaired motor coordination, leading to difficulties in fine motor skills such as grasping and holding objects, and gross motor skills like jumping and balancing.</p><p>Assessing Autism Spectrum Disorder (ASD) by collectively evaluating all the above-mentioned behavioral biomarkers presents a significant challenge. The scarcity of available data in existing literature compounds this difficulty. Current public datasets primarily concentrate on specific aspects such as repetitive movements, as seen in SSBD <ref type="bibr" target="#b11">[12]</ref> and ESSBD <ref type="bibr" target="#b9">[10]</ref>, or on facial expressions and eye-gaze patterns, as in the case of MMBD <ref type="bibr" target="#b12">[13]</ref>. Additionally, certain datasets such as De-Enigma <ref type="bibr" target="#b13">[14]</ref> are not publicly accessible.</p><p>In this paper, we propose a video-based weakly-supervised method that leverages spatio-temporal features of a long video to learn typical and atypical behavior patterns for autism detection. The resulting weakly-supervised network is further exploited to train a shallow regression model in a supervised manner to infer different severity levels according to the Autism Diagnostic Observation Schedule (ADOS) protocol.</p><p>We evaluate our method on actual evaluation videos of children with autism collected and annotated by clinical professionals. Experimental results demonstrate the effectiveness of spatio-temporal behavior patterns in accurately identifying autistic children. This could greatly influence the early detection and treatment of ASD by offering a dependable, non-disruptive, and effective means for autism categorization. Furthermore, the focus on actions simplifies the evaluation of children with restricted verbal communication. To sum-up, the main contributions are as follows:</p><p>• We propose a weakly-supervised network to learn discriminative markers in untrimmed videos related to typical and atypical behaviors. • Our severity score regressor module can automatically regress the autism severity score according to ADOS. • We evaluate our method on real-world autism assessment videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Current studies have investigated diverse methods for autism evaluation, with a significant focus on techniques based on facial expressions, eye gaze patterns, and gestures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Action Detection</head><p>Temporal Action Localization (TAL) is a fundamental task in video understanding. In terms of supervised methods, <ref type="bibr" target="#b14">[15]</ref> proposed a multi-stage architecture for temporal action segmentation. The first stage generates an initial prediction which is refined by the next stages. PDAN <ref type="bibr" target="#b15">[16]</ref> introduces a Dilated Attention Layer (DAL) for allocating attention weights to local frames and constructs a pyramid of DALs with different dilation rates to capture both short-term and longterm temporal relations. In this work, we experiment with PDAN <ref type="bibr" target="#b15">[16]</ref> and MS-TCN <ref type="bibr" target="#b14">[15]</ref> for SOTA comparison on supervised methods. However, such a fully supervised setting suffers from limitations like expensive frame-level labeling and subjective, prone to manual errors.</p><p>On the other hand, Weakly Supervised Temporal Action Localization (WTAL) methods have been developed. WTAL involves classifying and localizing all action instances in untrimmed videos under the supervision of only video-level category labels. <ref type="bibr" target="#b16">[17]</ref> utilizes ViT-encoded visual features from CLIP <ref type="bibr" target="#b17">[18]</ref> to extract discriminative representation and models temporal dependencies using Temporal Self-Attention (TSA). The OE-CTST <ref type="bibr" target="#b18">[19]</ref> enhances the CLIP-TSA <ref type="bibr" target="#b16">[17]</ref> by introducing an anomaly-aware temporal position encoding and a cross-temporal scale transformer. Our idea is borrowed from OE-CTST <ref type="bibr" target="#b18">[19]</ref> for autistic behavioral coding.</p><p>The majority of Temporal Action Localization (TAL) techniques frequently take advantage of large-scale Foundation Models (FMs) to extract high-dimensional features. In this work, we experiment with the DinoV2 <ref type="bibr" target="#b19">[20]</ref> and the VideoMAE-v2 <ref type="bibr" target="#b20">[21]</ref> features to understand atypical ASD behaviors in untrimmed videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Facial and Eye-Gaze Based</head><p>Physical appearance is a distinguishable characteristic of autism. In <ref type="bibr" target="#b21">[22]</ref>, developmental setbacks can be discerned from physical appearances in home-recorded videos. Asymmetry in facial appearance is studied in <ref type="bibr" target="#b12">[13]</ref>. The research indicates that people with a history of ASD often exhibit more asymmetric features. The pattern of eye-gaze is also a significant indicator of autism, as children with ASD tend to exhibit less attention compared to typically developed children <ref type="bibr" target="#b22">[23]</ref>. Their facial expressions and direction of gaze do not interact with their environment. This pattern of reduced eye gaze is consistently observed in all age groups and cultures <ref type="bibr" target="#b23">[24]</ref>. The cumulative stack histogram, as suggested in <ref type="bibr" target="#b24">[25]</ref>, identifies these irregularities in the trajectory of eye movement. AttentionGazeNet <ref type="bibr" target="#b8">[9]</ref> creates a mapping of screen coordinates from 3D gaze vectors. Experimental results suggest that gaze vectors are more scattered in children with ASD.</p><p>However, recognizing ASD from facial and eye gaze analysis is limited to only a few cues of autism, neglecting other atypical behaviors such as uncontrolled or limited body movements, impaired motor coordination and repetitive behaviors, etc. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Gesture-Based</head><p>The study <ref type="bibr" target="#b25">[26]</ref> reveals a notable difference in hand gesture patterns between children with Autism Spectrum Disorder and those who are typically developing. When these children engage in games on a smart tablet, those with ASD tend to apply more force and pressure in their gestures, and also utilize a larger average area. Another study <ref type="bibr" target="#b26">[27]</ref>, proposes that differences in gesture patterns when performing actions may also be apparent from the very beginning, incorporating information about intention. Therefore, the intended gestures can serve as a diagnostic tool for children with ASD. These studies underscore the potential to use motor functions in the analysis of ASD.</p><p>In the study <ref type="bibr" target="#b4">[5]</ref>, features crafted from skeletal data are utilized to categorize children with ASD. The attention-focused ASD screening technique in <ref type="bibr" target="#b6">[7]</ref> leverages various modalities to incorporate complementary multimodal information into a common space.</p><p>Another line of research is centered on identifying atypical actions from videos. The approach known as Bag-of-visualwords <ref type="bibr" target="#b29">[30]</ref> interprets image grids as visual words to identify pertinent feature descriptors. <ref type="bibr" target="#b7">[8]</ref> employed a two-stream architecture to classify repetitive autistic actions. In <ref type="bibr" target="#b27">[28]</ref>, a temporal pyramid network is employed to generate layers of feature maps from long-duration videos. A distinct discriminator for repetitive behavior is utilized to enhance the training process by differentiating samples that exhibit unusual actions.</p><p>Though extensive research has been done on skeleton and appearance-related approaches, these approaches are limited to short gestures of a few seconds such as jumping, flapping, and/or rocking, etc. They mostly use end-to-end deep learning methods and do not incorporate attention to the underlying mechanisms of atypical behaviors in children with ASD. Thus, in this study, we delve into the atypical behavioral patterns present in long videos and assimilate them into the learning process to amplify the representation of discriminative markers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>The architecture we propose is comprised of three distinct stages. In the initial stage, we extract features at the videolevel from each untrimmed video. Subsequently, we employ a weakly-supervised method to classify autistic and typical children. Ultimately, we train a shallow architecture to derive the final severity score for each individual.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Visual Encoder</head><p>The primary goal of the visual encoder is to derive spatiotemporal features from long, untrimmed videos. Initially, the input video V is split into T non-overlapping consecutive temporal segments, each containing a series of 16 successive frames. For each segment, we utilize a VideoMAE-v2 <ref type="bibr" target="#b20">[21]</ref> architecture to generate a feature map of dimension 1 × D. Each segment-level feature can be interpreted as a temporal token, and for a given V with T segments, the visual encoder produces a video feature map of dimension T × D. During the training phase, the visual encoder produces two batches of video feature maps i.e., one from typical and the other from mixed distribution "mixed includes both typical and atypical segments", denoted as F O and F M respectively, which are then processed by OE and CTST modules of the weakly-supervised method <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Weakly-Supervised Autism Detection</head><p>We borrowed OE-CTST <ref type="bibr" target="#b18">[19]</ref>, a WTAL anomaly detection architecture, to learn the atypical and typical behavioral patterns of children with and without ASD. The architecture consists of four components: i) Outlier Embedder (OE), ii) Cross-Temporal Scale Transformer (CTST) and iii) Detector. The weakly-supervised module takes two batches of inputs F O , and F M for binary classification of typical and atypical videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Outlier Embedder OE:</head><p>To create pseudo-temporal position embeddings that are aware of atypical (autistic behaviors in this case) in untrimmed videos, it is crucial to understand the representations at the typical segment level. This way, any temporal segment that significantly deviates from the established typical patterns is identified as an outlier, or an ASD. In such situations, it makes sense to learn the spatio-temporal cues of videos that belong to a one-class (i.e., typical) distribution. The outlier embedder focuses on understanding the temporal patterns rather than visual signals in non-autistic videos.</p><p>2) Cross-Temporal Scale Transformer (CTST): The Cross Temporal Scale Transformer (CTST) aims to learn distinct representations for atypical behaviors of varying lengths in relation to their typical counterparts. Given that short and long atypical behaviors are defined by separate cues (i.e., sharp progressive spatio-temporal cues, respectively), it is advantageous to encode temporal relationships at multiple semantic levels (i.e., temporal scale). The CTST employs a multi-level architecture based on a temporal feature pyramid to accommodate both long-and short-length ASD. The lower levels of the CTST capture the fine-grained, sharp temporal changes associated with short ASD markers, while the higher levels compile the contextual temporal progression of long ASD markers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Detector:</head><p>The detector is a Multi-Layer Perceptron (MLP) consisting of three fully-connected layers. It takes in video feature maps of dimension T × nm and assigns ASD scores to each temporal token. The final layer of the MLP contains a single neuron with a sigmoid activation function, which independently ranks each temporal token. Ultimately, the detector produces a score map S of dimension T × 1, which is utilized for ASD detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Severity Score Regressor</head><p>The proposed shallow architecture is designed to understand both coarse-fine discriminative markers, using ADOS severity score labels as a basis. This shallow architecture consists of two TCN layers followed by three MLP layers. The module accepts inputs from the visual encoder, represented as T × D, and combines them with feature embeddings of size T × 128 from the trained weakly-supervised module to estimate the severity score. Given that ADOS provides a severity score at the video-level for each child, we max-pool the output to compute the final score as shown in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>Weakly-Supervised Architecture Optimization: The suggested structure, which includes an Outlier Embedder (OE) and a Cross Temporal Scale Transformer (CTST) with a detector, can be trained together using two separate batches of input video feature maps. The visual encoder, similar to the ones used in references <ref type="bibr" target="#b28">[29]</ref>, is a pre-trained module that is frozen and is only used for feature extraction. The OE, which only takes the typical video feature maps (F O ) during training, is optimized with a reconstruction loss as indicated in Equation <ref type="formula" target="#formula_0">1</ref>. The CTST with the detector considers both typical and ASD video feature maps F M ∈ R T ×nm to calculate typical (S t ∈ R T ) and ASD (S a ∈ R T ) temporal token-wise scores. It optimizes itself with a self-rectifying loss proposed by <ref type="bibr" target="#b21">[22]</ref>, as shown in Equations 2 and 3.</p><formula xml:id="formula_0">L R (F O ) = ||F O -F R O || 2<label>(1)</label></formula><formula xml:id="formula_1">L D (S a , St) = λ 1 max(0, 1 - T i=1 (S i a ) + T i=1 (S i t ))</formula><p>+ λ 2 ||Err(Typical) -Err(Autistic)|| ( <ref type="formula">2</ref>)</p><formula xml:id="formula_2">Err(X) =                          1 T T i=1 (S i t -Y i t ) 2 , if X = Typical, ∀i, Y i t = Typical M SE(St) , 1 T T i=1 (S i a -Y i a ) 2 , if X = Autistic, ∀i, S i a &lt; S ref ⇒ Y i a = Typical, ∀i, S i a &gt; S ref ⇒ Y i a = Autistic M SE(Sa)<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL DETAILS</head><p>This section first details the dataset collected for all experiments, called Autism dataset. Then, it provides the experimental details used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Autism Dataset</head><p>The Autism dataset comprises real-life assessment sessions of children, which were conducted by clinicians at a hospital. These sessions, totaling 132 hours, were recorded in accordance with the ADOS-2 protocol to examine the visual behavior of children based on the severity of their autism. Each child was evaluated for potential autism disorder during various interactive ADOS-2 activities. Untrimmed videos were categorized into nine modules, namely, anniversary, playing with bubbles, playing with ball, construction, demonstration, describing-image, imitation, joint-game, and puzzle, as per the ADOS evaluation protocol. Each module corresponds to a specific evaluation criterion. For instance, the module 'playing with ball or bubble' assesses repetitive behaviors, while the 'joint-game' analyzes a child's social skills. These experiments utilize a total of 75 unique hour-long videos of children for the study. The dataset is divided according to the subjects (children) and the severity score of each child assessed by the clinicians, as shown in Table <ref type="table" target="#tab_1">I</ref>. Thus, only one child is present in either train or test set. We split the 75 unique videos into train and test sets in a ratio of 85% and 15% respectively, keeping a balanced ratio of severity levels in each set.</p><p>The dataset will be made public in modalities such as skeleton, optical-flow and depth information after receiving approval from the ethical team.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>Before extracting features, we detect and crop child tracklets from videos across frames using SOTA Track-Anything <ref type="bibr" target="#b29">[30]</ref> and AgeFormer <ref type="bibr" target="#b30">[31]</ref> networks. We consider VideoMAE-v2 <ref type="bibr" target="#b20">[21]</ref>, and DinoV2 <ref type="bibr" target="#b19">[20]</ref> for spatio-temporal feature extraction. For each 16-frame snippet, a 1408D feature vector is extracted from the backbone pre-trained on Kinetics dataset <ref type="bibr" target="#b31">[32]</ref> from VideoMAE-v2-giant, and a T ×257×1024 feature vector from the last hidden layer of DinoV2. We pre-process T frames into 32 averaged temporal length for dimensionality reduction. We use VideoMAE-v2 features for the final experiments due to its robust spatio-temporal features. Initially, we adopt the same experimental protocols outlined for OE-CTST in <ref type="bibr" target="#b18">[19]</ref> to train a binary classifier, distinguishing between typical and atypical. Training is carried out using the Adam optimizer with a learning rate of 0.001 over 4000 epochs on our Autism dataset. Upon the completion of OE-CTST training, we freeze the architecture and utilize the 128D embedded features of the Detector for subsequent processing.</p><p>Subsequently, we design a shallow TCN-MLP network to enhance learning at the different levels of autism severity. This network is composed of three Temporal Convolution Network (TCN) layers and three MLP layers, which are used to train a score regressor. The TCNs aid in down-sampling the features from the visual encoder, which are then combined with the 128D features of OE-CTST prior to the application of MLP. We employ a supervised training approach for this network, using severity scores as labels over 40 epochs with the Adam optimizer and a learning rate of 0.0001. Furthermore, for the severity score regression we use a ranking loss, specifically Corn Loss <ref type="bibr" target="#b32">[33]</ref>. We conduct experiments with MSE and MAE for the evaluation of regression scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RESULTS AND DISCUSSION</head><p>In an hour-long ASD diagnostic session, various atypical biomarkers are observed. These biomarkers represent a range of discriminative patterns, including emotions, repetitive gestures, social interactions, atomic gestures, and unusual movements, among others. Each session is assigned a single severity label. Traditional action recognition methods are not suitable for evaluating or classifying severity scores due to the complexity and diversity of these patterns. As a result, we employ existing Temporal Action Localization (TAL) methods to encode these discriminative markers in long videos. Initially, we conduct experiments with existing supervised TAL methods as depicted in Table <ref type="table" target="#tab_1">III</ref>. PDAN, which are purely TAL methods, did not perform well on the Autism dataset for severity score evaluation. These methods are designed primarily to learn the temporal relations of spatio-temporal features of untrimmed videos. Consequently, applying temporal maxpooling to the last feature embedding layer of these methods did not yield the desired results for severity score computation. Another key factor for the effectiveness of these methods is the availability of densely annotated data, either at the frame-level or segment-level, for each action class in an untrimmed video. As we do not have these annotations, we opt for the Weakly Supervised Temporal Action Localization (WTAL) method. These methods are capable of learning various discriminative biomarkers (both known and unknown) in a weakly supervised setting, thereby enabling the model to discern between typical and atypical behavior patterns. The features derived from the WTAL method are then used to train a regression model for the final score. This proposed approach is proven to be successful, achieving the highest accuracy.</p><p>Clip-TSA and OE-CTST, which are state-of-the-art Weakly Supervised Temporal Action Localization (WTAL) methods, are used for anomaly detection in untrimmed videos. We have adapted these methods to learn a binary classification between typical and ASD behavior patterns, as shown in Table <ref type="table" target="#tab_1">II</ref>. OE-CTST outperformed Clip-TSA due to its specialized Outlier Positional Embedding and CTST modules. To illustrate the effectiveness of the OE-CTST network, we visualized the last feature vector of the Detector T × 128D (T=32) of four randomly selected participants for each severity level, as shown in Figure <ref type="figure" target="#fig_1">2</ref>. Figure <ref type="figure" target="#fig_1">2</ref> shows a denser heatmap for videos with a higher severity score, validating the proposed WTAL architecture's suitability for this task. It also demonstrates the amount of biomarkers we identified. For example, for participant having higher severity score, we identify around 50 biomarkers. However, not all biomarkers related to ASD could be identified and is left for future work. Based on these identified biomarkers and features from the visual encoder we train a supervised network on top of the WTAL for the final regression of the severity score, as shown in Table <ref type="table" target="#tab_1">III</ref>.</p><p>The confusion matrix computed on test-set depicted in Figure <ref type="figure" target="#fig_2">3</ref> offers a comprehensive insight into the evaluations of severity assessment. The model exhibits superior performance for the high and moderate classes in comparison to the noautism and weak classes. This performance can be attributed to the higher correlations between these classes, as illustrated in Figure <ref type="figure" target="#fig_1">2</ref>. Furthermore, the one outlier confusion between the high class and the no-autism classes is because the child is not autistic but hyperactive. We present and deliberate on this particular case with the clinician to ascertain whether it is an error in the analysis or if the child is genuinely enthusiastic about playing with bubbles and does not have autism. The clinicians confirmed that this child is merely extroverted and hyperactive. However, such scenarios can lead the model to mistakenly identify a higher autism case for hyperactive children. As a result, we plan to introduce an additional class for hyperactive cases in the future to prevent such inaccuracies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>Capturing autistic biomarkers without dense annotations is a challenging task, particularly in long untrimmed videos. The wide array of biomarkers, such as facial expressions, uncontrolled movements, repetitive behaviors, and eye-gaze, present in a long video with a single severity label, complicates accurate detection by the model. Additionally, the complexity is further increased by human errors and the subjectivity of the severity score. In this study, we strive to learn these discriminative markers in a weakly-supervised manner for atypical behaviors, which are then divided into four distinct severity levels for ASD evaluations. Despite these challenges, our proposed method achieves the highest accuracy compared to the baseline results. Our method, which is based on WTAL, offers numerous advantages. It provides clinicians with a tool to validate these biomarkers, enabling them to make more objective decisions. In addition, it aids clinicians to perform a comprehensive diagnosis by considering all discriminative biomarkers, known and unknown. This can be highly beneficial.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The network comprises three major stages i.e. (A) Visual Encoder, (B) Weakly-supervised ASD Detector to detect typical and atypical behaviors, (C) Severity Score Regressor to further regress the final severity score. Here, F O = feature map of one-class, F M = feature map of mixed distribution, T = 32 temporal segments, D = 1408 features, 128 is the feature vector from detector final layer. nm is the m video features obtained from n-levels of CTST module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Analysis of WTAL T ×D features for 4 randomly selected participants from each level, where T = 16 and D = 128 (feature vector). The density of the heatmap defines the atypical biomarkers. A higher density on the heatmap corresponds to a higher severity score.</figDesc><graphic coords="5,311.98,122.69,251.06,283.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Confusion matrix for severity score assessment.</figDesc><graphic coords="6,61.52,196.71,225.96,193.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I AUTISM</head><label>I</label><figDesc>DATASET ANALYSIS BASED ON SEVERITY SCORE. THE HOUR-LONG VIDEOS ARE SUBDIVIDED INTO MINUTES LONG ADOS MODULES.</figDesc><table /><note><p><p><p><p><p><p><p>Method</p>Typical / Autistic frame-level AUC (%)</p>Clip-TSA</p><ref type="bibr" target="#b16">[17]</ref> </p>60.01</p>OE-CTST [19]</p>68.58</p></note></figure>
		</body>
		<back>

			<div type="funding">
<div><p>BoostUrCAreer program received funding from the <rs type="funder">European Union</rs>'s <rs type="programName">Horizon 2020 under Marie Curie</rs> grant agreement No <rs type="grantNumber">847581</rs>. This work is also supported by the <rs type="funder">French government</rs>, through the <rs type="projectName">ACTIVIS</rs> project managed by the <rs type="funder">National Research Agency (ANR)</rs> with the reference number <rs type="grantNumber">ANR-19-CE19-0004</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_UUNg65X">
					<idno type="grant-number">847581</idno>
					<orgName type="program" subtype="full">Horizon 2020 under Marie Curie</orgName>
				</org>
				<org type="funded-project" xml:id="_EY6JpfC">
					<orgName type="project" subtype="full">ACTIVIS</orgName>
				</org>
				<org type="funding" xml:id="_Ggjf5ku">
					<idno type="grant-number">ANR-19-CE19-0004</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Autism genetics: strategies, challenges, and opportunities</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>O'roak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>State</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autism Research</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="17" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">World Health Organization (WHO): Autism spectrum disorders Key Facts</title>
		<ptr target="https://www.who.int/news-room/fact-sheets/detail/autism-spectrum-disorders" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Autism prevalence increases from 1 in 60 to 1 in 54: Cdc</title>
		<author>
			<persName><forename type="first">A</forename><surname>Knopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Brown University Child and Adolescent Behavior Letter</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="4" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A computer vision approach for the assessment of autism-related behavioral markers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">V</forename><surname>Spina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tepper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Esler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Morellas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papanikolopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE International Conference on Development and Learning and Epigenetic Robotics (ICDL</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gait and full body movement dataset of autistic children classified by rough set classifier</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Al-Jubouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rajihy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Physics: Conference Series</title>
		<imprint>
			<biblScope unit="volume">1818</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">12201</biblScope>
			<date type="published" when="2021">2021</date>
			<publisher>IOP Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Increased facial asymmetry in autism spectrum conditions is associated with symptom presentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Boutrus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Gilani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Alvares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Maybery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Whitehouse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autism Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1774" to="1783" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Attention-based autism spectrum disorder screening with privileged modality</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1181" to="1190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Video-based behavior understanding of children for objective diagnosis of autism</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">F</forename><surname>Negin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">F</forename><surname>Bremond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thümmler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VISAPP 2022-17th International Conference on Computer Vision Theory and Applications</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Appearance-based gaze estimation for asd diagnosis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-K</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="6504" to="6517" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Visionassisted recognition of stereotype behaviors for early diagnosis of autism spectrum disorders</title>
		<author>
			<persName><forename type="first">F</forename><surname>Negin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ozyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agahian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kacdioglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">T</forename><surname>Ozyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">446</biblScope>
			<biblScope unit="page" from="145" to="155" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Healthdirect australia</title>
		<ptr target="https://www.healthdirect.gov.au" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2024" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Self-stimulatory behaviours in the wild for autism diagnosis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rajagopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="755" to="761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Decoding children&apos;s social behavior</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Abowd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rozga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Clements</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ousley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3414" to="3421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">3d human sensing, action and emotion recognition in robot assisted therapy of children with autism</title>
		<author>
			<persName><forename type="first">E</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2158" to="2167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ms-tcn: Multi-stage temporal convolutional network for action segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">A</forename><surname>Farha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3575" to="3584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pdan: Pyramid dilated attention network for action detection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Minciullo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Garattoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Francesca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bremond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2970" to="2979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Clip-tsa: Clip-assisted temporal self-attention for weakly-supervised video anomaly detection</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yamazaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2023 IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="3230" to="3234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<ptr target="https://arxiv.org/abs/2103.00020" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Oe-ctst: Outlier-embedded cross temporal scale transformer for weaklysupervised video anomaly detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Majhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Garattoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Francesca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Brémond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="8574" to="8583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<author>
			<persName><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darcet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moutakanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Szafraniec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Khalidov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Haziza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>El-Nouby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.07193</idno>
	</analytic>
	<monogr>
		<title level="m">Learning robust visual features without supervision</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Videomae v2: Scaling video masked autoencoders with dual masking</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">560</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Detecting developmental delay and autism through machine learning models using home videos of bangladeshi children: Development and validation study</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Tariq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Fleming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dunlap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Corbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Washington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kalantarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">Z</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Darmstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Wall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of medical Internet research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">e13822</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Looking at movies and cartoons: Eyetracking evidence from williams syndrome and autism</title>
		<author>
			<persName><forename type="first">D</forename><surname>Riby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Intellectual Disability Research</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="169" to="181" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Atypical gaze patterns to facial feature areas in autism spectrum disorders reveal age and culture effects: A meta-analysis of eye-tracking studies</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autism Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2625" to="2639" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Classifying asd children with lstm based on raw videos</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">390</biblScope>
			<biblScope unit="page" from="226" to="238" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Toward the autism motor signature: Gesture patterns during smart tablet gameplay identify children with autism</title>
		<author>
			<persName><forename type="first">A</forename><surname>Anzulewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sobota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Delafield-Butt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">31107</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Video gesture analysis for autism spectrum disorder detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zunino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Morerio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cavallo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ansuini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Podda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Veneselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Becchio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 24th international conference on pattern recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3421" to="3426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Video-based early asd detection via temporal pyramid networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="272" to="277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Real-world anomaly detection in surveillance videos</title>
		<author>
			<persName><forename type="first">W</forename><surname>Sultani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6479" to="6488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.11968</idno>
		<title level="m">Track anything: Segment anything meets videos</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">P-age: Pexels dataset for robust spatio-temporal apparent age classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Marisetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bremond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="8606" to="8615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep neural networks for rankconsistent ordinal regression based on conditional probabilities</title>
		<author>
			<persName><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Raschka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="941" to="955" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
