<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Running head: Semantic Knowledge &amp; Visual Working Memory Associating Everything with Everything Else, All at Once: Semantic Associations Facilitate Visual Working Memory Formation for Real-World Objects</title>
				<funder ref="#_qU6Y5kd">
					<orgName type="full">National Institute of Neurological Disorders and Stroke</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xinchi</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Program in Neuroscience and Cognitive Sciences</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Linguistics</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sanikaa</forename><forename type="middle">P</forename><surname>Thakurdesai</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weizhen</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Program in Neuroscience and Cognitive Sciences</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Running head: Semantic Knowledge &amp; Visual Working Memory Associating Everything with Everything Else, All at Once: Semantic Associations Facilitate Visual Working Memory Formation for Real-World Objects</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9BCFF8CEF3A7DFB54EFE228415AC1938</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-23T06:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>visual working memory</term>
					<term>long-term memory</term>
					<term>prior knowledge</term>
					<term>semantic association</term>
					<term>natural language processing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Integrating prior semantic knowledge with environmental information is essential for everyday cognition, yet how this process affects ongoing perception and memory remains unclear. We investigate this by studying how associative semantic knowledge interacts with perceptual constraints induced by brief encoding times, thereby supporting visual working memory (VWM) for real-world objects. Study 1 reanalyzed data from Quirk et al. ( <ref type="formula">2020</ref>), involving 75 participants across 13,750 trials of a VWM task with randomly chosen objects and verbal distraction. We found that objects' semantic associations, estimated by a natural language processing model, predicted trial-level VWM accuracy under brief but not prolonged encoding times (0.2s vs. 1-2s). These results, unaffected by image similarity from computer vision models, were replicated in Study 2 with 50 participants across 11,880 trials.</p><p>Combined, these findings suggest that semantic associations facilitate effective grouping among VWM items to mitigate perceptual constraints, highlighting the role of semantic knowledge in VWM formation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Public significance</head><p>This study demonstrates that human observers frequently use their prior associative semantic knowledge to enhance memory when viewing brief visual displays of random everyday objects. These findings underscore the role of prior knowledge in overcoming perceptual processing limitations, thereby supporting effective everyday visual cognition.</p><p>Understanding how prior knowledge integrates with environmental information to sustain ongoing perception and memory remains a critical challenge in comprehending everyday cognition <ref type="bibr" target="#b8">(Chase &amp; Simon, 1973;</ref><ref type="bibr" target="#b12">Collins &amp; Olson, 2014;</ref><ref type="bibr" target="#b38">Potter, 2012;</ref><ref type="bibr" target="#b45">Smith et al., 2022)</ref>. Inquiry into this challenge has gradually shifted research from well-controlled laboratory stimuli to more naturalistic stimuli, such as images of real-world scenes and objects, highlighting the dynamic interplay between prior knowledge and moment-bymoment cognition <ref type="bibr" target="#b6">(Brady et al., 2019;</ref><ref type="bibr" target="#b57">Xie &amp; Zhang, 2022)</ref>. For instance, recent studies have demonstrated that everyday objects may be memorized more effectively than simple stimulus features such as colors in visual working memory, VWM <ref type="bibr" target="#b5">(Brady et al., 2016;</ref><ref type="bibr" target="#b10">Chung et al., 2023</ref>; but see <ref type="bibr" target="#b27">Li et al., 2020;</ref><ref type="bibr" target="#b39">Quirk et al., 2020</ref>) -a capacity-limited mental faculty crucial for higher cognition such as fluid intelligence <ref type="bibr" target="#b13">(Cowan, 2001;</ref><ref type="bibr" target="#b29">Luck &amp; Vogel, 2013)</ref>. These findings prompt research into computer vision algorithms that capture stimulus-driven attributes in VWM processes <ref type="bibr">(Brady &amp; St√∂rmer, 2023;</ref><ref type="bibr" target="#b28">Liu et al., 2020)</ref> and observer-focused research emphasizing prior knowledge -either from past encounters driving visual familiarity <ref type="bibr" target="#b23">(Jackson &amp; Raymond, 2008;</ref><ref type="bibr" target="#b54">Xie &amp; Zhang, 2017c</ref><ref type="bibr">, 2018)</ref>, from the meaningfulness/integrity of a visual item relative to its visual distortion <ref type="bibr" target="#b0">(Asp et al., 2021;</ref><ref type="bibr" target="#b11">Chung et al., 2024;</ref><ref type="bibr" target="#b42">Sahar et al., 2024;</ref><ref type="bibr" target="#b46">Thibeault et al., 2024)</ref>, or from the categorical knowledge about an object's class information <ref type="bibr" target="#b9">(Chiou &amp; Lambon Ralph, 2018;</ref><ref type="bibr" target="#b14">Endress &amp; Potter, 2014;</ref><ref type="bibr" target="#b21">Hu &amp; Jacobs, 2021;</ref><ref type="bibr" target="#b30">Markov et al., 2021;</ref><ref type="bibr" target="#b38">Potter, 2012;</ref><ref type="bibr" target="#b50">Wong et al., 2008)</ref>.</p><p>Compared to these progresses, surprisingly little is known concerning the influence of our prior associative semantic knowledge on VWM. Separate from the class information of object categories, semantic associations can link any arbitrary pair of concepts, reflecting a distributed process of how information is organized in our minds for more efficient computation <ref type="bibr" target="#b24">(Klix, 1978;</ref><ref type="bibr" target="#b25">Kumar, 2021;</ref><ref type="bibr" target="#b31">McClelland &amp; Rogers, 2003;</ref><ref type="bibr" target="#b44">Sipser, 2012)</ref>. This form of knowledge begins to take root early in life, coinciding with the onset of language acquisition during infancy <ref type="bibr" target="#b1">(Barbir et al., 2023;</ref><ref type="bibr" target="#b26">Lany &amp; Saffran, 2011)</ref>. As it continues to reinforce through our written and spoken communication, semantic associations exert a pervasive impact on our perception <ref type="bibr">(Xie &amp; Zhang, 2023b</ref>) and memory <ref type="bibr" target="#b48">(Tompary &amp; Thompson-Schill, 2021;</ref><ref type="bibr" target="#b51">Xie et al., 2020)</ref>. For example, semantic associations can effectively group perceptually distinct stimuli within a single display <ref type="bibr" target="#b19">(Green &amp; Hummel, 2006;</ref><ref type="bibr" target="#b32">Nah &amp; Geng, 2022;</ref><ref type="bibr" target="#b40">Roberts &amp; Humphreys, 2011)</ref>, serving as a chunking mechanism underlying the interaction between pre-existing long-term memory and enhanced VWM storage <ref type="bibr" target="#b13">(Cowan, 2001;</ref><ref type="bibr" target="#b16">Gobet et al., 2001</ref>). Yet, despite this intuition, except for some context-specific associations (e.g., using a "hammer" to drive a "nail" under the action/function context, <ref type="bibr" target="#b34">O'Donnell et al., 2018;</ref><ref type="bibr" target="#b22">Humphreys &amp; Riddoch, 2006</ref>; e.g., a "hair blower" is not frequently seen in a "forest", <ref type="bibr" target="#b49">V√µ, 2021)</ref>, a notable gap persists in understanding how human observers leverage diverse, arbitrary semantic associations to mitigate VWM constraints, potentially due to various known challenges.</p><p>First, given the vast number of possible combinations between things in everyday life, it is challenging to identify the strength of associations between any arbitrary pairs of objectrelated concepts. Some of these arbitrary associations may carry little explicit action or functional meaning, yet they remain semantically related to each other (e.g., "bunny" and "Easter egg"). Revealing these arbitrary semantic associations in our everyday language has only become feasible with the advancements in natural language processing (NLP) models, such as the GloVe (Global Vector for Word Representation, <ref type="bibr" target="#b36">Pennington et al., 2014)</ref>. GloVe is an effective model that processes word-word co-occurrences in large corpora from printed and online texts to extract associations between any concepts in our everyday language <ref type="bibr" target="#b18">(Grand et al., 2022;</ref><ref type="bibr" target="#b36">Pennington et al., 2014)</ref>. Although features from feed-forward convolutional neural network (CNN) models trained solely on pixel-level information of images can capture the relationship between objects based on their visual similarity within the same semantic category <ref type="bibr" target="#b43">(Simonyan &amp; Zisserman, 2015)</ref>, NLP word embeddings capture semantic associations between any arbitrary pair of objects across any categories, regardless of their visual resemblance. Using these word embeddings, recent research has suggested that properties of semantic associations can modulate early visual perception <ref type="bibr">(Xie &amp; Zhang, 2023b)</ref> and episodic memory retrieval <ref type="bibr" target="#b51">(Xie et al., 2020)</ref>, even when perceptual contributions to these processes are strictly controlled. Nonetheless, these models or analyses have yet been applied to VWM research, leaving a gap in our understanding concerning the role of semantic associations among everyday objects in VWM and how it may differ from the contribution of image similarity predicted by CNN features <ref type="bibr">(Brady &amp; St√∂rmer, 2023)</ref>.</p><p>Second, the interplay between semantic associations and perceptual processing during VWM formation has remained unclear. Past research has implied that semantic and perceptual factors could exert either a compensatory or addictive impact on VWM. On the one hand, as retaining multiple pieces of information in VWM is inherently effortful <ref type="bibr">(Xie &amp; Zhang, 2023a)</ref>, semantic associations -as higher-order information of a visual display with multiple objects -may not become relevant when each individual object is sufficiently encoded. This compensatory relationship would predict more semantic contribution to VWM only when perceptual encoding is limited. On the other hand, it also remains possible that leveraging semantic associations may reflect a deeper level of processing following sufficiency perceptual encoding <ref type="bibr" target="#b3">(Brady &amp; St√∂rmer, 2022;</ref><ref type="bibr" target="#b17">Graham &amp; Golan, 1991)</ref>. Thus, an alternative account would predict that limited perceptual encoding may hinder the extraction of semantic associations and their subsequent impacts on VWM. Testing these possibilities requires manipulating perceptual processing during VWM formation, for example, by adjusting the time allowed for VWM encoding (e.g., <ref type="bibr" target="#b60">Ye et al., 2024)</ref>.</p><p>The current study therefore aims to investigate how semantic associations among everyday objects -quantified as NLP word embedding similarities across object labelsinteracts with perceptual encoding constraints induced by different encoding times to affect VWM. Relevant data meeting these criteria for testing the proposed hypotheses are available from <ref type="bibr">Quirk and colleagues (2020)</ref>, where participants attempted to encode a multi-object display in a VWM task within a limited (0.2s) or longer (&gt;=1s) encoding time. As Quirk and colleagues have primarily focused on contrasting participants' VWM for these real-world objects with that for the simpler stimulus feature of colors, they have not tested the role of semantic associations among objects during VWM formation. We therefore first analyzed Quirk and colleague's data with a fresh perspective. This is followed by a pre-registered study (<ref type="url" target="https://osf.io/t3nf5">https://osf.io/t3nf5</ref>) with a similar design to further test our hypotheses. Combined, as we gather evidence based on existing data with a replication, our findings underscore structured semantic knowledge -operationalized as semantic associations in NLP modelsas an important avenue to enrich our understanding of everyday VWM function in real-world contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>Study 1. We used the dataset from a previous study <ref type="bibr" target="#b39">(Quirk et al., 2020;</ref><ref type="bibr"></ref> <ref type="url" target="https://osf.io/vq37u/">https://osf.io/vq37u/</ref>), where 75 participants completed a VWM task involving real-world objects and colored squares in 3 behavioral experiments (i.e., Experiments 1a-c in the original paper). These participants were reported to be aged 18-31 with self-reported normal or corrected-to-normal visual acuity and normal color vision and completed the study for monetary compensation ($10/h). Data from these participants were included for these following reasons. First, all participants had completed a VWM task with real-world objects randomly chosen and presented in a single display. Second, the authors have provided triallevel data with information regarding which objects had been presented during the study.</p><p>Third, these experiments also included a within-subject manipulation of encoding time, containing a relatively limited and a longer encoding time (e.g., 0.2s vs. &gt;1s) within a conventional VWM task. Fourth, the research sample were tested within an English-speaking environment, considering that the current NLP model was trained and tested within an English-speaking context.</p><p>Apart from <ref type="bibr">Quirk et al. (2020, Experiments 1a-c)</ref>, as of the time in preparation of the manuscript, there have been a few other studies that have tested VWM for multiple realworld objects in a single visual display <ref type="bibr" target="#b5">(Brady et al., 2016;</ref><ref type="bibr" target="#b3">Brady &amp; St√∂rmer, 2022;</ref><ref type="bibr" target="#b10">Chung et al., 2023;</ref><ref type="bibr" target="#b27">Li et al., 2020;</ref><ref type="bibr" target="#b46">Thibeault et al., 2024;</ref><ref type="bibr">Experiment 2 in Quirk et al., 2020)</ref>. These additional studies were not included for the current research due to at least one of these following reasons: (1) raw data were not available <ref type="bibr" target="#b5">(Brady et al., 2016)</ref>; (2) participants were not English speakers <ref type="bibr" target="#b27">(Li et al., 2020)</ref>; or (3) the experiment only involved one encoding time condition, making the within-subject contrast between encoding time conditions implausible <ref type="bibr" target="#b3">(Brady &amp; St√∂rmer, 2022;</ref><ref type="bibr" target="#b10">Chung et al., 2023;</ref><ref type="bibr" target="#b46">Thibeault et al., 2024;</ref><ref type="bibr">Experiment 2 in Quirk et al., 2020)</ref>.</p><p>Study 2. We recruited 50 college students (19.96 ¬± 1.56 [mean ¬± SD] years old; 34 females, 16 males) from the University of Maryland, College Park, who reported normal or corrected-to-normal vision and being native English speakers. They participated in the study for course credits, following a protocol approved by the local IRB. This study was preregistered (<ref type="url" target="https://osf.io/t3nf5">https://osf.io/t3nf5</ref>) with the goal of replicating the findings of Study 1. All data and analytical codes are available online via the Open Science Framework: <ref type="url" target="https://osf.io/v3ckh/">https://osf.io/v3ckh/</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stimuli</head><p>Following prior research <ref type="bibr" target="#b5">(Brady et al., 2016;</ref><ref type="bibr" target="#b39">Quirk et al., 2020)</ref>, images of real-world objects from 200 distinct categories in our everyday visual environment (e.g., couch, cup, bag, etc.) were used for the current study. Each of the 200 category-specific image sets contains 15 perceptually distinct exemplars, yielding a total of 3000 images of objects in the entire stimulus set (<ref type="url" target="http://bradylab.ucsd.edu/stimuli.html">http://bradylab.ucsd.edu/stimuli.html</ref>). Each trial contained a study and a test display of these everyday objects. For the study display, 6 categorically distinct objects were randomly chosen from the 200 category-specific image sets. For the test display, the target object would repeat from the study display, whereas the foil object was randomly selected from any of the categories not included in the study array. Stimuli were displayed on an invisible ring in fixed, equidistant positions. Stimuli were generated and displayed on a white background using MATLAB and the Psychtoolbox <ref type="bibr" target="#b7">(Brainard, 1997)</ref> in both Study 1 and 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedures</head><p>All participants completed a VWM task with concurrent verbal distraction with a similar task structure as outlined in Figure <ref type="figure" target="#fig_2">1A</ref>. On each trial, participants first saw two digits for 1s, for which they would need to rehearse explicitly or silently in mind throughout the trial. Afterwards, 6 placeholder dots around the center of the screen appeared for 1s, followed by six real-world objects randomly sampled from 200 distinct semantic categories (15 unique objects/category) at the corresponding locations. Within each experiment, these objects were presented for either 0.2s, 1s or 2s (see Table <ref type="table" target="#tab_0">1</ref> for task design across different experiments), followed by blank screen of 6 dots for 0.8s. To prepare the participants for response, one of the placeholders enlarged for 0.5s, cuing the location that would be later tested <ref type="bibr" target="#b39">(Quirk et al., 2020)</ref>, making the delay period a total of 1.3s. Upon the test, two objects were presented above and below the cued location, one matching with the object previously shown at that location, and the other being randomly sampled from another unshown object category to act as the foil. Participants were instructed to press a set of buttons (e.g., UP or DOWN arrow keys) to indicate which item appeared in the study display at that location. Afterwards, participants tried to make a response based on the digits that they were rehearsing by either typing them out (Study 1) or recognizing whether a newly presented digit pair was the same or different as compared with the previous ones at the beginning of the trial (Study 2). In the latter case, half of the trials contained the same digit pair whereas another half of trials contained a different pair of digits, and participants used another set of buttons to make a match-or-not judgment (e.g., LEFT or RIGHT arrow key for match and not-match, respectively). In Study 1, participants also performed a similar VWM task using colored squares instead of real-world objects <ref type="bibr" target="#b39">(Quirk et al., 2020)</ref>. These different stimulus types were blocked and randomly intermixed throughout a testing session. For the current study, we only retained the data using the real-world objects for subsequent analyses.</p><p>Study 1 also employed multiple experimental designs to test the generalization of experimental findings across different conditions <ref type="bibr" target="#b39">(Quirk et al., 2020)</ref>. These designs varied by employing different encoding times in the VWM task (0.2, 1, 2s in Experiment 1a; 0.2 and 2s in Experiments 1b-c), by either blocking or randomly intermixing different encoding times within a block (Experiments 1a-b used blocking, while Experiment 1c used random intermixing), and by implementing explicit or silent verbal rehearsal for the concurrent digit task (Experiment 1b). We coded these design variables as potential predictors of participants' trial-level performance, such as encoding time as either short (0.2s) or long (1-2s). Since explicit or silent verbal rehearsal did not affect task performance <ref type="bibr" target="#b39">(Quirk et al., 2020)</ref>, we combined trials with explicit and silent verbal rehearsal tasks. Other experimental factors, including the location of the presented objects, the categories and identities of individual objects, and the tested locations, were randomly chosen across trials. Participants completed a minimum of 100 trials per encoding time condition to allow reliable estimates of VWM task performance, yielding a total of 13,750 trials across participants (see Table <ref type="table" target="#tab_0">1</ref> for details).</p><p>Study 2 used the same experimental design as in Study 1, with the following exceptions. First, to focus on VWM for real-world objects, we only retained the object condition in Study 1. Second, to ensure consistency with Experiments 1b-c in Study 1, we only included two encoding time conditions, presenting objects for either 0.2s or 2s in a blocked design. The order of these different conditions/blocks was randomized across participants. Third, we employed a 5s time limit for the VWM response for the interest of time. Fourth, participants completed blocks of 60 trials for 4-6 blocks depending on the availability of testing time within a 1h experimental session, yielding 120 or 180 trials per encoding time condition and a total of 11,880 trials across participants (see Table <ref type="table" target="#tab_0">1</ref> for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modeling Trial-level Features</head><p>To identify how the semantic associations among presented real-world objects affect VWM formation, we first quantified the strength of semantic association between any pair of presented objects in each trial based on the GloVe word embeddings of the objects' labels.</p><p>Additionally, to account for other trial-level factors, for example, encoding time, the image-based visual resemblance of the presented objects, and the image or semantic similarity between the presented target and foil items, we extracted these trial-level measures based on the methods outlined below. These trial-level predictors were then included in a general linear mixed model as outlined in Statistical Analysis to predict participants' trial-by-trial VWM recognition accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic Associations Among Study Objects</head><p>On each trial, we defined and calculated the semantic association between any nonredundant object pairs as the cosine similarity of the GloVe word embeddings of each object's categorical labels. These categorical labels are available in the original stimulus set, and we also have inspected these labels and edited them when necessary to ensure that they accurately reflect the presented real-world objects. As some object categories may be better described using more than one word (e.g., "cooking pan"), we retained the word embeddings for each individual word (e.g., "cooking" and "pan"). With a visual display of 6 randomly selected objects, there may be more than 6 words to describe these items. For each pair words i and j, namely Wi and Wj, we calculated their cosine similarity to capture their semantic association, as follows,</p><formula xml:id="formula_0">ùëÜ"ùëä ! , ùëä " % = ùëä ! ‚Ä¢ ùëä " ||ùëä ! || ||ùëä ! ||</formula><p>We first obtained an estimate of semantic association between a pair of objects by averaging S(Wi, Wj) over all non-redundant word pairs between the two categories. We then computed the average of these cosine similarity measures across all non-redundant pairs of objects as a measure of the overall semantic association across all objects in a visual display.</p><p>By chance, some visual display would contain objects that are mostly semantically related with one another (e.g., "bunny" and "Easter egg", "hat" and "shoe" on the right side of Figure <ref type="figure" target="#fig_2">1B</ref>), whereas other is less semantically related (e.g., "chessboard", "compass", and "microscope" on the left side of Figure <ref type="figure" target="#fig_2">1B</ref>). There is, therefore, a continuum of semantic association strength among objects within a given multi-object display, suggesting a source of variance for trial-by-trial VWM recognition accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Similarity Among Study Objects</head><p>Based on a similar approach, we also attempted to quantify the visual resemblance among the presented objects based on a pre-trained feedforward CNN model, namely VGG-16 <ref type="bibr" target="#b43">(Simonyan &amp; Zisserman, 2015)</ref>. VGG-16 is trained based on the pixel-level information of images containing real-world objects, making it an appropriate model in the current context. Although layers of VGG-16 features have been used to infer perceptual or conceptual information underlying object recognition and visual memory <ref type="bibr" target="#b41">(Rust &amp; Mehrpour, 2020;</ref><ref type="bibr" target="#b43">Simonyan &amp; Zisserman, 2015)</ref>, they primarily capture the image-level information available for visual categorization, considering that objects from the same category tend to be visually similar. This factor is in sharp contrast with the GloVe word embeddings that contain little information regarding the visual resemblance of concepts; instead, the semantic association estimate from GloVe completely abstracts away from any visual inputs. To distinguish these factors in predicting trial-level VWM success, we therefore quantified each object image's VGG-16 features of the last max pooling layer <ref type="bibr">(Brady &amp; St√∂rmer, 2023)</ref> and calculated the average cosine similarity of these VGG-16 features between non-redundant object pair as a measure of visual similarity of the object images within a given display.</p><p>Considering that this image-driven metric may capture primarily categorical information instead of semantic associations among objects, it serves as a good control variable in the current study to parse out image-related variances in participants' VWM task performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic Association and Image Similarity between Target and Foil Objects at Test</head><p>To factor out the variance in trial-level VWM recognition performance driven by how similar or dissimilar the target and foil items were to each other <ref type="bibr">(Brady &amp; St√∂rmer, 2023;</ref><ref type="bibr" target="#b53">Xie &amp; Zhang, 2017b)</ref>, we quantified both the semantic association and image similarity between the target and foil objects at test for each trial. These metrics can be directly calculated as the cosine similarity between the target and foil items, based on their GloVe word embeddings or VGG-16 features respectively, serving as additional covariates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistical Analyses</head><p>We used a generalized linear mixed model to fit trial-level data to predict participants' trial-by-trial VWM recognition performance, which was coded as 1 for correct and 0 for incorrect responses. This approach allows us to evaluate the variances accounted for by trial- and ùõΩ -12,&amp;_.*$/0√ó&amp;(%_*!+&amp; to evaluate the extent to which adding these terms is necessary to better account for participants' trial-by-trial VWM task performance. In this approach, the potential influence of other predictors on participants' VWM task performance is less consequential for our interpretation, as the variance related to these covariates has been factored out. Additionally, to enhance interpretation, we conducted separate regression analyses for data from each encoding time condition. Statistical significance was evaluated using Wald's Z test, and all reported p-values are two-tailed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study 1: Quirk et al., 2020</head><p>We first re-analyzed the data from 3 experiments across 75 participants and 13,750 trials from <ref type="bibr" target="#b39">Quirk et al. (2020)</ref>. To ensure that participants actively retained the presented objects in VWM under concurrent verbal distraction, we restricted our analysis only to the trials where participants had successfully recalled the presented digits in each trial.</p><p>Furthermore, we also excluded occasional trials where participants failed to respond using the instructed keys for the VWM task (&lt; 0.1% of all trials). This reduced our trials count to 12,939 trials (94.10% of all 13,750 trials). As a sanity check, we first replicated the original study and others <ref type="bibr" target="#b5">(Brady et al., 2016;</ref><ref type="bibr" target="#b27">Li et al., 2020;</ref><ref type="bibr" target="#b39">Quirk et al., 2020)</ref> and observed a clear enhancement effect of encoding time on VWM task performance (Œ≤ = 0.59, SE = 0.045, Z = 12.98, p = 2.94√ó10 -38 ). That is, participants' VWM recognition success is much higher under a longer encoding time of 1-2s (mean = 83.51%) relative to a shorter one of 0.2s (mean = 73.77%). Complementary analysis based on all 13,750 trials yield highly comparable findings (see Supplementary Figure <ref type="figure" target="#fig_2">S1</ref>).</p><p>Next, of primary interest, we then focus on the effect of semantic associations among objects in the study array, after controlling for various covariates including image similarity among study objects, semantic association between target and foil objects, and the image similarity between target and foil objects (see Statistical Analyses for the full general linear mixed model). We identified a significant main effect of the study objects' overall semantic association on VWM task performance (Œ≤ = 0.087, SE = 0.031, Z = 2.78, p = 0.0054), in that the higher average semantic association of a visual display, the more likely an observer would correctly recognize the target item after a short delay. Notably, there was a significant interaction effect between the study objects' overall semantic association and encoding time (Œ≤ = -0.095, SE = 0.046, Z = -2.08, p = 0.038; see Table <ref type="table" target="#tab_1">2</ref>). As demonstrated in the left panel in Figure <ref type="figure" target="#fig_3">2A</ref>, these effects were primarily driven by the greater improvement of VWM task performance driven by higher semantic associations among objects in the study display when it was presented with a shorter encoding time of 0.2s relative to a longer encoding time (1-2s). This was confirmed by separately analyzing the short and long encoding time trials (see Table <ref type="table" target="#tab_2">3</ref>). That is, for short encoding time trials, semantic association was a significant predictor for VWM accuracy (Œ≤ = 0.087, SE = 0.031, Z = 2.81, p = 0.0050), improving VWM accuracy from below 70% to near 80% from a semantically unrelated visual display to a semantically related display. In contrast, this clear increase was absent for long encoding time trials, in that semantic associations among objects within the study display did not have a significant contribution to VWM recognition accuracy (Œ≤ = -0.0066, SE = 0.034, Z = -0.20, p = 0.84).</p><p>Critically, the significant interaction effect between semantic associations among objects in the study display and encoding time could not be accounted for by other trial-level covariates. Based on a comparison between models without and with semantic association and its interaction with encoding time as additional predictors, we found that the latter provided a significantly better fit to the data (ŒîAIC = -4, log-likelihood ratio test: œá 2 (2) = 7.76, p=0.021; see Table <ref type="table" target="#tab_1">2</ref>), suggesting unique semantic contributions to VWM formation under perceptual processing constraints. Alternatively, can these accuracy effects be accounted for by response bias? We found this unlikely, considering that the mapping between the target location and the correct response was randomized across trials.</p><p>Furthermore, additional analysis replacing participants' response accuracy in each trial with key responses corresponding to the top versus bottom objects (coded as 1 and 0 respectively) did not reveal any significant findings from the predictors, yielding chance-level predictions (see the right panel in Figure <ref type="figure" target="#fig_3">2A</ref>). Hence, the current effect of semantic associations among study objects on VWM accuracy should not be confounded by response bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study 2: Pre-registered Replication</head><p>Building upon Study 1, we next aimed to replicate the key interaction effect between the average semantic association among study objects of a visual display and encoding time on VWM formation. As we simplified the original task design by removing the color condition, we were able to test more trials per participant. Across 50 participants, we analyzed a total of 11,880 trials, where 10,822 trials contained correct responses for the concurrent digit task and proper key presses within a 5s response time window for the VWM task, accounting for 91.09% of all trials. Again, using these trials as an indicator for participants' active retention of the task content within a given trial, we replicated the widely observed enhancement effect of prolonged encoding time on VWM for real-world objects (Œ≤ = 0.89, SE = 0.052, Z = 17.14, p = 5.72√ó10 -65 ). Consistent with Study 1, participants' VWM recognition success was much higher under a longer encoding time of 2s (mean = 86.07%)</p><p>relative to a shorter one of 0.2s (mean = 73.00%). Data including all trials also provide similar findings (see Supplementary Figure <ref type="figure" target="#fig_2">S1</ref>).</p><p>Next, we examined the extent to which findings from Study 1 could be replicated in Study 2. Using the same analytical approach, we identified a significant main effect of semantic associations among study objects of the study display on VWM task performance (Œ≤ = 0.14, SE = 0.032, Z = 4.38, p = 1.18√ó10 -5 ). Critically, there was a significant interaction effect between semantic associations among study objects and encoding time (Œ≤ = -0.12, SE = 0.053, Z = -2.34, p = 0.020; see Table <ref type="table" target="#tab_1">2</ref>). Again, these effects were primarily driven by a greater improvement in VWM recognition accuracy as a function of semantic associations among study objects under the short encoding time condition (Œ≤ = 0.14, SE = 0.032, Z = 4.31, p = 1.65√ó10 -5 ), relative to the long encoding time condition (Œ≤ = 0.011, SE = 0.043, Z = 0.26, p = 0.79), as demonstrated in separate regression analyses (see Table <ref type="table" target="#tab_2">3</ref>). Consistent with the previous results, VWM recognition accuracy can increase by up to ~10% as the semantic association among objects within a visual display increase under a brief encoding time of 0.2s, whereas such an effect would disappear when the encoding time extends to 10 times longer (see the left panel in Figure <ref type="figure" target="#fig_2">1B</ref>).</p><p>Furthermore, as these effects were estimated after controlling for covariates, factors like image similarity among study objects, semantic similarity between target and foil objects, and the image similarity between target and foil objects could not account for these results. Similarly, a comparison between models without and with semantic association and its interaction with encoding time as additional predictors suggested that adding trial-level predictors associated with semantic associations among study objects could significantly better account for the data (ŒîAIC = -15, log-likelihood ratio test: œá<ref type="foot" target="#foot_1">foot_1</ref> (2) = 19.32, p = 6.38√ó10 -5 , see Table <ref type="table" target="#tab_1">2</ref>), replicating the findings in Study 1. Furthermore, additional analysis replacing participants' response accuracy in each trial with key responses corresponding to the top versus bottom objects did not reveal any significant findings from the predictors (see the right panel in Figure <ref type="figure" target="#fig_3">2B</ref>).</p><p>While these findings were on par with the results in Study 1, we also identified some nuances. For example, while data from Study 1 did not reveal an effect of the image similarity between target and foil on VWM recognition performance<ref type="foot" target="#foot_0">foot_0</ref> , some recent work suggest otherwise <ref type="bibr">(Brady &amp; St√∂rmer, 2023)</ref>. Results from Study 2 appears to be in line with these recent findings, in that visually similar foil can lead to worse VWM recognition, potentially due to the interference at the retrieval phase (Œ≤ = -0.059, SE = 0.031, Z = -1.90, p = 0.058), despite with attenuated statistical evidence relative to the original study <ref type="bibr">(Brady &amp; St√∂rmer, 2023)</ref>. In contrast, semantically similar foil, however, did not lead to a similar detrimental effect for VWM accuracy in either Study 1 (Œ≤ = -0.019, SE = 0.031, p = 0.55) or</p><p>Study 2 (Œ≤ = 0.017, SE = 0.032, Z = 0.55, p = 0.59; see Table <ref type="table" target="#tab_1">2</ref>). These results suggest a potential distinction between the relevance of semantic similarity and image similarity between target and foil items during VWM retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>Human observers rarely encode real-world stimuli as mere data transferred onto a tabula rasa. A fundamental question in visual cognition within naturalistic settings is, therefore, how prior knowledge facilitates visual processing, particularly through associative semantic knowledge grounded in everyday language experiences. Building on a prior study <ref type="bibr" target="#b39">(Quirk et al., 2020)</ref> with a replication (Study 2), our trial-by-trial analysis reveals previously undocumented yet robust influences of associative semantic knowledge on VWM formation across two separate research samples with a total of 125 participants across over 25,000 trials in 4 experiments. These results cannot be explained by image similarity captured by a feedforward computer vision model that categorizes images based solely on pixel-level information, idiosyncratic trial-level covariates such as the similarity between target and foil items at test, nor participants' response biases. As these findings emphasize the impact of semantic associations among real-world objects beyond mere class information on VWM formation, they hold significant implications for scaling up our understanding of visual cognition in the real world.</p><p>Phenomenologically, our results suggest that human observers leverage prior semantic knowledge cumulated from everyday language experiences to group or chunk multiple arbitrary visual objects to facilitate ongoing VWM processes. This influence of long-term associative semantic knowledge on VWM processing is separate from other forms of long-term memory influences, such as the effects of visual familiarity of individual items <ref type="bibr" target="#b23">(Jackson &amp; Raymond, 2008;</ref><ref type="bibr" target="#b53">Xie &amp; Zhang, 2017b</ref><ref type="bibr" target="#b55">, 2017d)</ref>, the integrity of an individual item relative to visual distortion <ref type="bibr" target="#b0">(Asp et al., 2021;</ref><ref type="bibr" target="#b11">Chung et al., 2024;</ref><ref type="bibr" target="#b42">Sahar et al., 2024;</ref><ref type="bibr" target="#b46">Thibeault et al., 2024)</ref>, or categorical knowledge about an object's class information <ref type="bibr" target="#b9">(Chiou &amp; Lambon Ralph, 2018;</ref><ref type="bibr" target="#b14">Endress &amp; Potter, 2014;</ref><ref type="bibr" target="#b21">Hu &amp; Jacobs, 2021;</ref><ref type="bibr" target="#b30">Markov et al., 2021;</ref><ref type="bibr" target="#b38">Potter, 2012;</ref><ref type="bibr" target="#b50">Wong et al., 2008)</ref>. Instead, these findings align more with the rapid extraction of configural information or pattern goodness across multiple visual objects during VWM formation <ref type="bibr" target="#b2">(Brady &amp; Alvarez, 2011;</ref><ref type="bibr" target="#b20">Howe &amp; Brandau, 1983;</ref><ref type="bibr">Xie &amp; Zhang, 2017a)</ref>. Our findings extend these prior results from the perceptual domain to the conceptual domain, demonstrating that the contribution of configural information to VWM formation can occur rapidly within 0.2s at the semantic level.</p><p>Theoretically, why would such higher-order information be rapidly extracted during VWM formation? Conceptually, prior associative semantic knowledge may provide a cognitive map to scaffold the effective grouping or chunking of multiple perceptually distinct items in VWM <ref type="bibr" target="#b16">(Gobet et al., 2001)</ref>, thereby stabilizing these temporary memory information over time <ref type="bibr" target="#b35">(Peer et al., 2021)</ref>. If we assume that information in VWM is retained as a bound representation <ref type="bibr" target="#b37">(Peterson et al., 2015;</ref><ref type="bibr" target="#b47">Thyer et al., 2022;</ref><ref type="bibr" target="#b61">Yu &amp; Lau, 2023)</ref>, remembering semantically associated items may help reduce the need to remember multiple unique VWM representations, thereby reducing VWM load. Despite this intuition, however, the role of semantic associations among arbitrary pairs of everyday objects in this VWM grouping process has remained largely underspecified until now. Except for various context-dependent associations typically seen in the context of action/function <ref type="bibr" target="#b22">(Humphreys &amp; Riddoch, 2006;</ref><ref type="bibr" target="#b34">O'Donnell et al., 2018)</ref> or specific scenes <ref type="bibr" target="#b49">(V√µ, 2021)</ref>, in prior VWM research using real-world objects, the role of diverse arbitrary associative semantic knowledge in VWM encoding is often overlooked, especially under the typical randomization procedures of object selection and concurrent verbal distraction. Our findings refine this understanding by revealing the significant effect of long-term semantic memory on VWM, delineating and replicating conditions under which this effect may emerge.</p><p>First, the contribution of semantic associations to VWM formation is more pronounced when perceptual processing is constrained by brief encoding times (e.g., 0.2s).</p><p>Intuitively, prolonged encoding times may prompt deeper processing beneficial to integrating an ongoing memory item into prior associative knowledge <ref type="bibr" target="#b17">(Graham &amp; Golan, 1991)</ref>.</p><p>However, our findings favor an alternative account: longer encoding times may not provide ideal conditions for semantic contributions to VWM formation. Instead, semantic associations appear to be rapidly extracted within 0.2s of encoding time, contributing more to VWM formation when perceptual processing is interrupted, highlighting a compensatory relationship between semantic and perceptual information during visual memory formation <ref type="bibr" target="#b33">(Naspi et al., 2023)</ref>. Yet, our current study only tested a fixed set size and a limited range of perceptual encoding times. Future experiments with psychophysics methods can determine the necessary encoding times and set sizes for semantic effects to emerge or diminish.</p><p>Second, our findings indicate that the overall effects of semantic associations on VWM formation persist across variations in experimental conditions, for example, regardless of whether a concurrent digit task was explicitly or silently implemented as verbal distraction, as found in Study 1. As semantic information is rapidly extracted to interact with perceptual processing, the digit task may not fully block verbal encoding. Instead, this task may serve as an active interference, participants to retain information in working memory before further interruption. Future research may further investigate the extent to which other experimental procedures, such as perceptual masking <ref type="bibr" target="#b15">(Enns &amp; Di Lollo, 2000)</ref> and other forms of dual task (e.g., addition of a concurrent physical load; Xie &amp; Zhang, 2023a), may modulate this interplay between semantic and perceptual processing during      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc>level predictors, while partialling out the variances introduced by different participants or experimental designs. In this model, participants' overall likelihood of recognition success (Psuccess) across trials can therefore be directly modeled based on a linear combination of triallevel predictors with a logistic link, ln , ùëÉ #$%%&amp;## 1 -ùëÉ #$%%&amp;## 0 ~ ùõΩ ' + ùõΩ &amp;(%_*!+&amp; + ùõΩ ,--_.*$/0 + ùõΩ -12,&amp;_.*$/0 + ùõΩ ,--_3&amp;#* + ùõΩ -12,&amp;_3&amp;#* + ùõΩ ,--_.*$/0√ó&amp;(%_*!+&amp; + ùõΩ -12,&amp;_.*$/0√ó&amp;(%_*!+&amp; + ùõΩ ,--_3&amp;#*√ó&amp;(%_*!+&amp; + ùõΩ -12,&amp;_3&amp;#*√ó&amp;(%_*!+&amp; + 1|(Subject: Experiment) , where ùõΩ &amp;(%_*!+&amp; captures the variance accounted for by encoding time (coded as shorter: 0.2s vs. longer: &gt;=1s); ùõΩ -12,&amp;_.*$/0 and ùõΩ ,--_.*$/0 the average semantic association and image similarity among presented study objects, respectively; ùõΩ -12,&amp;_3&amp;#* and ùõΩ ,--_3&amp;#* the semantic association and image similarity between the target and foil objects at test, respectively; and ùõΩ 6√ó&amp;(%_*!+&amp; the interaction effects between encoding time and the respective variables X. All continuous variables (e.g., semantic association or image similarity among all study objects or between target and foil objects at test) were standardized before regression. Model fitting was implemented via the fitglme function in MATLAB. To achieve a focal test on the planned effect related to the semantic factors, we compared the models without and with the two key semantic predictors, namely ùõΩ -12,&amp;_.*$/0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><figDesc>current findings illuminate how human observers optimize the encoding of novel visual information by balancing perceptual inputs and semantic knowledge in a coordinated manner. Articulating how the human brain supports this coordination remains a key issue for future research -an important step toward elucidating how human visual cognition operates in the real world.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 .</head><label>1</label><figDesc>Figure1. Task design and analysis of semantic associations among objects within a visual display. (A) An example trial of the VWM task using real-world objects from different categories with concurrent verbal distraction. Study 1 required participants to directly recall the digits presented at the end of a trial, while Study 2 asked participants to recognize whether the digits presented at the end were the same or different compared to the digit shown at the beginning of the trial. The other parameters and procedures were consistent across both studies. (B) Illustrations of visual displays with objects depicting lower semantic association (left) and higher semantic association (right). The overall average semantic association among objects in a visual display is estimated as the mean association strength across all non-redundant pairs of object labels. The width of the lines connecting the objects represents the pair-wise semantic association strength.</figDesc><graphic coords="31,72.00,73.45,451.00,209.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Semantic associations among objects within the study display predict VWM accuracy but not response key press, under a short encoding time of 0.2s across Study 1 (A) and Study 2 (B). Solid lines represent the mean estimates across trials and participants, dashed lines represent the best fit prediction of the generalized linear mixed model. The shaded areas represent bootstrapped 95% confidence intervals. These results are based on trials with correct digit responses, excluding occasional non-response trials, which account for more than 90% of the data. Results based on all trials without any data exclusion remain consistent with these findings and are summarized in Supplementary Figure S1.</figDesc><graphic coords="32,116.95,78.25,361.10,525.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><figDesc>log-likelihood ratio test: œá 2 (2) = 7.76, p = 0.021 ŒîAIC = -15, log-likelihood ratio test: œá 2 (2) = 19.32, p &lt; 0.0001Note: (a) Model 1 removes the term related to the semantic association among objects within a study display and its interaction effect with encoding time. (b) Model 2 contains all terms. The comparison between these two models therefore informs us about the necessity of adding these semantic predictors in accounting for participants' trial-by-trial VWM task performance. Statistically significant effects with p &lt; 0.05 are bolded in the table.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="36,72.00,138.35,451.00,291.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Overview of the study design for the data involved in our current analyses.Note: (a) Study 1 contains data from<ref type="bibr" target="#b39">Quirk et al. (2020)</ref>. (b) The design here refers to whether the encoding time conditions are blocked or intermixed across blocks within each participant. Exp. = Experiment.</figDesc><table><row><cell>Study</cell><cell></cell><cell>Study 1 a N = 13,750 trials</cell><cell></cell><cell>Study 2 N = 11,880 trials</cell></row><row><cell></cell><cell>Exp. 1a</cell><cell>Exp. 1b</cell><cell>Exp. 1c</cell><cell>Replication experiment</cell></row><row><cell>Encoding time conditions</cell><cell>0.2, 1, 2s</cell><cell>0.2, 2s</cell><cell>0.2, 2s</cell><cell>0.2, 2s</cell></row><row><cell>Number of participants</cell><cell>25</cell><cell>25</cell><cell>25</cell><cell>50</cell></row><row><cell></cell><cell>within-</cell><cell>within-</cell><cell>within-</cell><cell>within-</cell></row><row><cell>Encoding time design b</cell><cell>subject,</cell><cell>subject,</cell><cell>subject</cell><cell>subject,</cell></row><row><cell></cell><cell>blocked</cell><cell>blocked</cell><cell>intermixed</cell><cell>blocked</cell></row><row><cell>Concurrent digit task</cell><cell>silent</cell><cell>half silent, half explicit</cell><cell>silent</cell><cell>silent</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Predicting VWM accuracy across encoding time condition based on generalized linear mixed models using trial-level features</figDesc><table><row><cell></cell><cell></cell><cell cols="4">Study 1 (Quirk et al., 2020)</cell><cell></cell><cell></cell><cell cols="4">Study 2 (Replication)</cell></row><row><cell>Predictors</cell><cell></cell><cell>Model 1 a</cell><cell></cell><cell></cell><cell>Model 2 b</cell><cell></cell><cell></cell><cell>Model 1</cell><cell></cell><cell></cell><cell>Model 2</cell></row><row><cell></cell><cell>Œ≤</cell><cell>SE</cell><cell>p</cell><cell>Œ≤</cell><cell>SE</cell><cell>p</cell><cell>Œ≤</cell><cell>SE</cell><cell>p</cell><cell>Œ≤</cell><cell>SE</cell><cell>p</cell></row><row><cell>enc_time</cell><cell>0.59</cell><cell cols="2">0.045 &lt;0.0001</cell><cell>0.59</cell><cell cols="2">0.045 &lt;0.0001</cell><cell>0.89</cell><cell cols="3">0.052 &lt;0.0001 0.89</cell><cell cols="2">0.052 &lt;0.0001</cell></row><row><cell>VGG_Study</cell><cell>0.011</cell><cell>0.031</cell><cell>0.71</cell><cell cols="2">0.0075 0.031</cell><cell>0.81</cell><cell cols="2">0.028 0.032</cell><cell>0.39</cell><cell cols="2">0.020 0.032</cell><cell>0.53</cell></row><row><cell>VGG_Test</cell><cell cols="2">-0.024 0.031</cell><cell>0.45</cell><cell cols="2">-0.023 0.031</cell><cell>0.47</cell><cell cols="2">-0.061 0.031</cell><cell>0.053</cell><cell cols="2">-0.059 0.031</cell><cell>0.058</cell></row><row><cell>GloVe_Test</cell><cell cols="2">-0.0012 0.031</cell><cell>0.97</cell><cell cols="2">-0.019 0.031</cell><cell>0.55</cell><cell cols="2">0.047 0.031</cell><cell>0.13</cell><cell cols="2">0.017 0.032</cell><cell>0.59</cell></row><row><cell>VGG_Study √ó enc_time</cell><cell>0.026</cell><cell>0.045</cell><cell>0.57</cell><cell cols="2">0.030 0.045</cell><cell>0.51</cell><cell cols="2">0.030 0.052</cell><cell>0.57</cell><cell cols="2">0.036 0.052</cell><cell>0.49</cell></row><row><cell>VGG_Test √ó enc_time</cell><cell cols="2">-0.017 0.045</cell><cell>0.71</cell><cell cols="2">-0.018 0.045</cell><cell>0.69</cell><cell cols="2">-0.061 0.051</cell><cell>0.23</cell><cell cols="2">-0.062 0.051</cell><cell>0.22</cell></row><row><cell>GloVe_Test √ó enc_time</cell><cell>0.038</cell><cell>0.045</cell><cell>0.40</cell><cell cols="2">0.057 0.046</cell><cell>0.21</cell><cell cols="2">-0.043 0.052</cell><cell>0.41</cell><cell cols="2">-0.017 0.053</cell><cell>0.75</cell></row><row><cell>GloVe_Study</cell><cell></cell><cell></cell><cell></cell><cell cols="3">0.087 0.031 0.0054</cell><cell></cell><cell></cell><cell></cell><cell>0.14</cell><cell cols="2">0.032 &lt;0.0001</cell></row><row><cell>GloVe_Study √ó enc_time</cell><cell></cell><cell></cell><cell></cell><cell cols="2">-0.095 0.046</cell><cell>0.038</cell><cell></cell><cell></cell><cell></cell><cell cols="2">-0.12 0.053</cell><cell>0.020</cell></row><row><cell>Model fit metric (AIC)</cell><cell></cell><cell>12640</cell><cell></cell><cell></cell><cell>12636</cell><cell></cell><cell></cell><cell>10138</cell><cell></cell><cell></cell><cell>10123</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Predicting VWM accuracy for each encoding time condition based on generalized linear mixed models using trial-level features Note. Statistically significant effects with p &lt; 0.05 are bolded in the table.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Study 1 (Quirk et al., 2020)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Study 2 (Replication)</cell><cell></cell><cell></cell></row><row><cell>Predictors</cell><cell cols="3">short encoding time (0.2s)</cell><cell cols="3">long encoding time (1-2s)</cell><cell cols="3">short encoding time (0.2s)</cell><cell cols="3">long encoding time (2s)</cell></row><row><cell></cell><cell>Œ≤</cell><cell>SE</cell><cell>p</cell><cell>Œ≤</cell><cell>SE</cell><cell>p</cell><cell>Œ≤</cell><cell>SE</cell><cell>p</cell><cell>Œ≤</cell><cell>SE</cell><cell>p</cell></row><row><cell>VGG_Study</cell><cell>0.0087</cell><cell>0.031</cell><cell>0.78</cell><cell>0.036</cell><cell>0.034</cell><cell>0.28</cell><cell>0.019</cell><cell>0.032</cell><cell>0.55</cell><cell>0.054</cell><cell>0.042</cell><cell>0.20</cell></row><row><cell>VGG_Test</cell><cell>-0.027</cell><cell>0.031</cell><cell>0.39</cell><cell>-0.041</cell><cell>0.033</cell><cell>0.22</cell><cell>-0.055</cell><cell>0.031</cell><cell>0.074</cell><cell>-0.12</cell><cell>0.041</cell><cell>0.0028</cell></row><row><cell>GloVe_Study</cell><cell>0.087</cell><cell cols="3">0.031 0.0050 -0.0066</cell><cell>0.034</cell><cell>0.84</cell><cell>0.14</cell><cell cols="2">0.032 &lt;0.0001</cell><cell>0.011</cell><cell>0.043</cell><cell>0.79</cell></row><row><cell>GloVe_Test</cell><cell>-0.020</cell><cell>0.031</cell><cell>0.51</cell><cell>0.033</cell><cell>0.034</cell><cell>0.34</cell><cell>0.018</cell><cell>0.031</cell><cell>0.57</cell><cell>0.00038</cell><cell>0.044</cell><cell>0.99</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>In Study 1, this effect was numerically in the same direction (Œ≤ = -0.023, SE = 0.031, Z = -0.73, p = 0.47) as that in Study</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>and inBrady &amp; St√∂rmer (2023).</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>This study is made possible by funding support from the <rs type="funder">National Institute of Neurological Disorders and Stroke</rs> (<rs type="grantNumber">K99NS126492</rs>, PI: W. X.).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_qU6Y5kd">
					<idno type="grant-number">K99NS126492</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Materials</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Greater Visual Working Memory Capacity for Visually Matched Stimuli When They Are Perceived as Meaningful</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">E</forename><surname>Asp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>St√∂rmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Brady</surname></persName>
		</author>
		<idno type="DOI">10.1162/jocn_a_01693</idno>
		<ptr target="https://doi.org/10.1162/jocn_a_01693" />
	</analytic>
	<monogr>
		<title level="j">Journal of Cognitive Neuroscience</title>
		<imprint>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Rapid infant learning of syntactic-semantic links</title>
		<author>
			<persName><forename type="first">M</forename><surname>Barbir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Babineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-C</forename><surname>Fi√©vet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Christophe</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.2209153119</idno>
		<ptr target="https://doi.org/10.1073/pnas.2209153119" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2209153119</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hierarchical encoding in visual working memory: Ensemble statistics bias memory for individual items</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Alvarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="384" to="392" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The role of meaning in visual working memory: Realworld objects, but not simple features, benefit from deeper processing</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>St√∂rmer</surname></persName>
		</author>
		<idno type="DOI">10.1037/xlm0001014</idno>
		<ptr target="https://doi.org/10.1037/xlm0001014" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="942" to="958" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Comparing memory capacity across stimuli requires maximally dissimilar foils: Using deep convolutional neural networks to understand visual working memory capacity for real-world objects</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>St√∂rmer</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13421-023-01485-5</idno>
		<ptr target="https://doi.org/10.3758/s13421-023-01485-5" />
	</analytic>
	<monogr>
		<title level="j">Memory and Cognition</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Working memory is not fixedcapacity: More active storage capacity for real-world objects than for simple stimuli</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>St√∂rmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Alvarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences of the United States of America</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">27</biblScope>
			<biblScope unit="page" from="7459" to="7464" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scaling up visual attention and visual working memory to the real world</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>St√∂rmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shafer-Skelton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Chapman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Schill</surname></persName>
		</author>
		<idno type="DOI">10.1016/bs.plm.2019.03.001</idno>
		<ptr target="https://doi.org/10.1016/bs.plm.2019.03.001" />
	</analytic>
	<monogr>
		<title level="j">Psychology of Learning and Motivation -Advances in Research and Theory</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="29" to="69" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The psychophysics toolbox</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Brainard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Spatial Vision</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="433" to="436" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Perception in chess</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">G</forename><surname>Chase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="55" to="81" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The anterior-ventrolateral temporal lobe contributes to boosting visual working memory capacity for items carrying semantic information</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>&amp; Lambon Ralph</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuroimage.2017.12.085</idno>
		<ptr target="https://doi.org/10.1016/j.neuroimage.2017.12.085" />
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">169</biblScope>
			<biblScope unit="page" from="453" to="461" />
			<date type="published" when="2017">2018. December 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">No Fixed Limit for Storing Simple Visual Features: Realistic Objects Provide an Efficient Scaffold for Holding Features in Mind</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>St√∂rmer</surname></persName>
		</author>
		<idno type="DOI">10.1177/09567976231171339</idno>
		<ptr target="https://doi.org/10.1177/09567976231171339" />
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="784" to="793" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Conceptual information of meaningful objects is stored incidentally</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wyble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>St√∂rmer</surname></persName>
		</author>
		<idno type="DOI">10.1037/xlm0001339</idno>
		<ptr target="https://doi.org/10.1037/xlm0001339" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Knowledge is power: How conceptual knowledge transforms visual cognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">R</forename><surname>Olson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="843" to="860" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The magical number 4 in short-term memory: A reconsideration of mental storage capacity</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cowan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="87" to="185" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Large capacity temporary visual memory</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Endress</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Potter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="548" to="565" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">What&apos;s new in visual masking?</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Enns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Di Lollo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="345" to="352" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Chunking mechanisms in human learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Gobet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C R</forename><surname>Lane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Croker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Pine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="236" to="243" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Motivational influences on cognition: Task involvement, ego involvement, and depth of information processing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Golan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Educational Psychology</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="187" to="194" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semantic projection recovers rich human knowledge of multiple object features from word embeddings</title>
		<author>
			<persName><forename type="first">G</forename><surname>Grand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Blank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fedorenko</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41562-022-01316-8</idno>
		<ptr target="https://doi.org/10.1038/s41562-022-01316-8" />
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Familiar interacting object pairs are perceptually grouped</title>
		<author>
			<persName><forename type="first">C</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Hummel</surname></persName>
		</author>
		<idno type="DOI">10.1037/0096-1523.32.5.1107</idno>
		<ptr target="https://doi.org/10.1037/0096-1523.32.5.1107" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1107" to="1119" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The temporal course of visual pattern encoding: Effects of pattern Goodness</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Howe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Brandau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Quarterly Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="607" to="633" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semantic influence on visual working memory of object identity and location</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Jacobs</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cognition.2021.104891</idno>
		<ptr target="https://doi.org/10.1016/j.cognition.2021.104891" />
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">217</biblScope>
			<biblScope unit="page">104891</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Features, objects, action: The cognitive neuropsychology of visual object processing, 1984-2004</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Humphreys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Riddoch</surname></persName>
		</author>
		<idno type="DOI">10.1080/02643290542000030</idno>
		<ptr target="https://doi.org/10.1080/02643290542000030" />
	</analytic>
	<monogr>
		<title level="j">Cognitive Neuropsychology</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="156" to="183" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Familiarity Enhances Visual Working Memory for Faces</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Raymond</surname></persName>
		</author>
		<idno type="DOI">10.1037/0096-1523.34.3.556</idno>
		<ptr target="https://doi.org/10.1037/0096-1523.34.3.556" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="556" to="568" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">On the presentation of semantic information in human long-term memory</title>
		<author>
			<persName><forename type="first">F</forename><surname>Klix</surname></persName>
		</author>
		<editor>H. Ebbinghaus &amp; A. K√∂nig</editor>
		<imprint>
			<date type="published" when="1978">1978</date>
			<biblScope unit="volume">186</biblScope>
			<biblScope unit="page" from="26" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semantic memory: A review of methods, models, and current challenges</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13423-020-01792-x</idno>
		<ptr target="https://doi.org/10.3758/s13423-020-01792-x" />
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="40" to="80" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Interactions between statistical and semantic information in infant language development: Interactions between statistical and semantic information</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Saffran</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1467-7687.2011.01073.x</idno>
		<ptr target="https://doi.org/10.1111/j.1467-7687.2011.01073.x" />
	</analytic>
	<monogr>
		<title level="j">Developmental Science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1207" to="1219" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Visual memory benefits from prolonged encoding time regardless of stimulus type</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Theeuwes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1037/xlm0000847</idno>
		<ptr target="https://doi.org/10.1037/xlm0000847" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning Memory and Cognition</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1998" to="2005" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stable maintenance of multiple representational formats in human visual short-term memory</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Heinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Axmacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xue</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.2006752117</idno>
		<ptr target="https://doi.org/10.1073/pnas.2006752117" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences of the United States of America</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">51</biblScope>
			<biblScope unit="page" from="32329" to="32339" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visual working memory capacity: From psychophysics and neurobiology to individual differences</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Luck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Vogel</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2013.06.006</idno>
		<ptr target="https://doi.org/10.1016/j.tics.2013.06.006" />
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="391" to="400" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Real-world objects are not stored in holistic representations in visual working memory</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">A</forename><surname>Markov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Utochkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Brady</surname></persName>
		</author>
		<idno type="DOI">10.1167/jov.21.3.18</idno>
		<ptr target="https://doi.org/10.1167/jov.21.3.18" />
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The parallel distributed processing approach to semantic cognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Rogers</surname></persName>
		</author>
		<idno type="DOI">10.1038/nrn1076</idno>
		<ptr target="https://doi.org/10.1038/nrn1076" />
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neuroscience</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="310" to="322" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Thematic object pairs produce stronger and faster grouping than taxonomic pairs</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Geng</surname></persName>
		</author>
		<idno type="DOI">10.1037/xhp0001031</idno>
		<ptr target="https://doi.org/10.1037/xhp0001031" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1325" to="1335" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Effects of Aging on Successful Object Encoding: Enhanced Semantic Representations Compensate for Impaired Visual Representations</title>
		<author>
			<persName><forename type="first">L</forename><surname>Naspi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stensholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Karlsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">A</forename><surname>Monge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cabeza</surname></persName>
		</author>
		<idno type="DOI">10.1523/JNEUROSCI.2265-22.2023</idno>
		<ptr target="https://doi.org/10.1523/JNEUROSCI.2265-22.2023" />
	</analytic>
	<monogr>
		<title level="j">The Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">44</biblScope>
			<biblScope unit="page" from="7337" to="7350" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semantic and functional relationships among objects increase the capacity of visual working memory</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>O'donnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Clement</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Brockmole</surname></persName>
		</author>
		<idno type="DOI">10.1037/xlm0000508</idno>
		<ptr target="https://doi.org/10.1037/xlm0000508" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1151" to="1158" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Structuring Knowledge with Cognitive Maps and Cognitive Graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Peer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">K</forename><surname>Brunec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Epstein</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2020.10.004</idno>
		<ptr target="https://doi.org/10.1016/j.tics.2020.10.004" />
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="54" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">GloVe: Global Vectors for Word Representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1162</idno>
		<ptr target="https://doi.org/10.3115/v1/D14-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Contralateral delay activity tracks the influence of Gestalt grouping principles on active visual working memory representations</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>G√∂zenman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Arciniega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Berryhill</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13414-015-0929-y</idno>
		<ptr target="https://doi.org/10.3758/s13414-015-0929-y" />
	</analytic>
	<monogr>
		<title level="j">Attention, Perception, &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2270" to="2283" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Conceptual Short Term Memory in Perception and Thought</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Potter</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2012.00113</idno>
		<ptr target="https://doi.org/10.3389/fpsyg.2012.00113" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">113</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">No evidence for an object working memory capacity benefit with extended viewing time</title>
		<author>
			<persName><forename type="first">C</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C S</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Vogel</surname></persName>
		</author>
		<idno type="DOI">10.1523/ENEURO.0150-20.2020</idno>
		<ptr target="https://doi.org/10.1523/ENEURO.0150-20.2020" />
	</analytic>
	<monogr>
		<title level="j">eNeuro</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Action-related objects influence the distribution of Visuospatial attention</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Humphreys</surname></persName>
		</author>
		<idno type="DOI">10.1080/17470218.2010.520086</idno>
		<ptr target="https://doi.org/10.1080/17470218.2010.520086" />
	</analytic>
	<monogr>
		<title level="j">Quarterly Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="669" to="688" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Understanding Image Memorability</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Rust</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mehrpour</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2020.04.001</idno>
		<ptr target="https://doi.org/10.1016/j.tics.2020.04.001" />
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="557" to="568" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Semantic Meaning Enhances Feature-Binding but not Quantity or Precision of Locations in Visual Working Memory</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gronau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Makovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Memory and Cognition</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1409.1556" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations</title>
		<meeting>the 3rd International Conference on Learning Representations<address><addrLine>ICLR, San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-04-10">2015, April 10</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Introduction to the Theory of Computation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sipser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Cengage Learning</publisher>
		</imprint>
	</monogr>
	<note>3rd ed.</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Prior knowledge shapes older adults&apos; perception and memory for everyday events</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Pitts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Newberry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Elbishari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Bailey</surname></persName>
		</author>
		<idno type="DOI">10.1016/bs.plm.2022.07.005</idno>
		<ptr target="https://doi.org/10.1016/bs.plm.2022.07.005" />
	</analytic>
	<monogr>
		<title level="m">Psychology of Learning and Motivation</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="233" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Investigating the effects of perceptual complexity versus conceptual meaning on the object benefit in visual working memory</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M L</forename><surname>Thibeault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Stojanoski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Emrich</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13415-024-01158-z</idno>
		<ptr target="https://doi.org/10.3758/s13415-024-01158-z" />
	</analytic>
	<monogr>
		<title level="j">Cognitive, Affective, &amp; Behavioral Neuroscience</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="453" to="468" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Storage in visual working memory recruits a content-independent pointer system</title>
		<author>
			<persName><forename type="first">W</forename><surname>Thyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">N V</forename><surname>S√°nchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Awh</surname></persName>
		</author>
		<ptr target="https://osf.io/uhbx5/" />
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Semantic influences on episodic memory distortions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tompary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Thompson-Schill</surname></persName>
		</author>
		<idno type="DOI">10.1037/xge0001017</idno>
		<ptr target="https://doi.org/10.1037/xge0001017" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">150</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1800" to="1824" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The meaning and structure of scenes</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L H</forename><surname>V√µ</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.visres.2020.11.003</idno>
		<ptr target="https://doi.org/10.1016/j.visres.2020.11.003" />
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">181</biblScope>
			<biblScope unit="page" from="10" to="20" />
			<date type="published" when="2021-01">2021. January</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Visual working memory capacity for objects from different categories: A face-specific maintenance effect</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Thompson</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cognition.2008.06.006</idno>
		<ptr target="https://doi.org/10.1016/j.cognition.2008.06.006" />
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="719" to="731" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Memorability of words in arbitrary verbal associations modulates memory retrieval in the anterior temporal lobe</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Bainbridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Inati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Zaghloul</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41562-020-0901-2</idno>
		<ptr target="https://doi.org/10.1038/s41562-020-0901-2" />
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="937" to="948" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Discrete item-based and continuous configural representations in visual short-term memory</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1080/13506285.2017.1339157</idno>
		<ptr target="https://doi.org/10.1080/13506285.2017.1339157" />
	</analytic>
	<monogr>
		<title level="j">Visual Cognition</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="21" to="33" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Dissociations of the number and precision of visual shortterm memory representations in change detection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13421-017-0739-7</idno>
		<ptr target="https://doi.org/10.3758/s13421-017-0739-7" />
	</analytic>
	<monogr>
		<title level="j">Memory and Cognition</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1423" to="1437" />
			<date type="published" when="2017">2017b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Familiarity increases the number of remembered Pok√©mon in visual short-term memory</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13421-016-0679-7</idno>
		<ptr target="https://doi.org/10.3758/s13421-016-0679-7" />
	</analytic>
	<monogr>
		<title level="j">Memory and Cognition</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="677" to="689" />
			<date type="published" when="2017">2017c</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Familiarity speeds up visual short-term memory consolidation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1037/xhp0000355</idno>
		<ptr target="https://doi.org/10.1037/xhp0000355" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1207" to="1221" />
			<date type="published" when="2017">2017d</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Familiarity speeds up visual short-term memory consolidation: Electrophysiological evidence from contralateral delay activities</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1162/jocn_a_01188</idno>
		<ptr target="https://doi.org/10.1162/jocn_a_01188" />
	</analytic>
	<monogr>
		<title level="j">Journal of Cognitive Neuroscience</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Pre-existing long-term memory facilitates the formation of visual short-term memory</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.4324/9781003158134-6</idno>
		<ptr target="https://doi.org/10.4324/9781003158134-6" />
	</analytic>
	<monogr>
		<title level="m">Visual Memory</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Brady</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Bainbridge</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="84" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Effortfulness of visual working memory: Gauged by physical exertion</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1037/xge0001391</idno>
		<ptr target="https://doi.org/10.1037/xge0001391" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Pupillary evidence reveals the influence of conceptual association on brightness perception</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13423-023-02258-6</idno>
		<ptr target="https://doi.org/10.3758/s13423-023-02258-6" />
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page">4228</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Perceptual encoding benefit of visual memorability on visual memory formation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cognition.2024.105810</idno>
		<ptr target="https://doi.org/10.1016/j.cognition.2024.105810" />
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">248</biblScope>
			<biblScope unit="page">105810</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">The Binding Problem 2.0: Beyond Perceptual Features</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lau</surname></persName>
		</author>
		<idno type="DOI">10.1111/cogs.13244</idno>
		<ptr target="https://doi.org/10.1111/cogs.13244" />
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">13244</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
