<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unimodal speech perception predicts stable individual differences in audiovisual benefit for phonemes, words and sentences a</title>
				<funder ref="#_MxGvGNx">
					<orgName type="full">Cognition and Brain Sciences Unit</orgName>
				</funder>
				<funder>
					<orgName type="full">Cambridge Trust Scholarship</orgName>
				</funder>
				<funder ref="#_GbQxNT4">
					<orgName type="full">MRC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Jacqueline</forename><surname>Von Seth</surname></persName>
							<email>jacqueline.vonseth@mrc-cbu.cam.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Medical Research Council Cognition and Brain Sciences Unit</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<addrLine>15 Chaucer Road</addrLine>
									<postCode>CB2 7EF</postCode>
									<settlement>Cambridge</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Máté</forename><surname>Aller</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Medical Research Council Cognition and Brain Sciences Unit</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<addrLine>15 Chaucer Road</addrLine>
									<postCode>CB2 7EF</postCode>
									<settlement>Cambridge</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matthew</forename><forename type="middle">H</forename><surname>Davis</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Medical Research Council Cognition and Brain Sciences Unit</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<addrLine>15 Chaucer Road</addrLine>
									<postCode>CB2 7EF</postCode>
									<settlement>Cambridge</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unimodal speech perception predicts stable individual differences in audiovisual benefit for phonemes, words and sentences a</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">42EF166E124195BA42223A87915E90EC</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-23T06:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Individuals differ substantially in the benefit they can obtain from visual cues during speech perception. Here, 113 normally-hearing participants between ages 18 and 60 completed a threepart experiment investigating the reliability and predictors of individual audiovisual benefit for acoustically degraded speech. Audiovisual benefit was calculated as the relative intelligibility (at the individual-level) of approximately matched (at the group-level) auditory-only and audiovisual speech for materials at three levels of linguistic structure: meaningful sentences, monosyllabic words, and consonants in minimal syllables. This measure of audiovisual benefit was stable across sessions and materials, suggesting that a shared mechanism of audiovisual integration operates across levels of linguistic structure. Information transmission analyses suggested that this may be related to simple phonetic cue extraction: sentence-level audiovisual benefit was reliably predicted by the relative ability to discriminate place of articulation at the consonant-level. Finally, while unimodal speech perception was related to cognitive measures (matrix reasoning, vocabulary) and demographics (age, gender), audiovisual benefit was predicted uniquely by unimodal speech perceptual abilities: Better lipreading ability and subclinically poorer hearing (speech reception thresholds) independently predicted enhanced audiovisual benefit. This work has implications for best practices in quantifying audiovisual benefit and research identifying strategies to enhance multimodal communication in hearing loss.</p><p>a Portions of this work were presented in "Inter-individual variability and correlates of audiovisual speech benefit in behaviour and MEG" at the 15 th Annual Meeting of the Society for the Neurobiology of Language in Marseille, France, October 2023, in "Stable individual differences in audiovisual benefit for speech perception: Exploring the role of perceptual and cognitive abilities" at the 15th Speech-in-Noise workshop in Potsdam, Germany, January 2024, in "Perceptual and cognitive predictors of stable individual differences in audiovisual and unimodal speech perception" at the EPS Meeting in Nottingham, UK, April 2024.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>Speech production is inherently linked to observable motion in the face: jaw, lips, and tongue of the speaker. Throughout the course of our lives, we acquire substantial experience with these signals during face-to-face conversation. It is well-established that when the acoustic speech signal is degraded, speech cues encoded in facial movements can provide a significant benefit to speech perception <ref type="bibr" target="#b118">(Sumby and Pollack, 1954</ref>). Yet, despite the ubiquity of these signals in our everyday perceptual experience, not everyone benefits equally. Previous work has reported substantial individual differences in measures of the audiovisual advantage across a wide range of speech materials: from minimal non-sense syllables to meaningful sentences <ref type="bibr" target="#b1">(Aller et al., 2022;</ref><ref type="bibr">Grant et al., 1998;</ref><ref type="bibr">Grant and Seitz, 1998;</ref><ref type="bibr" target="#b112">Sommers et al., 2005;</ref><ref type="bibr" target="#b126">Tye-Murray et al., 2016;</ref><ref type="bibr" target="#b128">Van et al., 2014;</ref><ref type="bibr" target="#b129">Van Engen et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. What accounts for individual differences in audiovisual speech perception?</head><p>The reasons for this variability remain poorly understood: Only measures of lipreading ability have been reliably linked to individual differences in audiovisual speech perception (see <ref type="bibr">Bernstein, 2022, for review)</ref>. Some research has also suggested the degree of acquired hearing loss (HL), in mild-to-moderate hearing impaired (HI) listeners, may predict both better lipreading ability (e.g. <ref type="bibr" target="#b11">Bernstein et al., 2000;</ref><ref type="bibr" target="#b117">Suess et al., 2022;</ref><ref type="bibr" target="#b121">Tillberg et al., 1996)</ref> and enhanced audiovisual integration for speech perception (e.g. <ref type="bibr" target="#b2">Altieri &amp; Hudock, 2014;</ref><ref type="bibr" target="#b94">Puschmann et al., 2019)</ref>. However, this effect is not always found <ref type="bibr" target="#b104">(Rosemann and Thiel, 2018;</ref><ref type="bibr" target="#b113">Spehar et al., 2008;</ref><ref type="bibr">Tye-Murray et al., 2007a)</ref> and substantial individual differences remain, meaning that too few of those with age-related hearing loss can use visual speech to mitigate the negative consequences of HL (e.g. <ref type="bibr" target="#b93">Punch et al., 2019)</ref>.</p><p>Additionally, lipreading ability and audiovisual integration for speech perception are notoriously difficult to train <ref type="bibr" target="#b91">(Preminger and Ziegler, 2008;</ref><ref type="bibr" target="#b101">Richie and Kewley-Port, 2008)</ref>. The small improvements in phoneme-level recognition obtained in some lipreading programmes may not generalise to more natural or audiovisual speech stimuli (see <ref type="bibr">Bernstein et al., 2022, for review)</ref>.</p><p>Explanations for individual differences in lipreading and audiovisual speech perception have also been sought in terms of non-speech cognitive abilities. <ref type="bibr" target="#b35">Feld and Sommers (2009)</ref> suggested that processing speed and visuo-spatial working memory may account for a large amount of the substantial individual variability in lipreading ability, in both younger and older adults. They argued that if fundamentally stable cognitive traits underlie individual differences in lipreading and audiovisual speech perception, this may explain why training programmes often show limited success. It is well-established that cognitive abilities play a significant role in auditory-only speech perception in noise <ref type="bibr" target="#b0">(Akeroyd, 2008;</ref><ref type="bibr" target="#b32">Dryden et al., 2017;</ref><ref type="bibr" target="#b51">Heinrich et al., 2015)</ref>, especially when the signal is degraded <ref type="bibr" target="#b89">(Pichora-Fuller et al., 1995)</ref>. However, for measures of audiovisual speech perception, and audiovisual integration for speech perception specifically, the picture is less clear.</p><p>Dual task demands seem to impair performance on audiovisual speech tasks <ref type="bibr" target="#b37">(Fraser et al., 2010;</ref><ref type="bibr">Alsius et al., 2005;</ref><ref type="bibr" target="#b42">2014;</ref><ref type="bibr">Buchan &amp; Munhall, 2012)</ref>. However, susceptibility to the McGurk effect <ref type="bibr" target="#b80">(McGurk &amp; MacDonald, 1976)</ref>, which is frequently used as a measure of audiovisual integration for speech perception, is not related to processing speed, working memory, or attentional control <ref type="bibr" target="#b19">(Brown et al., 2018)</ref>. Similarly, visual enhancement of speech perception (i.e. enhanced report for auditory-visual compared to auditory-only speech) in school-aged children is also not predicted by performance on cognitive tasks measuring vocabulary knowledge, working memory or attentional control <ref type="bibr" target="#b64">(Lalonde and McCreery, 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Quantifying individual differences in audiovisual benefit</head><p>A key challenge in this line of research is the lack of reliable measures of audiovisual integration for speech perception. The lack of correlations among different audiovisual integration measures, including both speech-and non-speech illusions have been taken to suggest that only measures derived from congruent speech materials may be useful in predicting an individual's ability to use visual cues in ecological conditions <ref type="bibr" target="#b134">(Wilbiks et al., 2022</ref>; but see <ref type="bibr" target="#b31">Dong et al., 2024;</ref><ref type="bibr" target="#b78">Magnotti et al., 2020;</ref><ref type="bibr"></ref> for arguments that susceptibility to the McGurk effect may be related to audiovisual speechin-noise perception). Previous research has most frequently compared unimodal and auditory-visual performance at the same level of acoustic clarity or background noise, taking the auditory condition as a baseline (hereafter Visual enhancement). The choice of audiovisual integration measure has significant implications regarding the conclusions that may be drawn, for example, concerning the question of whether audiovisual integration for speech perception declines, or increases with age <ref type="bibr" target="#b30">(Dias et al., 2021;</ref><ref type="bibr" target="#b112">Sommers et al., 2005;</ref><ref type="bibr">Tye-Murray et al., 2007a)</ref>. However, even within the same measure establishing stable individual differences across different speech materials has proven difficult: visual enhancement of consonant report does not seem to predict visual enhancement for word or sentence report tasks <ref type="bibr">(Grant and Seitz, 1998;</ref><ref type="bibr" target="#b112">Sommers et al., 2005)</ref>, whereas individual differences in unimodal speech perception are highly related across levels of linguistic structure <ref type="bibr">(Grant et al., 1998;</ref><ref type="bibr" target="#b53">Humes et al., 1994;</ref><ref type="bibr" target="#b112">Sommers et al., 2005</ref>; but for lipreading ability see: <ref type="bibr" target="#b11">Bernstein et al., 2000)</ref>.</p><p>These inconsistencies pose problems for traditional models of audiovisual speech perception, which propose a separate stage of multisensory integration for speech, which should account for a significant amount of variability in audiovisual outcomes <ref type="bibr" target="#b2">(Altieri and Hudock, 2014;</ref><ref type="bibr">Grant et al., 1998;</ref><ref type="bibr" target="#b54">Huyse et al., 2014)</ref>. If individual differences in audiovisual speech perception are related to a domain-general audiovisual integration ability, measures of visual enhancement should generalise across materials and levels of linguistic structure. In a review of shortcomings of the McGurk effect as a measure of audiovisual speech integration ability, <ref type="bibr" target="#b129">Van Engen et al., (2017)</ref> suggested a potential explanation for the lack of correlations: could audiovisual integration rely on different mechanisms at different levels of linguistic structure (e.g. minimal syllables versus meaningful sentences)? In line with this, <ref type="bibr" target="#b111">Sommers (2021)</ref> proposed that audiovisual integration for speech perception may not be conceived of as an individual differences measure in the traditional sense: unlike working memory or processing speed, which may be tapped into by different tasks. However, shortcomings of the currently predominant visual enhancement measure provide an alternative explanation for these inconsistent results: (1) it may not adequately capture integration (see <ref type="bibr">Sommers, 2021, for review;</ref><ref type="bibr" target="#b112">Sommers et al., 2005;</ref><ref type="bibr" target="#b125">Tye-Murray et al., 2010)</ref>, ( <ref type="formula">2</ref>) is confounded by differences in intelligibility between conditions, and (3) is susceptible to both ceiling-and floor-effects, truncating the individual variability that is measured. So far, however, the development of more sophisticated capacity or efficiency measures to model individual differences has not yielded promising results in terms of predicting the ability to use visual cues at the sentence-level <ref type="bibr" target="#b2">(Altieri and Hudock, 2014;</ref><ref type="bibr" target="#b15">Blamey et al., 1989;</ref><ref type="bibr" target="#b18">Braida, 1991;</ref><ref type="bibr">Grant and Seitz, 1998;</ref><ref type="bibr" target="#b79">Massaro and Cohen, 1983;</ref><ref type="bibr" target="#b112">Sommers et al., 2005;</ref><ref type="bibr" target="#b134">Wilbiks et al., 2022)</ref>.</p><p>Here, we apply a relatively simple measure of individual differences in audiovisual speech perception, following <ref type="bibr" target="#b1">Aller et al. (2022)</ref>, based on approaches estimating speech-reception thresholds at 50% accuracy (e.g. <ref type="bibr" target="#b77">Macleod &amp; Summerfield, 1987)</ref>. By comparing audiovisual and auditory-only speech perception in materials approximately equated for intelligibility, we avoid confounds introduced by differences in intelligibility between conditions, as well as floor-and ceiling-effects which appear in the visual enhancement measure depending on which level of acoustic clarity is chosen for experimental conditions. We also assess whether our measure is stable across sessions (and items), as well as levels of linguistic structure (and speakers).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. The current study</head><p>Rapid advances in software tools for online experiments in recent years <ref type="bibr" target="#b27">(De Leeuw et al., 2023;</ref><ref type="bibr" target="#b67">de Leeuw, 2015;</ref><ref type="bibr" target="#b103">Rodd, 2024)</ref>, combined with online participant panels allow us to quickly collect reliable data from a balanced sample across age groups and gender. These include older participants with more diverse educational backgrounds who may not be easily targeted by university-based recruitment. This is especially useful for individual differences research, which require larger samples to achieve sufficient power in testing for cross-condition correlations (for example, comparing audiovisual benefit across levels of linguistic structure in consonant, word and sentence report tasks). In the current study, we aimed at quantifying the degree of audiovisual benefit using an intelligibility-matched measure in normally-hearing, working-age adults (18-60 years). We measured the relative intelligibility of matched auditory-only and audiovisual speech for materials at three levels of linguistic structure: meaningful sentences, monosyllabic words, and minimal consonant-vowel syllables. Isolating individual variability across speech materials, we tested the degree to which variability in lipreading ability, hearing status, linguistic and cognitive ability alongside demographic variables (age and self-reported gender) explain individual differences in unimodal outcomes and audiovisual benefit in speech perception.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Participants</head><p>142 British English native speakers were recruited via Prolific Academic (<ref type="url" target="www.prolific.com">www.prolific.com</ref>).</p><p>Participants provided informed consent using an online consent form approved by the Cambridge Psychology Research Ethics Committee (application number PRE.2022.056). Participants were screened at the beginning of each session using Wood's headphone test <ref type="bibr" target="#b138">(Woods, 2017)</ref>, excluding 14 participants who were compensated for their time.</p><p>113 participants successfully completed all audiovisual speech perception tasks across two sessions and 103 participants (55 female, 48 male, age range = 18-60, mean age ± SD = 38.54 ± 11.55) successfully completed all tasks across three sessions, not meeting any outlier exclusion criteria. The target sample size (n=101) was estimated using G*Power 3.1.9.4 <ref type="bibr" target="#b34">(Faul et al., 2009)</ref> based on pooled effect sizes (weighted for sample size, Hedge's g) from previous studies investigating the reliability of lipreading ability and visual enhancement across speech materials (g = .26), and Pearson's product correlations of audiovisual benefit and enhancement measures with lipreading ability (g = .76) and hearing status (g = .33) to achieve power ≥ 0.8 to test each of our three main hypotheses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Outlier exclusion and data quality</head><p>We administered self-report questionnaires of attention, technical difficulties, and task comprehension to identify any issues that might require participants to be excluded after each speech perception task and the cognitive and hearing tests. For any ratings of &gt;3 (on 6-point Likert scales for 1) attention, 2) technical difficulties and b) clarity of task instructions) typed responses were manually reviewed (n=22). Data from participants with a rating of &gt;3 and no typed responses or responses substantiating difficulties were excluded (n=6), but we decided to retain participants whose responses indicated task comprehension, engagement and attention (for example, correctly describing task instructions) while acknowledging that they found the task difficult. Additional preset outlier exclusion criteria for the cognitive and speech perception tasks included: &lt;80% in catch trials (n=4) and lapse rate of &gt;0.0625 (n=2) as well as performance of 1.5 interquartile ranges (IQRs) below the first or above the third quartile (n=5). Additional data quality checks leading to the exclusion of individual trials (but not participants) are detailed in individual task descriptions below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Stimuli</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">General description</head><p>Consonants in minimal syllables, monosyllabic words, and meaningful sentences were presented to participants in separate audiovisual speech perception tasks across two sessions. Video recordings were drawn from <ref type="bibr" target="#b1">Aller et al. (2022)</ref>, <ref type="bibr" target="#b62">Krason et al. (2023)</ref> and <ref type="bibr" target="#b90">Pimperton et al. (2019)</ref> for sentence-, word-, and consonant-level materials, each produced by a different level. Videos were cropped to show only the face of the speakers, who performed minimal head movements. Example images and links to video recordings can be found in the original publications.</p><p>We manipulated the availability of visual speech cues and the degree of acoustic clarity using noise vocoding <ref type="bibr" target="#b107">(Shannon et al., 1995)</ref>  conditions AVlow and AOhigh for intelligibility and to ensure that they fall at an intermediate level of intelligibility (40-60%) in order to avoid floor-or ceiling effects in either the audiovisual or the auditory-only condition (see Figure <ref type="figure">1</ref>). Mixing proportions p were chosen based on visual inspection and psychometric curves fit to pilot data collected for each task (n=9 for reporting isolated words and forced-choice identification of minimal syllables, 8 female and 1 male, mean age ± SD = 31.11 ± 5.15) using the quickpsy package in R <ref type="bibr" target="#b69">(Linares and López-Moliner, 2016)</ref>. This resulted in p=0 (low clarity) and p=1 (high clarity) for consonants, p=0.47 (low clarity) and p=1</p><p>(high clarity) for words. Additionally, we retained the p=0.2 and p=0.7 conditions for sentences based on a previous pilot study in <ref type="bibr" target="#b1">Aller et al. (2022)</ref>. The difference between these measures was intended to show a mean of 0 and a spread of positive and negative values indicating the degree of audiovisual benefit obtained by individual participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Item characteristics a. Consonants</head><p>Recordings of 20 consonants in minimal syllables followed by /ə/ spoken by a single female speaker, with a neutral (mouth closed) start and end position were presented in the consonant identification tasks. Participants were instructed to classify the sounds as if they occurred at the start of words, with each consonant followed by a variation of /aed/, /aet/, /ɛt/, or /ɛd/, or a closely related syllable (with the exception of "thaw" for/θ/), to form a real monosyllabic word.</p><p>These words, with the initial sound highlighted made up the closed-set response options for the task. Clear speech auditory-only recordings of the same sounds produced by a male speaker were presented in the practice phase to familiarise participants with the isolated speech sounds and their corresponding word contexts, while preventing participants from learning lip configurations associated with each sound. Identical recordings of the 20 consonants, presented once in each of the five conditions, were repeated across both sessions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>b. Words</head><p>Video recordings of 200 common, monosyllabic words, spoken in isolation by a single female speaker were selected from a set of items previously used in <ref type="bibr" target="#b62">Krason et al. (2023)</ref> and <ref type="bibr" target="#b61">Krason et al. (2022)</ref>. Orthographic responses were cleaned by removing spaces and non-alphabetic symbols, as well as responses where participants indicated they could not identify the target word (by typing "dk"). Responses were then scored using the Levenshtein ratio calculated using the fast Levenshtein edit distance implemented in the PanPhon package for Python <ref type="bibr" target="#b86">(Mortensen et al., 2016)</ref>.</p><p>The edit distance measure for each target stimulus-response pair was expressed as a percentage of the length of the longer string, and then subtracted from 100 in order to convert the metric into a ratio measure of word report accuracy. Previous work has indicated that using the Levenshtein distance to automatically score orthographic transcriptions is highly consistent with manually scored responses (e.g. <ref type="bibr">Themistocleus et al., 2020)</ref>, as spelling errors are not unduly penalised when manual scoring is not feasible due to the large number of responses to be evaluated (see <ref type="bibr">Baese-Berk et al., 2023)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>c. Sentences</head><p>Recordings of 100 meaningful sentences (number of words: M±SD=13.97±2.20, length:</p><p>M±SD=5.11±0.71 seconds), a subset of the stimuli used in <ref type="bibr" target="#b1">Aller et al. (2022)</ref>, were presented across two sessions in the sentence-level word report tasks. Participant responses were cleaned in the same way as responses in the isolated word report task, removing extraneous spaces and symbols. Responses were scored using the Token Sort Ratio (TSR) fuzzy logic string matching metric <ref type="bibr" target="#b17">(Bosker, 2021)</ref> as implemented in the FuzzyWuzzy Python package <ref type="bibr">(SeatGeek Inc, 2014)</ref>.</p><p>We decided to use fuzzy string-matching metrics to score word report over more conservative item-correct measures as they provide a more fine-grained measure of individual differences in perceptual recognition, allowing for partial matches and not unduly penalising spelling errors and homophones (compared to scoring the more stringent %words correct as originally pre-registered, see <ref type="bibr" target="#b17">Bosker, 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Counterbalancing and randomisation</head><p>Item-session and item-condition assignments were counterbalanced across participants for the word-and sentence-level audiovisual speech perception tasks. Individual items were randomly assigned to sessions, resulting in 5 splits of items for each task. Ten item-condition assignments were created across all splits, and each participant was assigned to a split (item-session assignment)</p><p>and version (item-version assignment). Additionally, in each audiovisual speech perception task, the presentation order of individual items was shuffled for each participant while ensuring an equal number of conditions per block (2 blocks in each task for consonants, 5 blocks in each task for words and sentences) was retained.</p><p>C. Procedure</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">General Procedure</head><p>Participants completed three experimental sessions over a period of 2-3 weeks, with at least 7 days between the first two sessions. All tasks were coded in jsPsych versions 6.3 or 7.3 <ref type="bibr" target="#b27">(De Leeuw et al., 2023)</ref>. At the start of each session participants adjusted their volume to a comfortable level and completed a headphone test using anti-phase sounds designed by <ref type="bibr" target="#b138">Woods et al. (2017)</ref> to ensure that they were all wearing binaural headphones.</p><p>In the first two sessions, participants performed three audiovisual speech perception tasks with materials at one of three levels of linguistic structure (sentences, words and consonants in minimal syllables) with items presented in 5 different conditions varying in the availability of visual speech cues and acoustic clarity. In both sessions, completion of these tasks was preceded by a period of vocoded speech training, in which 10 sentences of degraded speech were presented in auditoryand auditory-visual conditions (mixing proportions p varying from 0.2-1), each preceded by a written transcription of the sentence. This was to ensure that the initial rapid perceptual learning that occurs with vocoded speech was completed by the start of the main experiment <ref type="bibr" target="#b110">(Sohoglu and Davis, 2016)</ref>. At the start of session 1, participants additionally completed a short language questionnaire which screened for language and hearing difficulties, non-native British English speakers and collected (voluntarily disclosed) demographic data on age, gender identity, regional accent familiarity, proficiency in languages other than English as well as significant periods of time (&gt;6 months) spent abroad.</p><p>In session 3, participants completed the Digits-in-Noise Test <ref type="bibr" target="#b109">(Smits et al., 2013)</ref> to assess individual speech-in-noise perception thresholds and completed section one of the Abbreviated Profile of Hearing Aid Benefit (APHAB) <ref type="bibr" target="#b25">(Cox, 1997)</ref>. Additionally, the Listen-Up Task <ref type="bibr" target="#b26">(Davis et al., 2019)</ref> was administered to assess phonological discrimination thresholds. The Matrix Reasoning Task (MaRs) <ref type="bibr" target="#b24">(Chierchia et al., 2019)</ref>, a non-proprietary version of the Raven's progressive matrices test, and the Spot-the-Word (STW) lexical decision task <ref type="bibr" target="#b6">(Baddeley et al., 1993)</ref>, were used to assess domain-general and verbal IQ, respectively. The order of tasks was randomised for each participant.</p><p>Finally, at the end of each audiovisual speech perception task, and at the end of each of the three sessions participants were asked to rate their comprehension of the instructions, ability to pay attention and technical difficulties during the task or session on a scale of 1-6 and provided with a textbox to provide further details if any issues that had occurred.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Audiovisual speech perception tasks</head><p>For each task, participants first completed a clear speech practice block, introducing the paradigm, and for consonants, the target stimuli. For the sentence-level and word-level tasks, participants first viewed three example items and were then asked to type back the words they understood into a textbox to the best of their ability. They were encouraged to guess if unsure and warned that some of the trials may appear very difficult. They then completed 5 blocks of word report tasks in sentence-and word-level audiovisual speech tasks consisting of 50 and 100 unique items and trials per session, respectively. Each item (sentence or word) was only presented once to each participant across the entire experiment. At the level of consonants, participants performed a forced-choice task, and were asked to select the target consonant out of all 20 possible options, presented in the context of a monosyllabic real word. Participants heard a male speaker pronouncing each target consonant during practice trials and were provided with feedback on their responses, to ensure participants could correctly match each consonant to the answer options available. After this, they completed 2 blocks of AFC trials each of which contained one presentation of each item per each of the five conditions, for a total number of 100 trials per session.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Subjective hearing experience</head><p>The first section of the revised Form A of the Abbreviated Profile of Hearing Aid Benefit (APHAB) <ref type="bibr" target="#b25">(Cox, 1997)</ref> was administered to assess individual participant's subjective experience of the frequency of hearing and speech-in-noise perception difficulties in everyday life. The APHAB includes 24 items which can be summarised into four subscales relating to hearing difficulties in everyday situations: Ease of Communication (EC), Reverberation (RV), Background Noise (BN)</p><p>and Aversiveness (AV). The overall APHAB score for each individual participant was derived from the mean of the first three of these scales. Participants are asked to rate statements such as: "I miss a lot of information when I'm listening to a lecture" (BN) from Never (1% of the time) to Always (99% of the time). Six items were scored in reverse order, where 99% indicates no difficulties and 1% severe difficulties. Participant's attention was drawn to that fact to ensure they answer each item carefully. Higher overall scores indicate more substantial hearing difficulties. This task was included as previous work had indicated that when including participants with known hearing loss, scores correlate with individual lipreading ability <ref type="bibr" target="#b117">(Suess et al., 2022)</ref>. Additionally, including a subjective measure may diverge from an objective measure of hearing difficulties especially in mild cases or early-onset while perceived listening effort may predict everyday face-viewing behaviour which could be linked to lipreading ability or individual audiovisual benefit <ref type="bibr" target="#b94">(Puschmann et al., 2019;</ref><ref type="bibr" target="#b99">Rennig et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Speech reception threshold</head><p>The Digits-in-Noise (DiN) test is an established measure of speech-in-noise reception thresholds (SRT), first introduced by <ref type="bibr" target="#b109">Smits et al. (2013)</ref> as a task to screen SRTs over the telephone.</p><p>In our implementation we followed the procedure described and validated in <ref type="bibr" target="#b109">Smits et al. (2013)</ref>.</p><p>Digit triplets consisting of a randomly chosen combination sampled from digits 0-9 were presented in long-term average speech-spectrum noise, modulated via a 1-up, 1-down adaptive procedure with a step size of 2dB. In each of the 24 trials, participants were asked to report all three heard digits using a number pad presented on the screen, and answers were scored as triplets. SRTs are calculated as the mean SNR (dB) in the final 20 trials. The DiN was chosen as it has a high testretest reliability, correlates significantly with pure tone audiometry thresholds and is highly sensitive to mild-moderate hearing loss <ref type="bibr" target="#b127">(Van den Borre et al., 2021)</ref>, which is indicated by an SRT of -7.4 dB SNR or above <ref type="bibr" target="#b109">(Smits et al., 2013)</ref>. Since none of our participants reported a clinical diagnosis of hearing loss, and our online version is not yet sufficiently validated, we refer to any differences in hearing measured here as "subclinical".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Categorical speech perception</head><p>Individual phonological speech discrimination thresholds were measured using the Listen-Up Task <ref type="bibr" target="#b26">(Davis et al., 2019)</ref>. Monosyllabic, common target words (e.g. "fan" in a female voice) were accompanied by a picture of the target word, followed by presentation of two real-word audiomorphed stimuli using the target word and a minimal-pair foil ("fan" and "van" spoken by a male voice). Participants were asked to indicate which of the two words was closer to the target word.</p><p>The acoustic difference between both words was progressively reduced, using an adaptive procedure (3 down: 1 up, <ref type="bibr" target="#b68">Levitt, 1971)</ref>. Trials started with a 100% acoustic difference between the foil stimuli (i.e. resynthesised versions of the original speech) and the acoustic form of each token was reduced by 16% following three correct responses (i.e. 84% and 16% tokens were presented, subsequently reducing to 68% and 32% etc).</p><p>Step size was reduced by 1/sqrt(2) at each turning point. Therefore, the difficulty of this two-alternative forced choice task (2AFC) increased progressively throughout the task until step size reached 2% and performance converged on thresholds for distinguishing target and foil spoken words. The outcome measure is the minimum proportion of acoustic difference (PADRI) between speech sounds allowing an individual to identify the spoken words with 79.4% accuracy (PADRI threshold). Each participant completed two blocks of the Listen Up task and the PADRI threshold was averaged across two blocks. Where performance in only one of the blocks met outlier exclusion criteria, the PADRI threshold estimated in the other block was retained. The inclusion of the Listen-Up task was not originally</p><p>preregistered, but we decided to include it as an exploratory predictor as it provides a brief complementary test of participant-level variability in speech perception in addition to auditory perceptual acuity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Verbal IQ</head><p>Linguistic skill was assessed using the Spot-the-Word (STW) lexical decision task, developed by <ref type="bibr" target="#b6">Baddeley et al. (1993)</ref>. In the STW task, participants were presented with 60 pairs of words and non-words and asked to identify each real word in a pair. Real words ranged from frequent to obscure words, whereas non-words were plausible and followed English orthographic conventions.</p><p>A practice trial consisting of 6 word-nonword pairs preceded the task, and participants were instructed to complete each trial page consisting of 6 word-non word pairs as quickly as possible.</p><p>Vocabulary knowledge as a proxy measure of verbal IQ was scored as % correct identification of the real word in non-word-word pairs. Participants were re-assured that perfect performance in this task was not expected. Additionally, trials for which reaction times significantly exceeded the expected completion time (1.5 IQRs &gt; Q3 across all participants) were excluded from analyses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Non-verbal IQ</head><p>Domain-general cognitive abilities were assessed using the MaRs reasoning task (<ref type="url" target="https://sites.google.com/site/blakemorelab/research/mars-ib">https://sites.google.com/site/blakemorelab/research/mars-ib</ref>), with individual items drawn from the open-source MaRs-IB item bank <ref type="bibr" target="#b24">(Chierchia et al., 2019)</ref>. Each item of the MaRs-IB was made up of a 3x3 matrix, with eight cells containing abstract shapes. Participants were asked to "complete the puzzle" by selecting the missing shape from four options presented below within 30s of trial onset, indicated by a countdown presented for the entire 30s. Relationships between items may be uni-or three-dimensional, and relate to the colour, shape and positions between cells.</p><p>Participants saw up to 80 items, depending on how many items they manage to complete within 8 minutes at which time the task finished automatically. All participants were shown the same randomly sampled items and distractor types (we used a paired difference strategy for all items) in identical order to ensure that individual differences in task performance did not arise from itemlevel variation in difficulty <ref type="bibr" target="#b141">(Zorowitz et al., 2024)</ref>.</p><p>Trials with rapid responses (&lt;250ms) were excluded from analyses. We computed the measures described in Chierchia et al. ( <ref type="formula">2019</ref>): (i) productivity (absolute number of puzzles completed), (ii) median response time (RT) for correctly completed items, (iii) accuracy (items correct divided by items attempted), and (iv) inverse efficiency (median response times divided by accuracy). For interpretability, and to index accuracy and processing speed separately, our main measures of interest to be included as predictors were reaction time for correctly completed items and accuracy of items attempted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Statistical Analysis</head><p>The study was pre-registered under: <ref type="url" target="https://aspredicted.org/34C_Z4L">https://aspredicted.org/34C_Z4L</ref>. Data were preprocessed and scored in R (version 4.2.2), Matlab (version 2020b) and Python (version 3.11), while statistical analyses were performed in R (version 4.2.2). Anonymised data and analysis scripts are available under: <ref type="url" target="https://osf.io/j56y4/">https://osf.io/j56y4/</ref>.</p><p>Linear/logistic mixed effects models were used to estimate main effects of acoustic clarity, modality (added visual speech) and session on accuracy measures in all three audiovisual speech perception tasks using the lme4 package in R. Audiovisual benefit was calculated by taking the difference between intelligibility-matched audiovisual and auditory-only listening conditions AVlow -AOhigh. For completeness, we estimated three different measures of test-retest reliability across sessions: the Spearman-Brown formula, Cohen's α, and the intra-class correlation coefficient (ICC) (even though only two of these measures were pre-registered we report all three; ICC allowed us to compare consistency across three levels in our cross-task comparison). Cronbach's α was computed using the psych package in R <ref type="bibr" target="#b100">(Revelle, 2024)</ref>, while the Intraclass Correlation Coefficient (ICC) was calculated using a two-way mixed-effects model for absolute agreement, treating separate sessions as individual raters, according to:</p><formula xml:id="formula_0">𝐼𝐶𝐶 = 𝑉𝑎𝑟𝑖𝑎𝑛𝑐𝑒 𝑏𝑒𝑡𝑤𝑒𝑒𝑛 𝑝𝑎𝑟𝑡𝑖𝑐𝑖𝑝𝑎𝑛𝑡𝑠 𝑉𝑎𝑟𝑖𝑎𝑛𝑐𝑒 𝑏𝑒𝑡𝑤𝑒𝑒𝑛 𝑝𝑎𝑟𝑡𝑖𝑐𝑖𝑝𝑎𝑛𝑡𝑠 + 𝐸𝑟𝑟𝑜𝑟 𝑣𝑎𝑟𝑖𝑎𝑛𝑐𝑒 + 𝑉𝑎𝑟𝑖𝑎𝑛𝑐𝑒 𝑏𝑒𝑡𝑤𝑒𝑒𝑛 𝑠𝑒𝑠𝑠𝑖𝑜𝑛𝑠 (2)</formula><p>To assess across-task reliability, we calculated correlations which were ceiling-corrected for within-level test-retest reliability using the Spearman-Brown formula (adjusted by the square root of the product of the test-retest reliability of both tasks), and estimated consistency across all three tasks using a two-way mixed-effects ICC.</p><p>Finally, in order to isolate condition-specific rather than level-specific variance as the independent variable in the regression analysis, and given the significant correlations we observed across levels, we decided to perform principal component analysis (PCA) on standardised unimodal and audiovisual benefit measures across levels of linguistic structure. PCA scores (isolating variability in audiovisual benefit across tasks) were then predicted using multiple linear regression analysis including standardised perceptual and cognitive measures as well as demographic variables (age and self-reported gender) as independent predictors.</p><p>We additionally performed information transmission analysis <ref type="bibr" target="#b82">(Miller and Nicely, 1955)</ref> to explore the role of phonetic feature perception (voicing, manner and place of articulation) in predicting sentence-level audiovisual benefit. This analysis was not pre-registered; therefore, we deem it exploratory here. Due to the small number of presentations per item per subject, confusion matrices for phonetic features of interest: voicing, manner and place of articulation (see Table <ref type="table">1</ref> for classification scheme), were pooled across participants prior to the calculation of relative transmitted feature information according to the following formula:</p><formula xml:id="formula_1">𝐼𝑇 𝑟𝑒𝑙 = 𝐼(𝑈, 𝑉) 𝐻 (𝑈) (3)</formula><p>Here, I(U,V) describes the mutual information between the discrete variables describing presented and identified features, also known as the absolute information transmitted (ITabs), and H(U) describes the feature entropy of the target variable (see <ref type="bibr" target="#b87">Oosthuizen and Hanekom, 2016</ref>, for a detailed methodological description of the classic FITA approach). Analyses were conducted using functions from the entropy package in R <ref type="bibr" target="#b47">(Hausser and Strimmer, 2009)</ref>. To estimate subjectlevel variability, a jackknife resampling procedure was used to produce subaverage ITrel scores for 115 confusion matrices for n-1. Individual estimates oi were then retrieved from the set of subaverage scores ji using the following formula (Smulders, 2010):</p><formula xml:id="formula_2">𝑜 𝑖 = 𝑛𝐽 ̅ -(𝑛 -1) 𝑗 𝑖 (4)</formula><p>Here 𝐽 ̅ represents the mean of subaverage scores across n participants. In words, we computed the information transmission for an individual participant as the difference between information transmission for all participants, and information transmission for all participants except that individual. We refer to this dependent measure as retrieved relative information transmission (Retrieved ITrel) and investigated the effect of added visual speech (modality) and acoustic clarity on this dependent measure using one-way analyses of variance (ANOVAs) on ranks (Kruskal-Wallis tests, as assumptions of normality were not met, see Results E). We also computed pairwise comparisons of interest and investigated the relationship of Retrieved ITrel values to sentence-level measures of lipreading ability and audiovisual benefit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Substantial individual differences in audiovisual benefit for sentences, words and phonemes</head><p>As in previous work <ref type="bibr" target="#b1">(Aller et al., 2022;</ref><ref type="bibr">Grant et al., 1998;</ref><ref type="bibr">Grant and Seitz, 1998;</ref><ref type="bibr" target="#b112">Sommers et al., 2005;</ref><ref type="bibr" target="#b128">Van et al., 2014;</ref><ref type="bibr" target="#b129">Van Engen et al., 2017)</ref>, we observed substantial inter-individual variability in lipreading ability and audiovisual speech processing across all three audiovisual speech tasks (Figure <ref type="figure" target="#fig_2">3</ref>). In each of the tasks, performance was lowest in the AOlow condition (Sentences:</p><formula xml:id="formula_3">M±SD=0.08±0</formula><p>.09:, Words: M±SD=0.19±0.07, Consonants: M±SD=0.23±0.08), as expected, and increased with added acoustic clarity and added visual speech (see Figure 3a), as indicated by improved fit when including fixed effects of clarity and modality in logistic (for consonants) and linear (for words and sentences) mixed effects models according to the following specification (in the Wilkinson notation, Wilkinson &amp; Rogers (1973): 𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦 ~ 1 + 𝑀𝑜𝑑𝑎𝑙𝑖𝑡𝑦 + 𝐶𝑙𝑎𝑟𝑖𝑡𝑦 + 𝑀𝑜𝑑𝑎𝑙𝑖𝑡𝑦: 𝐶𝑙𝑎𝑟𝑖𝑡𝑦 + 𝑆𝑒𝑠𝑠𝑖𝑜𝑛 (5) +(1 + 𝑀𝑜𝑑𝑎𝑙𝑖𝑡𝑦 + 𝐶𝑙𝑎𝑟𝑖𝑡𝑦 | 𝑃𝑎𝑟𝑡𝑖𝑐𝑖𝑝𝑎𝑛𝑡) + (1 | 𝐼𝑡𝑒𝑚) For sentences, model comparisons using Kenward-Roger's F-tests suggested that a model including clarity, F(1,112.12)=1282, p&lt;.001, and modality, F(1,112.12)=667.83, p&lt;.001, provided a better fit than models without. Examination of the summary output indicated that added acoustic clarity improved word report by 33% (low versus high acoustic clarity: β= 0.36, SE= 0.006, t=55.246) while the audiovisual modality improved word report by 36% (auditory-only versus auditory-visual condition: β= 0.360, SE= 0.015, t= 24.15). There was a small, but significant interaction between clarity and modality, F(1,8584)=4.87, p=.027, β=0.02, SE= 0.008, t=2.207, possibly driven by non-linearities in the data introduced by floor effects in the AOlow condition. There was also a significant improvement in overall accuracy between sessions, F(1,8556.59)=141.90, β=0.05, SE=0.004, t=11.912. We observed a similar pattern of results for the word-level task, with an increased accuracy of word report with added clarity, F(1,113.43)=4703.83, p&lt;.001, β=0.62, SE=0.01, t= 112.74, and added visual speech, F(1,112.98)=1743.60, p&lt;0.001, β=0.42, SE=0.01, t=55.99, as well as a very small decrease in performance between sessions, F(1,17548.61)=13.37, p&lt;.001, β=0.01, SE=0.003, t=-3.656. There was a significant interaction of clarity and modality also in the word-level task, F(1,17551.58)=1537.32, p&lt;.001, β=-0.30, SE=0.008, t= -39.209, p&lt;.001. This interaction effect is likely driven by a trend towards the ceiling in the word-level task (AOhigh). For the consonant-level task, we used mixed-effects logistic regression to explore the effect of clarity, modality and session on the binary accuracy measure. Since each participant saw each consonant per condition, by-item random slopes were also included in this model for a full random effects structure (see <ref type="bibr">Barr, 2013)</ref>. Model comparisons using likelihood ratio tests indicated that models including both acoustic clarity and modality provided the best fit to the data (Clarity:</p><p>χ2(1)=12. <ref type="bibr">837,</ref><ref type="bibr">p&lt;.001,</ref><ref type="bibr">β=1.78,</ref><ref type="bibr">SE=0.43,</ref><ref type="bibr">z=4.20</ref>, Modality: χ2(1)=17.295, p&lt;0.001, β=2.12, SE=0.41, z=5.18. There was no significant interaction between clarity and modality in the consonant-level task, χ2(1)=0.01, p=.939. Additionally, including session as a fixed effect improved model fit, suggesting a significant improvement in performance between sessions, χ2(1)=13.26, p&lt;0.001, β=0.19, SE=0.02, z=9.746. Finally, there were substantial individual differences in both effects of acoustic clarity and modality on accuracy. Our measure of audiovisual benefit was calculated as the difference in performance between the intermediate-intelligibility conditions AVlow and AOhigh. This measure demonstrates substantial differences between individual participants: some benefitted more from added visual speech than increased acoustic clarity levels, and vice versa (as indicated by the slopes of lines in column b, i.e., positive slopes indicating more benefit from visual speech, whereas negative slope means more benefit from increased acoustic clarity). For example, in the sentence level task, 59 out of 113 participants benefitted more from added visual speech than increase acoustic clarity i.e. showed better performance in the AVlow condition than the AOhigh conditions (for words and consonants these proportions were less balanced, with 6 and 68 out of 113 benefitting more from added visual speech, respectively). Over all participants, these measures of audiovisual benefit significantly differed from zero for words and consonants; but this difference was not reliable for sentencelevel report (Sentences: MD=0.030, t(112)= 1.57, p= .119; Words: MD=-0.19, t(112)= -16.72, p&lt;.001; Consonants: MD= 0.038, t(113)= 3.59, p=.001, see Figure <ref type="figure" target="#fig_2">3b</ref>). Nonetheless, all three differences straddle zero, and are largely unaffected by floor or ceiling effects in the underlying data. For all three speech tasks, the degree of audiovisual benefit (i.e. difference between AVlow and AOhigh) was significantly correlated with lipreading ability measured using performance in the visual-only condition (Sentences: r(111)=.59, p&lt;.001, Words: r(111)=.57, p&lt;.001, Consonants: r(111)=.35, p&lt;.001 see Figure <ref type="figure" target="#fig_2">3c</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Audiovisual benefit is stable across sessions and consistent across levels of linguistic structure</head><p>In order to establish whether our measure of audiovisual benefit can be considered a stable difference between individuals, we calculated test-retest reliability across two sessions (set at least one week apart, containing different items) and consistency across three levels of linguistic structure, adjusted for within-task test-retest reliability (sentences, words and consonants, produced by different speakers). According to standard interpretations of test-retest metric values <ref type="bibr" target="#b50">(Hedge et al., 2018)</ref>, audiovisual benefit for the sentence-level task shows good test-retest reliability, while the word-level task shows moderate and the consonant-level task shows poor-moderate test-retest reliability (see Table <ref type="table">I</ref> for a comparison of different measures and Figure <ref type="figure" target="#fig_3">4a</ref>). To assess whether audiovisual benefit is also consistent across the three tasks, we calculated pairwise ceiling-corrected Spearman-Brown correlations (accounting for within-task test-retest reliability, see Figure <ref type="figure" target="#fig_3">4b</ref>), as well as Cronbach's α and ICC3,k across all three levels in order to assess consistency (two-way mixed for consistency rather than absolute agreement due to magnitude differences between different scores, using the average across two sessions, making ICC identical to alpha), which yielded values of α=.</p><p>65, 95% CI [.53 .75]. Further assessing pairwise correlations between these three measures of audiovisual benefit demonstrates moderate consistency between monosyllabic words and sentences (α=.69 or ρ=.75, where ρ is corrected for within-task test-retest reliability), and poor to moderate consistency of audiovisual benefit measures for words and sentences with the consonant-level measure (Words: α=.35, ρ=.31, Sentences: α=.49, ρ=55). Low correlations may be driven by the moderate test-retest reliability of the consonant-level task (due to the relatively small number of trials presented). Nonetheless, these results show that across different items, stimulus types, and speakers we find meaningful correlations in the magnitude of audiovisual benefit individuals derive: A linear regression model including word-(β=0.693, SE=0.121, p&lt;.001) and consonant-level audiovisual benefit (β=0.370, SE=0.125, p&lt;.001) explained 33% of the variance in sentence-level audiovisual benefit, F(2,112)=28.32, p&lt;.001, R 2 = 0.324.</p><p>TABLE I. Test-retest reliability measures for within-task audiovisual benefit. Sentences Words Consonants Cronbach's α .83 [.76 .89] a .68 [.56 .80] .54 [.37 .71] ICC .71 [.60 .79] .51 [.36 .64] .37 [.20 .52]</p><p>a Confidence intervals for α were estimated using the Duhachek method <ref type="bibr" target="#b33">(Duhachek and Iacobucci, 2004)</ref>.</p><p>*** p &lt;.001</p><p>Previous studies have provided inconsistent results regarding whether measures of lipreading ability measured for materials at different levels of linguistic structure are positively correlated <ref type="bibr" target="#b11">(Bernstein et al., 2000)</ref>. Exploring this here, we observed that lipreading ability was reliably and positively correlated across tasks (Sentences-Words: r(111)=.57, p&lt; .001, Sentences-Consonants: r(111)=.43, p&lt; .001, Words-Consonants: r(111)=.53, p&lt; .001).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Audiovisual benefit is independently predicted by relatively poorer hearing and better lipreading ability</head><p>Having confirmed that our measure of audiovisual benefit shows sufficient convergent validity, we performed principal component analysis (PCA) using the principal function in the psych package on standardised audiovisual benefit scores to isolate participant-level variability across tasks. All three benefit measures loaded on one component, which explained 60% of the variance in the data, with loading strengths of 0.88 for sentences, 0.80 for words and 0.63 for consonants. Multiple linear regression was then used to predict variability in this PCA score from cognitive and hearing measures, as well as demographic variables (see Table <ref type="table">II</ref>).</p><p>TABLE II. Summary of results for cognitive and perceptual measures Measure N M SD Vocabulary knowledge (Spot-the-Word Accuracy) 103 0.675 0.191 Matrix Reasoning (RT correct in s) 103 8.283 3.650 Matrix Reasoning (Accuracy for items attempted) 103 0.805 0.084 Frequency of hearing difficulties (% APHAB) 103 0.323 0.049 Speech reception threshold (Digits-in-Noise dB SNR) 103 -10.233 0.933 PADRI threshold (Listen Up) 103 0.165 0.056 Age 103 38.544 11.549 Note. This table includes the measures and participants included in the multiple regression analysis. We decided to include response times (for correct items) and accuracy (for attempted items) for the matrix reasoning task as separate predictors to improve interpretability (compared to a trade-off measure such as inverse efficiency) and to index both processing speed (RT) and reasoning ability (accuracy). Both response time (RT) and accuracy (%) in the matrix reasoning task were weakly correlated with age (RT: r(101)=.26, p=.011, %: r(101)=.20, p=.038) indicating declines in reasoning speed, but increases in reasoning accuracy with age (but neither of those correlations survived correction for multiple comparisons, see Figure 5). Both these measures of matrix reasoning ability were positively associated with performance on the Spot the Word task (RT: r(101)=.34, p&lt;.001, %: r(101)=.35, p&lt;.001). As expected, Spot the Word performance was moderately positively correlated with age (r(101)=.49, p&lt;.001) in line with previous observations of increased verbal IQ in older individuals <ref type="bibr" target="#b46">(Hartshorne and Germine, 2015)</ref>. There were no significant relationships between the hearing or speech perception measures, and none of these measures were correlated with age (see Figure <ref type="figure" target="#fig_4">5</ref>). A series of model comparison F-tests suggested that only poorer hearing, as indicated by higher speech reception thresholds (worse performance) estimated using the Digits-in-Noise test (F(1)=4.298, p=.041) and better lipreading ability, measured as the mean of (standardised) visualonly performance across all three tasks (F(1)=87.845, p&lt;.001) predicted individual differences in audiovisual benefit, F(9,93)=12.33, R 2 =49.9%; dropping either of these, but none of the other predictors, significantly affected model fit (see Figure <ref type="figure" target="#fig_5">6a</ref>).</p><p>Previously, it has been suggested that speech-in-noise perception may explain some variability in lipreading ability <ref type="bibr" target="#b8">(Bernstein, 2018;</ref><ref type="bibr" target="#b133">Watson et al., 1996)</ref>. However, variance partitioning analysis indicated that lipreading ability and speech reception thresholds independently predicted .001, ** p &lt; .01, * p &lt; .05.</p><p>audiovisual benefit. Lipreading ability uniquely explained 45% (F(1,100)=77.448, p&lt;.001) of the variance in audiovisual benefit, while speech perception thresholds explained 7% (F(1,100)=8.644, p=.006) with only 2% of variance shared between the two predictors (Figure <ref type="figure" target="#fig_5">6C</ref>). A third variable included in this analysis, age, which has been associated previously with a decline in both lipreading ability and speech reception thresholds, and trended towards significance in the full regression model, did not explain any joint or unique variance in audiovisual benefit, F(1,100)=0.294, p=.589.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Unimodal speech perception is associated with demographic variables and domain-general cognitive abilities</head><p>We also explored whether hearing status, verbal and non-verbal cognitive abilities, age and gender explained any of the variability observed in performance in two unimodal conditions not used to calculate audiovisual benefit: Visual-Only and Auditory-Onlylow (Figure <ref type="figure" target="#fig_6">7</ref>).</p><p>We again extracted variability across levels using PCA to isolate modality-specific, levelindependent variability. Auditory-onlylow performance loaded on one component, explaining 53%</p><p>of the overall variance, with loading strengths of 0.76 for sentences, 0.82 for words and 0.57 for consonants. Model comparisons revealed that better performance in the auditory-only modality was associated with younger age (F(1)=12.724, p&lt;.001), and predicted by matrix reasoning accuracy (F(1)=5.106, p=.026) and performance in the Spot the Word task (F(1)=9.085, p=.003), explaining 31.6% of the variance in performance, F(8,94)=6.886, p&lt;.001. Due to some (but not substantial) indication of multicollinearity this model (VIF = 2.3), we also performed variance partitioning here to explore potential associations between age, and measures of verbal and non-verbal IQ. This suggested that matrix reasoning accounted for 16% of the variance (F(1,100)=21.047, p=.001), with 9% variance shared between the two cognitive measures and a non-significant unique contribution of Spot the Word performance (F(1,100)=2.949, p=0.087), while age independently explained 8 % of the variance in auditory-only word report, F(1,100)=5.05, p=.020. All three measures of lipreading ability loaded on a single PCA component, explaining 67% of the variance in the data, with loading strengths of 0.81 for sentences, 0.86 for words and 0.79 for consonants. Visual-only perception was related only to our two demographic measures, F(8,94)=2.662, p=.011, R 2 =11.5%. Model comparisons suggested that lipreading ability decreased with age (F(1)=5.812, p=0.017), and participants identifying as female were overall better at lipreading than those identifying as male (F(1)=4.675, p=.033). None of the other measures predicted individual differences in lipreading ability. Unlike for audiovisual benefit, including Speech Reception Thresholds estimated in the Digits-in-Noise task did not meaningfully contribute to model fit, F(1)=0.939, p=0.335. Overall, these results suggest that while audiovisual benefit is related to perceptual abilities, unimodal speech perception (both audio and visual-only) is associated with demographic variables (such as age/gender) and (for auditory speech perception) measures of domain-general cognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Exploratory: Sentence-level audiovisual benefit is predicted by visual perception of place and manner of articulation features</head><p>Our previous analyses have indicated that perceptual, rather than non-signal-related cognitive variables predict individual differences in audiovisual benefit for all three levels of linguistic structure tested. We therefore embarked on exploratory analyses to identify the perceptual cues that are most relevant to audiovisual speech perception and that may be better exploited by participant showing enhanced audiovisual benefit. Our focus here is on perception of specific articulatory features that might explain variability in consonant identification. To this end, we used a classic information theoretic approach to quantify transmission of phonetic cues in unimodal and audiovisual perception of consonants: feature information transmission analysis (FITA) (Files et only low clarity. b) Predictors of visual-only speech perception. *** p &lt; .001, * p &lt; .05. <ref type="bibr" target="#b67">al., 2015;</ref><ref type="bibr">Grant et al., 1998;</ref><ref type="bibr" target="#b56">Jesse and Massaro, 2010;</ref><ref type="bibr" target="#b65">Lalonde and Werner, 2019;</ref><ref type="bibr" target="#b82">Miller and Nicely, 1955;</ref><ref type="bibr" target="#b131">Walden et al., 1975)</ref>.</p><p>Consistent with the previous literature (e.g. <ref type="bibr">Grant et al., 1998)</ref>, we expected that place of articulation would be most easily transmitted in the visual-or audiovisual modality, whereas voicing and manner would be more easily recognised in the auditory modality. We statistically assessed two comparisons of interest: (1) Auditory-Onlylow compared to Visual-Only (i.e. which features are better transmitted in two low intelligibility conditions that convey only auditory or only visual information; (2) Auditory-Onlyhigh compared to Auditory-Visuallow, (i.e. which features are better transmitted in intermediate intelligibility, auditory-only and auditory-visual conditions, see Figure 9). We additionally conducted a factorial analysis to investigate main effects of auditory clarity (low/high) and visual information (absent/present) for the four auditory conditions (excluding VO). Since the data analysed are retrieved relative information transmission values, assumptions of normality are violated (as confirmed by Shapiro-Wilk tests, Voicing: W=0.821, p&lt;.001, Manner: W=0.778, p&lt;.001, Place: W=0.705, p&lt;.001), therefore, non-parametric tests were used for statistical analysis.</p><p>Kruskal-Wallis H tests indicated that there was a main effect of modality (χ 2 (1)=16.543, p&lt;.001), as well as a main effect of clarity (χ 2 (1)=231.299, p&lt;.001) for transmission of the voicing feature (see Figure <ref type="figure" target="#fig_8">9a</ref>). Transmission was significantly better in the Auditory-Onlylow condition than the Visual-Only condition (MD=0.00871, z=4403, p=.001 corrected for multiple comparisons using the Holm method), and better in the Auditory-Onlyhigh condition compared to the Auditory-Visuallow condition (MD=0.0348, z=6101, p&lt;.001). These observations suggest an overall auditory advantage for transmission of voicing information. These findings are consistent with the existing literature indicating that voicing information is typically considered to be absent in visual speech signals <ref type="bibr" target="#b70">(Lisker et al., 1977</ref>; but see: <ref type="bibr" target="#b97">Raphael, 1972</ref><ref type="bibr" target="#b98">Raphael, , 1975;;</ref><ref type="bibr" target="#b130">Van Son et al., 1994)</ref>. Nonetheless, the presence of a main effect of modality is interesting since it suggests that visual information can enhance perception of consonantal voicing contrasts when combined with auditory signals -for instance, because visual information can signal the timing of closure for stop consonants.</p><p>For manner of articulation, the picture was more complex: There was a main effect of modality (χ 2 (1)=85.506, p&lt;.001) and clarity (χ 2 (1)=206.027, p&lt;.001) on relative information transmission (see Figure <ref type="figure" target="#fig_8">9b</ref>). Manner cues were better transmitted in the Visual-Only than the Auditory-Onlylow condition (MD=0.0434, z=537, p&lt;.001), but more easily transmitted in the Auditory-Onlyhigh than the Auditory-Visuallow condition (MD=0.0274, z=5472, p&lt;.001), suggesting that while some manner information is available in the visual-only condition, at higher levels of acoustic clarity, the auditory modality contains more reliable cues to the manner feature.</p><p>For place of articulation, there was a main effect of modality χ 2 (1)=251.617, p&lt;.001), as well as a main effect of clarity (χ 2 (1)=37.419, p&lt;.001) (see Figure <ref type="figure" target="#fig_8">9c</ref>). Transmission was lowest in the Auditory-Onlylow condition, which was significantly worse than transmission in the Visual-Only condition according to a Wilcoxon sign-rank test (MD=0.0597, z=177, p&lt;.001). Finally, transmission of the place feature was better in the Auditory-Visuallow than the Auditory-Onlyhigh condition (MD=0.0467, z=513, p&lt;.001), indicating an overall advantage of the visual modality for transmitting place of articulation information. As for voicing, this is largely consistent with the existing literature; showing that visual speech provides valuable cues to place of articulation.</p><p>Previous studies have pointed to the importance of place of articulation extraction for audiovisual speech perception (e.g. <ref type="bibr">Grant et al., 1998)</ref>. For instance, ability to extract place information is a significant predictor of individual susceptibility to the McGurk effect <ref type="bibr" target="#b19">(Brown et al., 2018;</ref><ref type="bibr" target="#b116">Strand et al., 2014)</ref> Finally, we explored a possible relationship between our retrieved ITrel values for lipreading ability (VO) and audiovisual benefit (Retrieved ITrel(AVlow) -Retrieved ITrel(AOhigh)) at the consonant-level for each feature, and sentence-level lipreading and audiovisual benefit measures (see Figure <ref type="figure" target="#fig_8">9d-f</ref>). These analyses suggested that the ability to extract both manner and place of articulation features in the visual modality predicted individual differences in sentence-level lipreading ability (Manner: ρ(111)=.46, p&lt;.001, Place: ρ(111)=.42, p&lt;.001) and audiovisual benefit (Manner: ρ(111)=.36, p&lt;.001, Place: ρ(111)=.39, p&lt;.001), while the visual transmission of voicing did not explain any variability in either measure at the sentence-level (VO: ρ(111) <ref type="bibr">=.11,</ref><ref type="bibr">p=.712,</ref><ref type="bibr">Benefit: ρ(111)=.01,</ref><ref type="bibr">p=.888)</ref>. Furthermore, the relative transmission of place of articulation information in the matched AVlow and AOhigh conditions was meaningfully related to individual differences in sentence-level audiovisual benefit (ρ(111)=.27, p=.016), while this was not the case for audiovisual benefit for either voicing (ρ(111)=.09, p=.741) or manner (ρ(111)=.12, p=.616) feature transmission. Therefore, despite the small number of presentations our estimates are based on, we find a reliable relationship between perception of consonantal place and manner features and sentence-level measures of individual differences in visual and audiovisual speech. Overall, these results indicate that ability to extract manner and place of articulation cues visually, and the ability to extract place information audio-visually, relative to a participant's auditory-only performance when identifying individual consonants, is related to audiovisual benefit for sentencelevel speech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DISCUSSION</head><p>Not all listeners can benefit equally from visual information to enhance speech perception <ref type="bibr">(Grant et al., 1998)</ref>. Here, we investigated individual differences in audiovisual speech perception using a matched, intermediate-intelligibility measure of audiovisual benefit. <ref type="bibr" target="#b77">Macleod &amp; Summerfield (1987)</ref> similarly compared speech-in-noise reception thresholds (SRTs) at 50% accuracy in auditory-only (AO) and auditory-visual (AV) conditions, respectively. Measuring the relative intelligibility of matched auditory-only and audiovisual speech, rather than comparing changes in intelligibility due to added visual cues better avoids floor and ceiling effects and confirms that audiovisual benefit is stable across time. Crucially, unlike previous studies using more conventional visual enhancement measures <ref type="bibr">(Grant et al., 1998;</ref><ref type="bibr" target="#b112">Sommers et al., 2005;</ref><ref type="bibr" target="#b125">Tye-Murray et al., 2010)</ref> we found that this audiovisual benefit measure is correlated across different speech materials (sentences, words, consonants), suggesting that audiovisual integration relies on common mechanisms across levels of linguistic structure.</p><p>Isolating participant-level variability across levels of linguistic structure, we found that individual differences in audiovisual benefit were predicted by perceptual, rather than cognitive abilities: better lipreading abilities and higher Digits-in-Noise SRTs (relatively poorer hearing) independently predicted enhanced audiovisual benefit. Conversely, unimodal speech perception was associated with both cognitive measures (matrix reasoning, vocabulary) and demographic variables (age, gender). Using information transmission analyses, we further showed that visual speech perception and audiovisual benefit for sentence perception are predicted by individual differences in the perception of place of articulation (and to a lesserdegree, manner of articulation) features during a consonant identification task. These findings point to common speech perception mechanisms that support audiovisual benefit in speech listening.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. A common mechanism underlying audiovisual benefit across levels of linguistic structure</head><p>In the present study, we find reliable correlations for our measure of audiovisual benefit across speech materials probed at different levels of linguistic structure. That is, the degree of benefit obtained at the level of minimal syllables predicts the relative magnitude of benefit obtained at the level of monosyllabic words and meaningful sentences (each of which were additionally produced by a different speaker). Previous work has most commonly not been able</p><p>to establish such relationship, for example using the Visual Enhancement (VE) measure <ref type="bibr">(Grant &amp; Seitz, 1998;</ref><ref type="bibr" target="#b112">Sommers et al., 2005)</ref>. <ref type="bibr">Grant et al. (1998)</ref> found no reliable correlations between consonant-and sentence-level VE in older hearing-impaired (HI) listeners, while <ref type="bibr" target="#b112">Sommers et al. (2005)</ref> found only one moderate correlation between word-and sentence-level VE in younger, normally-hearing (NH) but not older (NH and HI) listeners, while no statistically reliable association could be established for consonant-level VE to higher-level measures.</p><p>These limited or null findings for AV speech are surprising given that in unimodal conditions similar cross-task correlations are typically reliable <ref type="bibr" target="#b11">(Bernstein et al., 2000;</ref><ref type="bibr">Grant et al., 1998;</ref><ref type="bibr" target="#b53">Humes et al., 1994;</ref><ref type="bibr" target="#b112">Sommers et al., 2005)</ref>.</p><p>A potential explanation for this lack of correlations proposed previously <ref type="bibr" target="#b111">(Sommers, 2021;</ref><ref type="bibr" target="#b112">Sommers et al., 2005;</ref><ref type="bibr" target="#b129">Van Engen et al., 2017)</ref> is that audiovisual integration for speech perception may rely on different mechanisms across levels of linguistic structure. In a multistage model of audiovisual speech perception <ref type="bibr" target="#b88">(Peelle and Sommers, 2015)</ref>, mechanisms relying on the complementarity of audiovisual information at the level of phonetic features (e.g. <ref type="bibr" target="#b120">Summerfield et al., 1997)</ref>, or whole words (e.g. auditory and visual neighbourhoods: <ref type="bibr" target="#b124">Tye-Murray et al., 2007b)</ref> could be differentially engaged depending on the linguistic complexity of the speech materials presented. This account might also extend to sentence perception -for example, if visually-mediated cortical entrainment is a mechanism that enhances sensitivity to upcoming, quasi-rhythmic continuous speech <ref type="bibr" target="#b88">(Peelle and Sommers, 2015)</ref>, this might not easily apply to isolated syllables or single words.</p><p>However, other speech perception mechanisms -e.g. predictive processing of mouthleading speech -more plausibly operate at multiple levels of linguistic structure <ref type="bibr" target="#b23">(Chandrasekaran et al., 2009;</ref><ref type="bibr" target="#b58">Karas et al., 2019)</ref>. Mouth-leading speech refers to cases in which visual cues precede corresponding acoustic speech in time. For example, when articulating the phoneme "m", a preparatory gesture of closing the lips can provide a visual speech cue before auditory cues to place are apparent; a "visual speech head start" <ref type="bibr" target="#b58">(Karas, 2019)</ref> that may facilitate audiovisual speech perception <ref type="bibr" target="#b132">(van Wassenhove et al., 2005)</ref>. However, the frequency of these mouth-leading events in natural speech remains unclear <ref type="bibr">(Schwartz and Savariaux, 2014)</ref>. By this view, perception of visual articulation activates phonological representations which can support speech perception when auditory cues are degraded or absent. This shared mechanism, relying on simple phonetic representations may explain common sources of variability that we observe when combining multiple levels of linguistic structure, and our finding of a link between perception of consonantal features and audiovisual benefit. That our observations also generalise across different speakers (which may introduce additional noise in across task comparisons: e.g. <ref type="bibr" target="#b48">Hazan et al., 2010;</ref><ref type="bibr" target="#b49">Heald &amp; Nusbaum, 2014</ref>) is striking and suggests that similar effects might also be observed in ecological listening situations.</p><p>Of course, the amount of lexical and semantic context available to listeners may impact speech recognition in both unimodal and audiovisual conditions (e.g. <ref type="bibr" target="#b55">Iverson et al., 1998;</ref><ref type="bibr" target="#b108">Smayda et al., 2016)</ref>. In our work, this is evident in the pilot data, explaining why our intermediate conditions of interest are created using different levels of acoustic clarity.</p><p>Measuring audiovisual benefit based on matched, intermediate-intelligibility conditions thus alleviates intelligibility confounds in comparing across both modalities and tasks. Unlike other measures used in studying audiovisual speech perception, our intelligibility-matched measure of AV benefit shows moderate or good test-retest reliability. Previous work has noted the apparent lack of success of audiovisual speech training at the group-level (e.g. <ref type="bibr" target="#b91">Preminger &amp; Ziegler, 2008)</ref>, and commented that audiovisual benefit may implicitly be assumed to be stable within an individual. However, this assumption has not explicitly been tested, especially in research specifically designed to investigate individual differences <ref type="bibr">(Grant et al., 1998;</ref><ref type="bibr" target="#b112">Sommers et al., 2005;</ref><ref type="bibr">Tye-Murray et al., 2007a</ref><ref type="bibr">, 2016)</ref>. Here, we show that our measure of individual differences generalises across time, when participants are tested on different items, and furthermore show reliable cross-task correlations even for tests assessing different levels of linguistic structure, with measures adjusted for within-task test-retest reliability. This approach, of (a) estimating audiovisual benefit at comparable, intermediate levels of acoustic clarity across materials, (b) avoiding floor and ceiling effects (c) estimating test-retest reliability and (d) taking task reliability into account for cross-task correlations was successful in showing consistent AV benefits for speech materials. We therefore encourage future studies of audiovisual speech training to consider the methods proposed here when testing for changes in the use of visual speech to support degraded speech perception.</p><p>A natural next step for this work will be to test whether individual differences in AV benefit are similarly apparent in hearing impaired individuals. However, in populations with more variable hearing abilities (e.g. in hearing loss, where acoustic degradation levels similar to the ones used here are likely to introduce floor-effects) we recommend the investigation of individual speech perception thresholds determined using an adaptive procedure to quantify the relative visual benefit <ref type="bibr" target="#b77">(Macleod and Summerfield, 1987)</ref>. Alternatively, researchers should consider testing a range of levels (guided by pilot experiments) rather than limiting their experiment to one or two levels of degradation determined a-priori. Where automatic scoring methods are more challenging -for example when working with sentence-level word reports tasks in online experiments (but see <ref type="bibr" target="#b16">Borrie et al., 2019;</ref><ref type="bibr" target="#b17">Bosker, 2021</ref> for recent advances in this area) -sampling at multiple levels would present an alternative option to prevent floor-and ceiling effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The role of cognitive, perceptual, and demographic variables in explaining individual differences</head><p>Having confirmed that individual differences in audiovisual benefit are stable over time, and consistent across levels of linguistic structure, we set out to investigate the role of cognitive, perceptual and demographic variables in explaining these individual differences. Importantly, we do not attempt to isolate the integration stage here, but instead take a holistic approach to understanding individual differences in audiovisual speech benefit, across levels of linguistic complexity. This represents a more ecological approach: assessing audiovisual speech perception in general, rather than understanding audiovisual integration as a discrete, separable part of the process. To understand the role of linguistic and cognitive abilities as well as auditory perceptual acuity we administered several well-established psychometric tests in our final session. We also recruited a balanced sample across the adult age range (18-60 years, mean = 38 years), and recorded participant's self-identified gender.</p><p>While auditory speech perception was predicted by demographic (age, gender) and domaingeneral cognitive abilities, we found that only unimodal perceptual abilities predicted individual differences in audiovisual benefit. This is in line with previous research: it is well-established that cognitive abilities correlate with performance on speech perception tasks (especially at the sentence-level: <ref type="bibr" target="#b51">Heinrich et al., 2015)</ref>, and that both auditory speech perception and lipreading ability decline with age <ref type="bibr" target="#b125">(Tye-Murray et al., 2010</ref><ref type="bibr">, 2016)</ref>. A common finding in previous work is a lipreading advantage for female participants (which may be due to differences in strategy or gaze behaviour, e.g. see <ref type="bibr" target="#b8">Bernstein, 2018)</ref>, which we also find here. Finally, the idea that individual differences in audiovisual enhancement is a consequence of differences in unimodal perceptual abilities has been suggested previously <ref type="bibr" target="#b111">(Sommers, 2021;</ref><ref type="bibr" target="#b126">Tye-Murray et al., 2016)</ref>.</p><p>We extend these previous findings by using a number of speech task-external measures, capturing different aspects of auditory perceptual acuity. We also explored the role of phonetic feature information across modalities, and how individual variability in their transmission at the consonant-level generalise to higher levels of linguistic structure. We will discuss our findings with regard to each of these factors in turn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Cognitive and linguistic abilities</head><p>Our results suggest that measures of language and domain-general cognition are associated with individual differences in auditory, but not visual speech perception or audiovisual benefit. This is in line with previous work: It is well-established that individual differences in cognitive measures and language proficiency predict performance on auditory-only speech recognition tasks, even after accounting for individual differences in audibility, in consonant-, word-and sentence-level tasks <ref type="bibr" target="#b0">(Akeroyd, 2008;</ref><ref type="bibr" target="#b12">Besser et al., 2013;</ref><ref type="bibr" target="#b53">Humes et al., 1994;</ref><ref type="bibr" target="#b84">Moradi et al., 2013</ref><ref type="bibr" target="#b85">Moradi et al., , 2014))</ref>. While more specific measures have previously been linked to speech recognition (specifically: measures of working memory such as n-back tasks, see <ref type="bibr" target="#b12">Besser et al., 2013)</ref>, we find that shared, domain-general, variance between our cognitive and linguistic tasks predicts better auditory-only performance across levels of linguistic structure. Specifically, after accounting for the unique variance in the matrix reasoning task and shared variance with the Spot-the-Word vocabulary measure, vocabulary knowledge itself no longer significantly explained variability in auditory-only performance. This finding could reflect influences of fluid intelligence on performance in working memory tasks <ref type="bibr" target="#b45">(Harrison et al., 2015;</ref><ref type="bibr" target="#b135">Wiley et al., 2011)</ref>, or the influence of domain general neural mechanisms shown by impaired perception of degraded speech in individuals with brain lesions affecting fluid intelligence <ref type="bibr" target="#b76">(MacGregor et al., 2022)</ref>. Alternatively, scoring word report tasks using more granular string-matching metrics <ref type="bibr" target="#b17">(Bosker, 2021)</ref> might have attenuated the influence of linguistic knowledge on relative performance <ref type="bibr" target="#b115">(Stevenson et al., 2015)</ref>.</p><p>Previous studies, however, have been less clear on whether cognitive and linguistic abilities predict individual differences in audiovisual enhancement. One aspect of this debate concerns whether the addition of visual speech leads to increased or decreased computational demands in speech recognition tasks <ref type="bibr" target="#b37">(Fraser et al., 2010;</ref><ref type="bibr" target="#b84">Moradi et al., 2013</ref><ref type="bibr" target="#b83">Moradi et al., , 2017))</ref>. In a dual-task paradigm, <ref type="bibr" target="#b37">Fraser et al. (2010)</ref> found that, compared to intelligibility-matched auditory-only speech, performance in an audiovisual speech recognition task was more disrupted by the presence of a secondary task. Like our intelligibility-matched method, this approach avoids a pitfall of traditional methods based on comparing conditions where the audiovisual task is naturally more intelligible, i.e. less cognitively demanding. However, for <ref type="bibr" target="#b37">Fraser et al. (2010)</ref>, this effect was only apparent in RTs, but not in accuracy scores or subjective listening effort ratings.</p><p>In the current study, we also compare matched conditions to avoid intelligibility-related confounds and found no evidence of any relationship between audiovisual benefit and domaingeneral cognitive or linguistic abilities.</p><p>It might be that variability in visual speech perception, and by extension, audiovisual benefit, relies on more domain-specific cognitive abilities, such as visuo-verbal or visuo-spatial working memory and processing speed, which had previously been linked to lipreading ability <ref type="bibr" target="#b35">(Feld &amp; Sommers, 2009;</ref><ref type="bibr" target="#b73">Lyxell &amp; Holmberg, 2000;</ref><ref type="bibr" target="#b122">Tye-Murray et al., 2014)</ref>. However, here, we found no evidence of a relationship to either of the MaRs measures, suggesting that -to the extent that processing speed and perceptual synthesis <ref type="bibr" target="#b133">(Watson et al., 1996)</ref> are measured by this non-verbal reasoning task, then neither, was related to lipreading ability or audiovisual benefit. Another explanation of the somewhat conflicting findings in the literature could be that proposals tying (audio-)visual speech perception to higher-level cognitive and linguistic abilities might be specific to research with special populations (i.e. school-aged children, or individuals with hearing loss that occurred early in life) <ref type="bibr" target="#b73">(Lyxell and Holmberg, 2000;</ref><ref type="bibr" target="#b74">Lyxell and Rönnberg, 1989)</ref>. More recent work generally confirms the idea that individual differences in cognitive or linguistic abilities are not correlated with audiovisual enhancement, even in these populations <ref type="bibr" target="#b64">(Lalonde and McCreery, 2020)</ref>. In a sample of school-aged children, with and without hearing loss, <ref type="bibr" target="#b64">Lalonde and McCreery (2020)</ref> found no relationship of vocabulary, working memory and executive function measures with audiovisual enhancement in a sentencerecognition task: Only the degree of hearing loss predicted the magnitude of audiovisual enhancement, consistent with our findings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Unimodal speech perception abilities</head><p>We found that relatively poorer hearing and better lipreading ability independently predicted individual differences in audiovisual benefit across levels of linguistic structure. As expected <ref type="bibr">(Bernstein et al., 2022, for review)</ref>, lipreading ability itself accounted for a large amount of the variance in audiovisual benefit. The idea that variability in unimodal perceptual abilities explain individual differences in the audiovisual speech advantage is not a new proposition: <ref type="bibr" target="#b126">Tye-Murray et al. (2016)</ref> for example, conducted a principal component analysis on a closed-set word identification task in 11 conditions of auditory-only, visual-only and auditory-only speech which returned only two components, suggesting that variability was entirely explained by two unimodal variability factors, rather than requiring a third, distinct integration ability. Importantly, when using the term "perceptual" here, we refer to "speech perception", which involves modality-specific phonetic categorisation (e.g. Holt, 2010). Our use of this term is therefore not limited to pre-linguistic perceptual processes. This is to distinguish accounts which consider auditory-visual integration as a distinct ability (similar to working memory or processing speed, as addressed in <ref type="bibr" target="#b126">Tye-Murray et al., 2016)</ref> which might be associated with supramodal cognitive abilities.</p><p>A key result from our regression analysis is that variability in SRTs estimated in the Digitsin-Noise test <ref type="bibr" target="#b109">(Smits et al., 2013)</ref> predicted individual differences in audiovisual benefit. It is well-established that hearing impairment is associated with improved lipreading ability (likely due to early developmental experiences: <ref type="bibr" target="#b4">Auer &amp; Bernstein, 2007;</ref><ref type="bibr" target="#b11">Bernstein et al., 2000;</ref><ref type="bibr" target="#b122">Tye et al., 2014)</ref>. Older adults with age-related hearing loss also generally show increased audiovisual enhancement <ref type="bibr" target="#b2">(Altieri &amp; Hudock, 2014;</ref><ref type="bibr" target="#b83">Moradi et al., 2017;</ref><ref type="bibr" target="#b94">Puschmann et al., 2019</ref><ref type="bibr">, but see: Rosemann &amp; Thiel, 2018;</ref><ref type="bibr" target="#b113">Spehar et al., 2008;</ref><ref type="bibr">Tye-Murray et al., 2007a)</ref>. Since we find that mild differences in hearing predict audiovisual benefit independently of lipreading ability, we interpret this in line with a re-weighting of visual perceptual cues during audiovisual speech processing as information conveyed through the auditory modality becomes less reliable (even though visual cues in isolation are not necessarily more reliably identified, as we find no evidence of a relationship here). This is in line with Causal Inference Models of audiovisual perception <ref type="bibr" target="#b60">(Körding et al., 2007;</ref><ref type="bibr" target="#b75">Ma et al., 2009)</ref>, whereby the sensory uncertainty introduced by poorer hearing induces shifts in perceptual weighting. For example, relatedly, a recent study has suggested that children with developmental dyslexia (DD) may increasingly rely on the visual modality to compensate for (auditory) phonological processing difficulties compared to children without DD <ref type="bibr" target="#b41">(Gijbels et al., 2024)</ref>.</p><p>Of course, the cross-sectional nature of our study, and lack of longitudinal data, limits the strength of the conclusions that we can draw. We do not find for instance that individual differences in our hearing measures are correlated with age, and thus cannot draw any conclusions regarding the onset and length of relative difficulties in speech-in-noise perception, or by extension, any role of cross-modal plasticity, which has been proposed to underlie increase audiovisual enhancement in age-related hearing loss <ref type="bibr" target="#b22">(Campbell &amp; Sharma, 2014;</ref><ref type="bibr" target="#b95">Puschmann &amp; Thiel, 2017)</ref>. Nonetheless, it is interesting that even mild differences in speechin-noise perception are reliably associated with enhanced audiovisual benefit.</p><p>At the same time, we find no evidence that audiovisual benefit is related to subjective experiences of hearing impairment (which may result to intentional changes in gaze behaviour, e.g. <ref type="bibr" target="#b99">Rennig et al., 2020)</ref>, or phonetic perceptual gradiency specifically (see also <ref type="bibr" target="#b19">: Brown et al., 2018</ref>, for a similar lack of evidence that categorical perception accounts for individual differences in the McGurk effect). It might be that additional self-report measures, such as the more extensive speech, spatial and qualities of hearing scale (SSQ, <ref type="bibr" target="#b40">Gatehouse and Noble, 2004)</ref> would provide a more reliable score. Alternatively, perhaps such mild differences in hearing are not subjectively noticeable by participants. <ref type="bibr" target="#b117">Suess et al. (2022)</ref> found that subjective hearing impairment measured using the APHAB predicted enhanced lipreading abilities in a sample including participants with moderate hearing-loss.</p><p>To better understand the role of phonetic feature recognition in explaining variability in lipreading ability and audiovisual benefit, we conducted exploratory information transmission analyses. While, overall, our results are in line with previous work <ref type="bibr">(Grant et al., 1998;</ref><ref type="bibr" target="#b65">Lalonde and Werner, 2019)</ref> in showing an auditory advantage for voicing and manner, as well as increased transmission of place information in the visual modality, we also successfully identify a directly relationship between phonetic feature recognition and sentence-level listening benefit.</p><p>We find that while place of articulation is predominantly transmitted through the visual modality, manner is more easily transmitted through the auditory modality, in line with the classic VPAM framework <ref type="bibr" target="#b13">(Binnie et al., 1974;</ref><ref type="bibr" target="#b119">Summerfield, 1979)</ref>, emphasising the complementarity of visual and auditory cues to phonetic perception. This is also in line with what we know from incongruent contexts: in minimal syllables, <ref type="bibr" target="#b65">Lalonde &amp; Werner (2019)</ref> showed that consonant identification was most likely determined by auditory manner and voicing information, or visual place information. Interestingly, we see a main effect of added visual speech even for voicing, for which transmission in the visual modality alone was negligible. We explain this effect, which is super-additive in nature, via temporal cues to voiceonset time <ref type="bibr" target="#b97">(Raphael, 1972</ref><ref type="bibr" target="#b98">(Raphael, , 1975))</ref>, transmitted in combination by combining visual cues for the timing of stop-release with auditory cues to voicing.</p><p>In line with the VPAM framework, <ref type="bibr">Grant et al. (1998)</ref> set out to show that cue complementarity, i.e. the ability to extract manner information in the auditory modality, and place information in the visual modality explains a significant amount of variance in individual differences in audiovisual speech perception. While this was true for nonsense syllables, it failed to generalise to their measure of audiovisual enhancement at the level of sentences. By contrast, we found that ability to extract both manner and place of articulation cues positively predicted individual differences in lipreading ability and audiovisual benefit at the sentence-level. This suggests that better perception of place and manner cues in visual speech (independent of speech perception differences in auditory-only conditions) generalises to improved lipreading and to the use of visual cues at the sentence-level. Interestingly, only for place information did relative feature transmission at the consonant-level (AVlow &gt; AOhigh) predict individual differences in sentence-level audiovisual benefit. This suggests that the complementary nature of visual relative to auditory cue extraction for place of articulation plays a role in an individual's ability to benefit from visual cues in more ecological listening conditions. Our findings therefore confirm that place of articulation perception is an important avenue for audiovisual speech rehabilitation. Item transmission analysis is usually performed on datasets containing a large number of presentations in a small sample (e.g. <ref type="bibr">Lalonde &amp; Werner, 2019: n=9</ref>). Here we show that based on only two presentation of each item we can retrieve sufficiently reliable measures of individual differences in feature transmission to be predictive of audiovisual performance in more ecological speech stimuli.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Demographic variables and clinical implications</head><p>As expected, in our speech perception task we replicate the well-documented age-related decline in both auditory speech perception <ref type="bibr" target="#b39">(Füllgrabe et al., 2014;</ref><ref type="bibr" target="#b42">Gordon-Salant, 2014)</ref> and in lipreading <ref type="bibr" target="#b35">(Feld and Sommers, 2009;</ref><ref type="bibr">Tye-Murray et al., 2007a</ref><ref type="bibr">, 2016)</ref>. A combination of sensory, perceptual and cognitive changes likely contribute to this effect <ref type="bibr" target="#b39">(Füllgrabe et al., 2014;</ref><ref type="bibr" target="#b102">Roberts and Allen, 2016)</ref>. In our sample, older adults were no more likely to have poorer speech reception thresholds than younger adults and did not provide subjective reports of a higher incidence of hearing difficulties. This may be because we intentionally recruited participants without known hearing difficulties and recruited a younger sample (aged ≤60 years) than studies explicitly aimed at investigating age-related changes in (audiovisual) speech perception. This may also explain why we found matrix reasoning performance to remain largely intact in older individuals, whereas previous studies including older adults reported a linear decline in scores <ref type="bibr" target="#b28">(Der et al., 2009;</ref><ref type="bibr" target="#b105">Salthouse, 1993)</ref>. However, as expected, linguistic skills (performance in the Spot-the-Word task) improved with age <ref type="bibr" target="#b46">(Hartshorne and Germine, 2015)</ref>. The age-related decline in unimodal speech perception we observe here may therefore be a combined consequence of both perceptual and cognitive changes throughout the lifespan, including changes to more domain-specific cognition, for example working memory. For example, <ref type="bibr" target="#b39">Füllgrabe et al. (2014)</ref> found that performance in digit span tests accounted for some agerelated deficits in speech-in-noise identification).</p><p>Previous suggestions that deficits in unimodal speech perception could be compensated for by an audiovisual integration capacity <ref type="bibr" target="#b38">(Freiherr et al., 2013;</ref><ref type="bibr" target="#b66">Laurienti et al., 2006)</ref>, in line with the principle of inverse effectiveness <ref type="bibr" target="#b114">(Stein and Meredith, 1993)</ref> have been of great clinical interest. However, this proposal has not been substantiated by the literature on audiovisual speech perception <ref type="bibr" target="#b113">(Spehar et al., 2008;</ref><ref type="bibr" target="#b115">Stevenson et al., 2015;</ref><ref type="bibr">Tye-Murray et al., 2007a;</ref><ref type="bibr" target="#b137">Winneke &amp; Phillips, 2011</ref>, but see <ref type="bibr" target="#b30">Dias et al., 2021)</ref>, or found in the current study. We found no agerelated changes in audiovisual benefit. At similar, intermediate levels of intelligibility for auditory-only and audiovisual speech, it therefore seems that, unlike unimodal speech perception, audiovisual benefit is generally preserved with age.</p><p>Understanding determiners of individual differences in lipreading and audiovisual benefit can help to further our understanding of the mechanisms underlying audiovisual speech perception <ref type="bibr" target="#b59">(Kidd et al., 2018)</ref> and also identify potential rehabilitative strategies to restore speech communication in hearing loss, by helping us understand what participants with better recognition "do right". Our results support the notion that improvements in visual phonemic perception can have substantial positive implications for sentence-level speech perception.</p><p>Recent advances in lipreading training are especially promising in this regard, even though lipreading training has traditionally been notoriously challenging <ref type="bibr" target="#b10">(Bernstein et al., 2022</ref><ref type="bibr" target="#b9">(Bernstein et al., , 2023))</ref>. <ref type="bibr" target="#b36">Files et al. (2015)</ref> suggest that while sub-visemic contrasts are not usually processed, they are available to participants i.e. normally hearing adults can discriminate between phonemes that are usually grouped into the same viseme class, such as /ʒa/ and /da/, suggesting this as a potential avenue for training which can generalise to natural speech (e.g. <ref type="bibr" target="#b9">Bernstein, 2023)</ref>.</p><p>Additionally, lipreading training targeted specifically at the phonemic contrasts that are increasingly degraded in the auditory modality may be especially beneficial in supporting speech communication in multimodal environments.</p><p>Additionally, in our study, we observed a slight lipreading advantage for female participants, which had been reported previously <ref type="bibr" target="#b8">(Bernstein, 2018;</ref><ref type="bibr" target="#b57">Johnson et al., 1988;</ref><ref type="bibr" target="#b133">Watson et al., 1996)</ref>; but see: <ref type="bibr" target="#b4">Auer &amp; Bernstein, 2007;</ref><ref type="bibr">Tye-Murray et al., 2007a)</ref>. The source of these gender differences, and whether they can provide insights for potential avenues for rehabilitation, however, remain elusive. <ref type="bibr" target="#b8">Bernstein (2018)</ref> speculates that differences in response strategiesspecifically, increased guessing -may underlie better lipreading scores. Gender differences in face processing may also underlie this effect: women tend to rate faces as more salient <ref type="bibr" target="#b92">(Proverbio, 2017)</ref>, which may in turn affect face viewing behaviour, in line with the idea that "social tuning", a measure of the frequency of mouth and eye fixations, predicts enhanced visual speech identification in children with and without HL <ref type="bibr" target="#b139">(Worster et al., 2018)</ref>. When speech is degraded, participants have a general tendency to fixate the mouth <ref type="bibr" target="#b99">(Rennig et al., 2020)</ref>, therefore, simply encouraging mouth-looking behaviour in adults is unlikely to make a sustained difference. Overcoming selection bias <ref type="bibr" target="#b5">(Awh et al., 2012)</ref>, towards prioritising the encoding of social rather than phonetic information from the face may also play a role <ref type="bibr" target="#b9">(Bernstein et al., 2023)</ref>. Finally, the effect of visual acuity on audiovisual speech perception remains unclear and understudied, despite well-documented age-related declines in visual abilities <ref type="bibr" target="#b3">(Andersen, 2012)</ref>. Mild differences in general visual acuity in older adults do not seem to predict audiovisual speech <ref type="bibr" target="#b52">(Hickson et al., 2004)</ref>, while early visual deprivation in congenital cataract patients permanently impairs lipreading ability <ref type="bibr" target="#b96">(Putzar et al., 2010)</ref>. In our study, we do not explicitly investigate the role of visual acuity and its relationship with lipreading ability and audiovisual speech perception. Future work employing a similar paradigm, measuring the relative intelligibility of auditory-and audiovisual speech, combined with assessment of domain-general visual abilities would be well-suited to address this question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Effects of different types of acoustic clarity manipulations</head><p>In our work we used a form of artificially-degraded speech -noise vocoded speech -that allows for careful matching of intelligibility (necessary for our comparison of visual and audiovisual speech perception) and provides an approximate simulation of speech transduced by a cochlear implant <ref type="bibr" target="#b107">(Shannon et al, 1995)</ref>. However, our use of this form of artificial degradation leaves unanswered important questions concerning the relationship between our findings and effects of background noise or competing talkers on audiovisual speech perception in ecological listening situations. Visual speech can both a) provide information about the content of speech (object formation) and b) aid the listener in segregating target speech from background noise or distractor speakers (object selection) (e.g. <ref type="bibr" target="#b29">Devergie et al., 2011)</ref>. Our use of noise-vocoded speech focused predominantly on the first case, whereas different types of background noise may introduce additional task demands related to sound source segregation and selective attention to the target sound source.</p><p>It is important to consider whether our results might also apply to more typical listening challenges, such as a speech in noise. It is possible that audiovisual integration for speech perception may not follow the same principles where it is needed to segregate target speech from background noise <ref type="bibr" target="#b14">(Blackburn et al., 2019;</ref><ref type="bibr" target="#b81">Micula et al., 2024)</ref>. Future work is needed to determine how demands related to object selection in ecological settings are affected by the presence of visual speech in listeners with hearing loss or CIs -and its association with supramodal abilities including attention. However, energetic masking introduced by background noise or distractor speakers also leads to the physical degradation of the target signal <ref type="bibr" target="#b20">(Brungart, 2001)</ref>, which can be compensated for by visual speech. For this aspect (i.e.</p><p>object formation) we believe our results should hold true independent of the type of auditory manipulation. Additionally, our work is not inconsistent with conclusions drawn from studies investigating audiovisual speech-in-noise perception (see <ref type="bibr">Sommers, 2021, for review)</ref>.</p><p>Importantly, ceiling effects can introduce confounds when studying predictors of individual differences regardless of the type of acoustic manipulation. It is therefore an important detail that floor/ceiling effects did not contribute to our critical audiovisual benefit measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>Substantial individual differences in lipreading and audiovisual speech perception exist in the general population. Therefore, only for some can the negative consequences of hearing loss be alleviated substantially via increased reliance on visual cues. In our study, added visual cues improved speech scores at the sentence-level by an average of 36% (AV &gt; AO) with a range of 0 to 86%. However, rather than investigating individual differences using these estimates, we used the relative benefit obtained by comparing matched, intermediate-intelligibility auditory-only and auditory-visual speech (AVlow &gt; AOhigh). We found that this measure of audiovisual benefit was stable across sessions and correlated across speech materials: meaningful sentences, monosyllabic words and minimal syllables. This suggests that if we avoid intelligibility confounds, we find evidence that audiovisual speech benefit relies on a shared mechanism across levels of linguistic structure. Information transmission analysis suggested that even at the sentence-level, audiovisual benefit relies fundamentally on the ability to perceive simple articulatory features (such as place of articulation) in visual speech.</p><p>Additionally, we found that individual differences in audiovisual benefit were predicted by better lipreading ability and subclinical indicators of poorer hearing (speech reception thresholds in the Digits-in-Noise task). Overall, this is in line with the idea that variability in unimodal perceptual abilities underlies individual differences in audiovisual speech processing.</p><p>Future research exploring how to best to support older individuals with declining hearing would be best served by focussing on supporting their declining unimodal perceptual abilities (specifically those most relevant in ecological, multimodal contexts, as can be identified using for example information transmission analysis). Rather than resulting from a simple linear combination of auditory and visual speech perception skills (e.g. <ref type="bibr" target="#b126">Tye-Murray et al., 2016)</ref>, however, it seems that individuals with mild speech-in-noise recognition difficulties are more adept at using visual cues in audiovisual context. This was independent of any improvements in lipreading ability. We interpret this in line with a causal inference framework, which has previously been applied to explain perception of incongruent AV stimuli. While we do not find any other behavioural correlates of this enhanced audiovisual benefit, our conclusions are perhaps limited by the cross-sectional nature of this study. Future research adopting a longitudinal approach, or carefully controlled and validated neuroimaging measures (considering intelligibility explicitly as a potentially confounding variable), may be better suited to identifying strategies to aid multimodal speech communication in individuals with hearing loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head><p>See tables III-IV for further details and summary statistics of consonant-and word stimuli. A full list of stimuli can be found in the OSF repository.</p><p>TABLE III. Feature classification scheme for consonants. Consonant Context a Voicing b Manner c Place d b bad 1 0 0 p pet 0 0 0 m met 1 1 0 f fat 0 2 0 v vet 1 2 0 θ thaw 0 2 1 t tad 0 0 1 d dad 1 0 1 n net 1 1 1 k cat 0 0 2 g get 1 0 2 z zed 1 2 1 ʃ shed 0 2 2 tʃ chat 0 3 2 dʒ jet 1 3 2 h hat 0 2 2 j yet 1 4 2 l let 1 4 1 r rat 1 4 1 w wet 1 4 2 a</p><p>The written context in which each response option was presented in the consonant AFC task.</p><p>In the video recording, each consonant was embedded in a minimal CV syllable: each consonant was followed by a /ə/.</p><p>b Describes whether the voicing feature is present (1) or absent (0) for a given consonant.</p><p>c Manner of articulation was grouped into five categories: (0) stop, (1) nasal, (2) fricative, (3) affricate and (4) approximant.</p><p>d Place of articulation was grouped into three categories: (0) front place (bilabial and labiodental) (1) mid place (dental and alveolar), (2) back place (remaining consonants), after <ref type="bibr" target="#b82">Miller &amp; Nicely (1955)</ref> as reported in <ref type="bibr" target="#b56">Jesse &amp; Massaro (2010)</ref>.</p><p>TABLE IV. Summary statistics for the word stimuli presented. See tables V-VII for detailed statistics on model comparisons performed via stepwise deletion as reported in Sections III.C and III.D. Mouth and Facial Informativeness a Frequency HAL Phonological Neighbourhood Density Age of Acquisition (AoA) Number of Phonemes Number of Letters M 0.82 160094.8 23.60 5.26 3.35 4.30 SD 0.38 262900.4 14.45 1.51 0.73 0.91</p><p>Note. All lexical variables were calculated from data in <ref type="bibr" target="#b62">Krason et al. (2023)</ref>, including the Hyperspace Analogue to Language (HAL) frequency norms <ref type="bibr" target="#b7">(Balota et al., 2007;</ref><ref type="bibr" target="#b72">Lund and Burgess, 1996)</ref>, Phonological Neighbourhood Density <ref type="bibr" target="#b71">(Luce and Pisoni, 1998)</ref> and Age of Acquisition (AoA) <ref type="bibr" target="#b63">(Kuperman et al., 2012)</ref>.</p><p>a Mouth and Facial Informativeness (MaFI) is a normed measure quantifying the degree of visual informativeness for each word, based on the phonological distance between target word and speechreading guess <ref type="bibr" target="#b62">(Krason et al., 2023)</ref>.</p><p>TABLE V. Stepwise deletion to compare models predicting audiovisual benefit. df SS RSS AIC F p 43.916 -67.803 MaRs % 0.989 44.905 -67.509 2.095 0.151 MaRs RT 0.255 44.171 -69.207 0.540 0.464 STW 1.18 45.095 -67.073 2.498 0.117 APHAB 0.277 44.193 -69.155 0.587 0.445 DiN SRT 2.03 45.946 -65.149 4.298 0.041* PADRI 0.005 43.921 -69.791 0.011 0.916 Age 1.731 45.647 -65.82 3.666 0.059. Gender 0.236 44.152 -69.251 0.499 0.482 Visual-Only 41.481 85.397 -1.304 87.845 &lt;0.001*** Note. *** p&lt;.001, * p&lt;.05, . p&lt;.06. All predictor variables are scaled.</p><p>TABLE VI. Stepwise deletion to compare models predicting performance in AOlow. df SS RSS AIC F p 66.722 -26.722 MaRs % 1.548 68.27 -26.36 2.181 0.143 MaRs RT 3.625 70.346 -23.274 5.106 0.026* STW 6.449 73.171 -19.219 9.085 0.003** APHAB 0.659 67.381 -27.71 0.928 0.338 DiN SRT 0.024 66.746 -28.686 0.034 0.855 PADRI 0.983 67.704 -27.216 1.384 0.242 Age 9.032 75.753 -15.646 12.724 0.001*** Gender 0.104 66.826 -28.562 0.146 0.703 Note. *** p&lt;.001, ** p&lt;.01, * p&lt;.05. All predictor variables are scaled.</p><p>TABLE VII. Stepwise deletion to compare models predicting performance in VO. df SS RSS AIC F p 79.594 -8.552 MaRs % 1 2.931 82.525 -6.828 3.461 0.066 MaRs RT 1 0.000 79.594 -10.552 0.000 0.988 STW 1 1.748 81.342 -8.314 2.065 0.154 APHAB 1 0.578 80.172 -9.808 0.682 0.411 DiN SRT 1 0.795 80.389 -9.529 0.939 0.335 PADRI 1 0.238 79.832 -10.245 0.280 0.598 Age 1 4.921 84.515 -4.373 5.812 0.018* Gender 1 3.959 83.553 -5.553 4.675 0.033* Note. * p&lt;.05. All predictor variables are scaled.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc>FIG. 1. Experimental paradigms and conditions for audiovisual speech tasks. a) Video stimuli are presented at three levels of linguistic structure: meaningfull sentences, monosyllabic words and minimal consonantvowel syllables, each produced by a different speaker. Participants perform word report in the sentenceand word-level tasks, and a 20 alternative forced choice for the consonant-level task. b) Pilot data for each task illustrates the acoustic clarity levels chosen in the main expriment, matching intelligibility in intermediate auditory-only (blue) and auditory-visual (purple) conditions, seperately for each level of lingusitic structure. Audiovisual benefit is calculated as the difference between intermediate intelligibility conditions (50% accuracy for sentences and words, 60% for consonants). A fifth condition included in the experiment, silent videos (visual-only) is not illustrated here.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FIG. 2 .</head><label>2</label><figDesc>FIG.2. Procedure. Participants completed three sessions, with session 1 and 2 set at least 7 days apart. At the start of each session, Wood's headphone test was used to ensure participants were wearing working headphones. At the start of session 1, participants additionally completed a language background questionnaire, and a practice session was used to familiarise participants with vocoded speech. In sessions 1 and two, participants completed three audiovisual speech tasks each, and in session 3 they completed three hearing and speech perception tasks and two cognitive tasks. Task order was randomised for each participants, and the item-session and item-conditions assigments were counterbalanced.</figDesc><graphic coords="12,78.00,366.72,465.84,181.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FIG. 3 .</head><label>3</label><figDesc>FIG. 3. Individual differences in audio-visual speech perception. a) Results of the sentence-, word-and consonant-level audiovisual speech perception tasks (mean ± SEM), as well as marginal probability densities. Asterisks (***) indicate significant main effects of clarity and modality, p &lt; 0.001. b) Audiovisual benefit for individual benefits is calculated as the difference in performance between auditory-visual low clarity and auditory-only high clarity conditions (grey lines). c) Individual audiovisual benefit is significantly correlated with within-level visual-only perception, with marginal distributions displayed at the right-and top-of the plot, respectively. The dashed grey horizontal line shows the median audiovisual benefit over participants, and the dashed black horizontal line shows zero audiovisual benefit -i.e. equivalent accuracy for AVlow and AOhigh.</figDesc><graphic coords="21,69.60,460.56,453.84,225.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FIG. 4 .</head><label>4</label><figDesc>FIG. 4. Test-retest reliability of audiovisual benefit a) across sessions and b) across levels of linguistic structure. Values shown represent the Spearman-Brown measure, ceiling corrected for within-task re-test reliability in the across-task measure in b).</figDesc><graphic coords="23,77.28,711.36,453.60,63.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>FIG. 5 .</head><label>5</label><figDesc>FIG. 5. Correlation matrix of cognitive, perceptual and demographic predictors included in multiple linear regression analyses. *** p &lt; .001, ** p &lt; .01, * p &lt; .05, corrected for multiple comparisons using the Holm-Bonferroni adjustment. %=Accuracy, RT=Reaction time, MaRs=Matrix Reasoning, STW=Spot the Word, APHAB=Abbreviated Profile of Hearing Aid Benefit (Subjective Hearing), DiN = Digits-in-Noise speech reception thresholds (SRTs; Objective Hearing), PADRI=Percentage of Acoustic Difference Required for Identification (Listen Up).</figDesc><graphic coords="26,87.12,622.56,432.48,158.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>FIG. 6 .</head><label>6</label><figDesc>FIG. 6. Results of multiple linear regression analysis predicting audiovisual benefit. a) Forest plot illustrating results for the full regression model predicting audiovisual benefit PCA scores across levels. Filled circles indicate a significant predictor. b) Partial regression plot for the two predictors which significantly contribute to model fit. c) Variance partitioning results indicate that hearing and lipreading ability independently explain variability in audiovisual benefit. Significance of partitions was tested using regularized discriminant analysis (RDA) across 999 permutations. *** p &lt; .001, ** p &lt; .01, * p &lt; .05.</figDesc><graphic coords="27,36.00,239.80,508.40,320.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>FIG. 7 .</head><label>7</label><figDesc>FIG. 7. Results of multiple linear regression analysis predicting speech perception performance in unimodal conditions. Forest plots illustrating multiple regression results for the unimodal conditions not used to calculate audiovisual benefit: Auditory-Onlylow and Visual-Only (PCA scores across levels of linguistics structure). Colour coding and significance testing as in Fig. 6. *** p &lt;</figDesc><graphic coords="29,106.10,70.90,339.90,258.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>FIG. 8 .</head><label>8</label><figDesc>FIG. 8. Partial regression plots for cognitive and demographic predictors of unimodal speech perception. a) Partial regression plots and variance partitioning results for the regression analysis predicting auditory-</figDesc><graphic coords="30,77.90,125.70,444.85,278.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>FIG. 9 .</head><label>9</label><figDesc>FIG. 9. Results of the feature information transmission analysis (FITA) a) Retrieved values reflecting relative information transmitted for a) voicing, b) manner of articulation and c) place of articulation features across five conditions, including significance levels for pairwise comparisons for conditions of interest, *** p&lt;.001. d) Correlations of sentence-level lip-reading ability (accuracy in the visual-only condition) with visual transmission of voicing, manner and place of articulation features. e) Relationship of sentence-level audiovisual benefit with visual feature transmission. f) Correlation of audiovisual benefit calculated using retrieved ITrel with sentence-level audiovisual benefit. Correlations are corrected for multiple comparisons using the Holm adjustment.</figDesc><graphic coords="32,72.65,96.40,458.13,288.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="8,60.75,272.95,456.88,295.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="8,76.08,586.08,448.56,228.00" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This research was funded by an <rs type="funder">MRC</rs> <rs type="grantName">DTP Studentship</rs> (Reference: <rs type="grantNumber">MR/N013433/1</rs>) and a <rs type="funder">Cambridge Trust Scholarship</rs> to J.V.S, and <rs type="funder">MRC</rs> funding of the <rs type="funder">Cognition and Brain Sciences Unit</rs> (Reference: <rs type="grantNumber">MC_UU_00030/6</rs> supporting M.H.D and M.A). The authors would like to thank <rs type="person">Thu Ngan Dang</rs>, <rs type="person">Shangqiguo Wang</rs> and <rs type="person">Tobias Goehring</rs> for providing stimuli for the Digits-in-Noise Task; <rs type="person">Anna Krason</rs>, <rs type="person">Ye Claudia Zhang</rs> and <rs type="person">Gabriella Vigliocco</rs>, and <rs type="person">Elizabeth Buchanan-Worster</rs> and <rs type="person">Mairéad MacSweeney</rs> for sharing video recordings for the speech stimuli. Finally, we would like to thank <rs type="person">Lucy MacGregor</rs>, <rs type="person">Adam Attahari</rs> and <rs type="person">Harriet J. Smith</rs> for sharing jsPsych scripts for the Spot-the-Word and Listen Up tasks.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_GbQxNT4">
					<idno type="grant-number">MR/N013433/1</idno>
					<orgName type="grant-name">DTP Studentship</orgName>
				</org>
				<org type="funding" xml:id="_MxGvGNx">
					<idno type="grant-number">MC_UU_00030/6</idno>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DATA AVAILABILITY</head><p>The data that support the findings of this study are openly available at: <ref type="url" target="https://osf.io/j56y4/">https://osf.io/j56y4/</ref>, DOI: 10.17605/OSF.IO/J56Y4. Further details on the on the speech stimuli used in this study are available in the original publications for which they were recorded. Sentence-level stimuli have been made available by Aller et al. (2020)  under: <ref type="url" target="https://osf.io/st6fe/">https://osf.io/st6fe/</ref>. Clear speech recordings from which our word-level stimuli were drawn <ref type="bibr" target="#b61">(Krason et al., 2022</ref><ref type="bibr" target="#b62">(Krason et al., , 2023) )</ref> are available under: <ref type="url" target="https://osf.io/gudj6/">https://osf.io/gudj6/</ref>. Examples of consonant-level stimuli have been printed in Pimperton et al. (2019) and Buchanan-Worster et al. (2021).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AUTHOR DECLARATIONS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict of Interest</head><p>The authors declare that they have no conflict of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics Approval</head><p>Ethics approval was obtained by the Cambridge Psychology Research Ethics Committee (application number PRE.2022.056).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Are individual differences in speech reception related to individual differences in cognitive ability? A survey of twenty experimental studies with normal and hearing-impaired adults</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Akeroyd</surname></persName>
		</author>
		<idno type="DOI">10.1080/14992020802301142</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Audiol</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="53" to="S71" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Differential Auditory and Visual Phase-Locking Are Observed during Audio-Visual Benefit and Silent Lip-Reading for Speech Perception</title>
		<author>
			<persName><forename type="first">M</forename><surname>Aller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Økland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Macgregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Blank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Davis</surname></persName>
		</author>
		<idno type="DOI">10.1523/JNEUROSCI.2476-21.2022</idno>
	</analytic>
	<monogr>
		<title level="j">J. Neurosci</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="6108" to="6120" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Assessing variability in audiovisual speech integration skills using capacity and accuracy measures</title>
		<author>
			<persName><forename type="first">N</forename><surname>Altieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hudock</surname></persName>
		</author>
		<idno type="DOI">10.3109/14992027.2014.909053</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Audiol</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="710" to="718" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Aging and vision: changes in function and performance from optics to perception</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Andersen</surname></persName>
		</author>
		<idno type="DOI">10.1002/wcs.1167</idno>
	</analytic>
	<monogr>
		<title level="j">WIREs Cogn. Sci</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="403" to="410" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Enhanced Visual Speech Perception in Individuals With Early-Onset Hearing Impairment</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Bernstein</surname></persName>
		</author>
		<idno type="DOI">10.1044/1092-4388(2007/080</idno>
	</analytic>
	<monogr>
		<title level="j">J. Speech Lang. Hear. Res</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="1157" to="1165" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Top-down versus bottom-up attentional control: a failed theoretical dichotomy</title>
		<author>
			<persName><forename type="first">E</forename><surname>Awh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Belopolsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Theeuwes</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2012.06.010</idno>
	</analytic>
	<monogr>
		<title level="j">Trends Cogn. Sci</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="437" to="443" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The Spot-the-Word test: A robust estimate of verbal intelligence based on lexical decision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Baddeley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Emslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Nimmo-Smith</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.2044-8260.1993.tb01027.x</idno>
	</analytic>
	<monogr>
		<title level="j">Br. J. Clin. Psychol</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="55" to="65" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The English Lexicon Project</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Balota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Yap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Hutchison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Cortese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Loftis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Neely</surname></persName>
		</author>
		<idno type="DOI">10.3758/BF03193014</idno>
	</analytic>
	<monogr>
		<title level="j">Behav. Res. Methods</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="445" to="459" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Response Errors in Females&apos; and Males&apos; Sentence Lipreading Necessitate Structurally Different Models for Predicting Lipreading Accuracy</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Bernstein</surname></persName>
		</author>
		<idno type="DOI">10.1111/lang.12281</idno>
	</analytic>
	<monogr>
		<title level="j">Lang. Learn</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="127" to="158" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Modality-Specific Perceptual Learning of Vocoded Auditory versus Lipread Speech: Different Effects of Prior Information</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Eberhardt</surname></persName>
		</author>
		<idno type="DOI">10.3390/brainsci13071008</idno>
	</analytic>
	<monogr>
		<title level="j">Brain Sci</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">1008</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Lipreading: A Review of Its Continuing Importance for Speech Recognition With an Acquired Hearing Loss and Possibilities for Effective Training</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Eberhardt</surname></persName>
		</author>
		<idno type="DOI">10.1044/2021_AJA-21-00112</idno>
	</analytic>
	<monogr>
		<title level="j">Am. J. Audiol</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="453" to="469" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Speech perception without hearing</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Demorest</surname></persName>
		</author>
		<idno type="DOI">10.3758/BF03205546</idno>
	</analytic>
	<monogr>
		<title level="j">Percept. Psychophys</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="233" to="252" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">How Linguistic Closure and Verbal Working Memory Relate to Speech Recognition in Noise-A Review</title>
		<author>
			<persName><forename type="first">J</forename><surname>Besser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Koelewijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Zekveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Festen</surname></persName>
		</author>
		<idno type="DOI">10.1177/1084713813495459</idno>
	</analytic>
	<monogr>
		<title level="j">Trends Amplif</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="75" to="93" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Auditory and Visual Contributions to the Perception of Consonants</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Binnie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Montgomery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Jackson</surname></persName>
		</author>
		<idno type="DOI">10.1044/jshr.1704.619</idno>
	</analytic>
	<monogr>
		<title level="j">J. Speech Hear. Res</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="619" to="630" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Visual Speech Benefit in Clear and Degraded Speech Depends on the Auditory Intelligibility of the Talker and the Number of Background Talkers</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Blackburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Kitterick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Sumner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Stacey</surname></persName>
		</author>
		<idno type="DOI">10.1177/2331216519837866</idno>
	</analytic>
	<monogr>
		<title level="j">Trends Hear</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">2331216519837866</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Speech perception using combinations of auditory, visual, and tactile information</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Blamey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">I</forename><surname>Alcantara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Whitford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Rehabil. Res. Dev</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="15" to="24" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Autoscore: An open-source automated tool for scoring listener perception of speech</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Borrie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Yoho</surname></persName>
		</author>
		<idno type="DOI">10.1121/1.5087276</idno>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="page" from="392" to="399" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Using fuzzy string matching for automated assessment of listener transcripts in speech intelligibility studies</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Bosker</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13428-021-01542-4</idno>
	</analytic>
	<monogr>
		<title level="j">Behav. Res. Methods</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="1945" to="1953" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Crossmodal integration in the identification of consonant segments</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Braida</surname></persName>
		</author>
		<idno type="DOI">10.1080/14640749108400991</idno>
	</analytic>
	<monogr>
		<title level="j">Q. J. Exp. Psychol. Sect. A</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="647" to="677" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">What accounts for individual differences in susceptibility to the McGurk effect?</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hedayati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mayn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dillman-Hasso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Strand</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0207160</idno>
	</analytic>
	<monogr>
		<title level="j">PloS One</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">207160</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Informational and energetic masking effects in the perception of two simultaneous talkers</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Brungart</surname></persName>
		</author>
		<idno type="DOI">10.1121/1.1345696</idno>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="1101" to="1109" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Speechreading in hearing children can be improved by training</title>
		<author>
			<persName><forename type="first">E</forename><surname>Buchanan-Worster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hulme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dennan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Macsweeney</surname></persName>
		</author>
		<idno type="DOI">10.1111/desc.13124</idno>
	</analytic>
	<monogr>
		<title level="j">Dev. Sci</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page">13124</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cross-Modal Re-Organization in Adults with Early Stage Hearing Loss</title>
		<author>
			<persName><forename type="first">J</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0090594</idno>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">90594</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The natural statistics of audiovisual speech</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Trubanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stillittano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Caplier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Ghazanfar</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pcbi.1000436</idno>
	</analytic>
	<monogr>
		<title level="j">PLoS Comput. Biol</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">1000436</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The matrix reasoning item bank (MaRs-IB): novel, open-access abstract reasoning items for adolescents and adults</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chierchia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fuhrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Knoll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Pi-Sunyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Sakhardande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-J</forename><surname>Blakemore</surname></persName>
		</author>
		<idno type="DOI">10.1098/rsos.190232</idno>
	</analytic>
	<monogr>
		<title level="j">R. Soc. Open Sci</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">190232</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Administration And Application Of The APHAB</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hear. J</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Lexical learning shapes the development of speech perception until late adolescence</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Giannakopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S H</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="DOI">10.31234/osf</idno>
		<ptr target=".io/ktsey.doi:10.31234/osf.io/ktsey" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">jsPsych: Enabling an Open-Source CollaborativeEcosystem of Behavioral Experiments</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>De Leeuw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Luchterhandt</surname></persName>
		</author>
		<idno type="DOI">10.21105/joss.05351</idno>
	</analytic>
	<monogr>
		<title level="j">J. Open Source Softw</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">5351</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Age-related Changes in Memory and Fluid Reasoning in a Sample of Healthy Old People</title>
		<author>
			<persName><forename type="first">G</forename><surname>Der</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Allerhand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Starr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Hofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Deary</surname></persName>
		</author>
		<idno type="DOI">10.1080/13825580903009071</idno>
	</analytic>
	<monogr>
		<title level="j">Aging Neuropsychol. Cogn</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="55" to="70" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The effect of lipreading on primary stream segregation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Devergie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Grimault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gaudrain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Berthommier</surname></persName>
		</author>
		<idno type="DOI">10.1121/1.3592223</idno>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="page" from="283" to="291" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Audiovisual speech is more than the sum of its parts: Auditory-visual superadditivity compensates for age-related declines in audible and lipread speech intelligibility</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Mcclaskey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Harris</surname></persName>
		</author>
		<idno type="DOI">10.1037/pag0000613</idno>
	</analytic>
	<monogr>
		<title level="j">Psychol. Aging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="520" to="530" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Perceptual uncertainty explains activation differences between audiovisual congruent speech and McGurk stimuli</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Noppeney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1002/hbm.26653</idno>
	</analytic>
	<monogr>
		<title level="j">Hum. Brain Mapp</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page">26653</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The Association Between Cognitive Performance and Speech-in-Noise Perception for Adult Listeners: A Systematic Literature Review and Meta-Analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dryden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Henshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Heinrich</surname></persName>
		</author>
		<idno type="DOI">10.1177/2331216517744675</idno>
	</analytic>
	<monogr>
		<title level="j">Trends Hear</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">2331216517744675</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Alpha&apos;s Standard Error (ASE): An Accurate and Precise Confidence Interval Estimate</title>
		<author>
			<persName><forename type="first">A</forename><surname>Duhachek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Iacobucci</surname></persName>
		</author>
		<idno type="DOI">10.1037/0021-9010.89.5.792</idno>
	</analytic>
	<monogr>
		<title level="j">J. Appl. Psychol</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="792" to="808" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Statistical power analyses using G*Power 3.1: Tests for correlation and regression analyses</title>
		<author>
			<persName><forename type="first">F</forename><surname>Faul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Erdfelder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Buchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-G</forename><surname>Lang</surname></persName>
		</author>
		<idno type="DOI">10.3758/BRM.41.4.1149</idno>
	</analytic>
	<monogr>
		<title level="j">Behav. Res. Methods</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1149" to="1160" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Lipreading, Processing Speed, and Working Memory in Younger and Older Adults</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Feld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Sommers</surname></persName>
		</author>
		<idno type="DOI">10.1044/1092-4388(2009/08-0137</idno>
	</analytic>
	<monogr>
		<title level="j">J. Speech Lang. Hear. Res</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="1555" to="1565" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Visual speech discrimination and identification of natural and synthetic consonant stimuli</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Files</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Tjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Bernstein</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2015.00878</idno>
		<ptr target="https://www.frontiersin.org/article/10.3389/fpsyg.2015.00878" />
	</analytic>
	<monogr>
		<title level="j">Front. Psychol</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Evaluating the Effort Expended to Understand Speech in Noise Using a Dual-Task Paradigm: The Effects of Providing Visual Speech Cues</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Gagn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alepins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dubois</surname></persName>
		</author>
		<idno type="DOI">10.1044/1092-4388(2009/08-0140</idno>
	</analytic>
	<monogr>
		<title level="j">J. Speech Lang. Hear. Res</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="18" to="33" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multisensory integration mechanisms during aging</title>
		<author>
			<persName><forename type="first">J</forename><surname>Freiherr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Lundström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Habel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Reetz</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnhum.2013.00863</idno>
		<idno>doi:10.3389/fnhum.2013.00863</idno>
	</analytic>
	<monogr>
		<title level="j">Front. Hum. Neurosci</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Age-group differences in speech identification despite matched audiometrically normal hearing: contributions from auditory temporal processing and cognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Füllgrabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C J</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Stone</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnagi.2014.00347</idno>
	</analytic>
	<monogr>
		<title level="j">Front. Aging Neurosci</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">347</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The Speech, Spatial and Qualities of Hearing Scale (SSQ)</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gatehouse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Noble</surname></persName>
		</author>
		<idno type="DOI">10.1080/14992020400050014</idno>
		<idno>doi:10.1080/14992020400050014</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Audiol</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Children with developmental dyslexia have equivalent audiovisual speech perception performance but their perceptual weights differ</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gijbels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Yeatman</surname></persName>
		</author>
		<idno type="DOI">10.1111/desc.13431</idno>
	</analytic>
	<monogr>
		<title level="j">Dev. Sci</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">13431</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Aging, Hearing Loss, and Speech Recognition: Stop Shouting, I Can&apos;t Understand You</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gordon-Salant</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-4614-9102-6_12</idno>
	</analytic>
	<monogr>
		<title level="m">Perspect. Audit. Res</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Popper</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Fay</surname></persName>
		</editor>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="211" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Measures of auditory-visual integration in nonsense syllables and sentences</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Seitz</surname></persName>
		</author>
		<idno type="DOI">10.1121/1.423751</idno>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="2438" to="2450" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Auditory-visual speech recognition by hearingimpaired subjects: Consonant recognition, sentence recognition, and auditory-visual integration</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Walden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Seitz</surname></persName>
		</author>
		<idno type="DOI">10.1121/1.422788</idno>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="2677" to="2690" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Why is working memory capacity related to matrix reasoning tasks?</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shipstead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Engle</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13421-014-0473-3</idno>
	</analytic>
	<monogr>
		<title level="j">Mem. Cognit</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="389" to="396" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">When Does Cognitive Functioning Peak? The Asynchronous Rise and Fall of Different Cognitive Abilities Across the Life Span</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Hartshorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Germine</surname></persName>
		</author>
		<idno type="DOI">10.1177/0956797614567339</idno>
	</analytic>
	<monogr>
		<title level="j">Psychol. Sci</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="433" to="443" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Entropy Inference and the James-Stein Estimator, with Application to Nonlinear Gene Association Networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Strimmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1469" to="1484" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Audiovisual perception in adverse conditions: Language, speaker and listener effects</title>
		<author>
			<persName><forename type="first">V</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.specom.2010.05.003</idno>
	</analytic>
	<monogr>
		<title level="j">Speech Commun., Non-native Speech Perception in Adverse Conditions</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="996" to="1009" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Talker variability in audio-visual speech perception</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L M</forename><surname>Heald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Nusbaum</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2014.00698</idno>
		<idno>doi:10.3389/fpsyg.2014.00698</idno>
	</analytic>
	<monogr>
		<title level="j">Front. Psychol</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The reliability paradox: Why robust cognitive tasks do not produce reliable individual differences</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hedge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sumner</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13428-017-0935-1</idno>
	</analytic>
	<monogr>
		<title level="j">Behav. Res. Methods</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="1166" to="1186" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">The relationship of speech intelligibility with hearing sensitivity, cognition, and perceived hearing difficulties varies for different speech perception tests</title>
		<author>
			<persName><forename type="first">A</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Henshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Ferguson</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2015.00782</idno>
		<idno>doi:10.3389/fpsyg.2015.00782</idno>
	</analytic>
	<monogr>
		<title level="j">Front. Psychol</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Auditory-visual Speech Perception in Older People: The Effect of Visual Acuity</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hollins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lovie-Kitchin</surname></persName>
		</author>
		<idno type="DOI">10.1375/audi.26.1.3.55988</idno>
	</analytic>
	<monogr>
		<title level="j">Aust. N. Z. J. Audiol</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3" to="11" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Factors Associated With Individual Differences in Clinical Measures of Speech Recognition Among the Elderly</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Humes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">U</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Christensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Cokely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Halling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1044/jshr.3702.465</idno>
	</analytic>
	<monogr>
		<title level="j">J. Speech Lang. Hear. Res</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="465" to="474" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Effects of aging on audio-visual speech integration</title>
		<author>
			<persName><forename type="first">A</forename><surname>Huyse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leybaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Berthommier</surname></persName>
		</author>
		<idno type="DOI">10.1121/1.4894685</idno>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">136</biblScope>
			<biblScope unit="page" from="1918" to="1931" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Modeling the interaction of phonemic intelligibility and lexical structure in audiovisual word recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Iverson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Auer</surname><genName>Jr</genName></persName>
		</author>
		<idno type="DOI">10.1016/S0167-6393(98)00049-1</idno>
	</analytic>
	<monogr>
		<title level="j">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="45" to="63" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">The temporal distribution of information in audiovisual spokenword identification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Massaro</surname></persName>
		</author>
		<idno type="DOI">10.3758/APP.72.1.209</idno>
	</analytic>
	<monogr>
		<title level="j">Atten. Percept. Psychophys</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="209" to="225" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Sex differences in lipreading</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Myslobodsky</surname></persName>
		</author>
		<idno type="DOI">10.3758/BF03334875</idno>
	</analytic>
	<monogr>
		<title level="j">Bull. Psychon. Soc</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="106" to="108" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">The visual speech head start improves perception and reduces superior temporal cortex responses to auditory speech</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Karas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Magnotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Metzger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yoshor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Beauchamp</surname></persName>
		</author>
		<idno type="DOI">10.7554/eLife.48116</idno>
	</analytic>
	<monogr>
		<title level="j">eLife</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Pasternak</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>King</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Mahon</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">48116</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Individual Differences in Language Acquisition and Processing</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kidd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Donnelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Christiansen</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2017.11.006</idno>
	</analytic>
	<monogr>
		<title level="j">Trends Cogn. Sci</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="154" to="169" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Causal Inference in Multisensory Perception</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Körding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Beierholm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Quartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shams</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0000943</idno>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">943</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">The role of iconic gestures and mouth movements in face-to-face communication</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fenton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Varley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Vigliocco</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13423-021-02009-5</idno>
	</analytic>
	<monogr>
		<title level="j">Psychon. Bull. Rev</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Mouth and facial informativeness norms for 2276 English words</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Man</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Vigliocco</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13428-023-02216-z</idno>
		<idno>doi:10.3758/s13428-023-02216-z</idno>
	</analytic>
	<monogr>
		<title level="j">Behav. Res. Methods</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Age-of-acquisition ratings for 30,000 English words</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kuperman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Stadthagen-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brysbaert</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13428-012-0210-4</idno>
	</analytic>
	<monogr>
		<title level="j">Behav. Res. Methods</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="978" to="990" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Audiovisual Enhancement of Speech Perception in Noise by School-Age Children Who Are Hard of Hearing</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lalonde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Mccreery</surname></persName>
		</author>
		<idno type="DOI">10.1097/AUD.0000000000000830</idno>
	</analytic>
	<monogr>
		<title level="j">Ear Hear</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page">705</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Perception of incongruent audiovisual English consonants</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lalonde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Werner</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0213588</idno>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">213588</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Enhanced multisensory integration in older adults</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Laurienti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Burdette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Maldjian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Wallace</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neurobiolaging.2005.05.024</idno>
	</analytic>
	<monogr>
		<title level="j">Neurobiol. Aging</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1155" to="1163" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">jsPsych: A JavaScript library for creating behavioral experiments in a Web browser</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>De Leeuw</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13428-014-0458-y</idno>
	</analytic>
	<monogr>
		<title level="j">Behav. Res. Methods</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Transformed Up-Down Methods in Psychoacoustics</title>
		<author>
			<persName><forename type="first">H</forename><surname>Levitt</surname></persName>
		</author>
		<idno type="DOI">10.1121/1.1912375</idno>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="467" to="477" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">quickpsy: An R Package to Fit Psychometric Functions for Multiple Groups</title>
		<author>
			<persName><forename type="first">D</forename><surname>Linares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>López-Moliner</surname></persName>
		</author>
		<idno type="DOI">10.32614/RJ-2016-008</idno>
	</analytic>
	<monogr>
		<title level="j">R J</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">122</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">On Pushing the Voice-Onset-Time (Vot) Boundary About</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lisker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Liberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Erickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dechovitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mandler</surname></persName>
		</author>
		<idno type="DOI">10.1177/002383097702000303</idno>
	</analytic>
	<monogr>
		<title level="j">Lang. Speech</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="209" to="216" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Recognizing Spoken Words: The Neighborhood Activation Model</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Luce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Pisoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ear Hear</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Producing high-dimensional semantic spaces from lexical cooccurrence</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
		<idno type="DOI">10.3758/BF03204766</idno>
	</analytic>
	<monogr>
		<title level="j">Behav. Res. Methods Instrum. Comput</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="203" to="208" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Visual speechreading and cognitive performance in hearing-impaired and normal hearing children (11-14 years)</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lyxell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Holmberg</surname></persName>
		</author>
		<idno type="DOI">10.1348/000709900158272</idno>
	</analytic>
	<monogr>
		<title level="j">Br. J. Educ. Psychol</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="505" to="518" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Information-processing skill and speech-reading</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lyxell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rönnberg</surname></persName>
		</author>
		<idno type="DOI">10.3109/03005368909076523</idno>
		<ptr target="https://www.tandfonline.com/doi/abs/10.3109/03005368909076523" />
	</analytic>
	<monogr>
		<title level="j">Br. J. Audiol</title>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Lip-Reading Aids Word Recognition Most in Moderate Noise: A Bayesian Explanation Using High-Dimensional Feature Space</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Foxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Parra</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0004638</idno>
		<idno>doi:10.1371/journal.pone.0004638</idno>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Causal Contributions of the Domain-General (Multiple Demand) and the Language-Selective Brain Networks to Perceptual and Semantic Challenges in Speech Comprehension</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Macgregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Balewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Erzinçlioğlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Rodd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Duncan</surname></persName>
		</author>
		<idno type="DOI">10.1162/nol_a_00081</idno>
	</analytic>
	<monogr>
		<title level="j">Neurobiol. Lang</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="665" to="698" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Quantifying the contribution of vision to speech perception in noise</title>
		<author>
			<persName><forename type="first">A</forename><surname>Macleod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Summerfield</surname></persName>
		</author>
		<idno type="DOI">10.3109/03005368709077786</idno>
	</analytic>
	<monogr>
		<title level="j">Br. J. Audiol</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="131" to="141" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Weak observer-level correlation and strong stimulus-level correlation between the McGurk effect and audiovisual speech-in-noise: A causal inference explanation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Magnotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Dzeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wegner-Clemens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rennig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Beauchamp</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cortex.2020.10.002</idno>
	</analytic>
	<monogr>
		<title level="j">Cortex J. Devoted Study Nerv. Syst. Behav</title>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="page" from="371" to="383" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Evaluation and integration of visual and auditory information in speech perception</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Massaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="DOI">10.1037/0096-1523.9.5.753</idno>
	</analytic>
	<monogr>
		<title level="j">J. Exp. Psychol. Hum. Percept. Perform</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="753" to="771" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Hearing lips and seeing voices</title>
		<author>
			<persName><forename type="first">H</forename><surname>Mcgurk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Macdonald</surname></persName>
		</author>
		<idno type="DOI">10.1038/264746a0</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">264</biblScope>
			<biblScope unit="page" from="746" to="748" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Relationships Between Hearing Status, Cognitive Abilities, and Reliance on Visual and Contextual Cues</title>
		<author>
			<persName><forename type="first">A</forename><surname>Micula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Holmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Danielsson</surname></persName>
		</author>
		<idno type="DOI">10.1097/AUD.0000000000001596</idno>
		<idno>doi:10.1097/AUD.0000000000001596</idno>
	</analytic>
	<monogr>
		<title level="j">Ear Hear</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">An Analysis of Perceptual Confusions Among Some English Consonants</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Nicely</surname></persName>
		</author>
		<idno type="DOI">10.1121/1.1907526</idno>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="338" to="352" />
			<date type="published" when="1955">1955</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Visual Cues Contribute Differentially to Audiovisual Perception of Consonants and Vowels in Improving Recognition and Reducing Cognitive Demands in Listeners With Hearing Impairment Using Hearing Aids</title>
		<author>
			<persName><forename type="first">S</forename><surname>Moradi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lidestam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Danielsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H N</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<idno type="DOI">10.1044/2016_JSLHR-H-16-0160</idno>
	</analytic>
	<monogr>
		<title level="j">J. Speech Lang. Hear. Res</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="2687" to="2703" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Gated audiovisual speech identification in silence vs. noise: effects on time and accuracy</title>
		<author>
			<persName><forename type="first">S</forename><surname>Moradi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lidestam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rönnberg</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2013.00359</idno>
		<idno>doi:10.3389/fpsyg.2013.00359</idno>
	</analytic>
	<monogr>
		<title level="j">Front. Psychol</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Gated auditory speech perception: effects of listening conditions and cognitive capacity</title>
		<author>
			<persName><forename type="first">S</forename><surname>Moradi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lidestam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saremi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rönnberg</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2014.00531</idno>
		<idno>doi:10.3389/fpsyg.2014.00531</idno>
	</analytic>
	<monogr>
		<title level="j">Front. Psychol</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">PanPhon: A Resource for Mapping IPA Segments to Articulatory Feature Vectors</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Mortensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Littell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bharadwaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Levin</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/C16-1328" />
	</analytic>
	<monogr>
		<title level="m">Proc. COLING 2016 26th Int. Conf. Comput. Linguist. Tech. Pap., The COLING 2016 Organizing Committee</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Matsumoto</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Prasad</surname></persName>
		</editor>
		<meeting>COLING 2016 26th Int. Conf. Comput. Linguist. Tech. Pap., The COLING 2016 Organizing Committee<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3475" to="3484" />
		</imprint>
	</monogr>
	<note>Presented at the COLING 2016</note>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Information transmission analysis for continuous speech features</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J J</forename><surname>Oosthuizen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Hanekom</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.specom.2016.06.003</idno>
	</analytic>
	<monogr>
		<title level="j">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="53" to="66" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Prediction and constraint in audiovisual speech perception</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Peelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Sommers</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cortex.2015.03.006</idno>
	</analytic>
	<monogr>
		<title level="j">Cortex J. Devoted Study Nerv. Syst. Behav</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="169" to="181" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">How young and old adults listen to and remember speech in noise</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Pichora-Fuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Daneman</surname></persName>
		</author>
		<idno type="DOI">10.1121/1.412282</idno>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="593" to="608" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Computerized Speechreading Training for Deaf Children: A Randomized Controlled Trial</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pimperton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hulme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Beedie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ralph-Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Worster</surname></persName>
		</author>
		<idno type="DOI">10.1044/2019_JSLHR-H-19-0073</idno>
	</analytic>
	<monogr>
		<title level="j">J. Speech Lang. Hear. Res</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="2882" to="2894" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Can auditory and visual speech perception be trained within a group setting?</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Preminger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Ziegler</surname></persName>
		</author>
		<idno type="DOI">10.1044/1059-0889(2008/009</idno>
	</analytic>
	<monogr>
		<title level="j">Am. J. Audiol</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="80" to="97" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Sex differences in social cognition: The case of face processing</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Proverbio</surname></persName>
		</author>
		<idno type="DOI">10.1002/jnr.23817</idno>
	</analytic>
	<monogr>
		<title level="j">J. Neurosci. Res</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="222" to="234" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Hearing loss and quality of life</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Punch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jcomdis.2019.01.001</idno>
	</analytic>
	<monogr>
		<title level="j">J. Commun. Disord</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="33" to="45" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Hearing-impaired listeners show increased audiovisual benefit when listening to speech in noise</title>
		<author>
			<persName><forename type="first">S</forename><surname>Puschmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Daeglau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stropahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mirkovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rosemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Thiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Debener</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuroimage.2019.04.017</idno>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">196</biblScope>
			<biblScope unit="page" from="261" to="268" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Changed crossmodal functional connectivity in older adults with hearing loss,&quot; Cortex, Is a</title>
		<author>
			<persName><forename type="first">S</forename><surname>Puschmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Thiel</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cortex.2016.10.014</idno>
	</analytic>
	<monogr>
		<title level="j">brain model sufficient?</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="109" to="122" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">The neural basis of lipreading capabilities is altered by early visual deprivation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Putzar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goerendt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Heed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Büchel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Röder</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuropsychologia.2010.04.007</idno>
	</analytic>
	<monogr>
		<title level="j">Neuropsychologia</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="2158" to="2166" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Preceding Vowel Duration as a Cue to the Perception of the Voicing Characteristic of Word-Final Consonants in American English</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Raphael</surname></persName>
		</author>
		<idno type="DOI">10.1121/1.1912974</idno>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="1296" to="1303" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">The physiological control of durational differences between vowels preceding voiced and voiceless consonants in English</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Raphael</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0095-4470(19)31284-7</idno>
	</analytic>
	<monogr>
		<title level="j">J. Phon</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="25" to="33" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Face viewing behavior predicts multisensory gain during speech perception</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rennig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wegner-Clemens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Beauchamp</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13423-019-01665-y</idno>
	</analytic>
	<monogr>
		<title level="j">Psychon. Bull. Rev</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="70" to="77" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">psych: Procedures for Psychological, Psychometric, and Personality Research</title>
		<author>
			<persName><forename type="first">W</forename><surname>Revelle</surname></persName>
		</author>
		<ptr target="https://cran.r-project.org/web/packages/psych/index.html" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">The effects of auditory-visual vowel identification training on speech recognition under difficult listening conditions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Richie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kewley-Port</surname></persName>
		</author>
		<idno type="DOI">10.1044/1092-4388(2008/07-0069</idno>
	</analytic>
	<monogr>
		<title level="j">J. Speech Lang. Hear. Res. JSLHR</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="1607" to="1619" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Perception and Cognition in the Ageing Brain: A Brief Review of the Short-and Long-Term Links between Perceptual and Cognitive Decline</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Allen</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnagi.2016.00039</idno>
		<idno>doi:10.3389/fnagi.2016.00039</idno>
	</analytic>
	<monogr>
		<title level="j">Front. Aging Neurosci</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Moving experimental psychology online: How to obtain high quality data when we can&apos;t see our participants</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Rodd</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jml.2023.104472</idno>
	</analytic>
	<monogr>
		<title level="j">J. Mem. Lang</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="page">104472</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Audio-visual speech processing in age-related hearing loss: Stronger integration and increased frontal lobe recruitment</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rosemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Thiel</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuroimage.2018.04.023</idno>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">175</biblScope>
			<biblScope unit="page" from="425" to="437" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Influence of working memory on adult age differences in matrix reasoning</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Salthouse</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.2044-8295.1993.tb02472.x</idno>
	</analytic>
	<monogr>
		<title level="j">Br. J. Psychol</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="171" to="199" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<title level="m" type="main">{fuzzywuzzy}: Fuzzy String Matching in Python}</title>
		<ptr target="https://github.com/seatgeek/fuzzywuzzy" />
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>SeatGeek Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Speech Recognition with Primarily Temporal Cues</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Shannon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-G</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wygonski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ekelid</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.270.5234.303</idno>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">270</biblScope>
			<biblScope unit="page" from="303" to="304" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Audio-Visual and Meaningful Semantic Context Enhancements in Older and Younger Adults</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Smayda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J V</forename><surname>Engen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0152773</idno>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">152773</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">The digits-in-noise test: Assessing auditory speech recognition abilities in noise</title>
		<author>
			<persName><forename type="first">C</forename><surname>Smits</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Theo Goverts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Festen</surname></persName>
		</author>
		<idno type="DOI">10.1121/1.4789933</idno>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="page" from="1693" to="1706" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Perceptual learning of degraded speech by minimizing prediction error</title>
		<author>
			<persName><forename type="first">E</forename><surname>Sohoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Davis</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1523266113</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl. Acad. Sci</title>
		<meeting>Natl. Acad. Sci</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="1747" to="E1756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title level="m" type="main">Santa Claus, the Tooth Fairy, and Auditory-Visual Integration</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Sommers</surname></persName>
		</author>
		<idno type="DOI">10.1002/9781119184096</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>John Wiley &amp; Sons, Ltd</publisher>
			<biblScope unit="page" from="517" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Auditory-Visual Speech Perception and Auditory-Visual Enhancement in Normal-Hearing Younger and Older Adults</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Sommers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tye-Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Spehar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ear Hear</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="263" to="275" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Intra-versus intermodal integration in young and older adults</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Spehar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tye-Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Sommers</surname></persName>
		</author>
		<idno type="DOI">10.1121/1.2890748</idno>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page" from="2858" to="2866" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<monogr>
		<title level="m" type="main">The Merging of the Senses</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Meredith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>MIT Press</publisher>
			<biblScope unit="page">231</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Deficits in audiovisual speech perception in normal aging emerge at the level of whole-word recognition</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Stevenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Nelms</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Baum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zurkovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Barense</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Newhouse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Wallace</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neurobiolaging.2014.08.003</idno>
	</analytic>
	<monogr>
		<title level="j">Neurobiol. Aging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="283" to="291" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Individual differences in susceptibility to the McGurk effect: links with lipreading and detecting audiovisual incongruity</title>
		<author>
			<persName><forename type="first">J</forename><surname>Strand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cooperman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Simenstad</surname></persName>
		</author>
		<idno type="DOI">10.1044/2014_JSLHR-H-14-0059</idno>
	</analytic>
	<monogr>
		<title level="j">J. Speech Lang. Hear. Res. JSLHR</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="2322" to="2331" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Influence of linguistic properties and hearing impairment on visual speech perception skills in the German language</title>
		<author>
			<persName><forename type="first">N</forename><surname>Suess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hauswald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Zehentner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Depireux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Herzog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rösch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Weisz</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0275585</idno>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">275585</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Visual Contribution to Speech Intelligibility in Noise</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Sumby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pollack</surname></persName>
		</author>
		<idno type="DOI">10.1121/1.1907309</idno>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="212" to="215" />
			<date type="published" when="1954">1954</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Use of Visual Information for Phonetic Perception</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Summerfield</surname></persName>
		</author>
		<idno type="DOI">10.1159/000259969</idno>
	</analytic>
	<monogr>
		<title level="j">Phonetica</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="314" to="331" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Lipreading and audio-visual speech perception</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Summerfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cowey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Perrett</surname></persName>
		</author>
		<idno type="DOI">10.1098/rstb.1992.0009</idno>
	</analytic>
	<monogr>
		<title level="j">Philos. Trans. R. Soc. Lond. B. Biol. Sci</title>
		<imprint>
			<biblScope unit="volume">335</biblScope>
			<biblScope unit="page" from="71" to="78" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Audio-visual Speechreading in a Group of Hearing Aid Users the Effects of Onset Age, Handicap Age, and Degree of Hearing Loss</title>
		<author>
			<persName><forename type="first">I</forename><surname>Tillberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rönnberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Svärd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ahlner</surname></persName>
		</author>
		<idno type="DOI">10.3109/01050399609074966</idno>
	</analytic>
	<monogr>
		<title level="j">Scand. Audiol</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="267" to="272" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Lipreading in School-Age Children: The Roles of Age, Hearing Status, and Cognitive Ability</title>
		<author>
			<persName><forename type="first">, -Murray</forename><surname>Tye</surname></persName>
		</author>
		<author>
			<persName><surname>Nancy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Spehar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Myerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Sommers</surname></persName>
		</author>
		<idno type="DOI">10.1044/2013_JSLHR-H-12-0273</idno>
	</analytic>
	<monogr>
		<title level="j">J. Speech Lang. Hear. Res</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="556" to="565" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Audiovisual Integration and Lipreading Abilities of Older Adults with Normal and Impaired Hearing</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tye-Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Sommers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Spehar</surname></persName>
		</author>
		<idno type="DOI">10.1097/AUD.0b013e31812f7185</idno>
	</analytic>
	<monogr>
		<title level="j">Ear Hear</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">656</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Auditory and Visual Lexical Neighborhoods in Audiovisual Speech Perception</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tye-Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sommers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Spehar</surname></persName>
		</author>
		<idno type="DOI">10.1177/1084713807307409</idno>
	</analytic>
	<monogr>
		<title level="j">Trends Amplif</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="233" to="241" />
			<date type="published" when="2007">2007b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Aging, Audiovisual Integration, and the Principle of Inverse Effectiveness</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tye-Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sommers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Spehar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Myerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hale</surname></persName>
		</author>
		<idno type="DOI">10.1097/AUD.0b013e3181ddf7ff</idno>
	</analytic>
	<monogr>
		<title level="j">Ear Hear</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">636</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Lipreading and audiovisual speech recognition across the adult lifespan: Implications for audiovisual integration</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tye-Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Spehar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Myerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sommers</surname></persName>
		</author>
		<idno type="DOI">10.1037/pag0000094</idno>
	</analytic>
	<monogr>
		<title level="j">Psychol. Aging</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="380" to="389" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">The digit triplet test: a scoping review</title>
		<author>
			<persName><forename type="first">E</forename><surname>Van Den Borre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Denys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Wieringen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wouters</surname></persName>
		</author>
		<idno type="DOI">10.1080/14992027.2021.1902579</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Audiol</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="946" to="963" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Enhancing Speech Intelligibility: Interactions Among Context, Modality, Speech Style, and Masker</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K J</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E B</forename><surname>Phelps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Smiljanic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<idno type="DOI">10.1044/JSLHR-H-13-0076</idno>
	</analytic>
	<monogr>
		<title level="j">J. Speech Lang. Hear. Res</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="1908" to="1918" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Audiovisual sentence recognition not predicted by susceptibility to the McGurk effect</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Van Engen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13414-016-1238-9</idno>
	</analytic>
	<monogr>
		<title level="j">Atten. Percept. Psychophys</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="396" to="403" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Viseme classifications of Dutch consonants and vowels</title>
		<author>
			<persName><forename type="first">N</forename><surname>Van Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M I</forename><surname>Huiskamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Bosman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Smoorenburg</surname></persName>
		</author>
		<idno type="DOI">10.1121/1.411324</idno>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="1341" to="1355" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Auditory and Audiovisual Feature Transmission in Hearing-Impaired Adults</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Walden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Prosek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Worthington</surname></persName>
		</author>
		<idno type="DOI">10.1044/jshr.1802.272</idno>
	</analytic>
	<monogr>
		<title level="j">J. Speech Hear. Res</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="272" to="280" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Visual speech speeds up the neural processing of auditory speech</title>
		<author>
			<persName><forename type="first">V</forename><surname>Van Wassenhove</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Poeppel</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.0408949102</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl. Acad. Sci</title>
		<meeting>Natl. Acad. Sci</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="1181" to="1186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Auditory and visual speech perception: Confirmation of a modality-independent source of individual differences in speech recognition</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1121/1.416300</idno>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="1153" to="1162" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Speech and non-speech measures of audiovisual integration are not correlated</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M P</forename><surname>Wilbiks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Strand</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13414-022-02517-z</idno>
	</analytic>
	<monogr>
		<title level="j">Atten. Percept. Psychophys</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="1809" to="1819" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">New rule use drives the relation between working memory capacity and Raven&apos;s Advanced Progressive Matrices</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wiley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Jarosz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Cushen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J H</forename><surname>Colflesh</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0021613</idno>
	</analytic>
	<monogr>
		<title level="j">J. Exp. Psychol. Learn. Mem. Cogn</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="256" to="263" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Symbolic Description of Factorial Models for Analysis of Variance</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">N</forename><surname>Wilkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rogers</surname></persName>
		</author>
		<idno type="DOI">10.2307/2346786</idno>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. Ser. C Appl. Stat</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="392" to="399" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Does audiovisual speech offer a fountain of youth for old ears? An event-related brain potential study of age differences in audiovisual speech perception</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Winneke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Phillips</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0021683</idno>
	</analytic>
	<monogr>
		<title level="j">Psychol. Aging</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="427" to="438" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Headphone screening to facilitate web-based auditory experiments</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J P</forename><surname>Woods</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Traer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Mcdermott</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13414-017-1361-2</idno>
	</analytic>
	<monogr>
		<title level="j">Atten. Percept. Psychophys</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="2064" to="2072" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Eye Movements During Visual Speech Perception in Deaf and Hearing Children</title>
		<author>
			<persName><forename type="first">E</forename><surname>Worster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pimperton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ralph-Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Monroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hulme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Macsweeney</surname></persName>
		</author>
		<idno type="DOI">10.1111/lang.12264</idno>
	</analytic>
	<monogr>
		<title level="j">Lang. Learn</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="159" to="179" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Perception of Rhythmic Speech Is Modulated by Focal Bilateral Transcranial Alternating Current Stimulation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zoefel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Allard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Davis</surname></persName>
		</author>
		<idno type="DOI">10.1162/jocn_a_01490</idno>
	</analytic>
	<monogr>
		<title level="j">J. Cogn. Neurosci</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="226" to="240" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">An item response theory analysis of the matrix reasoning item bank (MaRs-IB)</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zorowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chierchia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-J</forename><surname>Blakemore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Daw</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13428-023-02067-8</idno>
	</analytic>
	<monogr>
		<title level="j">Behav. Res. Methods</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="1104" to="1122" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
