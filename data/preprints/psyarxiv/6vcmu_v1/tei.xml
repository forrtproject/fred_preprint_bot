<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Effect of Lightness/Pitch Correspondence on Visual Working Memory Perceptual Load Modulates the Effect of Lightness/Pitch Correspondence on Visual Working Memory Performance</title>
				<funder>
					<orgName type="full">University of Derby</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Agnes</forename><surname>Rigo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Psychology</orgName>
								<orgName type="department" key="dep2">Department of Health, Psychology and Social Care</orgName>
								<orgName type="institution">University of Derby</orgName>
								<address>
									<settlement>Derby</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Division of Science and Mathematics</orgName>
								<orgName type="institution">New York University Abu Dhabi</orgName>
								<address>
									<settlement>Abu Dhabi</settlement>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Hettie</forename><surname>Roebuck</surname></persName>
							<email>h.roebuck@derby.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Psychology</orgName>
								<orgName type="department" key="dep2">Department of Health, Psychology and Social Care</orgName>
								<orgName type="institution">University of Derby</orgName>
								<address>
									<settlement>Derby</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Barbara</forename><surname>Manini</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Psychology</orgName>
								<orgName type="department" key="dep2">Department of Health, Psychology and Social Care</orgName>
								<orgName type="institution">University of Derby</orgName>
								<address>
									<settlement>Derby</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">School of Psychology</orgName>
								<orgName type="department" key="dep2">Department of Health, Psychology and Social Care</orgName>
								<orgName type="institution">University of Derby</orgName>
								<address>
									<settlement>Derby</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The Effect of Lightness/Pitch Correspondence on Visual Working Memory Perceptual Load Modulates the Effect of Lightness/Pitch Correspondence on Visual Working Memory Performance</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">364F0CDCE5E3C491648C52884C07E29A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-23T06:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>visual working memory</term>
					<term>cross-modal correspondence</term>
					<term>audiovisual stimuli</term>
					<term>perceptual load</term>
					<term>change detection task</term>
					<term>Bayesian learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Prior studies demonstrate that cross-modal correspondence, the seemingly arbitrary association of features across different sensory modalities, enhances working memory performance. Incongruent background noise has also been shown to aid visual working memory. The current study builds on these premises to investigate the effect of lightness/pitch audiovisual correspondence on visual working memory. We designed a black-and-white orientation change detection task, where the visual stimuli were paired with high-and low-pitched sounds. We compared change-detection performance in conditions with audiovisual correspondence with performance under non-corresponding and visual-only conditions. Additionally, we explored the impact of perceptual load, the stage of memory processing during which audiovisual correspondence is displayed, and the direction of attention to the auditory modality. We found that, in the lightness/pitch domain, cross-modal correspondence does not automatically enhance visual working memory accuracy and reaction time; instead, the salience of the effect is moderated by perceptual load. Lightness/pitch correspondence improved performance only under high perceptual load, with the effect being strongest when corresponding stimuli were displayed during both memory encoding and recall. In contrast, under conditions of low perceptual load, the mere presence of auditory pitch, irrespective of cross-modal correspondence, improved performance, likely by increasing alertness. The study demonstrates that lightness/pitch correspondence does not depend on conscious selective attention and suggests that cross-modal correspondence may serve a functional role beyond sensory integration. We frame our interpretation considering Running head: The Effect of Lightness/Pitch Correspondence on Visual Working Memory Bayesian theory with the cognitive system relying more heavily on statistical learning principles under high perceptual demands.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>In our daily lives, we experience a constant flow of information. Perception is formed through the integration of simultaneous multisensory stimuli, enabling people to perceive the world as a seamless, unified experience. To achieve coherent cognition and perception, the human brain identifies whether signals received through multiple sensory modalities originate from a common source and unites or segregates them accordingly <ref type="bibr" target="#b18">(Ernst &amp; Bülthoff, 2004;</ref><ref type="bibr" target="#b43">Noppeney, 2021)</ref>. Cross-modal correspondence (CC) refers to the association of often seemingly arbitrary features across senses. For example, the association between a dark or light color and a high-or low-pitched sound.</p><p>This correspondence has been shown to aid the integration and interpretation of multisensory information, contributing to coherent perception of the environment <ref type="bibr" target="#b55">(Spence, 2011)</ref>.</p><p>Prior research indicates that audiovisual intersensory binding is influenced by several types of CCs, such as associations between shape, elevation and darkness in the visual domain and pitch of sound in the auditory domain <ref type="bibr" target="#b8">(Brunetti et al., 2017;</ref><ref type="bibr" target="#b15">Ćwiek et al., 2022)</ref>. Over the years, the effect of CCs on perception and performance has been investigated in a variety of behavioral tasks (for reviews, see <ref type="bibr" target="#b55">Spence, 2011;</ref><ref type="bibr" target="#b57">Spence &amp; Deroy, 2013)</ref>. One of the most extensively studied auditory attributes in CC research is pitch <ref type="bibr" target="#b56">(Spence, 2020;</ref><ref type="bibr">Uno, 2022)</ref>, and it has repeatedly been shown to correspond with visual features such as brightness and darkness <ref type="bibr" target="#b39">(Marks, 1987;</ref><ref type="bibr" target="#b40">McEwan et al., 2024)</ref>.</p><p>Research by <ref type="bibr" target="#b64">Zeljko et al. (2019)</ref> indicates that light and dark objects are significantly easier to distinguish when they are paired with corresponding high or low pitches. In a Running head: The Effect of Lightness/Pitch Correspondence on Visual Working Memory later study, <ref type="bibr" target="#b63">Zeljko et al. (2021)</ref> found that lightness/pitch congruence can even influence the perception of ambiguous illusions such as the <ref type="bibr" target="#b51">Rubin (1915)</ref> face/vase.</p><p>CC not only influences perception but also higher-level cognitive processes such as working memory (WM). For example, research on grapheme-color synesthesia has shown that CC improves WM performance <ref type="bibr" target="#b59">(Terhune et al., 2013)</ref>. Moreover, <ref type="bibr" target="#b38">Makovac et al. (2014)</ref> found that a short delay period (150 ms) between cue and probe, compared to a long one (1150 ms), facilitates the recognition of shapes by a congruent sound and enhances visual working memory (VWM) performance. These findings suggest that the benefits of cross-modal cueing are time-sensitive. In their study on the effect of CC on WM, <ref type="bibr" target="#b8">Brunetti et al. (2017)</ref> found that CCs can enhance both WM accuracy and reaction time (RT). The authors also explored the influence of attended modality on the interaction between CC and WM, finding that performance remained stable when participants were instructed to pay attention to audiovisual stimuli over unimodal stimuli.</p><p>As we experience a continuous flow of information, some relevant and some irrelevant to our goals, our WM capacity is limited by our finite selective attention. In this context, perceptual load (e.g., the amount of sensory information that needs to be processed) moderates selective attention. Specifically, previous research suggests that a higher perceptual load facilitates filtering out task-irrelevant information <ref type="bibr" target="#b32">(Lavie &amp; Tsal, 1994;</ref><ref type="bibr" target="#b29">Lavie, 1995</ref><ref type="bibr" target="#b31">, Lavie et al., 2004)</ref>. In the context of cross-modal correspondence, more research is needed to explore how factors that influence perceptual and postperceptual processes may affect the relationship between CC and WM. The current study aims to fill this gap by investigating the possible modulating effect of perceptual Running head: The Effect of Lightness/Pitch Correspondence on Visual Working Memory load on the interaction between CC and VWM. This can provide insight into how attentional mechanisms operate under varying levels of sensory processing demand.</p><p>Additionally, how perceptual load may, in turn, affect cognitive control and the brain's information processing strategy.</p><p>The present study was designed to provide a comprehensive understanding of the effect of lightness/pitch CC on VWM performance. Specifically, we want to explore how the processing stage at which CC is displayed, perceptual load, and modalityspecific selective attention affect this interaction. Prior studies measuring the effect of CC on WM have used n-back tasks (e.g., <ref type="bibr" target="#b8">Brunetti et al., 2017;</ref><ref type="bibr" target="#b59">Terhune et al., 2013)</ref>.</p><p>While n-back is a popular method for assessing WM processing, it is known to evoke a relatively high cognitive load <ref type="bibr" target="#b28">(Lamichhane et al., 2020)</ref>. The present study manipulates the amount of perceptual load, utilizing an orientation change detection task designed explicitly for this experiment. Unlike n-back tasks, the change detection paradigm is thought to isolate VWM without extensively involving other perceptual and cognitive processes, which is suggested to result in purer VWM measurements <ref type="bibr" target="#b25">(Kane et al., 2007)</ref>. Moreover, by using an orientation change detection task perceptual load can be controlled more effectively <ref type="bibr" target="#b24">(Jaeggi et al., 2008)</ref>.</p><p>Past studies indicate that enhancement to VWM is not limited to audiovisual CC, but has also been observed with other sounds, such as white noise and background speech <ref type="bibr" target="#b21">(Han et al., 2013;</ref><ref type="bibr" target="#b22">Han et al., 2021)</ref>. Therefore, we aimed to investigate the effect of both corresponding and non-corresponding audiovisual stimuli on VWM performance. To assess at which stage of WM cross-modal sensory information might facilitate accuracy and RT, we created four audiovisual conditions by manipulating the Running head: The Effect of Lightness/Pitch Correspondence on Visual Working Memory presence and absence of CC during memory encoding (when the sample array was presented) and memory recall (when the target array was presented). We compared these conditions with VWM performance without any audio presentation ('Visual-only stimuli'). To control for perceptual load, we created a 'Low load' and a 'High load' condition of the task by manipulating the number of objects displayed in each array.</p><p>In line with previous research that found that audiovisual CC can enhance WM <ref type="bibr" target="#b8">(Brunetti et al., 2017;</ref><ref type="bibr" target="#b38">Makovac et al., 2014)</ref>, we hypothesized that audiovisual CC would increase participants' performance in the orientation change-detection task, both in terms of accuracy and RT. Specifically, we predicted that performance would be higher during the audiovisual correspondent conditions than during the non-correspondent, or 'Visual-only' conditions. We also assessed if CC has a more significant impact on VWM when it is presented during memory encoding (e.g., at the presentation of the sample stimulus), recall (e.g., at the presentation of the target stimulus), or both stages. If correspondence during encoding improves VWM performance more than in other conditions, it could indicate that the effect of CC on VWM is already present during the early processing stages <ref type="bibr" target="#b42">(Murray et al., 2008;</ref><ref type="bibr">Zlejko et al., 2019)</ref> implying that audiovisual correspondence plays a significant role in enhancing the initial formation of memories. However, if CC presented during the memory recall phase increases VWM performance, it might point towards a more post-perceptual effect, supporting the notion that WM selection relies on internally directed shifts of attention that highlight taskrelevant information <ref type="bibr" target="#b65">(Zhou et al., 2022)</ref>. Additionally, if VWM performance is most significantly enhanced in the scenario where CC occurs at both stages, it could mean Running head: The Effect of Lightness/Pitch Correspondence on Visual Working Memory that multisensory integration plays a role throughout the entire VWM process in a distributed nature <ref type="bibr" target="#b12">(Christophel et al., 2017;</ref><ref type="bibr" target="#b40">McEwan et al., 2024)</ref>.</p><p>The load theory of <ref type="bibr" target="#b31">Lavie et al. (2004)</ref> suggests that individuals have finite attentional capacity, and whether conceptually irrelevant information is processed depends on the perceptual load and attentional demand. Due to limited attentional resources, a high perceptual load prevents distractor processing through early selection <ref type="bibr" target="#b30">(Lavie, 2005)</ref>. In light of this theory, we predicted that different perceptual loads during the task would have a role in modulating the effect of CC on VWM processing. We hypothesized that CC would have a stronger effect on VWM performance under high perceptual load, where attentional resources are depleted by relevant tasks and noncorresponding auditory stimuli can be more effectively ignored.</p><p>Research indicates that the effect of CC on WM remains stable when attending to different modalities <ref type="bibr" target="#b8">(Brunetti et al., 2017)</ref>. Furthermore, while the attended modality (visual vs. auditory) can have a significant effect on WM performance, paying attention to audiovisual stimuli over unimodal stimuli was not found to have an effect. This aligns with findings of multiple studies which show that observers tend to prioritize signals from the modality considered most informative for the task (e.g., <ref type="bibr" target="#b9">Burr et al., 2009;</ref><ref type="bibr" target="#b10">Butler et al., 2010</ref><ref type="bibr" target="#b20">: Fetsch et al., 2009)</ref>. However, further research found that modality-specific selective attention reduces multisensory integration <ref type="bibr" target="#b2">(Badde et al., 2020;</ref><ref type="bibr">Mozolic et al., 2008)</ref>. Therefore, paying attention to both auditory and visual modalities should promote CC. To alleviate this discrepancy, we measured the role of the attended modality (visual vs. audiovisual) in this process. Building upon the studies mentioned above we hypothesized that the attended modality would significantly impact VWM performance.</p><p>Running head: The Effect of Lightness/Pitch Correspondence on Visual Working Memory Specifically, we predicted that participants instructed to attend to both visual and auditory stimuli would perform better than those instructed to attend only to visual stimuli.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>Participants were recruited online, using the University of Derby's recruitment space and opportunity sampling. While classically VWM research has been conducted in laboratory settings, <ref type="bibr" target="#b50">Ross-Sheehy et al. (2021)</ref> found that unsupervised online assessment of VWM yields similar results to laboratory experiments if environmental factors are controlled for.</p><p>Fifty-two individuals completed the online study. To determine the target sample size, a priori power analysis was conducted using G*Power <ref type="bibr">(version 3.1.9.7;</ref><ref type="bibr" target="#b19">Faul et al., 2007)</ref>. Based on a medium effect size (f2 = .25) and using a standard alpha level of .05, a minimum of 22 participants were required to have 80% power in the analyses. We excluded participants who had &gt; 5% missing data and those with low global accuracy &lt; 55% separately for the 'Low load' and 'High load' conditions. Furthermore, we removed participants whose RT fell outside the interquartile range (IQR) of 0.5-2.6 seconds in more than 20% of all trials. After removing outliers, N = 43 participants (Mage = 33.7 years; SD = 10.6; 65.1% female; range = 19-63 years) were included in the analysis for the 'Low load' condition. For the 'High load' condition, N = 41 participants (Mage = 32.2 years; SD = 9.6; 65.9% female; range = 19-59 years) were included in the analysis. This study followed the ethical standards of the British Psychological Society (2021), the American Psychological Association (2017), and was Running head: The Effect of Lightness/Pitch Correspondence on Visual Working Memory approved by the University of Derby's research ethics committee. All participants provided informed consent. There was no monetary compensation offered, but those who signed up for the study via the university's research participation scheme received points for participating upon completion. This study was not preregistered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transparency and Openness</head><p>We reported how we determined our sample size, all data exclusions, all manipulations, and all measures in the study. The study follows the Journal Article Reporting Standards <ref type="bibr" target="#b1">(Appelbaum, et al., 2018)</ref>. All data, analysis, and research materials are available at <ref type="url" target="https://osf.io/m8t5b/">https://osf.io/m8t5b/</ref>. The data was analyzed using SPSS version 29.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Apparatus and Stimuli</head><p>A black-and-white adaptation of the canonical orientation change detection task <ref type="bibr" target="#b35">(Luck &amp; Vogel, 1997)</ref> was built for this study using Pavlovia (<ref type="url" target="https://pavlovia.org">https://pavlovia.org</ref>). In this task, participants are asked whether the orientation of a visual target bar is the same or different between two arrays separated by a screen break with a fixation cross. We manipulated three aspects of the task: perceptual load, low (displaying 4 objects on the screen; see Figure <ref type="figure" target="#fig_0">1</ref>.B) or high (displaying 6 objects on the screen; see Figure <ref type="figure" target="#fig_0">1</ref>.B), the presence of audiovisual CC (modality conditions) during different phases of the task (see Figure <ref type="figure" target="#fig_1">2.A-E</ref>) and whether the participants were instructed to attend to both visual and auditory stimuli or only the visual stimuli. All participants performed the task in 'High load' and 'Low load' conditions and were exposed to all the modality conditions, while we split participants into two groups and randomly assigned to the 'Attend Visual' or 'Attend Both' conditions. The Low load and High load versions of the orientation change detection task. Note. Sample: sample array (memory encoding phase); Display until response: target array (memory recall phase). Modality conditions The visual stimuli consisted of black and white bars (size 0.10-0.01 height units) on a uniform grey (50%) background. The 'Low load' condition of the task consisted of the presentation of 4 bars (positioned respectively at bar 1) -0.10, 0.10; bar 2) 0.10, 0.10; bar 3) -0.10, -0.10; bar 4) 0.10, -0.10 height units; see Figure 1.A), while the 'High load' condition consisted of the presentation of 6 bars (positioned at bar 1) -0.10, 0.10; bar 2) 0, 0.15; bar 3) 0.10, 0.10; bar 4) -0.10, -0.10; bar 5) 0, -0.15; bar 6) 0.10, -0.10 height units see Figure 1.B). Each bar orientation could vary between 45, 90, 135 and 360 degrees of visual angle. The auditory stimuli consist of either low (300hz) or high (4500hz) pitched sounds presented for 150 ms at the onset of the sample array (encoding phase) and the target array (recall phase).</p><p>Audiovisual CC was created by utilizing lightness (black vs white) / pitch (low vs high) correspondence between visual and auditory stimuli. Previous research <ref type="bibr">(Zeljko et</ref> Running head: The Effect of Lightness/Pitch Correspondence on Visual Working Memory al., 2019) has shown that when audiovisual stimuli varied between two values along each sensory dimension (white or black in the visual dimension and low-pitched or highpitched in the auditory dimension), participants performed better and faster when the stimuli were paired as white and high-pitched or black and low-pitched, compared with the alternative pairing. Following this evidence, in our experiment, we manipulated the visual stimuli based on lightness (white or black) and the auditory stimuli based on pitch (high or low). The audiovisual correspondence was created on the visual target stimulus (e.g., on the bar that would be presented at a different position during the sample and target array when those were different). Following <ref type="bibr" target="#b64">Zeljko et al. (2019)</ref>, audiovisual CC was present when either a white target bar was presented with a high-pitched sound (4500hz), or a black target bar was presented with a low-pitched sound (300hz). On the other hand, the absence of audiovisual CC was obtained by presenting together a highpitched sound (4500hz) with a black target bar or a low-pitched sound (300hz) with a white target bar. We presented audiovisual correspondence during the different stages of memory processing, i.e., at the onset of the sample array (encoding phase) and the target array presentation (recall phase).</p><p>So, the task has five conditions pertaining to the presence or absence of audiovisual CC during different phases of the task: 'Visual-only' (without sound), 'CC at sample' (audiovisual CC during the sample array and non-corresponding stimuli during the target array), 'CC at target' (audiovisual CC during the target array and noncorresponding stimuli during the sample array), 'CC at both' (audiovisual CC during both arrays), 'CC at neither' (non-corresponding audiovisual pairing during both arrays), (see Figure <ref type="figure" target="#fig_1">2</ref>). There were two conditions pertaining to the auditory stimuli's attendance Running head: The Effect of Lightness/Pitch Correspondence on Visual Working Memory during the orientation change detection task. In the 'Attend visual' condition, participants were instructed to pay attention only to the visual stimuli during the task (instructions: "Please only pay attention to what you see on the screen and try to ignore the sound").</p><p>In the 'Attend both' condition, participants were instructed to pay attention to both the visual and auditory stimuli (instructions: "Please pay attention to both the sound and what you see on the screen").</p><p>Participants were required to use a computer for the study, be in a well-lit place, away from distraction, sit at a 50-80 cm distance from the computer screen, and use a set of headphones or earphones. Before the beginning of the study, participants were instructed to set the volume so that the tone was clearly audible but comfortable. To ensure that the volume was on and to avoid BOT performing the task, participants were asked to listen and identify a brief piece of music ("Happy Birthday").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Design and Procedure</head><p>The study followed a mixed factorial design. The presence or absence of audiovisual cross-modal correspondence during different phases of the task was set as the within-subject factor. The attendance of visual stimuli or both visual and auditory stimuli was set as between-subject factor. The 'Low load' and 'High load' conditions of the task were analyzed independently from each other, to determine the role perceptual load plays in the effect of CC's on VWM.</p><p>In the orientation change detection task, participants were asked to determine whether the orientation of the bars on the target array were the same or different compared to the sample array. Participants answered 'Same' by pressing the 'L' key, and 'Different' by pressing the 'A' key on the computer keyboard. The orientation of one Running head: The Effect of Lightness/Pitch Correspondence on Visual Working Memory bar on the target array changed 50% of the time. Before each trial, a fixation cross appeared for 600ms, followed by the sample array, paired with a synchronous corresponding or non-corresponding pitch in the cross-modal conditions (150ms), followed by a delay interval with a fixation cross (900ms), followed by the target array (until response or up to 5 seconds) paired with a synchronous corresponding or noncorresponding pitch in the cross-modal conditions (150ms). Each trial had an interstimulus interval of 1800ms (see Figure <ref type="figure" target="#fig_0">1</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis</head><p>Our study aimed at assessing how modality conditions (with the presence or absence of CC) at different task phases ('Visual-only', 'CC at sample', 'CC at target', 'CC at both', 'CC at neither') and attended sensory modality ('Attend Visual', 'Attend both') impacted participants' performance during a change detection task under different perceptual load conditions (low and high). We evaluated performance by looking at accuracy and RT as dependent variables.</p><p>For accuracy and RT in each load condition, a 5 × 2 mixed analysis of variance (ANOVA) was conducted. The modality conditions ('Visual-only', 'CC at sample', 'CC at target', 'CC at both', 'CC at neither') were set as within-subjects factor, and the attended modality ('Attend Visual', 'Attend both') was set as between-subjects factor. So, a total of four 5 × 2 ANOVAs were performed, two for the low perceptual load condition and two for the high perceptual load condition. When RT was set as the dependent variable, only RTs during accurate trials were included in the analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VWM under the Low load condition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Accuracy</head><p>A 2 × 5 ANOVA was conducted with the modality conditions set as within-subject factor ('Visual-only', 'CC at sample', 'CC at target', 'CC at both', 'CC at neither') and attended modality ('Attend Visual', 'Attend both') set as between-subject factor. Detection of target orientation accuracy was the dependent variable. There was a significant main effect of the modality conditions on accuracy F(4, 164) = 6.604, p &lt; .001, Running head: The Effect of Lightness/Pitch Correspondence on Visual Working Memory ηp 2 = .139. The main effect was explored by using post-hoc repeated measures t-tests, to which Bonferroni correction was applied. Only results with p &lt; .005 (p &lt; .05/10 = .005; corrected p &lt; .05) were considered significant. The accuracy during the 'Visual-only' condition resulted lower than the accuracy during every audiovisual condition: 'Visual-only' and 'CC at sample' t(42) = -3.807, p &lt; .001, 'Visual-only' and 'CC at target' t(42) = -3.229, p = .002, 'Visual-only' and 'CC at both' t(42) = -3.642, p &lt; .001, 'Visual-only' and 'CC at neither' t(42) = 3.120, p = .003. These findings indicate that the presence of sound, and not the presence of CC, improved participants' performance in the 'Low load' condition (see Table <ref type="table" target="#tab_0">1</ref> and Figure <ref type="figure" target="#fig_3">3-A</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.2).</head><p>There was no significant main effect of attended modality F(1, 41) = .003, p = .954, ηp 2 = .000, nor significant interaction between attended modality and the modality conditions F(4,164) = 2.193 p = .072, ηp 2 = .051.  CC at sample CC at target CC at both CC at neither Total Mean Mean Mean Mean Mean Mean (SD) (SD) (SD) (SD) (SD) (SD) Attend visual .735 .868 .879 .907 .886 .855 (.183) (.157) (.128) (.121) (.126) (.143) Attend both .813 .887 .850 .855 .858 .853 (.207) (.153) (.146) (.151) (.120) (.155) Total .775 .878 .864 .881 .871 .872 (.197) (.153) (.137) (.138) (.122) (.149) Note. N = 43. Attend visual group n = 21. Attend both auditory and visual group n = 22. Running head: The Effect of Lightness/Pitch Correspondence on Visual Working Memory</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reaction time</head><p>A 2 × 5 mixed factorial ANOVA was performed, with the modality conditions set as within-subject factor ('Visual-only', 'CC at sample', 'CC at target', 'CC at both', 'CC at neither') and attended modality set as between-subject factor ('Attend visual', 'Attend both'). RT for the accurate trials was set as the dependent variable. The sphericity was violated; therefore, the Greenhouse Geiser correction was applied when reporting the findings. There was a significant main effect of the modality conditions on RT F(3.044, 124.784) = 2.962, p = .034, ηp 2 = .067. The main effect was explored by using posthoc repeated measures t-tests, to which Bonferroni correction was applied. Only results with p &lt; .005 (p &lt; .05/10 = .005; corrected p &lt; .05) were considered significant. No significant differences were found between the modality conditions (see Table <ref type="table" target="#tab_3">3</ref> and 2). This suggests the ANOVA's significant result is likely due to the overall pattern of variation rather than specific group differences. There was no significant main effect of attended modality F(1, 41) = 2.800, p = .102, ηp 2 = .064, nor significant interaction between attended modality and the modality conditions F(3.044, 124.784) = 1.726, p = .164, ηp 2 = .040. Table 4 Means and standard deviations of RT during different modality conditions for both between-subject groups during the Low load condition (4-object). RT Visualonly CC at sample CC at target CC at both CC at neither Total Mean Mean Mean Mean Mean Mean (SD) (SD) (SD) (SD) (SD) (SD) Attend visual .982 .913 .899 .923 .891 .922 (.224) (.190) (.204) (.247) (.195) (.212) Attend both 1.029 .993 1.035 1.019 1.001 1.015 (.195) (.207) (.186) (.161) (.181) (.186) Total 1.001 .954 .969 .972 .950 .970 (.208) (.201) (.204) (.211) (.195) (.204) Note. N = 43. Attend visual group n = 21. Attend both auditory and visual group n = 22. Running head: The Effect of Lightness/Pitch Correspondence on Visual Working Memory</p><note type="other">Figure 3</note><p>Box plots show the accuracy and RT during the low perceptual load condition.</p><p>Running head: The Effect of Lightness/Pitch Correspondence on Visual Working Memory Note. The graphs show the interaction effects between modality conditions and attended modality for accuracy (A1) and RT (B1) and the main effects of modality conditions for accuracy (A2) and RT (B2). The main effect of modality condition is significant both for accuracy (p &lt;.001) and RT (p=.034); however, posthoc t-tests to which Bonferroni correction was applied did not show any significant comparison between the conditions when RT was set as dependent variable.</p><p>Visual: Visual-only condition; Sample: CC at Sample; Target: CC at Target; Both: CC at Both Target and Sample; Neither: absence of CC at both Target and Sample. *** p&lt;0.001; ** p&lt;.005. Bonferroni correction has been applied to the comparisons between CC conditions. Only comparisons having a p&lt;.005 have been considered significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VWM under the High load condition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Accuracy</head><p>Consistent with the analysis for the 'Low load' condition, a 2 × 5 ANOVA was conducted with the modality conditions set as within-subject factor ('Visual-only', 'CC at sample', 'CC at target', 'CC at both', 'CC at neither') and attended modality ('Attend Visual', 'Attend both') set as between-subject factor. Detection of target orientation accuracy was the dependent variable. The sphericity was violated; therefore, the Greenhouse Geiser correction was applied when reporting the findings. There was a significant main effect of the modality conditions on accuracy F(3.</p><p>305, 128.907) = 8.560, p &lt; .001, ηp 2 = .180. The main effect was explored by using post-hoc repeated measures t-tests, to which Bonferroni correction was applied. Only results with p &lt; .005 (p &lt; .05/10 = .005; corrected p &lt; .05) were considered significant. Participants' accuracy was lower during the 'Visual-only' condition compared to the conditions with audiovisual correspondence: 'Visual-only' and 'CC at sample' t(40) = -3.443, p &lt; .001, 'Visual-only' and 'CC at target' t(40) = -3.909, p &lt; .001, 'Visual-only' and 'CC at both' t(40) = -4.468, Running head: The Effect of Lightness/Pitch Correspondence on Visual Working Memory</p><p>p &lt; .001. We also found that participants were more accurate during the 'CC at both' condition compared to the 'CC at neither' condition t(40) = 3.117, p = .003. These findings indicate that the presence of CC, especially when presented both at memory encoding (during the presentation of the sample array) and at memory recall (during the presentation of the target array) improved participants' performance in the 'High load' condition (see Table <ref type="table" target="#tab_8">5</ref> and Figure <ref type="figure" target="#fig_4">4</ref>-A.2). There was no significant main effect of attended modality F(1, 39) = .486, p = .490, ηp 2 = .012, nor significant interaction between attended modality and the modality conditions F(3.305, 128.907) = .724, p = .552, ηp 2 = .018.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reaction time</head><p>A 2 × 5 ANOVA was conducted with the modality conditions set as within-subject factor ('Visual-only', 'CC at sample', 'CC at target', 'CC at both', 'CC at neither') and attended modality ('Attend visual', 'Attend both') set as between-subject factor. RT for the accurate trials was set as the dependent variable. The sphericity was violated; therefore, the Greenhouse Geiser correction was applied when reporting the findings.</p><p>There was a significant main effect of the modality conditions on RT F(2.856, 111.367) = 4.598, p = .005, ηp 2 = .105. The main effect was explored by using posthoc repeated measures t-tests, to which Bonferroni correction was applied. Only results with p &lt; .005 (p &lt; .05/10 = .005; corrected p &lt; .05) were considered significant.</p><p>Significant differences were found between the 'Visual-only' condition and the 'CC at</p><p>Running head: The Effect of Lightness/Pitch Correspondence on Visual Working Memory both' condition: t(40) = 3.811, p &lt; .001. These findings indicate that CC presented both at memory encoding (at the presentation of the sample array) and at memory recall (at the presentation of the target array) improved participants' performance compared to the 'Visual-only' condition. However, no significant differences were found between cross-modally corresponding and non-corresponding conditions (see Table <ref type="table" target="#tab_10">7</ref> and <ref type="table">Figure 4</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>-B.2).</head><p>There was no main effect of attended modality F(1, 39) = 1.645, p = .207, ηp 2 = .040, nor significant interaction between attended modality and the modality conditions F(2.856, 111.367) = 1.456, p = .232, ηp 2 = .036.   Box plots show the accuracy and RT during the high perceptual load condition.</p><p>Note. The graphs show the interaction effects between modality conditions and attended modality for accuracy (A1) and RT (B3) and the main effects of modality conditions for accuracy (A2) and RT (B2).</p><p>Running head: The Effect of Lightness/Pitch Correspondence on Visual Working Memory Visual: Visual-only condition; Sample: CC at sample; Target: CC at target; Both: CC at both target and sample; Neither: absence of CC at both target and sample. *** p&lt;.001; ** p&lt;.005. Bonferroni correction has been applied to the comparisons between CC conditions. Only comparisons having a p&lt;.005 have been considered significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>The present study investigated the effect of lightness/pitch correspondence on VWM performance testing the impact of perceptual load and instructed attended modality. We designed a black-and-white orientation change detection task where visual stimuli were paired with high-and low-pitched sounds. To manipulate perceptual load, the experiment consisted of a 4-object condition ('Low load') and a 6-object condition ('High load'). We instructed a group of participants only to pay attention to the visual stimuli and ignore the sound, while the other group was asked to pay attention to both visual and auditory stimuli. Our findings provide several critical insights into the dynamics of multisensory integration and its impact on WM processes, revealing that the cognitive system benefits from lightness/pitch correspondence under increased perceptual load, and this process is not influenced by conscious attentional focus to simultaneous auditory input.</p><p>Our results suggest that, in the lightness/pitch domain, CC does not automatically enhance performance on the orientation change detection task; instead, the effect is modulated by perceptual load. Under high perceptual load, when compared to the 'Visual-only' condition, participants exhibited significantly enhanced accuracy in the presence of corresponding audiovisual stimuli. This effect was consistently observed when CC was displayed during memory encoding, memory recall, and when it was displayed during both stages. Furthermore, when CC was presented during both</p><p>Running head: The Effect of Lightness/Pitch Correspondence on Visual Working Memory memory encoding and memory recall within a trial, it also significantly improved accuracy compared to the non-corresponding audiovisual condition. This result aligns with prior research by <ref type="bibr" target="#b8">Brunetti et al. (2017)</ref>, who reported a marginally significant effect suggesting that CC (e.g., elevation/pitch, shape/pitch, audiovisual numerosity) can improve WM accuracy in the n-back task. Specifically, this finding was observed when CC was present during both the target array and sample array, compared to the fully non-corresponding condition. Similarly, <ref type="bibr" target="#b13">Constant and Liesefeld (2020)</ref> demonstrated that stimulus saliency has a strong effect on VWM performance. Our findings (see Figure <ref type="figure" target="#fig_4">4</ref>) revealed that this effect is driven more strongly by the memory recall phase, even in trials where corresponding stimuli were presented during both encoding and recall stages. Specifically, CC presented during memory recall produced a similar pattern to CC presented during both memory encoding and recall.</p><p>Under high perceptual load, presenting CC during both memory processing stages also led to significantly faster RT compared to the conditions where only visual stimuli were presented. This finding is consistent with <ref type="bibr" target="#b8">Brunetti et al. (2017)</ref>, who demonstrated that various cross-modal correspondences (e.g., elevation/pitch, shape/pitch, audiovisual numerosity) can enhance RT. We also observed a similarity in the RT data pattern between CC at memory recall and CC at both stages (see Figure <ref type="figure" target="#fig_4">4</ref>). Our results show that, under high perceptual load, corresponding audiovisual stimuli presented during both encoding and recall stages led to the highest accuracy and fastest RT. Having CC present during either encoding or recall alone also enhanced performance, albeit to a lesser extent than when CC was present during both encoding and recall stages. These findings point towards cross-modal sensory integration playing Running head: The Effect of Lightness/Pitch Correspondence on Visual Working Memory a role throughout the entire VWM process in a distributed nature <ref type="bibr" target="#b12">(Christophel et al., 2017;</ref><ref type="bibr" target="#b40">McEwan et al., 2024)</ref>.</p><p>Conversely, under 'Low load' conditions, the presence of auditory stimuli, whether corresponding or not, significantly improved accuracy compared to the 'Visualonly' condition, but no significant differences were observed between corresponding and non-corresponding audiovisual conditions. These findings indicate that under low perceptual load, the cognitive system benefits from the general arousal and alertness effects induced by auditory stimuli <ref type="bibr" target="#b21">(Han et al., 2013;</ref><ref type="bibr" target="#b22">Han et al., 2021)</ref> without a specific advantage for CC. Our results suggest that the brain utilizes corresponding cross-modal information to enhance VWM performance when attentional resources are strained.</p><p>This suggests that strong associations between audiovisual stimuli play a crucial role in withstanding the effects of increased perceptual load on VWM processing.</p><p>The evidence that load modulates the effect of CC at different stages of WM processing aligns with Lavie's load theory <ref type="bibr" target="#b29">(Lavie, 1995;</ref><ref type="bibr" target="#b31">Lavie et al., 2004)</ref>, which proposes that high perceptual load restricts the processing of contextually irrelevant information. Our load specific results also corroborate with <ref type="bibr" target="#b34">Li et al. (2022)</ref>, who demonstrated that perceptual load plays a critical role in multisensory integration; higher loads can enhance the processing of congruent audiovisual stimuli while suppressing the processing of incongruent ones. An EEG study by <ref type="bibr" target="#b52">Simon et al. (2016)</ref> demonstrated similar results regarding WM load. The authors found that increased WM load is linked to stronger and more focused attention on the primary visual task, leading to reduced processing of cross-modal auditory stimuli that are irrelevant to the task. The attentional effect of CC is well established by prior research <ref type="bibr" target="#b8">(Brunetti et al., 2017;</ref><ref type="bibr" target="#b11">Chiou &amp; Rich,</ref> Running head: The Effect of Lightness/Pitch Correspondence on Visual Working Memory 2012; <ref type="bibr" target="#b26">Klapetek et al., 2012)</ref>. Similarly to <ref type="bibr" target="#b8">Brunetti et al. (2017)</ref>, our results show that the attentional effect evoked by CC affects not only the WM encoding stage but also the recall phase.</p><p>Our findings contribute to the ongoing debate about the conditions under which sound facilitates VWM performance and when it becomes a distractor. Our results</p><p>suggest that the interplay between audiovisual CC and perceptual load can be a key factor in determining whether auditory information serves as a beneficial cue (e.g., through contextual or attentional cueing) or a distracting element (e.g., through irrelevance). It is a widely accepted view that general WM capacity is 4±1 items under typical conditions <ref type="bibr" target="#b14">(Cowan, 2010)</ref>, and VWM capacity is estimated to be 3-4 items <ref type="bibr" target="#b16">(Dai et al., 2019;</ref><ref type="bibr" target="#b36">Luck &amp; Vogel, 2013)</ref>. Research <ref type="bibr" target="#b31">(Lavie et al., 2004;</ref><ref type="bibr" target="#b17">De Fockert et al., 2001)</ref> indicates that high perceptual load (the complexity or quantity of stimuli) decreases susceptibility to distractions; however, high WM load (the amount of information to be held and manipulated) increases it. Even so, <ref type="bibr" target="#b52">Simon et al. (2016)</ref> found that during audiovisual cross-modal tasks high WM load may reduce the processing of taskirrelevant auditory stimuli through focused attention. Furthermore, according to Liesefeld et al. ( <ref type="formula">2020</ref>) VWM capacity is aided by its ability to filter out irrelevant information and selectively encode relevant information. These findings suggest that WM's filtering ability may be influenced by contextual associations between stimuli, particularly when perceptual load is high.</p><p>Chunking (i.e., transforming smaller pieces of information into larger familiar units based on contextual association) is often believed to help overcome the limited capacity of WM.</p><p>Thalmann et al. (2019) suggested that chunks reduce WM load through memory Running head: The Effect of Lightness/Pitch Correspondence on Visual Working Memory retrieval of compact chunk representations, thereby replacing the representations of individual elements of the chunk. This account is supported by Son et al. (2020), who found that VWM clusters sensory items into larger representational units based on similarity. CC can be considered a form of chunking or clustering. <ref type="bibr" target="#b7">Brunel et al. (2015)</ref> demonstrated that the association between sensory information from different modalities modulates cross-modal integration during perceptual learning, leading to newly learned units. Clustering information based on CC into fewer units, when attentional resources are stretched, could explain why high perceptual load does not necessarily increase cognitive load, as suggested by <ref type="bibr" target="#b29">Lavie (1995;</ref><ref type="bibr" target="#b30">2005)</ref>. Consequently, our findings support this notion. Merging corresponding stimuli into cohesive units, combined with the attentional effect of CC <ref type="bibr" target="#b8">(Brunetti et al., 2017;</ref><ref type="bibr" target="#b11">Chiou &amp; Rich, 2012;</ref><ref type="bibr" target="#b26">Klapetek et al., 2012)</ref>, makes it more likely that visual items associated with the auditory stimuli are prioritized for processing when attentional resources are stretched. <ref type="bibr" target="#b47">Orbán et al. (2008)</ref> suggested that people use Bayesian learning to form efficient visual chunks from complex patterns based on statistical regularities in the environment. Statistical learning is the process through which the brain automatically and unconsciously learns the relationships between stimuli based on associations <ref type="bibr" target="#b3">(Barakat et al., 2012)</ref>. In addition, Bayesian learning also uses prior knowledge to predict sensory information and updates beliefs based on prediction errors <ref type="bibr" target="#b53">(Shams &amp; Beierholm, 2022)</ref>. Prior studies have shown a link between statistical and Bayesian learning and VWM. <ref type="bibr" target="#b6">Brady et al. (2009)</ref> found that participants' memory for the colors of concentric circle pairs improved when the colors were correlated (consistent color pairing patterns across trials) but dropped to control levels when correlations were</p><p>Running head: The Effect of Lightness/Pitch Correspondence on Visual Working Memory removed. Bates et al. (2019) explored how the brain leverages statistical regularities in visual environments to form efficient VWM representations, finding that participants implicitly learn and use these regularities to enhance VWM performance. Moreover, a study by Umemoto et al. (2010) demonstrates that statistical learning may help to optimize the allocation of limited resources in WM by biasing encoding towards behaviorally relevant items.</p><p>According to <ref type="bibr" target="#b55">Spence (2011)</ref>, certain cross-modal correspondences can be understood in terms of Bayesian learning. This framework suggests that people may integrate stimuli in a statistically optimal way by combining sensory information and prior knowledge (also called priors) and weighting each of them by their relative reliabilities <ref type="bibr">(Ernst, 2007;</ref><ref type="bibr" target="#b48">Parise &amp; Spence, 2009)</ref>. According to this model, the brain creates connections (or couplings) between stimuli to adapt to different situations and constraints. The more prior knowledge the brain has about the association between two stimuli, the stronger the coupling will be <ref type="bibr">(Ernst, 2007)</ref>. This means that with stronger coupling, unisensory signals from multimodal sources are more likely to merge into a single multisensory unit <ref type="bibr" target="#b55">(Spence, 2011)</ref>.</p><p>Previous research suggests that Bayesian models can support both parallel and integrated WM representations <ref type="bibr" target="#b37">(Ma et al., 2006;</ref><ref type="bibr" target="#b53">Shams &amp; Beierholm, 2022)</ref>. This study does not aim to determine if lightness/pitch stimuli are maintained separately in WM by sensory modality or recoded into multisensory representations. However, we hypothesize that when sensory information is processed based on associations, CC at memory recall may facilitate integrating parallel WM representations into a single memory response through attentional cueing. This integration can be achieved by treating recall as a Bayesian inference process, where chunk representations provide priors to interpret sensory input, as suggested by <ref type="bibr" target="#b45">Norris and Kalm (2021)</ref>.</p><p>Based on our findings, we propose that, as a dynamic adaptation to manage the increase in perceptual load, the cognitive system may rely more heavily on organizing, prioritizing, encoding, and recalling information based on learned statistical patterns.</p><p>Clustering crossmodally corresponding stimuli into fewer multisensory units based on highly probabilistic associations could be one such mechanism that aids WM processes when attentional resources are strained. Inference about whether cross-modal stimuli are correlated could also aid in filtering out irrelevant sensory information and prioritizing the selection of relevant information under increased perceptual load. Additionally, our results suggest that immediate priors, such as displaying corresponding stimuli during both memory encoding and recall, can further strengthen these associations and thereby aid VWM performance.</p><p>Contrary to our hypothesis, the attended modality (whether participants paid attention to both visual and auditory stimuli or only visual stimuli) did not significantly impact accuracy or RT. This suggests that the benefits of lightness/pitch CC on VWM are robust and not heavily dependent on top-down attentional control. Instead, the effects appear to be automatically driven by the characteristics of the cross-modal stimuli and their integration. This finding is consistent with previous research indicating that multisensory binding can occur automatically and does not always require focused attention <ref type="bibr" target="#b41">(Molholm et al., 2007;</ref><ref type="bibr">Zlejko et al., 2019</ref><ref type="bibr">Zlejko et al., , 2021))</ref>. This automaticity can also be explained by Hebb's law <ref type="bibr" target="#b23">(Hebb, 1949)</ref>. The repeated co-activation of neurons responding to visual and auditory stimuli during CC strengthens their connections. Due</p><p>Running head: The Effect of Lightness/Pitch Correspondence on Visual Working Memory saliency of the stimulus modality could have been modulated by the relevance of the type of stimuli for performing task. Future research that requires tasks to be performed either in the visual or in the auditory domain could provide further insights into how the interaction between attention and cross-modal correspondence affects the larger WM system and its domain-specific and domain-general networks (see, e.g., <ref type="bibr" target="#b33">Li et al., 2014)</ref>.</p><p>A further avenue of investigation is the neural mechanisms underlying the interplay between CC and perceptual load on WM processing using neuroimaging techniques.</p><p>Analytical approaches based on machine learning as Multi-Voxel-Pattern-Analysis <ref type="bibr" target="#b44">(Norman et al., 2006)</ref> or Representational Similarity Analysis <ref type="bibr" target="#b27">(Kriegeskorte et al., 2008)</ref> could help to shed a light on the nature of neural representations of cross-modal correspondences in the sensory and cognitive cortices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In conclusion, the current study found that, in the lightness/pitch domain, audiovisual CC does not automatically facilitate VWM performance; instead, the salience of the effect is modulated by perceptual load. Lightness/pitch correspondence only improves accuracy and RT under high perceptual load. Contrarily, under low perceptual load, the mere presence of auditory pitch, regardless of visual correspondence, enhances VWM performance, likely by increasing alertness.</p><p>Moreover, our results demonstrate that lightness/pitch CC is automatic and does not rely on conscious selective attention. Our results also suggest that CC serves a functional role beyond sensory integration. We hypothesize that the cognitive system adapts to high perceptual load by clustering CC stimuli into cohesive units and leveraging principles of statistical learning, primarily through the Bayesian approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 The</head><label>1</label><figDesc>Figure 1</figDesc><graphic coords="11,72.00,127.22,468.00,255.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2</head><label>2</label><figDesc>Figure 2</figDesc><graphic coords="12,72.00,127.22,468.00,257.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><figDesc>Before beginning the experiment, a training round for each block, consisting of 10 'Visual-only' trials without sound and 10 audiovisual trials, was administered in this order. Participants were provided feedback during the training, but no error feedback was provided during the experimental trials. The experiment took approximately 15 minutes to complete. Participants were semi-randomly (controlling for sex) assigned to one of two groups. The first group, 'Attend visual' (n = 21 'Low load' condition; n = 21 'High load' condition) were instructed to perform the change detection task focusing only on the visual stimuli while ignoring the auditory stimuli; the second group, 'Attend both' (n = 22 'Low load' condition; n = 20 'High load' condition) were instructed to pay attention to both the visual stimuli and the sound. Each participant, regardless of their group, was subjected to the same load and audiovisual correspondence conditions. The participants performed 20 trials of the 'Visual-only' condition first and then 80 trials of the four modality conditions in random order. The 'Low load' condition of the task was presented first. Running head: The Effect of Lightness/Pitch Correspondence on Visual Working Memory</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 -</head><label>3</label><figDesc>Figure 3-B.2). This suggests the ANOVA's significant result is likely due to the overall</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4</head><label>4</label><figDesc>Figure 4</figDesc><graphic coords="27,72.00,141.02,409.75,501.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Post-hoc paired sample t-tests for modality conditions: Visual-only, CC at sample, CC at target, CC at both, and CC at neither during the Low load condition.</figDesc><table><row><cell>Condition</cell><cell>t</cell><cell>df</cell><cell>p</cell></row><row><cell>Visual-only * CC at sample</cell><cell>-3.807</cell><cell>42</cell><cell>&lt; .001</cell></row><row><cell>Visual-only * CC at target</cell><cell>-3.229</cell><cell>42</cell><cell>.002</cell></row><row><cell>Visual-only * CC at both</cell><cell>-3.642</cell><cell>42</cell><cell>&lt; .001</cell></row><row><cell>Visual-only * CC at neither</cell><cell>-3.120</cell><cell>42</cell><cell>.003</cell></row><row><cell>CC at sample * CC at target</cell><cell>.527</cell><cell>42</cell><cell>.601</cell></row><row><cell>CC at sample * CC at both</cell><cell>-.122</cell><cell>42</cell><cell>.903</cell></row><row><cell>CC at sample * CC at neither</cell><cell>.301</cell><cell>42</cell><cell>.765</cell></row><row><cell>CC at target * CC at both</cell><cell>-.835</cell><cell>42</cell><cell>.409</cell></row><row><cell>CC at target * CC at neither</cell><cell>-.324</cell><cell>42</cell><cell>.747</cell></row><row><cell>CC at both * CC at neither</cell><cell>.423</cell><cell>42</cell><cell>.674</cell></row></table><note><p>Note. N = 43. Bonferroni correction for multiple comparison was applied: results are considered significant when p &lt; .005 (p &lt; .05/10 = .005; corrected p &lt; .05). Significant effects are shown in bold.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Means and standard deviations of accuracy during different modality conditions for bothbetween-subject groups during the Low load condition.</figDesc><table><row><cell>Accuracy</cell></row><row><cell>Visual-</cell></row><row><cell>only</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>Post-hoc paired sample t-tests for modality conditions: Visual-only, CC at sample, CC at target, CC at both, and CC at neither during the Low load condition.</figDesc><table><row><cell>CC condition</cell><cell>t</cell><cell>df</cell><cell>p</cell></row><row><cell>Visual-only * CC at sample</cell><cell>2.234</cell><cell>42</cell><cell>.031</cell></row><row><cell>Visual-only * CC at target</cell><cell>1.663</cell><cell>42</cell><cell>.104</cell></row><row><cell>Visual-only * CC at both</cell><cell>1.461</cell><cell>42</cell><cell>.151</cell></row><row><cell>Visual-only * CC at neither</cell><cell>2.774</cell><cell>42</cell><cell>.008</cell></row><row><cell>CC at sample * CC at target</cell><cell>-.956</cell><cell>42</cell><cell>.344</cell></row><row><cell>CC at sample * CC at both</cell><cell>-.1020</cell><cell>42</cell><cell>.313</cell></row><row><cell>CC at sample * CC at neither</cell><cell>.320</cell><cell>42</cell><cell>.751</cell></row><row><cell>CC at target * CC at both</cell><cell>-.204</cell><cell>42</cell><cell>.839</cell></row><row><cell>CC at target * CC at neither</cell><cell>1.512</cell><cell>42</cell><cell>.138</cell></row><row><cell>CC at both * CC at neither</cell><cell>1.322</cell><cell>42</cell><cell>.193</cell></row></table><note><p>Note. N = 43. Bonferroni correction for multiple comparison was applied: results are considered significant when p &lt; .005 (p &lt; .05/10 = .005; corrected p&lt; .05).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5</head><label>5</label><figDesc>Post</figDesc><table><row><cell>CC condition</cell><cell>t</cell><cell>df</cell><cell>p</cell></row><row><cell>Visual-only * CC at sample</cell><cell>-3.443</cell><cell>40</cell><cell>.001</cell></row><row><cell>Visual-only * CC at target</cell><cell>-3.909</cell><cell>40</cell><cell>&lt;.001</cell></row><row><cell>Visual-only * CC at both</cell><cell>-4.468</cell><cell>40</cell><cell>&lt;.001</cell></row><row><cell>Visual-only * CC at neither</cell><cell>-2.848</cell><cell>40</cell><cell>.007</cell></row><row><cell>CC at sample * CC at target</cell><cell>-1.507</cell><cell>40</cell><cell>.140</cell></row><row><cell>CC at sample * CC at both</cell><cell>-2.447</cell><cell>40</cell><cell>.019</cell></row><row><cell>CC at sample * CC at neither</cell><cell>-.383</cell><cell>40</cell><cell>.704</cell></row><row><cell>CC at target * CC at both</cell><cell>-1.038</cell><cell>40</cell><cell>.305</cell></row><row><cell>CC at target * CC at neither</cell><cell>1.503</cell><cell>40</cell><cell>.141</cell></row><row><cell>CC at both * CC at neither</cell><cell>3.117</cell><cell>40</cell><cell>.003</cell></row><row><cell cols="4">Note. N = 41. Bonferroni correction for multiple comparison was applied: results are considered significant</cell></row><row><cell cols="4">when p &lt; .005 (p &lt; .05/10 = .005; corrected p&lt; .05). Significant effects are shown in bold.</cell></row></table><note><p>-hoc paired sample t-tests for modality conditions: Visual-only, CC at sample, CC at target, CC at both, CC at neither during the High load condition.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6</head><label>6</label><figDesc>Means and standard deviations of accuracy during different modality conditions for bothbetween-subject groups during the High load condition.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Accuracy</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Visual-only</cell><cell>CC at sample</cell><cell>CC at target</cell><cell>CC at both</cell><cell>CC at neither</cell><cell>Total</cell></row><row><cell></cell><cell>Mean</cell><cell>Mean</cell><cell>Mean</cell><cell>Mean</cell><cell>Mean</cell><cell>Mean</cell></row><row><cell></cell><cell>(SD)</cell><cell>(SD)</cell><cell>(SD)</cell><cell>(SD)</cell><cell>(SD)</cell><cell>(SD)</cell></row><row><cell>Attend visual</cell><cell>.730</cell><cell>.797</cell><cell>.851</cell><cell>.877</cell><cell>.832</cell><cell>.817</cell></row><row><cell></cell><cell>(.209)</cell><cell>(.155)</cell><cell>(.160)</cell><cell>(.112)</cell><cell>(.154)</cell><cell>(.158)</cell></row><row><cell>Attend both</cell><cell>.699</cell><cell>.812</cell><cell>.826</cell><cell>.858</cell><cell>.754</cell><cell>.789</cell></row><row><cell></cell><cell>(.213)</cell><cell>(.152)</cell><cell>(.164)</cell><cell>(.133)</cell><cell>(.208)</cell><cell>(.174)</cell></row><row><cell>Total</cell><cell>.715</cell><cell>.804</cell><cell>.839</cell><cell>.868</cell><cell>.794</cell><cell>.804</cell></row><row><cell></cell><cell>(.209)</cell><cell>(.151)</cell><cell>(.160)</cell><cell>(.122)</cell><cell>(.184)</cell><cell>(.165)</cell></row></table><note><p>Note. N = 41. Attend visual group n = 21. Attend both auditory and visual group n = 20.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7</head><label>7</label><figDesc>Post</figDesc><table><row><cell>CC condition</cell><cell>t</cell><cell>df</cell><cell>p</cell></row><row><cell>Visual-only * CC at sample</cell><cell>1.876</cell><cell>40</cell><cell>.068</cell></row><row><cell>Visual-only * CC at target</cell><cell>2.921</cell><cell>40</cell><cell>.006</cell></row><row><cell>Visual-only * CC at both</cell><cell>3.811</cell><cell>40</cell><cell>&lt;.001</cell></row><row><cell>Visual-only * CC at neither</cell><cell>1.807</cell><cell>40</cell><cell>.078</cell></row><row><cell>CC at sample * CC at target</cell><cell>.985</cell><cell>40</cell><cell>.331</cell></row><row><cell>CC at sample * CC at both</cell><cell>1.830</cell><cell>40</cell><cell>.075</cell></row><row><cell>CC at sample * CC at neither</cell><cell>-.153</cell><cell>40</cell><cell>.879</cell></row><row><cell>CC at target * CC at both</cell><cell>.898</cell><cell>40</cell><cell>.375</cell></row><row><cell>CC at target * CC at neither</cell><cell>-1.205</cell><cell>40</cell><cell>.235</cell></row><row><cell>CC at both * CC at neither</cell><cell>-1.741</cell><cell>40</cell><cell>.089</cell></row><row><cell cols="4">Note. N = 41. Bonferroni correction for multiple comparison was applied: results are considered significant</cell></row><row><cell cols="4">when p &lt; .005 (p &lt; .05/10 = .005; corrected p&lt; .05). Significant effects are shown in bold.</cell></row></table><note><p>-hoc paired sample t-tests for modality conditions: Visual-only, CC at sample, CC at target, CC at both, CC at neither during the High load condition.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8</head><label>8</label><figDesc>Means and standard deviations of RT during different modality conditions for bothbetween-subject groups during the High load condition.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>RT</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Visual-only</cell><cell>CC at sample</cell><cell>CC at target</cell><cell>CC at both</cell><cell>CC at neither</cell><cell>Total</cell></row><row><cell></cell><cell>Mean</cell><cell>Mean</cell><cell>Mean</cell><cell>Mean</cell><cell>Mean</cell><cell>Mean</cell></row><row><cell></cell><cell>(SD)</cell><cell>(SD)</cell><cell>(SD)</cell><cell>(SD)</cell><cell>(SD)</cell><cell>(SD)</cell></row><row><cell>Attend visual</cell><cell>.927</cell><cell>.856</cell><cell>.852</cell><cell>.865</cell><cell>.861</cell><cell>.872</cell></row><row><cell></cell><cell>(.218)</cell><cell>(.198)</cell><cell>(.221)</cell><cell>(.194)</cell><cell>(.219)</cell><cell>(.210)</cell></row><row><cell>Attend both</cell><cell>.980</cell><cell>.958</cell><cell>.930</cell><cell>.889</cell><cell>.958</cell><cell>.943</cell></row><row><cell></cell><cell>(.181)</cell><cell>(.187)</cell><cell>(.173)</cell><cell>(.139)</cell><cell>(.190)</cell><cell>(.174)</cell></row><row><cell>Total</cell><cell>.953</cell><cell>.906</cell><cell>.890</cell><cell>.877</cell><cell>.908</cell><cell>.923</cell></row><row><cell></cell><cell>(.200)</cell><cell>(.197)</cell><cell>(.200)</cell><cell>(.168)</cell><cell>(.209)</cell><cell>(.195)</cell></row></table><note><p>Note. N = 41. Attend visual group n = 21. Attend both auditory and visual group n = 20. Running head: The Effect of Lightness/Pitch Correspondence on Visual Working Memory</p></note></figure>
		</body>
		<back>

			<div type="funding">
<div><p>received ethics approval from the <rs type="funder">University of Derby</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Running head: The Effect of Lightness/Pitch Correspondence on Visual Working Memory to this strengthening, even when attention is only directed to one of these modalities, the neural pathways linking them may operate automatically. <ref type="bibr" target="#b55">Spence (2011)</ref> suggested that some CCs involving pitch are likely statistical correspondences representing the internalization of the natural correlations between stimulus attributes present in the environment. Furthermore, any unimodal component of such multisensory pair can sufficiently activate the association representing the other unimodal component.</p><p>Expanding on these findings our study indicates that lightness/pitch CC could operate in a similar fashion.</p><p>The present study provides key critical insights into the impact of multisensory integration on VWM. However, some limitations should be noted, together with future research directions. The present study only included two load conditions and only explored lightness/pitch CC. Future research could investigate whether higher perceptual load increases the distractor effect of non-corresponding pitch on VWM and whether this effect is modulated by the number of stimuli included in the set through a gradient effect (e.g., the advantage is increased gradually as the number of stimuli in the set increases) or whether the effect is modulated in an on/off manner (e.g., the advantage is present for sets with a certain number of stimuli, but it does not increase when the number of stimuli increases after this number). Another opportunity for further studies is to explore whether the findings are generalizable to other types of CC correspondences, (e.g., auditory pitch/visual shape, numerosity, phonetic characteristics of the auditory stimulus/ visual shape, auditory frequency/ visual frequency) and other sensory modalities (e.g., in the tactile domain). In the current study, participants were required only to perform the WM task in the visual domain. The Our study provides a deeper understanding of how the cognitive system adjusts to perceptual demands by demonstrating the effects of the interplay between lightness/pitch CC and perceptual load on VWM performance. It also lays the foundation for future research and interventions that could optimize cognitive performance across various domains, including education, transport safety, and clinical settings.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="https://www.apa.org/ethics/code" />
		<title level="m">Ethical Principles of Psychologists and Code of Conduct</title>
		<imprint>
			<publisher>American Psychological Association</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Journal article reporting standards for quantitative research in psychology: The APA Publications and Communications Board task force report</title>
		<author>
			<persName><forename type="first">M</forename><surname>Appelbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Kline</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mayo-Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Nezu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Rao</surname></persName>
		</author>
		<idno type="DOI">10.1037/amp0000191</idno>
		<ptr target="https://doi.org/10.1037/amp0000191" />
	</analytic>
	<monogr>
		<title level="j">American Psychologist</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="25" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Modality-specific attention attenuates visual-tactile integration and recalibration effects by reducing prior expectations of a common source for vision and touch</title>
		<author>
			<persName><forename type="first">S</forename><surname>Badde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Landy</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cognition.2019.104170</idno>
		<ptr target="https://doi.org/10.1016/j.cognition.2019.104170" />
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">197</biblScope>
			<biblScope unit="page">104170</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">There is more to statistical learning than associative learning: Predictable items are enhanced even when not predicted</title>
		<author>
			<persName><forename type="first">B</forename><surname>Barakat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shams</surname></persName>
		</author>
		<idno type="DOI">10.1167/12.9.694</idno>
		<ptr target="https://doi.org/10.1167/12.9.694" />
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="694" to="694" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive allocation of human visual working memory capacity during statistical and categorical learning</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Lerch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Sims</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Jacobs</surname></persName>
		</author>
		<idno type="DOI">10.1167/19.2.11</idno>
		<ptr target="https://doi.org/10.1167/19.2.11" />
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="11" to="11" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bayesian priors are encoded independently from likelihoods in human multisensory perception</title>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">R</forename><surname>Beierholm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Quartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shams</surname></persName>
		</author>
		<idno type="DOI">10.1167/9.5.23</idno>
		<ptr target="https://doi.org/10.1167/9.5.23" />
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">23</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Compression in visual working memory: Using statistical regularities to form more efficient memory Running head: The Effect of Lightness/Pitch Correspondence on Visual Working Memory representations</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Konkle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Alvarez</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0016797</idno>
		<ptr target="https://www.bps.org.uk/guideline/bps-code-human-research-ethics" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="487" to="502" />
			<date type="published" when="2009">2009. 2021</date>
			<publisher>BPS Code of Human Research Ethics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">It does belong together: Crossmodal correspondences influence cross-modal integration during perceptual learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Brunel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Goldstone</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2015.00358</idno>
		<ptr target="https://doi.org/10.3389/fpsyg.2015.00358" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">121086</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The impact of cross-modal correspondences on working memory performance</title>
		<author>
			<persName><forename type="first">R</forename><surname>Brunetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Indraccolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mastroberardino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Spence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Santangelo</surname></persName>
		</author>
		<idno type="DOI">10.1037/xhp0000348</idno>
		<ptr target="https://doi.org/10.1037/xhp0000348" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="819" to="831" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Auditory dominance over vision in the perception of interval duration</title>
		<author>
			<persName><forename type="first">D</forename><surname>Burr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Banks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Morrone</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00221-009-1933-z</idno>
		<ptr target="https://doi.org/10.1007/s00221-009-1933-z" />
	</analytic>
	<monogr>
		<title level="j">Experimental Brain Research</title>
		<imprint>
			<biblScope unit="volume">198</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="57" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bayesian integration of visual and vestibular signals for heading</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Bulthoff</surname></persName>
		</author>
		<idno type="DOI">10.1167/10.11.23</idno>
		<ptr target="https://doi.org/10.1167/10.11.23" />
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="23" to="23" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cross-Modality Correspondence between Pitch and Spatial Location Modulates Attentional Orienting</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Rich</surname></persName>
		</author>
		<idno type="DOI">10.1068/p7161</idno>
		<ptr target="https://doi.org/10.1068/p7161Runninghead:TheEffectofLightness/Pitch" />
	</analytic>
	<monogr>
		<title level="m">Correspondence on Visual Working Memory</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="339" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The distributed nature of working memory</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Christophel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Klink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Spitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Roelfsema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Haynes</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2016.12.007</idno>
		<ptr target="https://doi.org/10.1016/j.tics.2016.12.007" />
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="111" to="124" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The role of saliency for visual working memory in complex visual scenes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Liesefeld</surname></persName>
		</author>
		<idno type="DOI">10.1167/jov.20.11.499</idno>
		<ptr target="https://doi.org/10.1167/jov.20.11.499" />
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">499</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The Magical Mystery Four</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cowan</surname></persName>
		</author>
		<idno type="DOI">10.1177/0963721409359277</idno>
		<ptr target="https://doi.org/10.1177/0963721409359277" />
	</analytic>
	<monogr>
		<title level="j">Current Directions in Psychological Science</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="51" to="57" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The bouba/kiki effect is robust across cultures and writing systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ćwiek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Draxler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Asu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dediu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hiovain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Koutalidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krifka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lippus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lupyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Petrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ridouane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Schümchen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Á</forename><surname>Szalontai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ö</forename><surname>Ünal-Logacev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Winter</surname></persName>
		</author>
		<idno type="DOI">10.1098/rstb.2020.0390</idno>
		<ptr target="https://doi.org/10.1098/rstb.2020.0390" />
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society B: Biological Sciences</title>
		<imprint>
			<biblScope unit="volume">377</biblScope>
			<date type="published" when="1841">2022. 1841</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The reliability of estimating visual working memory capacity</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Du</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41598-019-39044-1</idno>
		<ptr target="https://doi.org/10.1038/s41598-019-39044-1" />
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1155</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The Role of Working Memory in Visual Selective Attention</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>De Fockert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Frith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lavie</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.1056496</idno>
		<ptr target="https://doi.org/10.1126/science.1056496" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">291</biblScope>
			<biblScope unit="page" from="1803" to="1806" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Merging the senses into a robust percept</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Bülthoff</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2004.02.002</idno>
		<ptr target="https://doi.org/10.1016/j.tics.2004.02.002Runninghead" />
	</analytic>
	<monogr>
		<title level="m">The Effect of Lightness/Pitch Correspondence on Visual Working Memory</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="162" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">G*Power 3: A flexible statistical power analysis program for the social, behavioral, and biomedical sciences</title>
		<author>
			<persName><forename type="first">F</forename><surname>Faul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Erdfelder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Buchner</surname></persName>
		</author>
		<idno type="DOI">10.3758/bf03193146</idno>
		<ptr target="https://doi.org/10.3758/bf03193146" />
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="175" to="191" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dynamic Reweighting of Visual and Vestibular Cues during Self-Motion Perception</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Fetsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Deangelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Angelaki</surname></persName>
		</author>
		<idno type="DOI">10.1523/jneurosci.2574-09.2009</idno>
		<ptr target="https://doi.org/10.1523/jneurosci.2574-09.2009" />
	</analytic>
	<monogr>
		<title level="j">The Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">49</biblScope>
			<biblScope unit="page" from="15601" to="15612" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Low-Arousal Speech Noise Improves Performance in N-Back Task: An ERP Study</title>
		<author>
			<persName><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0076261</idno>
		<ptr target="https://doi.org/10.1371/journal.pone.0076261" />
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">76261</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Background white noise and speech facilitate visual working memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ku</surname></persName>
		</author>
		<idno type="DOI">10.1111/ejn.15455</idno>
		<ptr target="https://doi.org/10.1111/ejn.15455" />
	</analytic>
	<monogr>
		<title level="j">European Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="6487" to="6496" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The Organization of Behavior</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">O</forename><surname>Hebb</surname></persName>
		</author>
		<idno type="DOI">10.4324/9781410612403</idno>
		<ptr target="https://doi.org/10.4324/9781410612403" />
		<imprint>
			<date type="published" when="1949">1949</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improving fluid intelligence with training on working memory</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Jaeggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Buschkuehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jonides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Perrig</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.0801268105</idno>
		<ptr target="https://doi.org/10.1073/pnas.0801268105" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page" from="6829" to="6833" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Working memory, attention control, and the N-back task: A question of construct validity. Running head: The Effect of Lightness/Pitch Correspondence on Visual Working Memory</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R A</forename><surname>Conway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Miura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J H</forename><surname>Colflesh</surname></persName>
		</author>
		<idno type="DOI">10.1037/0278-7393.33.3.615</idno>
		<ptr target="https://doi.org/10.1037/0278-7393.33.3.615" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="615" to="622" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Does crossmodal correspondence modulate the facilitatory effect of auditory cues on visual search? Attention</title>
		<author>
			<persName><forename type="first">A</forename><surname>Klapetek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Spence</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13414-012-0317-9</idno>
		<ptr target="https://doi.org/10.3758/s13414-012-0317-9" />
	</analytic>
	<monogr>
		<title level="j">Perception, &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="1154" to="1167" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Representational similarity analysis-connecting the branches of systems neuroscience</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kriegeskorte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Bandettini</surname></persName>
		</author>
		<idno type="DOI">10.3389/neuro.06.004.2008</idno>
		<ptr target="https://doi.org/10.3389/neuro.06.004.2008" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in systems neuroscience</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">249</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Exploring brainbehavior relationships in the N-back task</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lamichhane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Westbrook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Braver</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuroimage.2020.116683</idno>
		<ptr target="https://doi.org/10.1016/j.neuroimage.2020.116683" />
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">212</biblScope>
			<biblScope unit="page">116683</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Perceptual load as a necessary condition for selective attention</title>
		<author>
			<persName><forename type="first">N</forename><surname>Lavie</surname></persName>
		</author>
		<idno type="DOI">10.1037/0096-1523.21.3.451</idno>
		<ptr target="https://doi.org/10.1037/0096-1523.21.3.451" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="451" to="468" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Distracted and confused?: Selective attention under load</title>
		<author>
			<persName><forename type="first">N</forename><surname>Lavie</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2004.12.004</idno>
		<ptr target="https://doi.org/10.1016/j.tics.2004.12.004" />
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="75" to="82" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Load Theory of Selective Attention and Cognitive Control</title>
		<author>
			<persName><forename type="first">N</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hirst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>De Fockert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Viding</surname></persName>
		</author>
		<idno type="DOI">10.1037/0096-3445.133.3.339</idno>
		<ptr target="https://doi.org/10.1037/0096-3445.133.3.339" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="339" to="354" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">How visual working memory handles distraction: cognitive mechanisms and electrophysiological correlates</title>
		<author>
			<persName><forename type="first">N</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tsal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Liesefeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Sauseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename></persName>
		</author>
		<idno type="DOI">10.1080/13506285.2020.1773594</idno>
		<ptr target="https://doi.org/10.1080/13506285.2020.1773594" />
	</analytic>
	<monogr>
		<title level="j">Perception &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">5-8</biblScope>
			<biblScope unit="page" from="372" to="387" />
			<date type="published" when="1994">1994. 2020</date>
		</imprint>
	</monogr>
	<note>Perceptual load as a major determinant of the locus of selection in visual attention Visual Cognition</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Domain-general and domain-specific functional networks in working memory</title>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Christ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cowan</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuroimage.2014.08.028</idno>
		<ptr target="https://doi.org/10.1016/j.neuroimage.2014.08.028" />
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="646" to="656" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Whether attentional loads influence audiovisual integration depends on semantic associations</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ejima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13414-022-02461-y</idno>
		<ptr target="https://doi.org/10.3758/s13414-022-02461-y" />
	</analytic>
	<monogr>
		<title level="j">Attention, Perception, &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2205" to="2218" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The capacity of visual working memory for features and conjunctions</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Luck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Vogel</surname></persName>
		</author>
		<idno type="DOI">10.1038/36846</idno>
		<ptr target="https://doi.org/10.1038/36846" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">390</biblScope>
			<biblScope unit="issue">6657</biblScope>
			<biblScope unit="page" from="279" to="281" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Visual working memory capacity: from psychophysics and neurobiology to individual differences</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Luck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Vogel</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2013.06.006</idno>
		<ptr target="https://doi.org/10.1016/j.tics.2013.06.006" />
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="391" to="400" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Bayesian inference with probabilistic population codes</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Latham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pouget</surname></persName>
		</author>
		<idno type="DOI">10.1038/nn1790</idno>
		<ptr target="https://doi.org/10.1038/nn1790" />
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1432" to="1438" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attentional cueing by cross-modal congruency produces both facilitation and inhibition on short-term visual recognition</title>
		<author>
			<persName><forename type="first">E</forename><surname>Makovac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gerbino</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.actpsy.2014.07.008</idno>
		<ptr target="https://doi.org/10.1016/j.actpsy.2014.07.008Runninghead" />
	</analytic>
	<monogr>
		<title level="m">The Effect of Lightness/Pitch Correspondence on Visual Working Memory</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="page" from="75" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">On cross-modal similarity: Auditory-visual interactions in speeded discrimination</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Marks</surname></persName>
		</author>
		<idno type="DOI">10.1037/0096-1523.13.3.384</idno>
		<ptr target="https://doi.org/10.1037/0096-1523.13.3.384" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">384</biblScope>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Involvement of the superior colliculi in crossmodal correspondences</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mcewan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kritikos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zeljko</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13414-024-02866-x</idno>
		<ptr target="https://doi.org/10.3758/s13414-024-02866-x" />
	</analytic>
	<monogr>
		<title level="j">Attention, Perception, &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="931" to="941" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Object-based attention is multisensory: Co-activation of an object&apos;s representations in ignored sensory modalities</title>
		<author>
			<persName><forename type="first">S</forename><surname>Molholm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shpaner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Foxe</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1460-9568.2007.05668.x</idno>
		<ptr target="https://doi.org/10.1111/j.1460-9568.2007.05668.x" />
	</analytic>
	<monogr>
		<title level="j">European Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="499" to="509" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Topographic ERP Analyses: A Stepby-Step Tutorial Review</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brunet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Michel</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10548-008-0054-5</idno>
		<ptr target="https://doi.org/10.1007/s10548-008-0054-5" />
	</analytic>
	<monogr>
		<title level="j">Brain Topography</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="249" to="264" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Perceptual Inference, Learning, and Attention in a Multisensory World</title>
		<author>
			<persName><forename type="first">U</forename><surname>Noppeney</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev-neuro-100120-085519</idno>
		<ptr target="https://doi.org/10.1146/annurev-neuro-100120-085519" />
	</analytic>
	<monogr>
		<title level="j">Annual Review of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="449" to="473" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Beyond mind-reading: multi-voxel pattern analysis of fMRI data</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Polyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Detre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Haxby</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2006.07.005</idno>
		<ptr target="https://doi.org/10.1016/j.tics.2006.07.005" />
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="424" to="430" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Chunking and data compression in verbal short-term memory</title>
		<author>
			<persName><forename type="first">D</forename><surname>Norris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kalm</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cognition.2020.104534</idno>
		<ptr target="https://doi.org/10.1016/j.cognition.2020.104534" />
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">208</biblScope>
			<biblScope unit="page">104534</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">The Effect of Lightness/Pitch Correspondence on Visual Working Memory</title>
		<author>
			<persName><forename type="first">Running</forename><surname>Head</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Bayesian learning of visual chunks by human observers</title>
		<author>
			<persName><forename type="first">G</forename><surname>Orbán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Aslin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lengyel</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.0708424105</idno>
		<ptr target="https://doi.org/10.1073/pnas.0708424105" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2745" to="2750" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">When birds of a feather flock together&apos;: Synesthetic correspondences modulate audiovisual integration in non-synesthetes</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">V</forename><surname>Parise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Spence</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0005664</idno>
		<ptr target="https://doi.org/10.1371/journal.pone.0005664" />
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">5664</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Perceptual Load Modulates the Effect of Lightness/Pitch Correspondence on Visual Working Memory Performance</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rigo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Roebuck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Manini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">osf</title>
		<imprint>
			<biblScope unit="issue">8t5</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>b</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Unsupervised online assessment of visual working memory in 4-to 10-year-old children: array size influences capacity estimates and task performance</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ross-Sheehy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Eschman</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2021.692228</idno>
		<ptr target="https://doi.org/10.3389/fpsyg.2021.692228" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">692228</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Rubin</surname></persName>
		</author>
		<title level="m">Synsoplevede Figurer</title>
		<imprint>
			<publisher>Gyldendal</publisher>
			<date type="published" when="1915">1915</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Increasing working memory load reduces processing of cross-modal task-irrelevant stimuli even after controlling for task difficulty and executive capacity</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Tusch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Holcomb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Daffner</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnhum.2016.00380</idno>
		<ptr target="https://doi.org/10.3389/fnhum.2016.00380" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Human Neuroscience</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">380</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Bayesian causal inference: A unifying neuroscience theory</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Beierholm</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neubiorev.2022.104619</idno>
		<ptr target="https://doi.org/10.1016/j.neubiorev.2022.104619" />
	</analytic>
	<monogr>
		<title level="j">Neuroscience &amp; Biobehavioral Reviews</title>
		<imprint>
			<biblScope unit="volume">137</biblScope>
			<biblScope unit="page">104619</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Similarity-based clusters are representational units of visual working memory</title>
		<author>
			<persName><forename type="first">G</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-I</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-S</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Chong</surname></persName>
		</author>
		<idno type="DOI">10.1037/xlm0000722</idno>
		<ptr target="https://doi.org/10.1037/xlm0000722" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Running head: The Effect of Lightness/Pitch Correspondence on Visual Working Memory Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="46" to="59" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Crossmodal correspondences: A tutorial review</title>
		<author>
			<persName><forename type="first">C</forename><surname>Spence</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13414-010-0073-7</idno>
		<ptr target="https://doi.org/10.3758/s13414-010-0073-7" />
	</analytic>
	<monogr>
		<title level="j">Attention, Perception, &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="971" to="995" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Simple and complex crossmodal correspondences involving audition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Spence</surname></persName>
		</author>
		<idno type="DOI">10.1250/ast.41.6</idno>
		<ptr target="https://doi.org/10.1250/ast.41" />
	</analytic>
	<monogr>
		<title level="j">Acoustical Science and Technology</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="6" to="12" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">How automatic are crossmodal correspondences?</title>
		<author>
			<persName><forename type="first">C</forename><surname>Spence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Deroy</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.concog.2012.12.006</idno>
		<ptr target="https://doi.org/10.1016/j.concog.2012.12.006" />
	</analytic>
	<monogr>
		<title level="j">Consciousness and Cognition</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="245" to="260" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Crossmodal processing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Spence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Senkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Röder</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00221-009-1973-4</idno>
		<ptr target="https://doi.org/10.1007/s00221-009-1973-4" />
	</analytic>
	<monogr>
		<title level="j">Experimental Brain Research</title>
		<imprint>
			<biblScope unit="volume">198</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="107" to="111" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Enhanced dimension-specific visual working memory in grapheme-color synesthesia</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Terhune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">A</forename><surname>Wudarczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kochuparampil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Kadosh</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cognition.2013.06.009</idno>
		<ptr target="https://doi.org/10.1016/j.cognition.2013.06.009" />
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="123" to="137" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">How does chunking help working memory?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Thalmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Oberauer</surname></persName>
		</author>
		<idno type="DOI">10.1037/xlm0000578</idno>
		<ptr target="https://doi.org/10.1037/xlm0000578" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="55" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Statistical learning induces discrete shifts in the allocation of working memory resources</title>
		<author>
			<persName><forename type="first">A</forename><surname>Umemoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Scolari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Awh</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0019324</idno>
		<ptr target="https://doi.org/10.1037/a0019324" />
	</analytic>
	<monogr>
		<title level="j">Journal of Running head: The Effect of Lightness/Pitch Correspondence on Visual Working Memory Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1419" to="1429" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Crossmodal binding: Evaluating the &quot;unity assumption&quot; using audiovisual speech stimuli</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vatakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Spence</surname></persName>
		</author>
		<idno type="DOI">10.3758/bf03193776</idno>
		<ptr target="https://doi.org/10.3758/bf03193776" />
	</analytic>
	<monogr>
		<title level="j">Perception &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="744" to="756" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">The lightness/pitch crossmodal correspondence modulates the Rubin face/vase perception</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zeljko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Grove</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kritikos</surname></persName>
		</author>
		<idno type="DOI">10.1163/22134808-bja10054</idno>
		<ptr target="https://doi.org/10.1163/22134808-bja10054" />
	</analytic>
	<monogr>
		<title level="j">Multisensory Research</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="763" to="783" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Lightness/pitch and elevation/pitch crossmodal correspondences are low-level sensory effects</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zeljko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kritikos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Grove</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13414-019-01668-w</idno>
		<ptr target="https://doi.org/10.3758/s13414-019-01668-w" />
	</analytic>
	<monogr>
		<title level="j">Attention, Perception, &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1609" to="1623" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Common Neural Mechanisms Control Attention and Working Memory</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Curtis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Sreenivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fougnie</surname></persName>
		</author>
		<idno type="DOI">10.1523/jneurosci.0443-22.2022</idno>
		<ptr target="https://doi.org/10.1523/jneurosci.0443-22.2022" />
	</analytic>
	<monogr>
		<title level="j">The Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">37</biblScope>
			<biblScope unit="page" from="7110" to="7120" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
