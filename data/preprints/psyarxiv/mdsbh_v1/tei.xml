<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Lexical Knowledge Enhances Consistency in Speech Categorization</title>
				<funder ref="#_7VMrteJ">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sita</forename><surname>Carraturo</surname></persName>
							<email>sita-carraturo@uiowa.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Otolaryngology</orgName>
								<orgName type="institution">University of Iowa</orgName>
								<address>
									<postCode>52242</postCode>
									<settlement>Iowa City</settlement>
									<region>IA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Department of Otolaryngology</orgName>
								<orgName type="institution">University of Iowa</orgName>
								<address>
									<postCode>52242</postCode>
									<settlement>Iowa City</settlement>
									<region>IA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Otolaryngology</orgName>
								<orgName type="institution">University of Iowa</orgName>
								<address>
									<postCode>52242</postCode>
									<settlement>Iowa City</settlement>
									<region>IA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Department of Otolaryngology</orgName>
								<orgName type="institution">University of Iowa</orgName>
								<address>
									<postCode>52242</postCode>
									<settlement>Iowa City</settlement>
									<region>IA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hyoju</forename><surname>Kim</surname></persName>
							<email>kim@uiowa.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Psychological &amp; Brain Sciences</orgName>
								<orgName type="institution">University of Iowa</orgName>
								<address>
									<postCode>52242</postCode>
									<settlement>Iowa City</settlement>
									<region>IA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">Department of Psychological &amp; Brain Sciences</orgName>
								<orgName type="institution">University of Iowa</orgName>
								<address>
									<postCode>52242</postCode>
									<settlement>Iowa City</settlement>
									<region>IA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Psychological &amp; Brain Sciences</orgName>
								<orgName type="institution">University of Iowa</orgName>
								<address>
									<postCode>52242</postCode>
									<settlement>Iowa City</settlement>
									<region>IA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">Department of Psychological &amp; Brain Sciences</orgName>
								<orgName type="institution">University of Iowa</orgName>
								<address>
									<postCode>52242</postCode>
									<settlement>Iowa City</settlement>
									<region>IA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ethan</forename><surname>Kutlu</surname></persName>
							<email>ethan-kutlu@uiowa.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Psychological &amp; Brain Sciences</orgName>
								<orgName type="institution">University of Iowa</orgName>
								<address>
									<postCode>52242</postCode>
									<settlement>Iowa City</settlement>
									<region>IA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Communication Sciences &amp; Disorders</orgName>
								<orgName type="institution">University of Iowa</orgName>
								<address>
									<postCode>52242</postCode>
									<settlement>Iowa City</settlement>
									<region>IA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">Department of Psychological &amp; Brain Sciences</orgName>
								<orgName type="institution">University of Iowa</orgName>
								<address>
									<postCode>52242</postCode>
									<settlement>Iowa City</settlement>
									<region>IA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff8">
								<orgName type="department">Department of Communication Sciences &amp; Disorders</orgName>
								<orgName type="institution">University of Iowa</orgName>
								<address>
									<postCode>52242</postCode>
									<settlement>Iowa City</settlement>
									<region>IA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Psychological &amp; Brain Sciences</orgName>
								<orgName type="institution">University of Iowa</orgName>
								<address>
									<postCode>52242</postCode>
									<settlement>Iowa City</settlement>
									<region>IA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Linguistics</orgName>
								<orgName type="institution">University of Iowa</orgName>
								<address>
									<postCode>52242</postCode>
									<settlement>Iowa City</settlement>
									<region>IA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Communication Sciences &amp; Disorders</orgName>
								<orgName type="institution">University of Iowa</orgName>
								<address>
									<postCode>52242</postCode>
									<settlement>Iowa City</settlement>
									<region>IA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">Department of Psychological &amp; Brain Sciences</orgName>
								<orgName type="institution">University of Iowa</orgName>
								<address>
									<postCode>52242</postCode>
									<settlement>Iowa City</settlement>
									<region>IA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff7">
								<orgName type="department">Department of Linguistics</orgName>
								<orgName type="institution">University of Iowa</orgName>
								<address>
									<postCode>52242</postCode>
									<settlement>Iowa City</settlement>
									<region>IA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff8">
								<orgName type="department">Department of Communication Sciences &amp; Disorders</orgName>
								<orgName type="institution">University of Iowa</orgName>
								<address>
									<postCode>52242</postCode>
									<settlement>Iowa City</settlement>
									<region>IA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bob</forename><surname>Mcmurray</surname></persName>
							<email>bob-mcmurray@uiowa.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Psychological &amp; Brain Sciences</orgName>
								<orgName type="institution">University of Iowa</orgName>
								<address>
									<postCode>52242</postCode>
									<settlement>Iowa City</settlement>
									<region>IA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Linguistics</orgName>
								<orgName type="institution">University of Iowa</orgName>
								<address>
									<postCode>52242</postCode>
									<settlement>Iowa City</settlement>
									<region>IA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Communication Sciences &amp; Disorders</orgName>
								<orgName type="institution">University of Iowa</orgName>
								<address>
									<postCode>52242</postCode>
									<settlement>Iowa City</settlement>
									<region>IA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">Department of Psychological &amp; Brain Sciences</orgName>
								<orgName type="institution">University of Iowa</orgName>
								<address>
									<postCode>52242</postCode>
									<settlement>Iowa City</settlement>
									<region>IA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff7">
								<orgName type="department">Department of Linguistics</orgName>
								<orgName type="institution">University of Iowa</orgName>
								<address>
									<postCode>52242</postCode>
									<settlement>Iowa City</settlement>
									<region>IA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff8">
								<orgName type="department">Department of Communication Sciences &amp; Disorders</orgName>
								<orgName type="institution">University of Iowa</orgName>
								<address>
									<postCode>52242</postCode>
									<settlement>Iowa City</settlement>
									<region>IA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Psychological &amp; Brain Sciences</orgName>
								<orgName type="institution">University of Iowa</orgName>
								<address>
									<postCode>52242</postCode>
									<settlement>Iowa City</settlement>
									<region>IA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Linguistics</orgName>
								<orgName type="institution">University of Iowa</orgName>
								<address>
									<postCode>52242</postCode>
									<settlement>Iowa City</settlement>
									<region>IA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Communication Sciences &amp; Disorders</orgName>
								<orgName type="institution">University of Iowa</orgName>
								<address>
									<postCode>52242</postCode>
									<settlement>Iowa City</settlement>
									<region>IA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">Department of Psychological &amp; Brain Sciences</orgName>
								<orgName type="institution">University of Iowa</orgName>
								<address>
									<postCode>52242</postCode>
									<settlement>Iowa City</settlement>
									<region>IA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff7">
								<orgName type="department">Department of Linguistics</orgName>
								<orgName type="institution">University of Iowa</orgName>
								<address>
									<postCode>52242</postCode>
									<settlement>Iowa City</settlement>
									<region>IA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff8">
								<orgName type="department">Department of Communication Sciences &amp; Disorders</orgName>
								<orgName type="institution">University of Iowa</orgName>
								<address>
									<postCode>52242</postCode>
									<settlement>Iowa City</settlement>
									<region>IA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Department of Psychological &amp; Brain Sciences</orgName>
								<orgName type="institution">University of Iowa</orgName>
								<address>
									<addrLine>PBSB 255E</addrLine>
									<postCode>52242</postCode>
									<settlement>Iowa City</settlement>
									<region>IA</region>
									<country key="US">United States of America</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff9">
								<orgName type="department">Department of Psychological &amp; Brain Sciences</orgName>
								<orgName type="institution">University of Iowa</orgName>
								<address>
									<addrLine>PBSB 255E</addrLine>
									<postCode>52242</postCode>
									<settlement>Iowa City</settlement>
									<region>IA</region>
									<country key="US">United States of America</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Lexical Knowledge Enhances Consistency in Speech Categorization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">13194D2EE5FF1EF79D0561B7D9909306</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-22T20:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Methodology</term>
					<term>Formal analysis</term>
					<term>Writing -Original Draft</term>
					<term>Visualization. Hyoju Kim: Conceptualization</term>
					<term>Methodology</term>
					<term>Formal Analysis</term>
					<term>Investigation</term>
					<term>Writing -Original Draft</term>
					<term>Visualization. Ethan Kutlu: Conceptualization</term>
					<term>Writing -Review &amp; Editing. Bob McMurray: Conceptualization</term>
					<term>Methodology</term>
					<term>Sofware</term>
					<term>Resources</term>
					<term>Writing -Original Draft</term>
					<term>Supervision</term>
					<term>Project administration</term>
					<term>Funding acquisition Speech categorization</term>
					<term>Visual analog scaling</term>
					<term>Lexical feedback</term>
					<term>Categorization consistency</term>
					<term>Individual differences</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Speech categorization is a gateway for downstream language processes. Recent work using the Visual Analog Scaling (VAS) task underscores the critical role of categorization consistency (trial-by-trial response variability around the mean response function) as a critical predictor of real-world outcomes such as language and reading abilities. Yet, the mechanisms that contribute to categorization consistency remain unknown. One hypothesis is that higher-level linguistic factors, such as lexical knowledge, may stabilize the percept by cleaning up lowerlevel perceptual noise. The first aim of this study was to test this hypothesis by examining whether categorization consistency is modulated by lexicality (word vs. nonword). Forty-eight adult American English listeners completed a VAS task involving both word (e.g., batch-patch) and matched nonword (e.g., bazg-pazg) continua. Listeners' categorization consistency for the word continua was significantly higher than for the nonword continua. This suggests that categorization consistency is, indeed, affected by higher-level linguistic factors. The second aim was to investigate whether individuals' broader language abilities influence their reliance on lexical information during speech categorization. Although individuals with greater language ability showed more consistent categorization, language scores did not modulate the lexical effect on categorization consistency. Together, these findings demonstrate the roles of top-down knowledge and language knowledge in stabilizing speech categorization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Understanding spoken language is contingent on speech perception, the process of mapping variable speech signals to stored phonological or lexical representations. Decades of psycholinguistic research has sought to uncover the nature of the categories that people use to represent speech (such as /b/ and /p/), and how listeners identify these categories despite the inherent acoustic variability in speech <ref type="bibr" target="#b5">(Clayards et al., 2008;</ref><ref type="bibr" target="#b13">Goldinger, 1998;</ref><ref type="bibr" target="#b14">Holt &amp; Lotto, 2010;</ref><ref type="bibr" target="#b24">Liberman et al., 1967;</ref><ref type="bibr" target="#b36">McMurray, 2022;</ref><ref type="bibr" target="#b38">McMurray &amp; Jongman, 2011;</ref><ref type="bibr" target="#b53">Stilp, 2020)</ref>.</p><p>Historically, such work emphasized broad characterizations that apply across listeners, but more recently, research has sought to characterize individual variation in speech categorization <ref type="bibr" target="#b9">(Fuhrmeister &amp; Myers, 2021;</ref><ref type="bibr" target="#b17">Kapnoula et al., 2017;</ref><ref type="bibr" target="#b19">Kim, Klein-Packard, et al., 2025;</ref><ref type="bibr" target="#b21">Kim et al., 2024;</ref><ref type="bibr" target="#b22">Kutlu et al., 2024;</ref><ref type="bibr" target="#b46">Ou et al., 2021)</ref>. This new focus has led to new questions about the underlying mechanisms that lead to those differences. Early individual differences work asked classic questions about categorical perception, category learning, and cue integration, but from an individual-differences perspective (e.g., <ref type="bibr" target="#b4">Clayards, 2018;</ref><ref type="bibr" target="#b17">Kapnoula et al., 2017;</ref><ref type="bibr">Kong &amp; Edwards, 2016;</ref><ref type="bibr" target="#b34">McHaney et al., 2021)</ref>, essentially extending classic theories and debates to individual differences. A critical recent finding, however, is that one of the most important indices that differs across people is something entirely new: how consistently a listener arrives at the same percept across repeated trials-their categorization consistency <ref type="bibr" target="#b15">(Honda et al., 2024;</ref><ref type="bibr" target="#b19">Kim, Klein-Packard, et al., 2025;</ref><ref type="bibr">Kim, McMurray, et al., 2025;</ref><ref type="bibr" target="#b41">Myers et al., 2024)</ref>.</p><p>The importance of categorization consistency is demonstrated by its relationship to realworld outcomes. Among children, higher categorization consistency is strongly linked to better oral language and word reading <ref type="bibr" target="#b19">(Kim, Klein-Packard, et al., 2025)</ref>. In adults, it predicts about 30% of the variance in language ability, even after controlling for consistency in a non-speech visual categorization task <ref type="bibr" target="#b21">(Kim et al., 2024)</ref>. Greater consistency also predicts better performance on speech-in-noise tasks <ref type="bibr" target="#b41">(Myers et al., 2024)</ref> and more accurate discrimination of speech sounds in second/foreign languages <ref type="bibr" target="#b15">(Honda et al., 2024)</ref>.</p><p>These findings raise a key question: where does categorization consistency (and individual variation therein) come from? Both bottom-up and top-down mechanisms are likely to contribute. Regarding bottom-up mechanisms, variability in low-level auditory encoding may be one source. For instance, children with more stable auditory brainstem responses to speech stimuli exhibit better reading ability <ref type="bibr" target="#b16">(Hornickel &amp; Kraus, 2013;</ref><ref type="bibr" target="#b42">Neef et al., 2017)</ref>. This common link-between consistency of a perceptual response and reading-raises the possibility of similar mechanisms at play and supports the idea that noise at the earliest levels of auditory processing can propagate upward, influencing how consistently listeners categorize ambiguous speech sounds.</p><p>At the same time, top-down mechanisms could also contribute. Higher-level factors like lexical knowledge <ref type="bibr" target="#b28">(Luthra et al., 2021)</ref>, talker familiarity <ref type="bibr" target="#b39">(McMurray &amp; Jongman, 2016)</ref>, and sublexical regularities <ref type="bibr" target="#b43">(Newman et al., 1997;</ref><ref type="bibr" target="#b47">Pitt &amp; McQueen, 1998)</ref> could exert feedback influences on perceptual processing and could help stabilize noisy or ambiguous input <ref type="bibr" target="#b27">(Luthra et al., 2024;</ref><ref type="bibr">Magnuson et al., 2024)</ref>. This connects to a long-running debate over how higherlevel information interacts with low-level input <ref type="bibr" target="#b29">(Magnuson, 2025;</ref><ref type="bibr">Magnuson &amp; Luthra, 2024;</ref><ref type="bibr" target="#b45">Norris et al., 2000)</ref>. We argue, however, that the feedback mechanisms usually proposed in this literature are distinct from what is needed to achieve greater categorization consistency.</p><p>Traditionally, feedback from higher-level representations to lower-level processing has been thought to serve one of two roles: (1) to bias the percept toward expectations (i.e., the Ganong Effect; <ref type="bibr" target="#b11">Ganong, 1980)</ref>, or (2) to generate a difference between percept and expectations <ref type="bibr" target="#b3">(Blank &amp; Davis, 2016;</ref><ref type="bibr" target="#b38">McMurray &amp; Jongman, 2011;</ref><ref type="bibr">and see Lupyan, 2017</ref> for an example in vision). Empirical evidence for such feedback has focused on detecting bias in phoneme categorization (e.g., a shift in the boundary; <ref type="bibr" target="#b7">Elman &amp; McClelland, 1988;</ref><ref type="bibr" target="#b45">Norris et al., 2000)</ref> or accuracy differences as a function of context (e.g., <ref type="bibr" target="#b39">McMurray &amp; Jongman, 2016)</ref>. Under these accounts, feedback alters existing representations to better account for broader context. This is largely studied through its effects on the average response across trials (as these effects are presumed to be equally operative over time).</p><p>In contrast to these traditional views, we propose an alternative function for top-down feedback-what we refer to as Auditory/Phonological Clean-Up (APCU). According to this hypothesis, feedback from higher-level representations (e.g., lexicality) does not merely bias perceptual outcomes, but actively contributes to stabilizing inherent noise in the perceptual system such that the percept is more consistently aligned with the signal. Lexical knowledge may act as a scaffold that reduces variability in the perceptual interpretation of speech sounds, leading to more consistent categorization across repeated presentations of the same input. This proposal aligns with recent computational modeling work using the TRACE model <ref type="bibr">(Magnuson et al., 2024)</ref>, which demonstrates that lexical feedback can "sharpen" noisy input representations.</p><p>Robust evidence for the APCU hypothesis would require clear evidence of feedback as the locus of the clean-up. As the long-running debate over lexical feedback attests, direct (or indirect) evidence of alterations to sublexical representations is an inherently difficult enterprise <ref type="bibr" target="#b8">(Firestone &amp; Scholl, 2016;</ref><ref type="bibr" target="#b45">Norris et al., 2000)</ref>, requiring clever chained effects <ref type="bibr" target="#b7">(Elman &amp; McClelland, 1988;</ref><ref type="bibr" target="#b49">Samuel, 2001)</ref>, or more direct access to sublexical representations via neuroscience (e.g., <ref type="bibr" target="#b12">Getz &amp; Toscano, 2019;</ref><ref type="bibr" target="#b44">Noe &amp; Fischer-Baum, 2020;</ref><ref type="bibr" target="#b50">Sarrett et al., 2020)</ref>. However, before attempting a more complex paradigm, this study seeks to first establish the viability of the APCU hypothesis by asking whether lexicality contributes to increased categorization consistency at all. By itself, such a finding would be consistent with both a feedback-induced locus and with other post-perceptual mechanisms. However, an absence of an effect would rule out the APCU hypothesis entirely.</p><p>Thus, as our first aim, we compared categorization consistency using speech continua that spanned two words (e.g., bond/pond) or two closely matched nonwords (bonf/ponf). Critically, the manipulated acoustic cue was identical in both conditions, making the broader lexical context the only relevant factor. Categorization consistency was measured using the Visual Analog Scaling (VAS) task <ref type="bibr" target="#b1">(Apfelbaum et al., 2022;</ref><ref type="bibr" target="#b33">Massaro &amp; Cohen, 1983;</ref><ref type="bibr" target="#b40">Munson et al., 2017)</ref>, in which listeners heard tokens from a continuum (e.g., beach-peach) and rated each token along a continuous analog scale. This provides a fine-grained index of perceptual gradiency (slope of the categorization function), which can speak to any effects that operate across trials on the average representation. More importantly, once the slope is estimated, we can precisely assess trial-by-trial variability around the mean function-an index of categorization consistency.</p><p>Our second (exploratory) aim was to clarify the relationship between categorization consistency and broader language abilities. We build on recent work showing that categorization consistency predicts both adults' and children's language ability <ref type="bibr" target="#b19">(Kim, Klein-Packard, et al., 2025;</ref><ref type="bibr" target="#b21">Kim et al., 2024)</ref>. However, these studies only used word-word continua, leaving it unclear whether variation in cleanup or variation in the inherent noise is more important. Thus, we included the same language measures employed by <ref type="bibr" target="#b21">Kim et al. (2024)</ref> and predicted an interaction with a stronger effect of language for words than nonwords. However, the exact nature of this interaction is unclear. One possibility is that individuals who show a greater effect of word-nonword continua may demonstrate better language abilities, as lexical feedback is known to enhance word recognition speed and accuracy <ref type="bibr">(Magnuson et al., 2024;</ref><ref type="bibr" target="#b32">Magnuson et al., 2018)</ref>. On the other hand, greater reliance on higher-level information may be a compensatory mechanism and, therefore, linked to poorer language abilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Open Science Practices</head><p>This study was pre-registered. Stimuli, registration, data, and analysis code are available at <ref type="url" target="https://osf.io/5b26j/?view_only=732dfad4b90f417e9b9e387db11d62ad">https://osf.io/5b26j/?view_only=732dfad4b90f417e9b9e387db11d62ad</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Participants</head><p>Participants were recruited via Prolific (<ref type="url" target="www.prolific.com">www.prolific.com</ref>). All subjects provided informed consent in accordance with the University of [redacted for review] Institutional Review Board and received compensation ($12/hour). Fifty-eight participants initially completed the tasks. After exclusions (detailed below), the final sample included 48 native speakers of American English (22 female, Mage = 30.5 years, SD = 6), all reporting no history of speech, hearing, language, reading, or neurological disorders.</p><p>Power. Our pilot study (see Supplement S1 for details) yielded a moderate effect size for the effect of lexicality (d=.55), indicating that a sample size of N = 28 would be sufficient to detect this effect with 80% power. To allow detection of smaller effects (d &gt; .45) with adequate power, we planned for N &gt; 40. Finally, to accommodate a 30% exclusion rate, we targeted 58 participants. We note, however, that detecting medium-sized interactions in a 2 × 2 design (f=.25, approximately d≈.5) would require substantially larger samples (≈180 participants in total) to achieve 80% power. Thus, while our study was well-powered to detect the predicted main effect of lexicality, the interaction analyses should be interpreted with caution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Overview</head><p>The experiment was conducted on participants' computers (mobile devices were not permitted) using Gorilla Experiment Builder <ref type="bibr" target="#b0">(Anwyl-Irvine et al., 2020)</ref>. Before the main tasks, participants completed a headphone screening <ref type="bibr" target="#b56">(Woods et al., 2017)</ref>; those who failed were excluded from further participation. The study comprised two sessions of a VAS task, separated by two language assessments; this took approximately 45 minutes per subject.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Visual Analog Scaling task</head><p>Stimuli. In its entirety, the VAS task included 10 matched sets of word and nonword continua: two bilabial stop voicing pairs, three alveolar stop voicing pairs, two velar stop voicing pairs, and three fricative place pairs (Table <ref type="table">1</ref>). Each continuum was constructed from seven acoustic steps, including the minimal pair endpoints. Because articulation may differ in words and nonwords <ref type="bibr" target="#b51">(Scarborough, 2012;</ref><ref type="bibr" target="#b52">Stephenson, 2004)</ref>, and in words with minimal pairs than words without competitors <ref type="bibr" target="#b54">(Wedel et al., 2018)</ref>, we also introduced a splicing manipulation that led to four continua for each pair, crossing whether the onset portion came from a word or nonword, and whether the stimulus as a whole was a word or nonword.</p><p>Each endpoint token was recorded within the carrier sentence "He said ____", with five repetitions by a male speaker<ref type="foot" target="#foot_0">foot_0</ref> . Noise reduction was applied to the audio files in Audacity before the target items were segmented from the carrier sentence, and then intensities were scaled to 70 dB across items.</p><p>For stop voicing continua, we used a Praat script <ref type="bibr" target="#b55">(Winn, 2020)</ref> to generate seven-step VOT continua (in which f0 was selected to covary across the seven steps). The range of VOTs for each stop voicing continuum differed as a function of the place of articulation (following <ref type="bibr" target="#b25">Lisker &amp; Abramson, 1964)</ref> with 0-40 msec for /b/-/p/; 5-50 msec for /t/-/d/; 10-60 msec for /k/-/g/.</p><p>Fricative place continua were created using a spectral averaging method <ref type="bibr" target="#b6">(Colby et al., 2023;</ref><ref type="bibr" target="#b10">Galle et al., 2019;</ref><ref type="bibr" target="#b37">McMurray et al., 2018)</ref>. This started by excising the frication portions of /s/-/ʃ/ endpoint recordings, estimating their spectrum, shifting the spectra in frequency space, and then filtering noise through that spectrum. To ensure that coarticulation also varied with continuum step, we cut the vocoid from the fricative and TANDEM-STRAIGHT to shift the formants of the vocoid in 7 even steps <ref type="bibr" target="#b18">(Kawahara et al., 1999)</ref> before splicing it onto the frication portion. More detailed stimulus manipulation procedures are provided in Supplemental S2.</p><p>Lastly, to ensure that the acoustic cues in the continua were the same across lexical conditions, items were reconstructed by splicing the onset consonant and vowel from each step of the continua onto the coda consonants to create four continua for each set. For each, the onset consonant and vowel from one token (e.g., ba from batch) was combined with the coda from the same item (e.g., tch from batch; the match-splice condition), or the other item (e.g., zg from bazg; the cross-spliced condition). In the match-splice condition, the coda was taken from another recording of the same item. This led to two splice conditions (match-spliced v. crossspliced) crossed with the word/nonword conditions to ensure that neither articulation nor splicing was a confounding variable.</p><p>Procedure. Each VAS trial displayed a horizontal line with orthographic labels of the two endpoints at each extreme. Participants heard an auditory stimulus over headphones/earphones and clicked a point on the line indicating how closely the stimulus matched either endpoint. A vertical tick mark appeared at their selection point. Listeners could revise their responses before continuing. Trials advanced automatically 300 msec after the final response.</p><p>Each participant was randomly assigned one continuum per contrast type (bilabial, alveolar, velar stop voicing, fricative place), and completed 448 trials (7 steps × 4 continua × 2 lexicality types × 2 repetitions × 2 splicing conditions × 2 sessions). Trials were blocked by continuum, with words, their corresponding nonwords, and both splice conditions appearing within the same block (e.g., batch-patch trials were interleaved with bazg-pazg trials). Trial order within blocks and block order were randomized. The assignment of labels to the side of the continua (e.g., batch on the left or right) was reversed in the second session. Each session took approximately 15 minutes.</p><p>Data Processing. Responses from the second session were reverse-scored so that ratings of 0 and 100 each referred to the same phonemes across sessions. Next, we visually inspected plots of each subject's data (grouped by lexicality and splicing condition); 10 participants were excluded for having flat functions, which could be indicative of lack of engagement in the task.</p><p>For the remaining data (48 participants), four-parameter logistic functions were fit to each subject, each continuum, and each splicing condition using a custom curve-fitting program developed in MATLAB (version 48, McMurray, 2017) (<ref type="url" target="https://osf.io/4atgv/">https://osf.io/4atgv/</ref>). Fits minimized leastsquares error with constraints (e.g., crossover within the continuum range, bounded asymptotes) and provided estimates for minimum and maximum asymptotes, slope, crossover, and root mean squared error (RMSE) of a subject's response ratings. Fits were visually inspected, with no exclusions; average model fit was high (mean R² = .78).</p><p>From this output, the following three variables were extracted: (1) slope, which inversely reflects categorization gradiency (i.e., a steeper slope reflects less gradiency); (2) response variability (RV), measured by trial-by-trial RMSE between individual responses and the predicted psychometric function, inversely reflecting categorization consistency <ref type="bibr" target="#b1">(Apfelbaum et al., 2022;</ref><ref type="bibr" target="#b19">Kim, Klein-Packard, et al., 2025)</ref>: lower RV indicates tighter clustering of responses around the estimated function, reflecting more consistent categorization; and (3) amplitude, the difference between the two asymptote values. Amplitude is an additional index, but its theoretical interpretation is less established compared to RV and slope.<ref type="foot" target="#foot_1">foot_1</ref> Accordingly, all results from amplitude analyses are reported separately in Supplemental S3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Measuring General Language Ability (Language assessment tasks)</head><p>To assess participants' overall language abilities, we administered two in-house assessments: an adapted Token Test and an Agent-Action Test (see <ref type="bibr" target="#b21">Kim et al., 2024)</ref>. The adapted Token Test evaluated sentence comprehension. On each trial, participants viewed a 6×2 grid containing 12 objects-circles and squares in six colors (white, black, red, green, blue, and yellow). They then heard a directive (e.g., "Click on the circles between two squares") and responded by clicking or moving shapes. The task included 30 trials of increasing complexity and lasted ~5 minutes.</p><p>The Agent-Action Test assessed receptive vocabulary. On each trial, participants heard a true/false question containing low-frequency lexical items (e.g., Is an esoteric topic widely understood?) and clicked on "Yes" or "No". The task included 25 randomly ordered trials and lasted fewer than 5 minutes.</p><p>The auditory stimuli for both tasks were generated by an AI text-to-speech tool (<ref type="url" target="https://ttsmaker.com/">https://ttsmaker.com/</ref>). A full list of test items is available on the OSF page.</p><p>Data Processing. Responses for each language test were scored as correct or incorrect (1 or 0). Scores from the two assessments were significantly correlated, r(46) = .76, p &lt; .0001; thus, we computed a composite by z-scoring the accuracy of each task and averaging them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results</head><p>All analyses were performed in R (R Core Team, 2025) and R Studio (Posit team, 2025). Linear mixed-effects models were implemented using the lme4 <ref type="bibr" target="#b2">(Bates et al., 2015)</ref> and lmerTest <ref type="bibr" target="#b23">(Kuznetsova et al., 2017)</ref> packages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Effect of Lexicality on VAS Indices</head><p>The effects of lexicality and splicing condition in each analysis are illustrated in Figure <ref type="figure" target="#fig_0">1</ref>. Linear mixed-effects models assessed the effect of lexicality on (1) categorization consistency and (2) categorization gradiency. On an exploratory basis, we also examined amplitude (described in Supplement S3). These were z-scored to permit a comparison of the magnitude of the fixed effects across models. The two independent variables were lexicality (sum-coded as word = +1 and nonword = -1) and splice condition (match vs. cross, +1 vs. -1, respectively). Per our pre-registered analysis plan, we started with a model with a maximal random effects structure and simplified it until the model converged (see the Note under Table <ref type="table">2</ref> for the model syntax). <ref type="foot" target="#foot_2">3</ref>We started by analyzing RV. Word trials had significantly lower RV than nonword trials (β = -0.092, t(621) = -3.644, p &lt; .001) (Table <ref type="table">2A</ref>). This is consistent with our hypothesis that the availability of top-down knowledge (here, lexical knowledge) leads to more consistent categorization. There was no effect of splicing condition (p = .055), suggesting that categorization consistency did not vary significantly as a function of the coarticulatory cues in the productions of these tokens. Finally, the interaction between lexicality and splicing condition was not significant (p = .169), indicating that coarticulation did not modulate the effect of lexicality.</p><p>Next, we examined slope using the same model as above. None of the terms was significant (all ps &gt; .05; Table <ref type="table">2B</ref>), suggesting that neither top-down knowledge nor coarticulatory cues affects the extent to which categorization is either categorical or gradient. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Individual Differences</head><p>Next, we asked how individual differences in language ability related to the effects reported above. Before the main analysis, we conducted a simple regression predicting language scores from RV. This model revealed that categorization consistency alone accounted for 31% of the variance in language ability (R 2 = .31, p &lt; .001), closely mirroring the effect size reported previously.</p><p>For the main analysis, we used a linear mixed effects model to predict RV (z-scored) from lexicality and splice condition (sum-coded as before), the composite language score (zscored), and all two-way interactions. Figure <ref type="figure" target="#fig_1">2</ref> shows the large correlation between categorization consistency and language scores. Model summaries and model syntax are reported in Table <ref type="table" target="#tab_1">3</ref>.</p><p>Once again, the results showed that words led to greater categorization consistency (lower RV) than nonwords (β = -0.097, t(621) = -3.631, p &lt; .001). The effect of splicing was not significant (p = .085). The effect of language was such that individuals with higher language scores had higher categorization consistency (β = -0.660, t(46) = 7.452, p &lt; .001). However, there were no significant interactions (all ps &gt; .05): the effect of language on categorization consistency was not modulated by whether it was measured on word or nonword trials. Table <ref type="table">2</ref>. Summary of coefficients in the regression models on VAS indices, with lexicality, splicing condition, and their interaction as fixed effects.</p><p>A. Response Variability Fixed effects Estimate SE t p (Intercept) 0.005 0.124 0.039 .970 Lexicality -0.092 0.025 -3.644 &lt; .001 Splicing Condition -0.049 0.025 -1.926 .055 Lexicality × Splicing Condition -0.035 0.025 -1.377 .169 B. Slope (Intercept) 0.007 0.052 0.141 .890 Lexicality 0.056 0.037 1.507 .132 Splicing Condition 0.027 0.037 0.735 .462 Lexicality × Splicing Condition 0.055 0.037 1.471 .142 Note. R syntax: VAS index.Z ~ Lexicality + SpliceCond + LexType:SpliceCond + (1 | Subject) + (1 | Continuum).</p><p>Next, we repeated the analysis with slope as the DV. Again, none of the terms was statistically significant, meaning that the categorical/gradient nature of categorization is not related to language ability, and this did not differ by lexicality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion</head><p>Mounting research with the Visual Analog Scale (VAS) task has revealed that categorization consistency is a critical predictor of real-world language and reading outcomes <ref type="bibr" target="#b15">(Honda et al., 2024;</ref><ref type="bibr" target="#b19">Kim, Klein-Packard, et al., 2025;</ref><ref type="bibr" target="#b21">Kim et al., 2024;</ref><ref type="bibr" target="#b41">Myers et al., 2024)</ref>. This construct reflects how much a listener's percept rating varies from trial to trial and is not well described by theoretical models of speech perception. To better understand the locus of individual differences in categorization consistency, the current study investigated the effect of higher-level knowledge on categorization consistency. Specifically, we used words and nonwords on separate trials to test the auditory/phonological clean-up (APCU) hypothesis: that higher-level knowledge cleans up lower-level noise and thereby increases categorization consistency.</p><p>Consistent with our prediction, categorization consistency was significantly higher on word trials than on nonword trials. Importantly, the splicing manipulation ensured that the segment /b/ in bonf on a cross-spliced trial was the same /b/ in bond on a match-spliced trial. The results, however, suggest that it was the effect of the coda-whether it formed a word or a nonword with the given onset-that affected how consistently the segment was perceived. Splicing condition (match vs. mismatch) was included in the model, but it neither had a significant main effect nor significantly interacted with lexicality. These findings together provide strong support for the hypothesis that, in the face of bottom-up noise, perception of a segment such as /b/ is stabilized by higher-level knowledge (lexicality, in this case).</p><p>Though we propose that the APCU mechanisms function via feedback, this remains speculative. The same observation could arise from clean-up at the level of the lexicon or the mapping between auditory and lexical representations and the VAS response (for an analogical model, see <ref type="bibr" target="#b45">Norris et al., 2000)</ref>. Our work serves as a first step: if there had been no difference as a function of lexicality, the APCU hypothesis would be unsupported. While it is premature to definitively say at which level the effect arises, the robust associations in the literature between categorization consistency and language/reading skills is important: it is unclear why differences solely in the response system would explain so much variance in language. Future work should build on this first step with more sophisticated designs and measures that can assess the perceptual representation more directly.</p><p>In addition to categorization consistency, we analyzed the more traditional categorization index: slope (gradiency). Slope was not significantly affected by lexicality, splicing condition, or their interaction, implying that whether a listener is gradient or categorical is not affected by higher-level knowledge or coarticulatory cues. This is perhaps not surprising, given that slope is also less robustly correlated across continua: a steep slope for stop voicing categorization does not predict a steeper slope for fricatives <ref type="bibr">(Kim, McMurray, et al., 2025)</ref>. The present data are consistent with this: slope showed no reliable cross-contrast correlations (words: r(46) = -.11, p = .46; nonwords: r(46) = .23, p = .112), whereas categorization consistency was strongly correlated across contrast types (words: r(46) = .71, p &lt; .0001; nonwords: r(46) = .75, p &lt; .0001). This suggests that slope may not be a product of larger language function, but rather, derives from differences in a listener's experience with a particular contrast (e.g., <ref type="bibr" target="#b5">Clayards et al., 2008;</ref><ref type="bibr">Kim, McMurray, et al., 2025;</ref><ref type="bibr" target="#b22">Kutlu et al., 2024)</ref>. However, the lack of an effect on slope also provides some discriminant validity regarding the APCU hypothesis, which predicts an effect specifically on categorization consistency.</p><p>A second aim was to evaluate how the effect of lexicality related to language ability. We evaluated language ability by using a composite language score of the same tests used in prior work <ref type="bibr" target="#b21">(Kim et al., 2024)</ref>. Here, we predicted an interaction with lexicality that would suggest either that the influence of higher-level knowledge on cleaning up lower-level noise could be dependent on greater language ability, or that it could be stronger for individuals with lower language ability as a manifestation of a compensatory mechanism.</p><p>We replicated the finding that individuals with higher language scores also show higher categorization consistency <ref type="bibr" target="#b21">(Kim et al., 2024)</ref>. However, there was no significant interaction with lexicality, providing no support for either of the proposed interpretations. One possibility is that word and nonword categorization reflect a large amount of shared variance. In fact, despite the significant effect of lexicality on RV, RV was strongly correlated between the word and nonword continua, r(46) = .92, p &lt; .0001. Slope was also significantly, though less strongly, correlated, r(46) = .52, p &lt; .001. To some extent, such correlations were predicted-the word and nonword continua used the same auditory instantiations, and certainly some variance in slope and RV reflects specific response to specific auditory cues. However, the magnitude of the correlation in RV was unexpected and does not favor a model in which differences in the ability to engage APCU can explain the high correlation between language and categorization consistency. This echoes previous work by <ref type="bibr">Kim, McMurray, et al. (2025)</ref>, who found that individuals' RV was more strongly correlated across continua than their slopes, suggesting that consistency is perhaps "trait-like".</p><p>We see two possible explanations for the lack of a moderation by language. First, although we expected the effect of APCU mechanisms on consistency to be moderated by language ability, at face value, our data are more consistent with the idea that categorization consistency is trait-like within individuals and individual differences in this trait are largely driven by differences in the auditory/perceptual system. APCU then acts relatively uniformly (across individuals) to clean up some of the inherent noise in the system. An alternative explanation is that the experimental design limited the extent to which individual variation in the APCU effect was apparent. For example, several factors could have encouraged lexicalization of the nonwords over the course of the experiment: their orthographic presentations at the ends of the scale; the use of monosyllabic, phonotactically legal stimuli; and repeated exposure. Indeed, the nonwords were fairly word-like (e.g., using a common CVCC structure, frequent consonants), thus providing APCU mechanisms a great deal of information to leverage. Follow up work should examine nonwords that are less familiar. Similarly, the use of just one talker likely promoted talker familiarity, a higher-level cue that the APCU mechanisms could leverage even in the context of nonwords.</p><p>Broadly, this study demonstrated a clear effect of lexicality on categorization consistency. Though more work is needed, it provides the first piece of support for the hypothesis that categorization consistency reflects, in part, the effect of APCU mechanisms. Future work may seek to clarify the role of feedback in APCU and to identify individual differences in the strength of these mechanisms.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Raw values of VAS parameter estimates as a function of lexicality (x-axis) and splice condition. Each panel displays mean values (±1 SE) for (A) response variability (RV; higher raw scores reflect lower categorization consistency), and (B) slope (higher raw scores reflect more categorical responses).</figDesc><graphic coords="8,276.58,194.55,242.61,303.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Correlation between categorization consistency and language score as a function of lexicality and splicing condition.</figDesc><graphic coords="9,240.00,258.85,263.15,193.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="18,72.00,72.00,468.00,260.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Summary of output from regression models of VAS indices with lexicality, splicing condition, and z-scored language composite scores as fixed effects.</figDesc><table><row><cell>A. Response Variability</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fixed Effects</cell><cell>Estimate</cell><cell>SE</cell><cell>t</cell><cell>p</cell></row><row><cell>(Intercept)</cell><cell>0.183</cell><cell>0.097</cell><cell>1.881</cell><cell>.069</cell></row><row><cell>Lexicality (word)</cell><cell>-0.097</cell><cell>0.027</cell><cell>-3.631</cell><cell>&lt; .001</cell></row><row><cell>Splice Condition (match -spliced)</cell><cell>-0.046</cell><cell>0.027</cell><cell>-1.727</cell><cell>.085</cell></row><row><cell>Language Composite</cell><cell>-0.660</cell><cell>0.089</cell><cell>-7.452</cell><cell>&lt; .001</cell></row><row><cell>Lexicality x Splice Condition</cell><cell>-0.035</cell><cell>0.024</cell><cell>-1.370</cell><cell>.171</cell></row><row><cell>Lexicality x Language</cell><cell>0.017</cell><cell>0.030</cell><cell>0.575</cell><cell>.565</cell></row><row><cell>Splice Condition x Language</cell><cell>-0.009</cell><cell>0.030</cell><cell>-0.310</cell><cell>.756</cell></row><row><cell>B. Slope</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(Intercept)</cell><cell>-0.002</cell><cell>0.056</cell><cell>-0.042</cell><cell>.967</cell></row><row><cell>Lexicality (word)</cell><cell>0.060</cell><cell>0.039</cell><cell>1.527</cell><cell>.127</cell></row><row><cell>Splice Condition (match-spliced)</cell><cell>0.016</cell><cell>0.039</cell><cell>0.416</cell><cell>.677</cell></row><row><cell>Language Composite</cell><cell>0.037</cell><cell>0.059</cell><cell>0.622</cell><cell>.537</cell></row><row><cell>Lexicality x Splice Condition</cell><cell>0.055</cell><cell>0.037</cell><cell>1.467</cell><cell>.143</cell></row><row><cell>Lexicality x Language</cell><cell>-0.014</cell><cell>0.045</cell><cell>-0.312</cell><cell>.755</cell></row><row><cell>Splice Condition x Language</cell><cell>0.041</cell><cell>0.045</cell><cell>0.923</cell><cell>.356</cell></row><row><cell cols="4">Note. R syntax: VAS.index ~ Lexicality + SpliceCond + z_LangComp + LexType:SpliceCond +</cell><cell></cell></row><row><cell cols="4">LexType:z_LangComp + SpliceCond:z_LangComp + (1 | Subject) + (1 | Continuum)</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The talker was a monolingual speaker of American English from the Midwest. Table 1. List of items used in the VAS task Word Nonword bond-pond bonf-ponf batch-patch bazg-pazg tent-dent tenf-denf tusk-dusk tups-dups tense-dense tench-dench cage-gage caish-gaish coat-goat coadge-goadge sift-shift sigged-shigged seep-sheep seeg-sheeg sip-ship sib-shib</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>There remain open questions regarding whether amplitude functions as a primary index of categorization or whether it largely reflects variance already captured by RV, in which case it may be redundant.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Random intercepts were included for subject and continuum. Continuum, in this case, is defined as word and nonword pairs for a given contrast; e.g., bonf-ponf and bond-pond are modeled as a single continuum.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>Part of the data presented here was presented at the 188 th Meeting of the <rs type="institution">Acoustical Society of America in New Orleans, Louisiana</rs>. This work was supported by the <rs type="funder">National Science Foundation</rs> under grant number <rs type="grantNumber">BCS-2444664</rs> to BM.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_7VMrteJ">
					<idno type="grant-number">BCS-2444664</idno>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Availability</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declaration of Interest</head><p>The authors declare that they have no competing interests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S1. Pilot Data</head><p>To inform the design of the present study, we conducted two pilot experiments examining whether lexical status (word vs. nonword) modulates phonetic categorization patterns, using a Visual Analog Scaling (VAS) task. Both experiments were administered online via Gorilla Experiment Builder and recruited participants through Prolific.</p><p>In Pilot Experiment 1, forty-eight adult native speakers of American English (30 female; mean age = 31.6 years, SD = 5.4) completed the task. The stimuli comprised four-word continua <ref type="bibr">(beach-peach, dime-time, gold-cold, dark-bark)</ref> and four corresponding nonword continua <ref type="bibr">(beag-peag, dich-tich, golss-kolss, darp-barp)</ref>, representing two stop voicing and two stop place contrasts. Each continuum was manipulated into nine acoustic steps. To ensure that lexical status was the only systematic difference between paired conditions, the manipulated portions of the word stimuli were spliced onto their corresponding nonword pairs, rendering each word-nonword pair acoustically identical in the critical region.</p><p>The results revealed no significant difference in categorization slope between word and nonword continua, t(47) = -1.14, p = .26 (Figure <ref type="figure">S1A</ref>). However, categorization consistency was significantly higher for word continua compared to nonwords, t(47) = -3.77, p &lt; .001 (Figure <ref type="figure">S1B</ref>). Amplitude measures also differed by lexical status, t(47) = -3.67, p &lt; .001, with greater amplitude observed in responses to word continua (Figure <ref type="figure">S1C</ref>). A linear mixed-effects model predicting each VAS index from lexical status (as a fixed effect), with subject as a random intercept, confirmed a significant effect of lexical status on categorization consistency (p &lt; .001) but not on slope. These findings suggest that lexical knowledge enhances the stability of speech categorization. However, this pilot did not include a non-spliced baseline condition, limiting our ability to determine whether the observed effects could be attributed to coarticulatory cues. This limitation motivated a second pilot experiment.</p><p>Pilot Experiment 2 addressed this gap by including both spliced and non-spliced versions of the stimuli and by broadening the range of phonological contrasts. Forty-one adult native speakers of American English (20 female; mean age = 29.8 years, SD = 5.3) participated. The stimulus set comprised four-word continua (beach-peach, save-shave, check-chuck, least-list) and four matched nonword continua (beesp-peesp, sague-shague, chesp-chusp, leedge-lidge), which included one stop voicing contrast, one fricative place contrast, and two vowel contrasts. Each continuum contained seven steps. As in the first pilot, continua were cross-spliced to ensure acoustic equivalence across lexical conditions, and a non-spliced version of each continuum was also included.</p><p>The results replicated the main findings of Pilot 1 (Figure <ref type="figure">S1D-F</ref>). A mixed-effects model predicting categorization consistency from lexical status (fixed effect), with subject as a random intercept, revealed a significant effect of lexical status (β = -2.07, SE = 0.60, p = .001), again indicating higher consistency for word continua. No significant effect was found for slope. A second model included lexical status, splicing condition (spliced vs. non-spliced), and their interaction. This model revealed only a significant main effect of lexical status, with no effect of splicing condition or interaction. These results indicate that top-down lexical information enhances perceptual stability irrespective of the presence of coarticulatory cues.</p><p>Despite these promising findings, the second pilot remained limited by the small number of contrast pairs and the inclusion of vowel contrasts, which proved challenging for acoustic manipulation. Vowels are highly coarticulated segments, making it difficult to create naturalsounding, tightly controlled stimuli.</p><p>Together, the results from both pilot experiments provide converging evidence that lexical knowledge stabilizes categorization consistency, independent of coarticulatory context. These findings informed the design of the current study by motivating the use of a new and more extensive set of continua focused on stop voicing and fricative place contrastsdimensions that allow for more precise acoustic manipulation and theoretical clarity. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2. Additional Stimulus Creation Details</head><p>For stop voicing continua, two recordings of each word were selected. The initial 100 msec of silence, the initial consonant, and the vowel from one file of each end of a continuum (e.g., ba from one recording of batch and the pa from one recording of patch) were isolated and run through a Praat script <ref type="bibr" target="#b55">(Winn, 2020)</ref> to create a seven-step VOT continuum. Each step of this ba/pa continuum was then spliced together with the coda and final 100 msec of silence from the other recording of the same word (e.g., tch from the other recording of batch), generating a "match-spliced" continuum. Then, the original seven steps were spliced together with the coda and 100ms of silence from the corresponding nonword (e.g., zg from bazg) to create a "crossspliced" continuum; the same is done for nonwords (e.g., after making a 7-step continuum from isolated ba/pa splices from bazg/pazg recordings, each step is spliced with zg from another recording of bazg and cross-spliced with tch from batch).</p><p>For fricative place continua, spliced endpoints were created following the same procedure as described above. To create /s/-/ʃ/ continua, we used a spectral averaging procedure developed in prior studies <ref type="bibr" target="#b6">(Colby et al., 2023;</ref><ref type="bibr" target="#b10">Galle et al., 2019;</ref><ref type="bibr" target="#b37">McMurray et al., 2018)</ref>. First, the frication portions from endpoint tokens (e.g., save and shave) were extracted from the selected recordings. Second, the longer frication segment was cut to match the length of the shorter one, ensuring consistency in segment length. Third, the spectral mean is calculated from the long-term average spectra of each fricative, and both spectra were aligned to the same average spectral mean. Fourth, a weighted average of the two spectra is constructed to create a series of seven spectra, representing each step along the continuum. Fifth, the spectral means of the spectra are shifted in frequency space to create seven steps. Sixth, the modified spectra are applied as filters to a segment of white noise, which has an envelope that is the average of the /s/ and /ʃ/ endpoints. These steps were implemented in MATLAB script (<ref type="url" target="https://osf.io/ut9wz/">https://osf.io/ut9wz/</ref>). Separately, a continuum of the vocoids of the endpoints was created in TANDEM-STRAIGHT <ref type="bibr" target="#b18">(Kawahara et al., 1999)</ref>. Lastly, we splice each step of this continuum onto the corresponding /s/-/ʃ/ continuum. Cross-spliced continua were created by removing the coda from each step of the continua and splicing the coda of the non-word onto the word and vice-versa (e.g., the /p/ from sheep onto sheeg and the /g/ from sheeg onto sheep).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S3. Amplitude Analyses and Results</head><p>First, we examined the effects of lexical type and splicing on amplitude (Table <ref type="table">S3A</ref>). Word continua, on average, had significantly higher amplitudes than nonword continua (β = 0.115, t(621) = 4.365, p &lt; .001). Similarly, match-spliced trials showed significantly higher amplitudes than cross-spliced trials (β = 0.077, t(621) = -2.931, p &lt; .01). This suggests that coarticulatory cues consistent with a natural production of a token lead to more robust categorization of endpoint tokens (or conversely that mismatching coarticulation can disrupt the ambiguous tokens). The interaction between lexical type and splicing condition was not significant (p = 0.190).</p><p>Next, we analyzed how language scores modulated the relationships of lexical type and splicing on amplitudes (Table <ref type="table">S3B</ref>). The analysis showed that both lexical type and splicing conditions affected amplitude. Words again showed larger amplitudes than nonwords (β = .135, t(619) = 4.905, p &lt; .001) and match-spliced tokens had larger amplitudes than crossspliced ones (β = 0.080, t(619) = 2.926, p &lt; .01).</p><p>There was also a main effect of language ability such that those with higher language scores also yielded more robust categorization of endpoint tokens (β = 0.357, t(46) = 3.273, p &lt; .01). Finally, there was also a significant interaction between language ability and lexical type such that the effect of lexical type on amplitude was attenuated for people with higher language scores (β = -0.076, t(619) = -2.429, p &lt; .05). Figure <ref type="figure">S3</ref> shows that this interaction is characterized by a sort of ceiling effect among individuals with high language scores, for whom amplitudes are already high, independent of lexical type.</p><p>In summary, lexical type had a significant effect on amplitude, which was higher in the word condition. That lexical type was unrelated to slope but related to both amplitude and consistency may indicate inherent relationships between these measures: whereas differences in slope are further characterized by consistency <ref type="bibr" target="#b1">(Apfelbaum et al., 2022)</ref>, high consistency may be a prerequisite for high amplitudes, but this remains to be investigated empirically.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Gorilla in our midst: An online behavioral experiment builder</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Anwyl-Irvine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Massonnie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Flitton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kirkham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Evershed</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13428-019-01237-x</idno>
		<ptr target="https://doi.org/10.3758/s13428-019-01237-x" />
	</analytic>
	<monogr>
		<title level="j">Behav Res Methods</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="388" to="407" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Don&apos;t force it! Gradient speech categorization calls for continuous categorization tasks</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Apfelbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcmurray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Kapnoula</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3728" to="3745" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Package &apos;lme4</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maechler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bolker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H B</forename><surname>Christensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Singmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Grothendieck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Bolker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">convergence</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Prediction errors but not sharpened signals simulate multivoxel fMRI patterns during speech perception</title>
		<author>
			<persName><forename type="first">H</forename><surname>Blank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS biology</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">1002577</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Differences in cue weights for speech perception are correlated for individuals within and across contrasts</title>
		<author>
			<persName><forename type="first">M</forename><surname>Clayards</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="172" to="L177" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Perception of speech reflects optimal use of probabilistic speech cues</title>
		<author>
			<persName><forename type="first">M</forename><surname>Clayards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Tanenhaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Aslin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="804" to="809" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Audiological and Demographic Factors that Impact the Precision of Speech Categorization in Cochlear Implant Users</title>
		<author>
			<persName><forename type="first">S</forename><surname>Colby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seedorff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcmurray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ear and hearing</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="572" to="587" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cognitive penetration of the mechanisms of perception: Compensation for coarticulation of lexically restored phonemes</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="143" to="165" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cognition does not affect perception: Evaluating the evidence for &quot;top-down&quot; effects</title>
		<author>
			<persName><forename type="first">C</forename><surname>Firestone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Scholl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and brain sciences</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page">e229</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Structural neural correlates of individual differences in categorical perception</title>
		<author>
			<persName><forename type="first">P</forename><surname>Fuhrmeister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Myers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain and Language</title>
		<imprint>
			<biblScope unit="volume">215</biblScope>
			<biblScope unit="page">104919</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">What are you waiting for? Real-time integration of cues for fricatives suggests encapsulated auditory memory</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Galle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Klein-Packard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schreiber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcmurray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">12700</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Phonetic categorization in auditory word perception</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Ganong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">110</biblScope>
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Electrophysiological evidence for top-down lexical influences on early speech perception</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Getz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Toscano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological science</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="830" to="841" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Echoes of echoes? An episodic theory of lexical access</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Goldinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">251</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Speech perception as categorization</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Lotto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Attention, Perception, &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1218" to="1227" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Exploring individual differences in native phonetic perception and their link to nonnative phonetic perception</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Honda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Clayards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Baum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">370</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unstable representation of sound: a biological marker of dyslexia</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hornickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kraus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3500" to="3504" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Evaluating the sources and functions of gradiency in phoneme categorization: An individual differences approach</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Kapnoula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcmurray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">1594</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Restructuring speech representations using a pitch-adaptive time-frequency smoothing and an instantaneous-frequency-based F0 extraction: Possible role of a repetitive structure in sounds</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Masuda-Katsuse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>De Cheveigne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech communication</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="187" to="207" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Speech Categorization Consistency Predicts Language and Reading Abilities in School-Age Children: Implications for Language and Reading Disorders</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Klein-Packard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sorensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Oleson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tomblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcmurray</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cognition.2025.106194</idno>
		<ptr target="https://doi.org/https://doi.org/10.1016/j.cognition.2025.106194" />
		<imprint>
			<date type="published" when="2025">2025. 106194</date>
			<biblScope unit="volume">263</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The consistency of categorizationconsistency in speech perception</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcmurray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sorensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Oleson</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13423-025-02700-x</idno>
		<ptr target="https://doi.org/10.3758/s13423-025-02700-x" />
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Individual differences in categorical perception of speech: Cue weighting and executive function</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tomblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcmurray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<idno type="DOI">10.31234/osf.io/</idno>
		<ptr target="https://doi.org/https://doi.org/10.31234/osf.io/" />
	</analytic>
	<monogr>
		<title level="m">Speech Categorization Consistency Predicts Overall Languae Abilities</title>
		<imprint>
			<date type="published" when="2016">2024. 2016</date>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="40" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Linguistic diversity shapes flexible speech perception in school age children</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Baxelbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sorensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Oleson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcmurray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">28825</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">lmerTest package: tests in linear mixed effects models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Brockhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Christensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of statistical software</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Perception of the speech code</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Liberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Shankweiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Studdert-Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">431</biblScope>
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A cross-language study of voicing in initial stops: Acoustical measurements</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lisker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Abramson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Word</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="384" to="422" />
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Objective effects of knowledge on visual perception</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lupyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">794</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Do They Know It&apos;s Christmash? Lexical Knowledge Directly Impacts Speech Perception</title>
		<author>
			<persName><forename type="first">S</forename><surname>Luthra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Crinnion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Saltzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Magnuson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">13449</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Robust lexically mediated compensation for coarticulation: Christmash time is here again</title>
		<author>
			<persName><forename type="first">S</forename><surname>Luthra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Peraza-Santiago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Beeson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Saltzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Crinnion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Magnuson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">12962</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">TRACE-ing fixations in the Visual World Paradigm: Extending linking hypotheses and addressing individual differences by simulating trial-level behavior</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Magnuson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain Research</title>
		<imprint>
			<biblScope unit="volume">1856</biblScope>
			<biblScope unit="page">149563</biblScope>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Contra assertions, feedback improves word recognition: How feedback and lateral inhibition sharpen signals over noise</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Magnuson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Crinnion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luthra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gaston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Grubb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">242</biblScope>
			<biblScope unit="page">105661</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Simple recurrent networks are interactive</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Magnuson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luthra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Interaction in spoken word recognition models: Feedback helps</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Magnuson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mirman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luthra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Strauss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Harris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in psychology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">369</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Categorical or continuous speech perception: A new test</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Massaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech communication</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="35" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Working memory relates to individual differences in speech category learning: Insights from computational modeling and pupillometry</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Mchaney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tessmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Roark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chandrasekaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain and Language</title>
		<imprint>
			<biblScope unit="volume">222</biblScope>
			<biblScope unit="page">105010</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Nonlinear curvefitting for Psycholinguistics (and other) Data</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mcmurray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Version</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The myth of categorical perception</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mcmurray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3819" to="3842" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Speech categorization develops slowly through adolescence</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mcmurray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Danelz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seedorff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Developmental psychology</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">1472</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">What information is necessary for speech categorization? Harnessing variability in the speech signal by integrating cues computed relative to expectations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mcmurray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jongman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">219</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">What comes after/f/? Prediction in speech derives from data-explanatory processes</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mcmurray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jongman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="52" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Bias in the perception of phonetic detail in children&apos;s speech: A comparison of categorical and continuous rating scales</title>
		<author>
			<persName><forename type="first">B</forename><surname>Munson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Schellinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Edwards</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clinical Linguistics &amp; Phonetics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="56" to="79" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Individual differences in the perception of phonetic category structure predict speech-in-noise performance</title>
		<author>
			<persName><forename type="first">E</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Skoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">156</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1707" to="1719" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Auditory brainstem responses to stop consonants predict literacy</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>Neef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Schaadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Friederici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clinical Neurophysiology</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="484" to="494" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Lexical neighborhood effects in phonetic processing</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Sawusch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Luce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">873</biblScope>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Early lexical influences on sublexical processing in speech perception: Evidence from electrophysiology</title>
		<author>
			<persName><forename type="first">C</forename><surname>Noe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fischer-Baum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">197</biblScope>
			<biblScope unit="page">104162</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Merging information in speech recognition: Feedback is never necessary</title>
		<author>
			<persName><forename type="first">D</forename><surname>Norris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mcqueen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cutler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="299" to="325" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Individual differences in categorization gradience as predicted by online processing of phonetic cues during spoken word recognition: Evidence from eye movements</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">12948</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Is compensation for coarticulation mediated by the lexicon?</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Pitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mcqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="347" to="370" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">RStudio: Integrated Development Environment for R</title>
		<ptr target="http://www.posit.co/RCoreTeam,R" />
	</analytic>
	<monogr>
		<title level="m">R: A language and environment for statistical computing</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2025">2025. 2025</date>
		</imprint>
		<respStmt>
			<orgName>Posit team</orgName>
		</respStmt>
	</monogr>
	<note>Posit Software, PBC</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Knowing a word affects the fundamental perception of the sounds within it</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Samuel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological science</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="348" to="351" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Dynamic EEG analysis during language comprehension reveals interactive cascades between perceptual processing and sentential expectations</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Sarrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcmurray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Kapnoula</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain and language</title>
		<imprint>
			<biblScope unit="volume">211</biblScope>
			<biblScope unit="page">104875</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Lexical similarity and speech production: Neighborhoods for nonwords</title>
		<author>
			<persName><forename type="first">R</forename><surname>Scarborough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lingua</title>
		<imprint>
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="164" to="176" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Lexical frequency and neighbourhood density effects on vowel production in words and nonwords</title>
		<author>
			<persName><forename type="first">L</forename><surname>Stephenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Australian International Conference on Speech Science and Technology</title>
		<meeting>the 10th Australian International Conference on Speech Science and Technology</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Acoustic context effects in speech perception</title>
		<author>
			<persName><forename type="first">C</forename><surname>Stilp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wiley Interdisciplinary Reviews: Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1517</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">The phonetic specificity of contrastive hyperarticulation in natural speech</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sharp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="61" to="88" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Manipulation of voice onset time in speech stimuli: A tutorial and flexible Praat script</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Winn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">147</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="852" to="866" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Headphone screening to facilitate web-based auditory experiments</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Woods</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Traer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Mcdermott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Attention, Perception, &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2064" to="2072" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
