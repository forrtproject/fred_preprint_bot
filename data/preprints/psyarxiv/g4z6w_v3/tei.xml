<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Addressing the Precision-Breadth-Simplicity Impossible Trinity in Psychological Research: A Comprehensive Exploration Approach</title>
				<funder ref="#_dYcceFq">
					<orgName type="full">Research Grants Council of Hong Kong</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>SAGE Publications</publisher>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-09-29">2025-09-29</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Liqiang</forename><surname>Huang</surname></persName>
							<email>lqhuang@cuhk.edu.hk</email>
							<idno type="ORCID">0000-0002-0007-9375</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Addressing the Precision-Breadth-Simplicity Impossible Trinity in Psychological Research: A Comprehensive Exploration Approach</title>
					</analytic>
					<monogr>
						<title level="j" type="main">Review of General Psychology</title>
						<title level="j" type="abbrev">Review of General Psychology</title>
						<idno type="ISSN">1089-2680</idno>
						<idno type="eISSN">1939-1552</idno>
						<imprint>
							<publisher>SAGE Publications</publisher>
							<date type="published" when="2025-09-29" />
						</imprint>
					</monogr>
					<idno type="MD5">316C1FB9F0EEA93A9D3782C4FF9368AB</idno>
					<idno type="DOI">10.1177/10892680251380430</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-22T14:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Breadth</term>
					<term>comprehensive exploration</term>
					<term>large-scale experiment</term>
					<term>precision</term>
					<term>simplicity</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Psychological research faces a fundamental challenge -the Precision-Breadth-Simplicity (PBS) impossible trinity. While experimental findings are often precise and simple, they tend to be narrow in scope. Conversely, broad and simple concepts frequently lack precision. Developing theories that are both precise and broad is scientifically valuable but inevitably introduces complexity, which conflicts with humans' cognitive limitations in processing complexity. To address this impossible trinity, I propose a comprehensive exploration (CE) approach-a data-guided theory-building framework that involves: (1) designing experimental conditions in a stimulus-driven way, with minimal upfront theoretical specification; (2) conducting experiments with tens of millions of observations (e.g., 40 million responses in Huang, 2025a); (3) modeling the results through iterative improvements; and (4) producing the outcome: a moderately complex quantitative information-processing model to integrate diverse empirical findings. Inspired by similar strategies that drove breakthroughs in artificial intelligence (e.g., ImageNet's role in advancing object recognition), the CE approach offers a promising path toward more integrative psychological theories. Initial implementations in visual working memory research demonstrate both its practicality and potential to transform how we study mental processes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A challenge for psychological research</head><p>The PBS impossible trinity of psychological research Psychological research over the past half-century has generated a wealth of experimental findings offering detailed insights into specific phenomena. Yet this knowledge remains fundamentally fragmented (e.g., <ref type="bibr" target="#b2">Almaatouq et al., 2024;</ref><ref type="bibr" target="#b5">Anvari et al, 2025)</ref> -like scattered pieces awaiting assembly into a coherent whole. The study of visual working memory (VWM) exemplifies this challenge: while decades of research have identified numerous factors (e.g., <ref type="bibr" target="#b50">Pashler, 1988;</ref><ref type="bibr" target="#b3">Alvarez &amp; Cavanagh, 2004;</ref><ref type="bibr" target="#b69">Zhang &amp; Luck, 2008;</ref><ref type="bibr" target="#b8">Bays &amp; Husain, 2008;</ref><ref type="bibr" target="#b11">Brady &amp; Alvarez, 2011;</ref><ref type="bibr" target="#b12">Brady &amp; Alvarez, 2015;</ref><ref type="bibr">Suchow, Brady, Fournier &amp; Alvarez, 2013;</ref><ref type="bibr" target="#b7">Bae et al., 2015;</ref><ref type="bibr" target="#b30">Huang, 2020;</ref><ref type="bibr" target="#b23">Fougnie, Suchow &amp; Alvarez, 2012;</ref><ref type="bibr"></ref> Van den <ref type="bibr" target="#b64">Berg &amp; Ma, 2018;</ref><ref type="bibr">Liesefeld, Liesefeld &amp; Müller, 2019)</ref>, comprehensive theoretical frameworks (e.g., <ref type="bibr" target="#b49">Oberauer, Farrell, Jarrold &amp; Lewandowsky, 2016;</ref><ref type="bibr" target="#b49">Oberauer et al., 2018;</ref><ref type="bibr" target="#b48">Oberauer, 2023;</ref><ref type="bibr" target="#b62">Suchow, Fougnie, Brady &amp; Alvarez, 2014;</ref><ref type="bibr">Liesefeld &amp; Müller, 2019</ref>) have yet to yield a truly unified account that precisely integrates all relevant elements.</p><p>Beyond fragmented findings, conceptual vagueness presents a parallel challenge. Core psychological constructs like "attention" and "working memory" frequently operate as overly broad, imprecise categories encompassing diverse phenomena (e.g., <ref type="bibr" target="#b57">Rosenholtz, 2024;</ref><ref type="bibr" target="#b28">Hommel et al., 2019;</ref><ref type="bibr" target="#b4">Anderson, 2011)</ref>. This dual challenge of fragmentation and vagueness stands in stark contrast to disciplines like physics, where theories from Newton's laws to Maxwell's equations achieve remarkable explanatory unity through fundamental principles like symmetry and conservation laws. Such exemplary theories successfully achieve three key attributes: precision, breadth and simplicity<ref type="foot" target="#foot_0">foot_0</ref> .</p><p>Why has psychology failed to develop optimal theories comparable to those in physics? <ref type="bibr" target="#b2">Almaatouq et al. (2024)</ref> identify several contributing factors (e.g., experimental incommensurability, theoretical imprecision). While these observations are accurate, I argue they reflect symptoms rather than root causes-resulting from a deeper, unified constraint: the "PBS impossible trinity" (Precision-Breadth-Simplicity), illustrated in Figure <ref type="figure">1</ref>. This principle asserts that psychological research can typically satisfy any two of these criteria simultaneously, but achieving all three proves exceptionally challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Three trade-off options</head><p>The PBS impossible trinity arises from the inherent complexity of mental mechanisms. As Figure <ref type="figure">1</ref> illustrates, this complexity forces three fundamental trade-offs in psychological research: one can either (1) narrow the focus to yield precise yet fragmented findings (precise-and-simple approaches); (2) broaden the scope to capture the "big picture" at the cost of precision (broad-and-simple approaches); or (3) embrace both breadth and precision while directly confronting the ensuing complexity (preciseand-broad approaches).</p><p>Most experimental studies exemplify the precise-and-simple approach. While methodologically rigorous, these typically examine isolated phenomena-for instance, demonstrating how VWM performance depends on categorical processing (i.e., how color categories such as red and blue affect memory, see <ref type="bibr" target="#b7">Bae et al., 2015)</ref>, interactions between items (i.e., how items affect each other's memorized appearance, see <ref type="bibr" target="#b11">Brady &amp; Alvarez, 2011)</ref>, quality-quantity trade-offs (i.e., the trade-off between memorizing a larger number of imprecise items versus a smaller number of precise ones, see <ref type="bibr" target="#b22">Fougnie et al., 2016)</ref>, or chunking (i.e., memorizing multiple items as a single unit, see <ref type="bibr" target="#b13">Brady &amp; Tenenbaum, 2013)</ref>. Such studies provide crucial but piecemeal insights, leaving the broader question ("How are these items memorized?") unanswered.</p><p>The broad-and-simple approach dominates theoretical discourse, where umbrella terms like "attention" explain diverse phenomena but often lack operational precision <ref type="bibr" target="#b57">(Rosenholtz, 2024;</ref><ref type="bibr" target="#b28">Hommel et al., 2019;</ref><ref type="bibr" target="#b4">Anderson, 2011)</ref>. While useful for organizing knowledge, these concepts become vague when applied across different experimental contexts.</p><p>The precise-and-broad approach represents the most scientifically valuable direction for psychological research, as it simultaneously expands the scope (breadth) and depth (precision) of our understanding of mental processes. While this approach aligns perfectly with psychology's fundamental goal of comprehensive theoretical understanding, it remains exceptionally rare in practice. Although some studies have begun moving in this direction, a truly ideal example has yet to be realized.</p><p>The "Blind Men and the Elephant" parable (Figure <ref type="figure" target="#fig_1">2</ref>) captures these trade-offs vividly: precise-andsimple research resembles isolated tactile explorations of individual body parts (Figure <ref type="figure" target="#fig_1">2b</ref>), broad-andsimple theories produce oversimplified "spherical elephant in a vacuum" generalizations (Figure <ref type="figure" target="#fig_1">2c</ref>), while precise-and-broad synthesis would reveal both components and their interconnections (Figure <ref type="figure" target="#fig_1">2d</ref>). This article investigates why precise-and-broad studies remain elusive despite their scientific value, proposing pathways to overcome these limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Clarifications regarding the PBS impossible trinity</head><p>Several clarifications are warranted regarding the PBS impossible trinity. First, while intuitively appealing, the PBS impossible trinity remains challenging to prove rigorously due to the difficulty of quantifying precision, breadth, and simplicity across diverse mental processes. Thus, I present it here as a conjecture.</p><p>Second, the severity of the PBS impossible trinity varies across mental processes, with rare exceptions occurring primarily in low-level vision. For instance, Trichromacy theory achieves all three criteria by using just three components to precisely explain diverse retinal color phenomena (for other cases, see also <ref type="bibr" target="#b0">Adelson &amp; Bergen, 1985;</ref><ref type="bibr" target="#b20">Ernst &amp; Banks, 2002;</ref><ref type="bibr" target="#b43">Li, 2002;</ref><ref type="bibr">Zhaoping &amp; Zhe, 2015)</ref>. These exceptions prove the impossible trinity is not absolute, yet they remain uncommon in psychological research precisely because they involve relatively simple early visual processes -their exceptional status actually reinforces the impossible trinity's general validity by demonstrating how most psychological phenomena, with their greater complexity, inevitably face these fundamental trade-offs. Third, while the PBS impossible trinity may seem self-evident, its implications are profound. For instance, it entails that theoretical understanding is better achieved through data-guided theory-building than through theory-driven hypothesis-testing (the paradox of theorizing below). Furthermore, it implies that comprehensive theory-driven hypothesis-testing is likely futile (See "challenges in large-scale collaboration" below), pointing to the necessity of fundamental paradigm changes in the field.</p><p>Finally, although discussed here in relation to psychology, the PBS impossible trinity likely applies to other behavioral, human, and social sciences. These disciplines may therefore also benefit from the solution proposed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Humans' cognitive limitations in processing complexity</head><p>Why do precise-and-broad studies remain rare despite their scientific value? This likely stems from a fundamental cognitive bias-the humans' cognitive limitations in processing complexity (HCLPC) -which systematically prioritizes easily processed information over more complex but scientifically rigorous alternatives. Empirical work demonstrates that both laypeople and experts consistently favor simpler explanations when evaluating competing theories <ref type="bibr" target="#b46">(Lombrozo, 2007;</ref><ref type="bibr" target="#b24">Gigerenzer &amp; Goldstein, 1996;</ref><ref type="bibr" target="#b27">Heath &amp; Heath, 2007)</ref>, even when more complex alternatives better account for the evidence <ref type="bibr" target="#b15">(Chater &amp; Vitányi, 2003)</ref>. This preference reflects a basic cognitive limitation-humans struggle with complexity and frequently choose simpler but inferior solutions to reduce mental effort <ref type="bibr" target="#b25">(Gilovich et al., 2002;</ref><ref type="bibr" target="#b38">Kahneman, 2011)</ref>.</p><p>The bias toward simplicity manifests across domains. In perception, we prefer simple visual patterns <ref type="bibr" target="#b56">(Reber et al., 2004)</ref>; in communication, simplistic messages outperform nuanced arguments <ref type="bibr" target="#b66">(Westen, 2008;</ref><ref type="bibr" target="#b9">Berger, 2013)</ref>; and in academia, this same preference likely disadvantages complex precise-andbroad research despite its greater scientific value. Faced with limited time and cognitive resources, researchers systematically favor simpler approaches-conducting studies with narrow focus or vague constructs-to avoid the complexity required for precise-and-broad synthesis. This is not a conscious choice against scientific ideals, but rather an inevitable compromise. In an ideal world, comprehensive precise-and-broad studies would dominate. In reality, cognitive constraints and external pressures (e.g., publishability, visibility) create an ecosystem where simpler, less informative research thrives-even when researchers recognize precise-and-broad work as ultimately more valuable.</p><p>Crucially, identifying HCLPC and its consequences is not an indictment of the research community's morals or abilities. Rather, it suggests that pervasive issues like fragmentation and vagueness stem from an innate, irresistible weakness of human cognition-not from practitioners' failures. The solution, therefore, is not to blame researchers but to develop stronger methodological tools that allow us to transcend these limitations. This study presents one candidate for such a tool.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Binary theorizing</head><p>Theory-driven hypothesis-testing is a cornerstone of scientific progress, valued for generating testable predictions that incrementally refine theoretical frameworks. In physics, this approach has yielded remarkable successes-as demonstrated by Poisson's spot, initially conceived as a falsification of wave theory but ultimately becoming decisive evidence for it. One might expect psychological theories to similarly evolve toward precise-and-broad frameworks through either: (1) enriching broadand-simple theories with mechanistic details, or (2) synthesizing precise-and-simple findings into integrative models.</p><p>Yet psychology shows little evidence of this progression, likely due to the aforementioned HCLPC.</p><p>Rather than converging toward comprehensive accounts, theoretical debates often devolve into binary oppositions: nature vs. nurture, early vs. late attentional selection, slot-based vs. resource-based VWM models. Like the blind men debating whether an elephant resembles a pillar or snake (Figure <ref type="figure" target="#fig_1">2e</ref>), these dichotomies oversimplify complex mental phenomena. Their persistence reflects pragmatic realities: binary frameworks are cognitively manageable, easily communicated, and experimentally tractablequalities that promote publication and impact despite often misrepresenting underlying complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A challenge for psychological research</head><p>Figure <ref type="figure" target="#fig_2">3</ref> summarizes the arguments presented so far: the PBS impossible trinity establishes that psychological research cannot simultaneously maximize precision, breadth, and simplicity. Yet the field's fundamental goal-developing comprehensive theoretical understanding of mental processesdemands that we prioritize precise-and-broad approaches despite their inherent complexity. This creates a fundamental tension: the most scientifically valuable work conflicts with HCLPC.</p><p>The challenge, therefore, is to develop a method for precise-and-broad research that reduces complexity-induced burden to a manageable level. While this does not resolve the PBS impossible trinity (which would require precise-and-broad research that is also simple), it represents the best achievable solution.</p><p>The proposed solution adapts successful artificial intelligence (AI) approaches by implementing standardized benchmarking-quantifiable evaluation criteria for complex work. Though complexity remains inherent, systematic benchmarking, as demonstrated in AI research, can significantly mitigate its practical challenges while maintaining scientific rigor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The proposed solution: comprehensive exploration AI revolution</head><p>Psychological mechanisms appear fundamentally more complex than physical systems, creating an inherent PBS impossible trinity. This suggests that psychology's current physics-inspired paradigmwhich prioritizes simple, universal theories-may be fundamentally mismatched to its subject matter.</p><p>Instead, we might look to AI as a more relevant model. AI has achieved remarkable progress in studying the same complex processes that psychology examines (perception, cognition, language), but through a distinctly different, data-driven approach.</p><p>The AI revolution offers an instructive case study. <ref type="bibr">Early AI research (pre-1990s)</ref> resembled much of contemporary psychology-small-scale, theory-driven, and lacking standardized assessment. The field's transformation began with the advent of large benchmark datasets like ImageNet, which enabled standardized assessment of model performance through benchmarking. As <ref type="bibr" target="#b40">LeCun et al. (2015)</ref> noted, the 2012 ImageNet competition-where AlexNet dramatically outperformed alternativesmarked a turning point in the rise of data-driven AI breakthroughs.</p><p>From a PBS impossible trinity perspective, modern AI methods achieve both precision (through quantitative models and metrics) and breadth (via large-scale datasets), while managing the resulting complexity through standardized benchmarking. Benchmarking allows models to be compared fairly using reproducible metrics (e.g., recognition accuracy), reducing reliance on subjective judgment. This creates a virtuous cycle: reliable feedback enables iterative improvement, democratizes innovation, and accelerates progress-all while mitigating complexity-induced burden.</p><p>To summarize, AI's solution to the PBS impossible trinity involves: (1) large datasets ensuring breadth, (2) quantitative modeling providing precision, and (3) benchmarking, which cannot reduce complexity but can alleviate the complexity-induced burden by enabling the evaluation of complex studies through simple, relatively objective metrics.</p><p>Even in physics, historical precedents like Kepler's analysis of Brahe's astronomical data also show how data-guided theory-building can revolutionize fields by replacing subjective theorizing with empirical pattern-finding. Psychology now stands to benefit similarly by adapting these strategies to mental phenomena.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adapting AI for Psychology</head><p>The AI revolution has inspired growing interest in adapting computational methods for psychological research. Pioneering work by <ref type="bibr" target="#b26">Griffiths (2015)</ref> highlighted big data's transformative potential, while <ref type="bibr" target="#b65">Watts (2017)</ref> advocated for more solution-focused approaches in social sciences. <ref type="bibr" target="#b68">Yarkoni and Westfall (2017)</ref> further argued that psychology should prioritize prediction over explanation, mirroring AI's success. Along these lines, a growing body of arguments, proposals, and empirical findings has emerged, advocating for the application of AI elements to advance psychology (e.g., <ref type="bibr" target="#b6">Awad et al., 2018;</ref><ref type="bibr" target="#b37">Jolly &amp; Chang, 2019;</ref><ref type="bibr" target="#b1">Agrawal, Peterson, &amp; Griffiths, 2020;</ref><ref type="bibr" target="#b58">Schrimpf et al., 2020;</ref><ref type="bibr" target="#b53">Peterson et al., 2021;</ref><ref type="bibr" target="#b14">Bryan, Tipton, &amp; Yeager, 2021;</ref><ref type="bibr" target="#b67">Yarkoni, 2022)</ref>.</p><p>The framework introduced in this work advances beyond prior approaches in three key respects:</p><p>1. It is grounded in the first principle PBS impossible trinity conjecture, providing a rigorous theoretical foundation.</p><p>2. It offers a complete and operationalizable methodology, systematically bridging experimental design and model construction.</p><p>3. It is explicitly designed for experimental psychologists, with theoretical understanding as its primary objective. This framework's viability is evidenced by multiple successful implementations, including three published studies <ref type="bibr" target="#b32">(Huang, 2023</ref><ref type="bibr">(Huang, , 2025a</ref><ref type="bibr" target="#b34">(Huang, , 2025b))</ref>, numerous ongoing projects, and an ongoing large-scale collaborative initiative (see "large-scale collaboration" below) -all rigorously following the methodological framework illustrated here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Six methodological dimensions</head><p>To develop an optimal framework for integrating AI approaches into psychology, Figure <ref type="figure" target="#fig_3">4</ref> compares typical AI studies (blue) and traditional experimental psychology studies (red) across six dimensions:</p><p>Dimension 1: Output Format. AI research typically generates quantitative models, whereas psychological studies often yield verbal interpretations of findings<ref type="foot" target="#foot_1">foot_1</ref> . The optimal approach should adopt AI's quantitative modeling practices to maximize precision, but with a critical modification: employing interpretable information-processing models rather than opaque neural networks, thereby leveraging mechanistic explanations to advance theoretical understanding (Dimension 3).</p><p>Dimension 2: Data Scale. Modern AI research leverages large-scale benchmark datasets, whereas psychology traditionally employs smaller, custom-designed experiments. Here, the optimal approach should adopt AI's large-scale paradigm to achieve greater breadth, but with a crucial modificationusing large-scale controlled experiments specifically designed for psychological research questions (Dimension 4).</p><p>Dimension 3: Research Objective. Whereas AI studies typically prioritize applied solutions, experimental psychology emphasizes theoretical understanding-its fundamental scientific purpose (e.g., <ref type="bibr" target="#b10">Bowers et al., 2023)</ref>. The optimal approach should therefore use interpretable information-processing models, whose mechanistic explanations provide theoretical understanding, rather than the opaque neural networks.</p><p>Dimension 4: Data Source. AI research often utilizes large observational datasets, whereas psychology prioritizes controlled experiments. The optimal approach should retain experimental control, as it is essential for isolating causal relationships and minimizing extraneous variables. This methodological rigor remains essential for developing valid theoretical understanding of mental processes.</p><p>Dimension 5: Design of Conditions. AI studies typically employ a stimulus-driven design, sampling from stimulus space (e.g., large datasets of diverse faces), whereas psychology traditionally relies on a theory-driven design, sampling from factor space (e.g., computer-generated faces systematically varying specific factors). As noted in Dimension 3, the optimal approach should be theory-oriented, which might seem to suggest that a theory-driven design should be used for consistency.</p><p>However, a paradox of theorizing arises from the PBS impossible trinity: for gaining theoretical understanding from precise-and-broad studies, stimulus-driven design is better suited than theorydriven design. Such studies inevitably grow complex, and theory-driven designs struggle under this complexity due to HCLPC. The most viable strategy is therefore to shift from a theory-driven to a stimulus-driven design, deferring theoretical commitments until empirical evidence can help reduce the burden of complexity (see "Precision, breadth, and standardized benchmarking" below). Simply put, when key factors are unforeseeable and mechanisms are highly complex, it is often more productive to let the data guide exploration than to impose arbitrary theoretical assumptions in advance <ref type="bibr" target="#b19">(Dubova et al., 2022;</ref><ref type="bibr" target="#b47">Musslick et al., 2023)</ref>.</p><p>Given the paradox of theorizing, stimulus-driven design (i.e., sampling from stimulus space) should be the primary strategy. However, while most uncontrollable factors are best addressed through natural stimulus variations, readily controllable factors should still be explicitly incorporated into the design. This synthesis leads to the optimal approach: sampling from factor-informed stimulus space (See below for details). Dimension 6: Analytical Approach. While psychology traditionally relies on confirmatory analyses (e.g., ANOVA), AI research emphasizes exploratory, iterative model building. The stimulusdriven design of the optimal approach (Dimension 5) deliberately minimizes upfront theoretical specification; consequently, there are no pre-defined hypotheses to test. This necessitates a data-guided theory-building process, using exploratory, iterative model building to elucidate the theoretical implications of the experimental conditions.</p><p>After introducing all six dimensions, Figure <ref type="figure" target="#fig_4">5</ref> summarizes the underlying rationale for these key methodological choices. The upper cyan region (Dimensions 1-2) reflects selections made to achieve precision, breadth, and standardized benchmarking, which will be further detailed below. The lower purple region (Dimensions 3-4) represents choices prioritizing theoretical understanding, while the central yellow region (Dimensions 5-6) comprises decisions enhancing the feasibility of designing experimental conditions, to be expanded below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comprehensive exploration</head><p>In summary, the optimal methodology for adapting AI approaches for psychology leverages key AI elements-including stimulus-driven design, large-scale benchmark datasets, quantitative modeling, and exploratory iterative model-building-to advance psychology's core goal of theoretical understanding.</p><p>This approach, termed comprehensive exploration (CE), employs stimulus-driven design for largescale experimentation, followed by iterative modeling to build interpretable quantitative models whose mechanistic explanations yield theoretical understanding. CE is so named because it diverges from traditional experimental psychology most distinctly in being comprehensive (Dimension 2) and exploratory (Dimension 6).</p><p>On one hand, the CE approach differs dramatically from theory-driven methods in psychology, as it constitutes a form of data-guided theory-building. This shift does not diminish the importance of theory but reframes its role: in CE, theories emerge from and are refined by empirical evidence, serving as the goal of the research process rather than its starting point.</p><p>On the other hand, the CE approach, while data-guided, remains distinct from data-driven AI approaches. In the CE framework, data serves as a GPS, providing guidance while the researcher actively steers toward theoretical understanding. In AI, by contrast, data often acts as a self-driving shuttle bus, delivering outputs automatically without revealing its operational mechanics.</p><p>To formalize these distinctions: traditional theory-driven methods in psychology align with deductive reasoning, while typical data-driven studies in AI reflect inductive reasoning (or inductive pattern-finding). In contrast, the exploratory, iterative model-building central to the CE approach constitutes abductive reasoning-inference to the best explanatory theory based on empirical patterns.</p><p>The typical CE study process, illustrated in Figure <ref type="figure" target="#fig_5">6</ref>, consists of several steps (details will be elaborated below using a recent example: Huang, 2025a):</p><p>Experimental design: Conditions are sampled from a factor-informed stimulus space.</p><p>Data collection: A large-scale controlled experiment is conducted to gather data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model development:</head><p>The experimental data undergo exploratory, iterative model building.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Outcome:</head><p>The above process yields a CE model as the final study output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Precision, breadth, and standardized benchmarking</head><p>Following the successful AI paradigms illustrated above (e.g., ImageNet), the CE approach simultaneously achieves the following:</p><p>1. Precision through quantitative information-processing models, 2. Breadth via large-scale experiments, and 3. Not simplicity-since complexity is inevitable in precise-and-broad research-but a significant reduction in complexity-induced burden through benchmarking.</p><p>In summary, the CE directly addresses the fundamental challenge posed above: methods for precise-and-broad research that reduce complexity-induced burden to a manageable level.</p><p>Several clarifications are needed. First, effective benchmarking requires standardized assessment, which in turn demands both breadth and precision, as demonstrated by foundational psychometric research (e.g., <ref type="bibr" target="#b17">Crocker &amp; Algina, 2006)</ref>. Breadth -comprehensive domain coverage -ensures construct validity, while precision -through reliable measurement -ensures consistency; neither dimension alone suffices for standardized assessment. This principle extends to research evaluation, where broad but vague theories produce non-falsifiable claims even when applied to extensive datasets, while precise yet narrow quantitative models generate non-comparable performance metrics lacking generalizability.</p><p>Consequently, the CE approach necessitates both large-scale experiments (to achieve breadth) and quantitative modeling (to achieve precision) to enable meaningful benchmarking.</p><p>Second, benchmarking mitigates complexity-induced burden by enabling the evaluation of complex studies through simple, relatively objective metrics. This also allows researchers to concentrate exclusively on top-performing models-a common practice in AI research. This contrasts sharply with traditional psychological research, where the absence of standardized benchmarking forces scholars to laboriously track all relevant theoretical claims and methodological details across studies. Consequently, the CE approach may yield a net reduction in researchers' cognitive workload despite employing more complex theoretical models, as the field shifts from evaluating every individual study to evaluating performance against shared benchmarks. Third, while large datasets are commonly associated with breadth and quantitative modeling with precision-relationships we employ for convenient description-these connections are neither automatic nor guaranteed. Poor design compromises dataset breadth, while weak conceptualization undermines model precision. Furthermore, benchmarking reduces but doesn't eliminate subjectivity-researcher biases still influence experimental designs, stimuli, metrics, and interpretations (e.g., <ref type="bibr" target="#b18">Daston &amp; Galison, 2021)</ref>. Thus, rigorous effort remains just as critical in data-guided CE studies as in traditional theorydriven research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Details of the CE approach</head><p>In this section, I will use <ref type="bibr">Huang (2025a)</ref> to illustrate the six methodological dimensions of the CE approach and further justify their current implementation. Although two other published studies <ref type="bibr" target="#b32">(Huang, 2023</ref><ref type="bibr" target="#b34">(Huang, , 2025b) )</ref> and numerous ongoing projects further support these methodological choices, they are omitted here for conciseness of presentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Large-scale controlled experiment</head><p>To achieve comprehensive breadth without compromising experimental rigor, <ref type="bibr">Huang (2025a)</ref> implemented a large-scale controlled experiment examining VWM through 10,000 distinct color patterns, collecting 40 million responses (~33,000 hours of data). This design simultaneously fulfills two key dimensions of the CE approach: Dimension 2 (large-scale data) and Dimension 4 (controlled experimentation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantitative information-processing model</head><p>Huang (2025a) introduced the Quasi-Comprehensive Exploration of VWM (QCE-VWM) model, building on established information-processing models of VWM (e.g., <ref type="bibr" target="#b69">Zhang &amp; Luck, 2008;</ref><ref type="bibr" target="#b8">Bays &amp; Husain, 2008;</ref><ref type="bibr" target="#b63">Van den Berg et al., 2014)</ref>. Information-processing models are formal computational frameworks that simulate how humans encode, transform, and utilize information, serving as a cornerstone of cognitive psychology. The CE models presented here constitute a specialized class of such models-quantitative, mechanistic, interpretable, and fully specified. Their quantitative nature ensures precise specification of mechanisms, while their interpretability enables meaningful theoretical understanding through clear explanation of these mechanisms. As such, these models simultaneously fulfill two key dimensions of the CE approach: Dimension 1 (quantitative modeling) and Dimension 3 (theoretical research).</p><p>Specifically, the QCE-VWM takes as input 10,000 randomly generated color patterns (each consisting of 4 colors, totaling 40,000 items) and predicts response distributions for these items. These predictions are then compared against the results from the aforementioned large-scale experiment.</p><p>Crucially, it balances predictive accuracy with explanatory parsimony, integrating approximately a dozen transparent mechanisms into an integrative framework with just 57 parameters.</p><p>For instance, interactions between items are formalized using a combination of normal and Mexican-hat-like functions. Meanwhile, the quality-quantity trade-off is formalized by establishing a unified ratio that governs the co-variation between a representation's precision (standard deviation) and its prevalence (proportion within a mixture). All twelve mechanisms-along with their interrelationships-are explicitly formalized through such algorithmic implementations. This ensures the model remains interpretable, unlike "black-box" neural networks, while still accounting for the complexity of VWM operations. By maintaining this balance, the QCE-VWM exemplifies the CE approach's ability to reconcile precision 3 and breadth without sacrificing explanatory interpretability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sampling from stimulus space</head><p>As discussed, the paradox of theorizing maintains that stimulus-driven design achieves theoryoriented goals more effectively than theory-driven design in precise-and-broad studies. Huang (2025a)'s design supports this counterintuitive claim.</p><p>The use of 10,000 randomly-generated color patterns in Huang (2025a) represents a deliberate departure from traditional theory-driven design in psychology. Typically, researchers carefully select conditions based on specific hypotheses, sometimes using factorial designs to manipulate two variables simultaneously (or three on rare occasions). While this theory-driven design works well for testing focused questions, it becomes untenable for comprehensive investigations of complex phenomena. The challenge lies in operationalizing numerous interacting factors at once-a task that requires not only precisely defining each factor but also creating stimuli that unambiguously represent their myriad combinations. For example, consider designing a stimulus pattern for a VWM study that must simultaneously satisfy:</p><p>3 By their very nature, these models' algorithms are mathematically precise. Yet this computational precision does not preclude divergent theoretical interpretations. A telling example arose in discussions of <ref type="bibr">Huang's (2025a)</ref> model, where colleagues interpreted the same mechanisms as supporting either "slot-like" or "resource-like" frameworks. This divergence does not reflect a weakness in the modeling approach but rather highlights its strength: the capacity to expose and clarify vagueness inherent in purely verbal theoretical descriptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Chunking (low)</head><p>• Interactions between items (medium)</p><p>• Categorical effects (medium)</p><p>• Regularity (high)</p><p>• Quality-quantity trade-off (high)</p><p>• And five other specific criteria… Now imagine scaling this to 10,000 distinct factor combinations-each requiring peer-defensible implementation. The impossibility speaks for itself<ref type="foot" target="#foot_2">foot_2</ref> . Of course, one may choose to force definitions for these factors, but doing so would likely result in arbitrary, vague, or artificially constrained operationalizations.</p><p>This factor design difficulty is evident in the VWM literature, which includes three-factor designs (e.g., <ref type="bibr" target="#b63">Van den Berg et al., 2014;</ref><ref type="bibr" target="#b48">Oberauer, 2023)</ref> but nothing more complex than that. The absence of higher-dimensional factorial designs underscores the challenge faced by theory-driven designs in precise-and-broad studies. This factor design difficulty led me to adopt sampling from stimulus space-a strategy that minimizes upfront theoretical specification, thereby avoiding the challenges of manually designing high-dimensional factor spaces. This is inspired by successful AI paradigms like ImageNet. Rather than artificially constructing stimuli by pre-defining and combining specific factors relevant to object recognition-an approach that would be unworkable and counterproductive in ImageNet's context-ImageNet leverages natural variations in images to uncover underlying mechanisms. Similarly, in <ref type="bibr">Huang (2025a)</ref>, instead of manually designing stimuli based on predetermined factors, I generated 10,000 color patterns, each comprising four colors randomly selected from a color wheel. This allowed natural stimulus variations to reveal the underlying processes of interest.</p><p>To understand how theoretical questions can be addressed without upfront theoretical specification, consider various factors in VWM (e.g., interactions between items, chunking, and categorical effects).</p><p>Traditional sampling from factor space requires explicit, predefined mappings between experimental conditions and these factors-a demanding and inevitably subjective task. In contrast, <ref type="bibr">Huang (2025a)</ref> required no upfront theoretical specification regarding these factors. Since these effects are all related to how items are distributed on the color wheel, they were expected to emerge organically from the random variations among the 10,000 patterns. This approach allows the precise nature of these mechanisms to be determined from the data itself, offering a simpler strategy.</p><p>In addition to reducing the difficulty of the factor design, sampling from stimulus space offers another key advantage: the exploration of unanticipated factors. As Popper (2014) observed, "Our knowledge can only be finite, while our ignorance must necessarily be infinite." While sampling from factor space inherently restricts investigations to predefined variables, Sampling from stimulus space allows novel and meaningful factors to emerge organically from natural stimulus variation. For example, <ref type="bibr">Huang (2025a)</ref> identified four new mechanisms: concentration and crosstalk (both of which describe how the color categories of items interact with each other), as well as red advantage and red disadvantage (referring to the enhancement of memory for reddish colors in category-related processing and its reduction in a category-unrelated "unbiased component," respectively).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sampling from factor-informed stimulus space</head><p>Having advocated for sampling from stimulus space, it's crucial to clarify that it should still be theoretically constrained. In Huang (2025a), for example, fixing the number of items at four enabled a focused investigation of multi-item memory phenomena while controlling for the known effects of capacity limitations. Conversely, a CE study specifically designed to examine capacity limitations would instead incorporate systematic variation in number of items. In other words, although the upfront theoretical specification is greatly reduced, some minimal essential ones are still required. This leads to what I term sampling from factor-informed stimulus space (Dimension 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exploratory, iterative model building</head><p>Because the CE approach samples from a factor-informed stimulus space-minimizing upfront theoretical specification-the experimental conditions are not derived from pre-defined hypotheses. This makes exploratory, iterative model building (Dimension 6) a methodological necessity to elucidate the theoretical implications of the data, as there are no pre-defined theoretical predictions to test.</p><p>While this process shares methodological similarities with AI's progressive refinement techniques, it retains a crucial psychological distinction: each iteration of this data-guided theory-building process explicitly tests theoretical hypotheses through rigorous model comparison. For example, <ref type="bibr">Huang (2025a)</ref> used effect-size-based criteria (CAD<ref type="foot" target="#foot_3">foot_3</ref> ) to evaluate model improvements.</p><p>For example, a typical iteration involves refining the "interactions between items" mechanism in the QCE-VWM model. Initially, this mechanism was placed after categorical encoding, reflecting the theoretical view that color interactions depend on category membership (e.g., red, green, blue).</p><p>However, model comparison revealed significantly better performance when interactions were positioned at a pre-categorical stage, suggesting they operate on early continuous color values rather than later discrete categorical representations. This finding not only revised our understanding of VWM organization but also exemplified how data-guided theory-building can productively challenge theoretical assumptions. Through approximately 1,000 such model comparisons, the development process addressed an equivalent number of fine-grained theoretical questions-a scale of hypothesistesting impossible in traditional confirmatory frameworks. These theoretical hypotheses originate from two complementary sources (Figure <ref type="figure" target="#fig_5">6</ref>): established findings in the literature and empirical patterns emerging from the data itself.</p><p>This exploratory, iterative model building differs fundamentally from conventional practices in experimental psychology, warranting several important clarifications:</p><p>First, this method must be distinguished from the problematic practice of HARKing (Hypothesizing After Results are Known). While both involve post hoc analysis, they differ critically in transparency and intent. HARKing misleadingly presents exploratory findings as confirmatory, whereas the CE explicitly acknowledges its exploratory nature and avoids overinterpretation. The key distinction lies in their treatment of uncertainty: where HARKing obscures it, the CE approach makes it explicit.</p><p>Second, while multiple hypothesis-testing does raise legitimate concerns about false positives in traditional small-scale research, the CE approach mitigates this risk through two key features: (1) enormous data scale (e.g., 40 million responses in Huang, 2025a), and (2) strict statistical criteria (e.g., the effect-size-based index CAD in <ref type="bibr">Huang, 2025a)</ref>. This combination resulted in extraordinarily low false positive rates (e.g., &lt; 1.E-87 in Huang 2025a), rendering the multiple testing problem negligible despite evaluating ~1,000 models.</p><p>Third, exhaustive model testing proves impractical given the combinatorial explosion of possibilities. While <ref type="bibr" target="#b63">Van den Berg et al. (2014)</ref> tested 32 models-already ambitious by traditional standards-the QCE-VWM's parameter space contains 2 57 = ~1.4E+17 possible configurations <ref type="foot" target="#foot_4">6</ref> .</p><p>Exploratory, iterative model building thus represents the only feasible option at this scale.</p><p>Fourth, the CE approach explicitly embraces the provisional nature of its models. Like the iterative scientific process described by <ref type="bibr" target="#b54">Popper (2005)</ref>, initial models may contain inaccuracies that subsequent refinements gradually correct (Figure <ref type="figure" target="#fig_1">2d</ref>). Should future work surpass QCE-VWM's performance, this would demonstrate the CE approach's strength-mirroring AI's progressive improvement paradigmrather than indicating failure 7 .</p><p>Fifth, iterative exploration outperforms predefined testing by accommodating a wider range of potential solutions. As shown in Figure <ref type="figure">7</ref> Sixth, the conventional preference for exhaustive testing may stem less from methodological necessity than from a well-documented cognitive bias. Research on the paradox of choice <ref type="bibr" target="#b36">(Iyengar &amp; Lepper, 2000;</ref><ref type="bibr" target="#b59">Schwartz et al., 2002)</ref> demonstrates that decision-makers often favor limited but complete option sets over more expansive but necessarily incomplete ones -even when the latter objectively produces better outcomes. This preference may stem from HCLPC, leading individuals to prioritize ease of evaluation over optimal results. In academia, this manifests as an irrational preference for small, exhaustively testable model spaces despite their limited explanatory scope, while potentially more 7 A breakthrough-such as unifying the QCE-VWM's dozen mechanisms under a single principle-could challenge the CE approach's foundational assumption of irreducible complexity. Should such unification occur, contemporary complex models would nevertheless serve as necessary precursors to this future theoretical synthesis. Their role would be analogous to that of Ptolemy's system of epicycles: though ultimately incorrect, it provided the essential scaffolding for Kepler's elegant laws of planetary motion. The enduring lesson from Ptolemy is not final accuracy, but the value of systematic engagement with complexity. His model demonstrates that embracing intricacy is often a necessary path to ultimate simplicity.</p><p>productive but incompletely testable approaches are avoided due to the discomfort of unexplored alternatives. The CE approach challenges this bias by recognizing that in complex domains, iterative exploration of large solution spaces -while necessarily incomplete -yields superior theoretical and empirical outcomes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theory-driven confirmation vs. data-guided exploration</head><p>Theory-driven confirmatory analysis is strongly favored over data-guided exploratory analysis in experimental psychology. For instance, Pashler and colleagues, in their analysis of the replication crisis, advocated for solutions like pre-registration and strict adherence to pre-specified plans to ensure studies are truly confirmatory <ref type="bibr" target="#b51">(Pashler &amp; Harris, 2012;</ref><ref type="bibr" target="#b52">Pashler &amp; Wagenmakers, 2012)</ref>.</p><p>At first glance, the CE approach may appear to conflict with this preference for confirmation, given its explicitly exploratory nature. However, this approach can be viewed as an evolution of these methodological reforms-both are driven by the core principle of optimizing scientific methodology. A key insight is that methodological optimality is scale-dependent. Issues that are critical in small-scale studies may become negligible in large-scale research, and vice versa.</p><p>Figure <ref type="figure">8</ref> summarizes four key factors governing the choice between theory-driven confirmation and data-guided exploration, highlighting the divergent trade-offs for small-versus large-scale studies: Theory-driven confirmation faces two primary challenges:</p><p>1. Lack of exploration: As discussed above, data-guided exploration allows us to explore unanticipated factors-an advantage lacking in theory-driven confirmation. While this constraint is harmless in small-scale studies with limited data, it represents a significant opportunity cost in largescale research, where an abundance of data could allow for more comprehensive exploration.</p><p>2. Factor design difficulty: As discussed above, small studies can feasibly manipulate one or two factors, but large-scale confirmatory research struggles with the impracticality of manipulating numerous factors.</p><p>Data-guided exploration also faces two principal challenges:</p><p>1. False positive risk: Exploratory analysis traditionally raises concerns about inflated false positive rates due to multiple comparisons. However, as discussed above, in large-scale studies, massive datasets can support the application of exceedingly stringent statistical criteria (e.g., Huang, 2025a's false positive rates of &lt; 1.E-87), effectively mitigating this risk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Inefficient testing:</head><p>Exploratory designs like the CE approach use randomized sampling of stimulus space, which can allocate data points to uninformative or redundant conditions. This inefficiency poses a critical liability in small-scale studies, where data is limited and statistical power is precious. At large scales, however, a certain portion of wasted data becomes easily affordable, as the benefit of broad, unsupervised coverage outweighs the cost.</p><p>The shift from small-to large-scale research fundamentally transforms the cost-benefit landscape:</p><p>• The limitations of theory-driven confirmatory analysis (lack of exploration &amp; factor design difficulty) become severely amplified.</p><p>• The drawbacks of data-guided exploratory approaches (false positive risk &amp; inefficient testing) are greatly reduced. Thus, while theory-driven confirmation remains superior for small-scale experiments, data-guided exploration emerges as the more suitable strategy for large-scale investigations. This methodological shift mirrors the dominant paradigm in AI research, where data-driven exploration prevails-a choice necessitated by the field's reliance on large-scale datasets. This scaledependent perspective may also explain why such data-intensive fields have largely avoided the replication crises that have plagued psychology, despite being far more exploratory in nature.</p><p>Finally, Figure <ref type="figure">9</ref> uses a firearms analogy to illustrate three of the four factors discussed above.</p><p>Theory-driven confirmation in small-scale studies is like a sniper rifle-using a single bullet for precise, narrow targeting. In contrast, data-guided exploration in large-scale studies operates like a shotgun blast, scattering many pellets to achieve broad, comprehensive coverage.</p><p>For the sniper, the cost of precise control (analogous to factor design difficulty) is low, and the inability to hit unexpected targets (analogous to a lack of exploration) is acceptable. Conversely, the cost of a missed shot (analogous to inefficient testing) is high, making strict, precise control essential. This situation reverses for the shotgun. Controlling the trajectory of every pellet is prohibitively difficult, making broad coverage optimal. Here, "missing" with many pellets is inconsequential, while hitting unexpected targets becomes a significant advantage. Ultimately, applying the sniper's precise control to the shotgun's domain-like using theory-driven confirmation in large-scale studies-is both difficult and counterproductive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>An optimal balance between prediction and explanation</head><p>Cognitive science is defined by a fundamental trade-off between predictive accuracy and explanatory interpretability. While powerful deep neural networks achieve high accuracy on complex tasks like face recognition, they function as "black boxes" that offer little theoretical understanding <ref type="bibr" target="#b10">(Bowers et al., 2023)</ref>. Conversely, interpretable psychological models (e.g., featural vs. configural theories of face processing) provide clear explanations but lack robust predictive power <ref type="bibr" target="#b68">(Yarkoni &amp; Westfall, 2017)</ref>. Consequently, the field remains divided, lacking a unified framework that is both fully predictive and fully explanatory.</p><p>The CE approach aims to resolve this dilemma through comprehensive yet interpretable modeling.</p><p>The QCE-VWM model exemplifies this solution. With only 57 parameters-compared to 30,796 in a benchmark neural network-it achieves superior predictive accuracy while retaining full mechanistic interpretability (Figure <ref type="figure">7</ref>). This represents an optimal balance between prediction and explanation<ref type="foot" target="#foot_5">foot_5</ref> . Although this success is largely credited to two decades of VWM research that defined the core puzzles, it also proves the value of the CE approach in synthesizing those puzzles into a coherent, complete model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The utility of exploratory, iterative model building</head><p>The optimal balance achieved in <ref type="bibr">Huang (2025a)</ref> depends not only on large-scale experiments and quantitative models but also-critically-on exploratory, iterative model building. To appreciate this, consider how alternative analytical approaches would have performed if applied to the same data.</p><p>The first alternative is traditional confirmatory analysis. As noted above, simultaneously prespecifying numerous factors is exceptionally challenging. Even if achieved, applying a traditional confirmatory analysis (e.g., a multifactor ANOVA) would fragment QCE-VWM's integrated architecture into statistically significant but theoretically disconnected effects. Such an approach would reduce complex mechanistic relations to simplistic interaction terms, failing to address core questions like whether chunking operates on pre-categorical or categorical information. Moreover, its rigid operationalizations would not adapt to patterns emerging from the data. Consequently, while interpretable, this approach would likely yield inferior predictive accuracy. This limitation further supports the paradox of theorizing: rigid, pre-specified research plans can stifle discovery by preventing researchers from detecting unexpected yet meaningful patterns <ref type="bibr" target="#b19">(Dubova et al., 2022;</ref><ref type="bibr" target="#b47">Musslick et al., 2023)</ref>. This suggests that, when facing a large dataset, theoretical understanding may be more effectively achieved by carefully examining the data itself rather than following only pre-determined, theory-driven plans.</p><p>The second alternative is predictive modeling using neural networks. While such models excel in predictive accuracy, they function as opaque black boxes, offering minimal explanatory interpretability. This is not to say neural networks lack value; following the scientific regret minimization framework developed by <ref type="bibr" target="#b1">Agrawal, Peterson, &amp; Griffiths (2020)</ref>, Huang (2025a) employed a "guidance neural network" to facilitate the development of the QCE-VWM model. However, for psychological science, they should serve as intermediate tools rather than final products, as they cannot provide the mechanistic understanding that defines explanatory progress.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Large-scale collaboration</head><p>The need for large-scale collaboration While Huang (2025a) represents a significant advancement in scale over traditional studiesyielding several hundred times more data-its scope was deliberately restricted to how color variations affect VWM for four items presented simultaneously at fixed locations. This design intentionally excluded extraneous factors such as number of items, temporal order, and stimulus dimension (e.g., color versus object identity). Consequently, a critical future direction for the CE approach is to conduct a more comprehensive investigation of VWM that incorporate these and other dimensions of stimulus variation.</p><p>Such an ambitious undertaking demands large-scale collaboration across the field for two reasons.</p><p>First, the project's unprecedented scope-far exceeding Huang (2025a) in both scale and complexitynecessitates shared resources across multiple laboratories to ensure feasible data collection. Second, and more fundamentally, this collaborative effort could establish a standardized benchmark dataset for the VWM research community-a psychological equivalent to ImageNet in computer vision. Achieving this vision requires broad participation to ensure the resulting dataset reflects field-wide consensus while serving as a definitive resource for theoretical advancement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Challenges in large-scale collaboration</head><p>Despite its potential benefits, large-scale collaboration presents significant challenges. The Cogitate project <ref type="bibr" target="#b16">(Cogitate et al., 2025)</ref>, a major adversarial collaboration in consciousness research, illustrates these difficulties. Despite involvement from leading theorists, the project made limited progress <ref type="bibr" target="#b41">(Lenharo, 2024)</ref>, with critics noting it tested only "idiosyncratic predictions" and "hand-picked auxiliary components already known to be true" rather than core theoretical claims <ref type="bibr" target="#b21">(Fleming et al., 2023)</ref>.</p><p>These difficulties stem from the fundamental constraints of the PBS impossible trinity. The HCLPC makes developing precise-and-broad frameworks challenging even in individual work-a challenge magnified in collaborative settings by self-serving biases. Researchers naturally avoid tests threatening their theoretical positions 9 , resulting in designs that focus on safe, known findings rather than critical tests of competing theories<ref type="foot" target="#foot_6">foot_6</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Facilitating collaboration through the CE approach</head><p>9 There have been instances of progress, such as the collaboration on intuitive expertise <ref type="bibr" target="#b39">(Kahneman &amp; Klein, 2009)</ref>.</p><p>However, these examples are exceptions rather than the norm. Furthermore, the success of these collaborations often relies on both large datasets and quantitative modeling-two key components of the CE approach.</p><p>The CE approach offers a solution to these collaborative challenges. Like adversarial collaborations, a collaborative CE project brings together researchers with diverse perspectives. However, by sampling from stimulus space, it minimizes upfront theoretical specification-collaborators need only agree on relevant stimulus variations, not their theoretical interpretations. This theory-neutral foundation reduces conflict by shifting focus from defending positions to shared empirical exploration. For instance, it's unlikely that any reasonably open-minded researcher would view Huang's (2025a) design (i.e., 10,000 randomly-generated color patterns) as a direct threat to their theoretical positions.</p><p>Following the ImageNet model, a collaborative CE project would conclude with an open benchmark dataset, eliminating the need for consensus on theoretical interpretation-a persistent hurdle in traditional adversarial collaborations. Individual researchers could then independently develop and test their models against this standardized benchmark, competing on a more objective basis, as will be explained in "Benchmarking as the engine of change" below.</p><p>By reducing these demands-and leveraging the CE approach's inherent resistance to oversimplification-the CE approach enables more viable large-scale cooperation than theory-driven alternatives.</p><p>Guided by this design, a large-scale collaborative CE project on VWM is currently underway, coled by Klaus Oberauer and me. While this project specifically extends <ref type="bibr">Huang (2025a)</ref> to study VWM, the advantages of the collaborative CE approach are not domain-specific and can be generalized to other fields of <ref type="bibr">research. al., 2021)</ref>. Ultimately, the CE approach builds upon the intellectual tradition that led to IED, integrating its key insights as core components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theory-driven method vs data-guided theory building</head><p>While the CE inherited important insights from this tradition, it also introduced critical modifications.</p><p>A primary dimension on which IED and CE differ is their positioning along the theory-driven vs data-guided spectrum. In essence, IED maintains that researchers should use the best available theoretical understanding to structure experimental testing from the outset, thereby ensuring comparability and coherence across studies. In contrast, CE argues that existing theories are usually insufficient to guide such structuring and instead advocates for data-guided theory-building to uncover the underlying structure of phenomena, which can subsequently inform stronger, more robust theories.</p><p>Specifically, the IED approach emphasizes the construction of a well-defined "explicit design space" to guide experimental testing and theoretical development. This approach demands even greater upfront theoretical specification than traditional theory-driven methods, placing the formalization of theory at the forefront. While IED acknowledges the importance of data-guided theory-building and suggests that the explicit design space should be updated when data suggest error or inadequacy, such data-guided adjustments remain secondary within this theory-first framework and do not eliminate the challenge of upfront theoretical specification. After all, to be practically useful for the research community, the explicit design space needs to be stable enough to accommodate ongoing studies and is therefore only updated periodically.</p><p>In contrast, CE advocates for a decisive shift toward data-guided theory-building, arguing that excessive upfront theoretical specification is a fundamental limitation of traditional experimental psychology. Therefore, as stated above, CE proposes that upfront theoretical specification should be minimized, and theories should emerge organically through iterative, exploratory model-building.</p><p>Although CE incorporates some theoretical constraints (i.e., sampling from factor-informed stimulus spaces), these are subordinate to the overarching goal of data-guided theory-building.</p><p>For example, Huang (2025a)'s only upfront theoretical constraint is that the number of items is fixed at four, making it a predominantly data-guided approach. In contrast, using the IED to study VWM would require a mainly theory-driven approach. This would involve extensive upfront theoretical specifications such as designing the conditions for "chunking", "interactions between items", "qualityquantity trade-off", and many other factors, as well as their fully factorial combinations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Iterative experimentation vs. iterative modeling</head><p>Another key distinction between IED and CE lies in their approaches to data accumulation. <ref type="bibr" target="#b2">Almaatouq et al. (2024)</ref> describe IED as requiring an iterative cycle: first constructing an explicit design space, then progressively generating theories and testing them through experiments systematically sampled from this space. In contrast, CE employs a single comprehensive data collection phase, with subsequent iterations focused exclusively on model refinement without additional experimentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feasibility of IED and CE</head><p>Having delineated the key distinctions between CE and IED, it is worth noting that while IED offers a strong theoretical framework for ideal scenarios, its practical implementation can be challenging. In contrast, the CE approach provides a more readily applicable methodological path forward.</p><p>CE's emphasis on data-guided theory-building is grounded in the PBS impossible trinity conjecture, which highlights the inherent complexity of precise-and-broad studies and the difficulty of managing this complexity prior to data collection due to HCLPC. Consequently, the explicit design space proposed by IED-though conceptually valuable-faces practical hurdles, as it requires a degree of upfront theoretical specification that may be difficult to achieve in early stages of research. This challenge is compounded when attempting to coordinate such specifications across diverse research teams. In this context, CE's stimulus-driven design offers a more adaptable and feasible alternative.</p><p>Additionally, IED's call for iterative experimentation may present greater logistical demands compared to CE's model-focused iteration within a single comprehensive dataset. For example, Huang (2025a) evaluated approximately 1,000 theoretical hypotheses through model comparisons using one dataset-a scale of inquiry that would be difficult to support if each iteration required new experimental data.</p><p>These expectations are reflected in current research outcomes. On one hand, CE has been successfully implemented across multiple published studies <ref type="bibr" target="#b32">(Huang, 2023</ref><ref type="bibr">(Huang, , 2025a</ref><ref type="bibr" target="#b34">(Huang, , 2025b) )</ref> and is currently being applied in several ongoing projects and the above-mentioned large-scale collaborative CE project . On the other hand, while influential, no published study has yet fully implemented the complete IED framework as originally described. <ref type="bibr" target="#b2">Almaatouq et al. (2024)</ref> cite <ref type="bibr" target="#b53">Peterson et al. (2021)</ref> and <ref type="bibr" target="#b6">Awad et al. (2018)</ref> as examples of work aligned with IED principles-and indeed, these studies represent significant advances in scale and systematicity. However, they did not centrally organize around a pre-specified high-dimensional explicit design space, nor did they adopt IED's iterative experimentation protocol. Thus, while they valuably demonstrate the power of large-scale research, they</p><p>do not yet establish the feasibility of IED's full methodological vision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IED and CE: Complementary Futures</head><p>In summary, IED provides a valuable blueprint for addressing foundational challenges in psychological research, rightly emphasizing the importance of large-scale experimentation and integrative analysis. That said, certain features of its proposal-particularly its reliance on extensive upfront theoretical specification and iterative experimentation-may limit practical implementation in many current research contexts.</p><p>The CE approach can be seen as building upon IED's insights while offering a more accessible entry point. It retains IED's core strengths-large-scale data collection and systematic analysis-while shifting emphasis from theoretical pre-specification to data-guided theory-building. This adjustment enhances feasibility, especially in domains where theoretical understanding is still evolving.</p><p>It should be emphasized that IED remains a compelling framework for contexts where strong theoretical consensus exists. In such settings, its structured approach could efficiently guide experimental design. As fields mature through the accumulation of empirical results-for instance, via CE-style research-the more demanding IED framework may become increasingly applicable. For now, however, CE offers a practical and powerful pathway for exploring complex, under-specified domains without requiring premature theoretical convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Advantages of the CE approach</head><p>Within the PBS impossible trinity framework, the CE approach's primary advantage is successfully combining breadth and precision -two dimensions traditionally in tension in psychological research.</p><p>Where conventional studies yield either precise-but-fragmented findings or broad-but-vague theories, CE creates precise linkages between diverse phenomena to form cohesive theoretical frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Breadth-related advantages</head><p>The CE approach's comprehensive scope offers three key advantages for theoretical progress:</p><p>First, it provides an integrative, system-level perspective. Traditional studies often examine cognitive mechanisms in isolation, offering fragmented insights akin to possessing scattered puzzle pieces without the picture on the box (Figure <ref type="figure" target="#fig_1">2b</ref>). The CE approach, in contrast, assembles this picture (Figure <ref type="figure" target="#fig_1">2d</ref>). For instance, while prior research has identified numerous elements of VWM, the QCE-VWM model is the first to offer a direct and comprehensive answer to the broad question: "How are these items memorized?" Crucially, although its constituent mechanisms were largely known, the model's novel contribution lies in its integrated computational architecture, which fully specifies the relationships between these mechanisms. The value of a complete puzzle exceeds that of its scattered pieces; this added value resides in understanding how the pieces connect to form a coherent whole.</p><p>Second, it transcends binary theoretical debates. Traditional approaches often force artificial dichotomies (e.g., slot vs. resource models), whereas CE models synthetically incorporate elements from competing theories. The QCE-VWM model exemplifies this by demonstrating how "slot-like" features (e.g., representation precisions that are largely stable against external factors) and "resource-like" features (e.g., quality-quantity trade-offs) coexist within a single architecture. This offers a more nuanced account than either extreme position. Indeed, within the QCE-VWM framework, neither feature serves as the sole guiding principle. Thus, to achieve a precise and broad understanding of VWM, it is necessary to move beyond these debates and embrace a more complex, integrative truth.</p><p>Third, it serves as a quantified literature review. This approach combines the systematic coverage of traditional reviews with the empirical rigor of experimental studies (Figure <ref type="figure">10</ref>). Huang (2025a) exemplifies this by formally integrating decades of VWM findings into a unified computational framework, thereby validating and refining prior insights through data-guided theory-building.</p><p>Crucially, this method offers unique advantages over conventional reviews: model fitting provides a quantifiable tool for identifying missing or redundant mechanisms. Patterns of poor fit reveal theoretical gaps (e.g., undiscovered processes), while a lack of fit improvement indicates when proposed mechanisms are unnecessary-capabilities beyond the reach of traditional narrative reviews.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Precision-related advantages</head><p>While traditional experimental studies are already very precise, CE can enhance the precision even further by utilizing larger datasets and broader coverage of the stimulus space, allowing us to make finer mechanistic distinctions. For example, Huang (2025a) revealed that:</p><p>• Bayesian integration follows multiplicative rather than additive rules</p><p>• Memory errors follow a truncated normal rather than von Mises distribution These subtle but theoretically important distinctions often elude smaller-scale studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Complexity of the CE approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Complexity is theoretically necessary</head><p>The CE approach's ability to achieve both precision and breadth within the PBS impossible trinity inevitably requires embracing theoretical complexity. Huang (2025a)'s QCE-VWM model exemplifies this necessity, incorporating approximately a dozen distinct mechanisms through 57 parameters while generating 20 theoretical implications about VWM (Table <ref type="table">1</ref>, <ref type="table">Huang 2025a</ref>). This complexity should not be viewed as a methodological flaw, but rather as an epistemically necessary response to the inherent complexity of cognitive systems. Traditional literature reviews face similar challenges when synthesizing diverse findings, suggesting that CE's complexity simply makes explicit what was always implicitly present in comprehensive theoretical accounts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Complexity is data-justified</head><p>The line between scientifically justified simplicity and unjustified simplicity (driven by HCLPC or other biases) must be drawn through evidence-based, statistically rigorous criteria. As demonstrated in previous studies (e.g., <ref type="bibr" target="#b53">Peterson et al., 2021)</ref>, the optimal complexity of a model scales with data size (see also Figure <ref type="figure">11</ref>). This justifies the moderate complexity of CE models like the 57-parameter QCE-VWM, which is calibrated to large-scale datasets (40 million responses). Nevertheless, researchers accustomed to small-scale studies may intuitively perceive such models as inherently problematic-a bias stemming from misgeneralized heuristics and HCLPC rather than scientific rationale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>General discussions A personal journey with theory-driven hypothesis-testing</head><p>While incorporating personal experiences into scientific publications is uncommon, I share my own here to clarify the perspectives that shaped this article. My twelve-year effort <ref type="bibr">(2007)</ref><ref type="bibr">(2008)</ref><ref type="bibr">(2009)</ref><ref type="bibr">(2010)</ref><ref type="bibr">(2011)</ref><ref type="bibr">(2012)</ref><ref type="bibr">(2013)</ref><ref type="bibr">(2014)</ref><ref type="bibr">(2015)</ref><ref type="bibr">(2016)</ref><ref type="bibr">(2017)</ref><ref type="bibr">(2018)</ref><ref type="bibr">(2019)</ref> to develop the Boolean map theory <ref type="bibr" target="#b35">(Huang &amp; Pashler, 2007)</ref> into a comprehensive framework of visual attention proved unexpectedly illuminating through its failures. This odyssey through three distinct phases fundamentally reshaped my understanding of psychological research methods:</p><p>Phase 1: Initially, I aimed to develop a theory through traditional hypothesis-testing, striving for an ideal balance of precision, breadth, and simplicity-unaware of the constraints imposed by the PBS impossible trinity. However, years of intensive research led me to conclude this goal may be fundamentally unattainable. A persistent pattern emerged: attempts to simultaneously increase both precision and breadth inevitably resulted in greater complexity, as additional factors and mechanisms came to light. This impasse forced a difficult realization: the cognitive phenomena I studied resisted elegant simplification.</p><p>Phase 2: Upon recognizing that precision, breadth, and simplicity cannot be optimized simultaneously, I shifted toward precise-and-broad (yet complex) frameworks, still relying on one-at-atime hypothesis-testing. However, progress remained limited. A fundamental barrier was the lack of standardization across experiments, which rendered their results incomparable and thereby necessitated a more integrative and systematic approach.</p><p>Phase 3: I adopted a broader, more integrative approach while remaining theory-driven-an approach similar to Almaatouq et al. ( <ref type="formula">2024</ref>)'s IED. For instance, building on Huang (2015a)'s findings (2 stimulus types × 3 tasks), I subsequently attempted to standardize attentional processing across a broader range: first to 16 stimulus types × 8 tasks <ref type="bibr">(Huang, 2015b)</ref>, and ultimately to 16 stimulus types × 26 tasks <ref type="bibr" target="#b31">(Huang, 2022)</ref>  12 . While some progress was made within this broad scope, more ambitious attempts consistently failed, compelling a radical methodological shift.</p><p>Three key limitations emerged: 12 Believing it self-evident that broader scope enhanced the work's quality and value, I was shocked when a colleagueinitially enthusiastic about <ref type="bibr">Huang's (2015a)</ref> focused finding-dismissed the far more comprehensive Huang (2022) as less interesting, overly complex, and lacking an elegant message. This prompted introspection, during which I realized I often react the same way to others' work. This realization led me to believe there is something fundamentally flawed about the socalled "theory-driven" approach. In reality, it is a "simplicity-driven" approach that penalizes rather than rewards the effort toward comprehensive theoretical understanding.</p><p>1. Data constraints: The largest-scale laboratory projects I can conduct (e.g., Huang 2022) yield only a few thousand hours of data-far too limited for broader ambitions.</p><p>2. Theoretical precision: Verbal theories proved inadequate for specifying complex cognitive mechanisms, necessitating quantitative information-processing models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Complexity in design:</head><p>Upfront theoretical specification became intractable when handling &gt;5 factors, revealing the need for data-guided theory-building.</p><p>These insights-coupled with the empirical successes of AI-motivated the development of the CE approach, which was subsequently validated in <ref type="bibr" target="#b32">Huang (2023</ref><ref type="bibr">Huang ( , 2025a</ref><ref type="bibr" target="#b34">Huang ( , 2025b))</ref>. Ironically, when applied to VWM in <ref type="bibr">Huang (2025a)</ref>, the QCE-VWM model rejected my initial Boolean map hypothesis. This outcome underscores the reasonable objectivity of the CE approach, demonstrating its ability to mitigate the theoretical confirmation biases inherent in traditional approaches.</p><p>inaccessible. The introduction of computers enabled studies with tens of thousands of observations, a hundredfold increase that empowered researchers to probe cognitive mechanisms, thereby catalyzing the cognitive revolution.</p><p>Today, psychology may be on the verge of another transformative shift. The internet now facilitates studies with tens of millions of observations-a further thousandfold leap-opening unprecedented opportunities for methodological innovation. Yet, data volume alone is not sufficient to drive a paradigm shift. The critical question is: how can this vast new data landscape be harnessed to propel psychological research into its next era? The answer may lie in a practice that has already revolutionized other dataintensive fields: benchmarking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Benchmarking as the engine of change</head><p>Benchmarking provides the essential link between large-scale data and paradigmatic change. As detailed above, benchmarking alleviates complexity-induced burdens. More broadly, benchmarkingpowered by standardized assessment-has the potential to fundamentally reorient academic evaluation.</p><p>By shifting the focus from impression management to the actual content being assessed <ref type="bibr" target="#b42">(Levashina &amp; Campion, 2007)</ref>, it creates a system that rewards substantive, long-term contributions over superficially impressive but shallow outputs.</p><p>The transformative power of standardized benchmarking is irrefutably demonstrated in AI, where it has catalyzed rapid progress by aligning professional incentives with reproducible achievements rather than subjective impressions. Psychology stands poised for a similar revolution. The ongoing large-scale than just provide data; it would establish a new arena for scientific progress, replacing cyclical and often inconclusive debates-such as those between slot-based and resource-based models-with rigorous, evidence-based competition. The goal would shift from winning rhetorical arguments to engineering superior models that achieve an optimal, quantifiable balance between prediction and explanation on a shared, standardized stage.</p><p>Beyond any single project, the widespread adoption of benchmarking could transform research workflows by making standardized evaluation a central organizing principle 13 . Researchers could shift from perpetually designing isolated small-scale experiments to iteratively refining models against shared benchmark datasets, redirecting energy from generating fragmented findings toward a cumulative, collaborative enterprise of theory-building.</p><p>Furthermore, benchmarking presents a powerful systemic solution to the replication crisis by mitigating two of its core drivers: it eliminates selective reporting (as all researchers are evaluated on the same data) and drastically reduces analytical flexibility (through standardized metrics).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Naturalistic tasks</head><p>13 While benchmarking provides a common foundation for communication, an over-reliance on specific benchmarks can stifle methodological diversity and narrow theoretical perspectives. Therefore, it should not be applied so rigidly that it limits intellectual creativity.</p><p>The CE approach has demonstrated its ability to achieve an optimal balance between prediction and explanation, as shown in Huang (2025a) using simplified artificial stimuli. An important next step involves extending this success to more complex, naturalistic tasks.</p><p>Our lab is currently pursuing two projects in this direction. The first examines aesthetic preferences by collecting ratings for 190,000 abstract artworks, including pieces by established artists, amateur creators, and AI systems. The second project analyzes poetry appreciation through ratings of 95,398 poems spanning various quality levels, from renowned poets to AI-generated verses. Both studies aim to develop interpretable mechanistic CE models that explain and predict human judgments in these domains.</p><p>These naturalistic tasks present greater challenges compared to studies using simplified artificial stimuli. The resulting models will likely be more complex and less predictively accurate than that in Huang (2025a). However, they offer significant advantages. Theoretically, they may open up entirely new research domains instead of optimizing existing ones. Practically, they demonstrate psychology's ability to address real-world problems -a crucial consideration as the field faces questions about its relevance in competition with AI approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Observational data</head><p>As discussed above, controlled experiments are preferred within the CE approach due to their ability to isolate causal relationships and minimize the influence of extraneous variables. Without such controls, observational data often reflects more complex and entangled underlying mechanisms, which can hinder the development of accurate and interpretable CE models.</p><p>That said, the CE approach is not fundamentally incompatible with observational data. In cases where ethical or practical constraints make experimental manipulation infeasible, the methodology of CE (i.e., data-guided theory-building) may still be applied. If the underlying mechanisms can be reasonably hypothesized or subsequently inferred, this approach remains capable of generating valuable theoretical understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sampling strategies</head><p>Sampling from a factor-informed stimulus space is a general principle that must be adapted to different research contexts.</p><p>First, Huang (2025a) illustrates a scenario with focused, artificial stimuli. Here, only a single constraint is imposed (i.e., number of items = 4) to isolate the phenomena of interest (i.e., multi-item memory). The relevant mechanisms are then captured through random variations within that constrained space.</p><p>Second, implementation is often more straightforward for naturalistic tasks. As with datasets like ImageNet, these require only a diverse and representative set of real-world examples (e.g., visual artworks or poems, as described above).</p><p>Third, more ambitious projects, whether using artificial or naturalistic stimuli, require a more complex stimulus space. For example, the ongoing large-scale collaborative CE project varies stimuli along several dimensions (e.g., number of items, color, object identity, temporal order). The design is guided by the principle of minimal sufficient constraints: the goal is to identify the simplest possible stimulus space that can accommodate the phenomena of interest. Stimuli are randomly generated from this space, ensuring coverage of both known experimental conditions and novel, untested regions that may lead to discovery.</p><p>Like traditional design principles, the "minimal sufficient constraints" principle does not prescribe a single correct solution. Just as classic methods require domain knowledge to operationalize variables, constructing an optimal CE stimulus space requires leveraging existing theoretical understanding and, ideally, the collective wisdom of the research community. The key advantage is that a CE design, while sometimes challenging, is invariably more feasible than a theory-driven design of comparable scope, as it focuses on manipulating stimulus dimensions rather than the far more complex task of pre-defining and controlling high-dimensional factor interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feasibility</head><p>While CE studies require larger datasets than traditional psychology research, their associated costs remain manageable through online data collection. Huang (2025a) demonstrated this by obtaining a high-quality dataset of approximately 33,000 participant hours for about $26,000 on a custom-built platform. Subsequent analyses showed that even one-third of this data volume yields reliable parameter estimates, making individual CE projects viable for most researchers.</p><p>For more ambitious, community-wide projects-such as the collaborative benchmark study described above-costs scale accordingly but remain practical. An estimated investment of $260,000 for a foundational benchmark dataset becomes feasible when distributed across a consortium of labs. This cost should be framed not merely as an expense, but as a critical one-time infrastructure investment.</p><p>Unlike individual studies, the resulting dataset would serve as a lasting, open resource for the entire research community, enabling years of cumulative theory-building and providing a high return on investment by eliminating redundant data collection efforts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concluding comments</head><p>In summary, I argue that psychological research should strive to achieve greater precision and breadth simultaneously. This is accomplished through a data-guided theory-building process that involves:</p><p>• leveraging large-scale experimentation</p><p>• employing quantitative information-processing models</p><p>• minimizing upfront theoretical specification in favor of exploratory, iterative model building • embracing moderate complexity as a necessary trade-off for more precise and broader understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1. The PBS Impossible trinity in Psychological Research</head><p>This figure illustrates the PBS Impossible trinity, a fundamental constraint in psychological research where only two of three criteria can typically be optimized simultaneously: precision, breadth, and simplicity. Experimental studies often yield precise and simple-but narrow-findings, whereas broad theoretical concepts sacrifice precision for generality. The comprehensive exploration (CE) approach proposed here prioritizes precision and breadth, embracing moderate complexity to integrate diverse phenomena into unified information-processing models.  Human cognition is inherently complex, making it challenging to achieve precision, breadth, and simplicity simultaneously in psychological theories. Although precise-and-broad research holds the greatest scientific value, its inherent complexity conflicts with humans' cognitive limitations in processing complexity (HCLPC). This creates a key challenge: developing methods for precise-andbroad research that reduce complexity-induced burden to a manageable level. The comprehensive exploration (CE) approach addresses this challenge by maintaining precision and breadth while mitigating complexity-induced burden through benchmarking.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 8. Confirmatory and exploratory analysis</head><p>This figure summarizes the factors influencing the choice between theory-driven confirmation and data-guided exploration. When shifting from small-scale to large-scale studies, the problems of confirmatory analysis-such as lack of exploration and factor design difficulty-become significantly more pronounced. In contrast, the drawbacks of exploratory analysis, including false positive risk and inefficient testing, are substantially mitigated. Consequently, while confirmatory methods remain wellsuited for small-scale experiments, exploratory analysis emerges as the preferable approach for largescale investigations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 9. A firearms analogy</head><p>Theory-driven confirmation in small-scale studies is like a sniper rifle: a single, precise shot for narrow targeting. The cost of a miss is high, so precise control is essential, while the inability to hit unexpected targets is acceptable. In contrast, data-guided exploration in large-scale studies is like a shotgun blast: many scattered pellets for broad coverage. Controlling the trajectory of every pellet is prohibitively difficult, making broad coverage optimal. Here, "missing" with many pellets is inconsequential, while hitting unexpected targets becomes a significant advantage. Therefore, applying the sniper's precise control to the shotgun's domain-using theory-driven confirmation for large-scale studies-is both difficult and counterproductive.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc>, Huang (2025a)'s model achieved superior fit compared to Van den Berg et al. (2014)'s precisely because its development wasn't constrained to a predetermined solution space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The "Blind Men and the Elephant" Parable (a) The elephant symbolizes the multifaceted nature of mental mechanisms. (b) Precise-butfragmented experimental findings examine isolated aspects, much like each blind man perceiving only one part of the elephant. (c) Broad-but-vague theories oversimplify complexity, akin to describing the elephant as a "sphere". (d) Precise-and-broad studies (the CE approach) integrate components, determine their interconnections, and iteratively refine understanding. (e) Binary theorizing reduces complexity to oversimplified dichotomies (e.g., "pillar vs. snake"), forcing artificial opposition.</figDesc><graphic coords="64,74.90,79.05,392.59,507.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. A challenge for psychological researchHuman cognition is inherently complex, making it challenging to achieve precision, breadth, and simplicity simultaneously in psychological theories. Although precise-and-broad research holds the greatest scientific value, its inherent complexity conflicts with humans' cognitive limitations in processing complexity (HCLPC). This creates a key challenge: developing methods for precise-andbroad research that reduce complexity-induced burden to a manageable level. The comprehensive exploration (CE) approach addresses this challenge by maintaining precision and breadth while mitigating complexity-induced burden through benchmarking.</figDesc><graphic coords="65,54.00,75.00,481.04,352.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Six Methodological Dimensions This figure compares six key methodological dimensions in typical AI studies (left column) and traditional theory-driven hypothesis-testing in psychology (middle column). The right column presents the methodological approach of the comprehensive exploration (CE), with blue and red indicating alignment with AI and psychology traditions, respectively.</figDesc><graphic coords="66,54.00,79.50,502.35,473.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Rationale behind the six methodological choicesThe upper cyan region (Dimensions 1-2) reflects choices made to achieve precision, breadth, and benchmarking. The lower purple region (Dimensions 3-4) represents choices made to prioritize theoretical understanding, while the central yellow region (Dimensions 5-6) comprises choices made to enhance the feasibility of experimental design.</figDesc><graphic coords="67,54.00,73.40,501.43,502.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Comprehensive exploration (CE) study process A typical CE study consists of several steps: experimental design, data collection, and model development, which ultimately produces the outcome of the study-a CE model.</figDesc><graphic coords="68,54.00,76.15,500.09,415.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="73,74.90,78.95,270.85,442.19" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>In this context, precision refers to the accuracy of defining and measuring concepts, breadth describes the scope of a theory's applicability-such as the range of different phenomena or findings it can explain-and simplicity reflects the conciseness of an explanation, exemplified by the number of parameters a model uses to account for the data.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>While exceptions exist in both fields, the discussion focuses on typical cases for simplicity, as with all subsequent dimensions.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>This is not a hypothetical example but reflects my actual experience attempting to design a VWM experiment that simultaneously manipulated ten factors. After four weeks of effort, I abandoned not only that specific plan but also my confidence in high-dimensional, theory-driven designs as a whole.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>The statistical index CAD (Complexity-Adjusted d) is a fit-parsimony index that evaluates whether each parameter in a model meaningfully contributes to its explanatory power. As an effect-size-based measure, CAD assesses the utility of a parameter independently of dataset scale, ensuring that the effect size of its contribution exceeds a prespecified threshold (e.g., 0.2 or 0.1) across observed patterns. For further details, seeHuang (2025a).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>Strictly speaking, the parameter space is infinite because it includes not only the current 57 parameters but also those that have been tried and discarded, as well as parameters that may be explored in the future.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5"><p>It should be noted that whileHuang (2025a)  presents an especially successful case, other CE models<ref type="bibr" target="#b32">(Huang, 2023</ref><ref type="bibr" target="#b34">(Huang, , 2025b) )</ref> have not fully matched the effectiveness of benchmark neural networks, though they have come close.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_6"><p>This tendency resembles the Motte and Bailey fallacy<ref type="bibr" target="#b60">(Shackel, 2005)</ref>, in which a person advocates for a controversial, ambitious stance (the "Bailey") but retreats to a more defensible, modest position (the "Motte") when challenged.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_7"><p><ref type="bibr" target="#b5">Anvari et al. (2025)</ref> also share a similar perspective. Their proposed solutions-such as developing more comprehensive datasets, iteratively improving frameworks, and promoting better standardization across the research community-are closely aligned with those of the present work.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_8"><p>The ChatGPT has been utilized to enhance grammar, spelling, and phrasing of this paper, but it has not been employed to generate any new text.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_9"><p>Zhaoping, L., &amp; Zhe, L. (2015). Primary visual cortex as a saliency map: a parameter-free prediction and its test by behavioral data. Plos ComputationalBiology, 11(10), e1004375.   </p></note>
		</body>
		<back>

			
			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>


			<div type="funding">
<div><head>Funding statement</head><p>The work described in this paper was supported by the <rs type="funder">Research Grants Council of Hong Kong</rs> (<rs type="grantNumber">CUHK 14606622</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_dYcceFq">
					<idno type="grant-number">CUHK 14606622</idno>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data availability and the next paradigm shift</head><p>The rapid growth of data availability and computational power has fueled extraordinary progress in AI over the past two decades. A similar pattern is observable in psychology's history: the field has undergone multiple paradigm shifts-from Freud's psychoanalysis to behaviorism, and later, to cognitive psychology. While these transitions are often attributed to theoretical advancements, the increasing richness and precision of empirical data have consistently played a catalytic role.</p><p>For instance, behaviorist research typically relied on studies with hundreds of observations-a scale that constrained investigations to observable behaviors, leaving internal mental processes largely</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with Almaatouq et al. (2024)'s integrative experimental design</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Similarity between IED and CE</head><p>The CE approach and <ref type="bibr" target="#b2">Almaatouq et al. (2024)</ref>'s integrative experiment design (IED) share a fundamental critique of psychology's methodological stagnation 11 . Both seek to advance comprehensive theoretical understanding and identify traditional small-scale, hypothesis-driven experiments as a primary obstacle to this goal. Specifically, they argue that such approaches perpetuate theoretical fragmentation, producing either narrow findings or artificial binary debates that fail to capture cognitive complexity. Crucially, they agree that advancing psychological science requires directly engaging with methodological and theoretical complexity rather than circumventing it.</p><p>Beyond their shared goals, CE and IED also converge partly in their proposed solutions. Both advocate for larger datasets than those typical in experimental psychology and emphasize more systematic, in-depth analyses that move beyond simple hypothesis-testing. These similarities emerge naturally from their shared intellectual heritage -CE is indeed inspired by earlier work that either influenced IED (e.g., <ref type="bibr" target="#b6">Awad et al., 2018;</ref><ref type="bibr" target="#b37">Jolly &amp; Chang, 2019)</ref> or was directly produced by IED's authors themselves (e.g., <ref type="bibr" target="#b26">Griffiths, 2015;</ref><ref type="bibr" target="#b65">Watts, 2017;</ref><ref type="bibr" target="#b1">Agrawal, Peterson, &amp; Griffiths, 2020;</ref><ref type="bibr">Peterson et</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflicts of interest statement</head><p>The author declares no competing interests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 7. An optimal balance between prediction and explanation</head><p>The QCE-VWM model <ref type="bibr">(Huang, 2025a)</ref> employs just 57 parameters, all of which are fully interpretable. Despite its explanatory parsimony, it outperforms a benchmark neural network with 30,796 parameters in predictive accuracy. This represents a rare case in which we can assert an optimal balance between prediction and explanation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 10. Quantified literature review</head><p>The CE approach functions as a quantified literature review, combining the precision and rigor of experimental studies with the integrative scope of traditional reviews.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 11. Optimal Complexity and Dataset Scale</head><p>The upper panel shows a subset of data points from the lower panel. Statistically, the simpler (linear) model is better suited for smaller datasets, while the more complex (quadratic) model fits larger datasets more effectively. In general, as dataset size increases, the optimal model tends to be more complex-justifying the moderate complexity of CE models.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Spatiotemporal energy models for the perception of motion</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Bergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the optical society of america A</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="284" to="299" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scaling up psychology via scientific regret minimization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="8825" to="8835" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Beyond playing 20 questions with nature: Integrative experiment design in the social and behavioral sciences</title>
		<author>
			<persName><forename type="first">A</forename><surname>Almaatouq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Suchow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Whiting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Watts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The capacity of visual short-term memory is set both by visual information load and by number of objects</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cavanagh</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.0963-7214.2004.01502006</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="106" to="111" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">There is no such thing as attention</title>
		<author>
			<persName><forename type="first">B</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in psychology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">246</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Defragmenting psychology</title>
		<author>
			<persName><forename type="first">F</forename><surname>Anvari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Alsalti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Oehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hussey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Arslan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<biblScope unit="page" from="1" to="4" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The moral machine experiment</title>
		<author>
			<persName><forename type="first">E</forename><surname>Awad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dsouza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Henrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shariff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename><surname>Rahwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">563</biblScope>
			<biblScope unit="issue">7729</biblScope>
			<biblScope unit="page" from="59" to="64" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Why some colors appear more memorable than others: A model combining categories and particulars in color working memory</title>
		<author>
			<persName><forename type="first">G.-Y</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Olkkonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Allred</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">I</forename><surname>Flombaum</surname></persName>
		</author>
		<idno type="DOI">10.1037/xge0000076</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">744</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dynamic shifts of limited working memory resources in human vision</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Bays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Husain</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.1158023</idno>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">321</biblScope>
			<biblScope unit="issue">5890</biblScope>
			<biblScope unit="page" from="851" to="854" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Contagious: How to build word of mouth in the digital age</title>
		<author>
			<persName><forename type="first">J</forename><surname>Berger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Simon and Schuster</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep problems with neural network models of human vision</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Bowers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Malhotra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dujmović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Montero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Biscione</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename><surname>Heaton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page">385</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hierarchical Encoding in Visual Working Memory: Ensemble Statistics Bias Memory for Individual Items</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Alvarez</surname></persName>
		</author>
		<idno type="DOI">10.1177/0956797610397956</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="384" to="392" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Contextual effects in visual working memory reveal hierarchically structured memory representations</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Alvarez</surname></persName>
		</author>
		<idno type="DOI">10.1167/15.15</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="6" to="6" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A probabilistic model of visual working memory: Incorporating higher order regularities into working memory capacity estimates</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0030779</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">85</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Behavioural science is unlikely to change the world without a heterogeneity revolution</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName><surname>Tipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Yeager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="980" to="989" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Simplicity: a unifying principle in cognitive science?</title>
		<author>
			<persName><forename type="first">N</forename><surname>Chater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vitányi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="22" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adversarial testing of global neuronal workspace and integrated information theories of consciousness</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cogitate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ferrante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Gorska-Klimowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Henin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hirschhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khalaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Crocker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Algina</surname></persName>
		</author>
		<title level="m">Introduction to classical and modern test theory</title>
		<imprint>
			<publisher>ERIC</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Objectivity</title>
		<author>
			<persName><forename type="first">L</forename><surname>Daston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Galison</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Princeton University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Against theory-motivated experimentation in science</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dubova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moskvichev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zollman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Humans integrate visual and haptic information in a statistically optimal fashion</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Banks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">415</biblScope>
			<biblScope unit="issue">6870</biblScope>
			<biblScope unit="page" from="429" to="433" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The integrated information theory of consciousness as pseudoscience</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fleming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Frith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goodale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Ledoux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename><surname>Slagter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Strategic trade-offs between quantity and quality in working memory</title>
		<author>
			<persName><forename type="first">D</forename><surname>Fougnie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Cormiea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kanabar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Alvarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">1231</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Variability in the quality of visual working memory</title>
		<author>
			<persName><forename type="first">D</forename><surname>Fougnie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Suchow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Alvarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1229">2012. 1229</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Reasoning the fast and frugal way: models of bounded rationality</title>
		<author>
			<persName><forename type="first">G</forename><surname>Gigerenzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">650</biblScope>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Heuristics and biases: The psychology of intuitive judgment</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gilovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kahneman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Manifesto for a new (computational) cognitive revolution</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">135</biblScope>
			<biblScope unit="page" from="21" to="23" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Made to stick: Why some ideas survive and others die: Random House</title>
		<author>
			<persName><forename type="first">C</forename><surname>Heath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Heath</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">No one knows what attention is</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hommel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Chapman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cisek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">F</forename><surname>Neyedli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Welsh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Attention, Perception, &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="2288" to="2303" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Color is processed less efficiently than orientation in change detection but more efficiently in visual search</title>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1177/0956797615569577</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="646" to="652" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unit of visual working memory: A Boolean map provides a better account than an object does</title>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1037/xge0000616</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">149</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">FVS 2.0: A unifying framework for understanding the factors of visual-attentional processing</title>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1037/rev0000314</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="696" to="731" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A quasi-comprehensive exploration of the mechanisms of spatial working memory</title>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="729" to="739" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Comprehensive exploration of visual working memory mechanisms using large-scale behavioral experiment</title>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">1383</biblScope>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Aesthetic preferences among spatial patterns: Large-scale experiment, comprehensive exploration, and a three-component regularity-based model</title>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychology of Aesthetics, Creativity, and the Arts</title>
		<imprint>
			<date type="published" when="2025">2025b</date>
		</imprint>
	</monogr>
	<note>Advance online publication</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A Boolean map theory of visual attention</title>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pashler</surname></persName>
		</author>
		<idno type="DOI">10.1037/0033-295x.114.3.599</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="599" to="631" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">When choice is demotivating: Can one desire too much of a good thing</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Iyengar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Lepper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">995</biblScope>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The flatland fallacy: Moving beyond low-dimensional thinking</title>
		<author>
			<persName><forename type="first">E</forename><surname>Jolly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Topics in Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="433" to="454" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Thinking, fast and slow</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kahneman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Farrar, Straus and Giroux</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Conditions for intuitive expertise: a failure to disagree</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kahneman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American psychologist</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">515</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The consciousness wars: can scientists ever agree on how the mind works</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lenharo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">625</biblScope>
			<biblScope unit="page" from="438" to="440" />
			<date type="published" when="2024">2024. 7995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Measuring faking in the employment interview: development and validation of an interview faking behavior scale</title>
		<author>
			<persName><forename type="first">J</forename><surname>Levashina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Campion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Psychology</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">1638</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A saliency map in primary visual cortex</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="16" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Two good reasons to say &apos;change!&apos;-ensemble representations as well as item representations impact standard measures of VWM capacity</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Liesefeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Liesefeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British Journal of Psychology</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="328" to="356" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Current directions in visual working memory research: An introduction and emerging insights</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Liesefeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British Journal of Psychology</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="193" to="206" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Simplicity and probability in causal explanation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lombrozo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="232" to="257" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">An evaluation of experimental sampling strategies for autonomous empirical research in cognitive science</title>
		<author>
			<persName><forename type="first">S</forename><surname>Musslick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Hewson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Strittmatter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">T</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename><surname>Holland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the annual meeting of the cognitive science society</title>
		<meeting>the annual meeting of the cognitive science society</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Paper presented at the</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Measurement models for visual working memory-A factorial model comparison</title>
		<author>
			<persName><forename type="first">K</forename><surname>Oberauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">841</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Benchmarks for models of short-term and working memory</title>
		<author>
			<persName><forename type="first">K</forename><surname>Oberauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jarrold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lewandowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">142</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">885</biblScope>
			<date type="published" when="2016">2016. 2018</date>
		</imprint>
	</monogr>
	<note>What limits working memory capacity? Psychological Bulletin</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Familiarity and visual change detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pashler</surname></persName>
		</author>
		<idno type="DOI">10.3758/bf03210419</idno>
	</analytic>
	<monogr>
		<title level="j">Perception &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="369" to="378" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Is the replicability crisis overblown? Three arguments examined</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pashler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Harris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perspectives on Psychological Science</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="531" to="536" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Editors&apos; introduction to the special section on replicability in psychological science: A crisis of confidence?</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pashler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Wagenmakers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perspectives on Psychological Science</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="528" to="530" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Using large-scale experiments and machine learning to discover theories of human decision-making</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Bourgin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Reichman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">372</biblScope>
			<biblScope unit="issue">6547</biblScope>
			<biblScope unit="page" from="1209" to="1214" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">The logic of scientific discovery</title>
		<author>
			<persName><forename type="first">K</forename><surname>Popper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Routledge</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Conjectures and refutations: The growth of scientific knowledge</title>
		<author>
			<persName><forename type="first">K</forename><surname>Popper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Routledge</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Processing fluency and aesthetic pleasure: Is beauty in the perceiver&apos;s processing experience?</title>
		<author>
			<persName><forename type="first">R</forename><surname>Reber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Winkielman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Personality and social psychology review</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="364" to="382" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Visual Attention in Crisis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rosenholtz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="page" from="1" to="32" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Integrative benchmarking to advance neurally mechanistic models of human intelligence</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schrimpf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kubilius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A R</forename><surname>Murty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ajemian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Dicarlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="413" to="423" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Maximizing versus satisficing: happiness is a matter of choice</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Monterosso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lyubomirsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Lehman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">1178</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">The vacuity of postmodernist methodology</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Metaphilosophy</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="295" to="320" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Modeling visual working memory with the MemToolbox</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Suchow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fougnie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Alvarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="9" to="9" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Terms of the debate on the format and structure of visual memory</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Suchow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fougnie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Alvarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Attention, Perception, &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="2071" to="2079" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Factorial comparison of working memory models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Van Den Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Awh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">124</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A resource-rational theory of set size effects in human visual working memory</title>
		<author>
			<persName><forename type="first">R</forename><surname>Van Den Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ELife</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">34963</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Should social science be more solution-oriented?</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Watts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">The political brain: The role of emotion in deciding the fate of the nation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Westen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>PublicAffairs</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">The generalizability crisis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yarkoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Choosing prediction over explanation in psychology: Lessons from machine learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yarkoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Westfall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perspectives on Psychological Science</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1100" to="1122" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Discrete fixed-resolution representations in visual working memory</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Luck</surname></persName>
		</author>
		<idno>doi:10.1038/</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">453</biblScope>
			<biblScope unit="issue">7192</biblScope>
			<biblScope unit="page" from="233" to="U213" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
