<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Comparing Perceptual Judgments in Large Multimodal Models and Humans</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Billy</forename><surname>Dickson</surname></persName>
							<email>dicksonb@iu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Indiana University Bloomington</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sahaj</forename><surname>Singh Maini</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Indiana University Bloomington</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Robert</forename><surname>Nosofsky</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Psychological and Brain Sciences</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="department" key="dep3">Computing, and Engineering</orgName>
								<orgName type="institution" key="instit1">Indiana University Bloomington</orgName>
								<orgName type="institution" key="instit2">Luddy School of Informatics</orgName>
								<orgName type="institution" key="instit3">Indiana University Bloomington</orgName>
								<address>
									<addrLine>700 N Woodlawn Ave</addrLine>
									<postCode>47408</postCode>
									<settlement>Bloomington</settlement>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zoran</forename><surname>Tiganj</surname></persName>
							<email>ztiganj@iu.edu.</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Indiana University Bloomington</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Psychological and Brain Sciences</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="department" key="dep3">Computing, and Engineering</orgName>
								<orgName type="institution" key="instit1">Indiana University Bloomington</orgName>
								<orgName type="institution" key="instit2">Luddy School of Informatics</orgName>
								<orgName type="institution" key="instit3">Indiana University Bloomington</orgName>
								<address>
									<addrLine>700 N Woodlawn Ave</addrLine>
									<postCode>47408</postCode>
									<settlement>Bloomington</settlement>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Comparing Perceptual Judgments in Large Multimodal Models and Humans</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">58696655E62363BFE1AF667428C86D13</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-23T06:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Cognitive scientists commonly collect participants' judgments regarding perceptual characteristics of stimuli to develop and evaluate memory, learning, and decision-making models. For instance, to model human responses in tasks of category learning and item recognition, researchers often have to collect perceptual judgments of images to embed the images in multidimensional feature spaces. This process is time-consuming and costly. Recent advancements in Large Multimodal Models (LMMs) provide a potential alternative since such models can respond to prompts that include both text and images and could potentially replace human participants. To test whether the available LMMs can indeed be useful for this purpose, we evaluated their judgments on a dataset consisting of rock images that has been widely used by cognitive scientists. The dataset includes human perceptual judgments along ten dimensions considered important for classifying rock images. While the models exhibited a strong positive correlation with human responses, we found that they fell short in replacing an average of a set of judgments from human participants. The models provided correlations with these averaged data that were roughly the same magnitude as observed for individual participants, especially for dimensions that are relatively general (such as lightness and chromaticity) as opposed to domain-specific dimensions (such as pegmatitic structure), where they struggled more. We also found that modifying prompts and providing additional examples of images with corresponding ratings had a positive but relatively modest impact on model performance. Our study provides a benchmark for evaluating future LMMs on human perceptual judgment data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Among the fundamental goals in the computational modeling of cognitive processes such as categorization, old-new recognition, and decision making is to account for human performance at the level of individual items <ref type="bibr" target="#b30">(Love et al., 2004;</ref><ref type="bibr" target="#b35">Nosofsky, 1986</ref><ref type="bibr" target="#b36">Nosofsky, , 1991;;</ref><ref type="bibr" target="#b38">Nosofsky et al., 2011;</ref><ref type="bibr" target="#b57">Shepard, 1957)</ref>. With respect to this goal, a major current trend in the psychological and cognitive sciences has involved the scaling up of the models from applications in simple, highly controlled, low-dimensional stimulus domains to complex, real-world high-dimensional ones <ref type="bibr" target="#b4">(Bainbridge, 2019;</ref><ref type="bibr" target="#b5">Battleday et al., 2020;</ref><ref type="bibr" target="#b16">Hebart et al., 2020;</ref><ref type="bibr" target="#b34">Meagher &amp; Nosofsky, 2023;</ref><ref type="bibr" target="#b40">Nosofsky, Sanders, &amp; McDaniel, 2018;</ref><ref type="bibr" target="#b63">Storms et al., 2000)</ref>. In addition, a current trend is to scale up the application of the models to cases in which performance is modeled for very large numbers of individual items from the domains of interest <ref type="bibr" target="#b5">(Battleday et al., 2020;</ref><ref type="bibr" target="#b16">Hebart et al., 2020;</ref><ref type="bibr" target="#b23">Kramer et al., 2023;</ref><ref type="bibr" target="#b34">Meagher &amp; Nosofsky, 2023)</ref>.</p><p>Applying the models for predicting individual-item performance requires the specification of a psychological "feature space" in which the items are embedded. Historically, one of the major approaches to deriving such an input space for the cognitive models has been to conduct independent tasks involving the collection of varied forms of "similarity-scaling" data <ref type="bibr" target="#b37">(Nosofsky, 1992;</ref><ref type="bibr" target="#b49">Roads &amp; Love, 2024)</ref>. For example, observers might be required to make direct judgments of the similarity between different pairs of items <ref type="bibr" target="#b58">(Shepard, 1962)</ref>; to judge whether pairs of presented items are "same" or "different" <ref type="bibr" target="#b24">(Kruskal &amp; Wish, 1978;</ref><ref type="bibr" target="#b51">Rothkopf, 1957)</ref>; to choose which item among a triad of presented items is the "odd-one-out" <ref type="bibr" target="#b16">(Hebart et al., 2020;</ref><ref type="bibr" target="#b50">Roads &amp; Mozer, 2019)</ref>; or to form spatial arrangements of entire sets of objects in which the distance between objects is proportional to their judged dissimilarity <ref type="bibr" target="#b13">(Goldstone, 1994;</ref><ref type="bibr" target="#b19">Hout, Goldinger, &amp; Ferguson, 2013)</ref>. Formal statistical models can then be applied for modeling the obtained similarity-scaling data by embedding the judged items in derived multidimensional feature spaces <ref type="bibr" target="#b20">(Hout, Papesh, &amp; Goldinger, 2012;</ref><ref type="bibr" target="#b26">Lee, 2001;</ref><ref type="bibr" target="#b58">Shepard, 1962</ref><ref type="bibr" target="#b59">Shepard, , 1980</ref><ref type="bibr" target="#b60">Shepard, , 1987))</ref>.</p><p>In modern work, however, a limitation of this classic approach is that the techniques eventually become intractable when the number of to-be-scaled items is extremely large and/or when the dimensionality of the feature space is large and complex (but for modern efforts along these lines, see, <ref type="bibr" target="#b16">Hebart et al., 2020;</ref><ref type="bibr" target="#b32">Marjieh et al., 2024;</ref><ref type="bibr" target="#b41">Nosofsky, Sanders, Meagher, et al., 2018.</ref> For example, to fill out a similarity-judgment matrix with just a single judgment for all pairs of n items requires something on the order of n 2 similarity judgments. If n=1,000, then something on the order of 1,000,000 judgments are needed.</p><p>Therefore, as an alternative to this similarity-scaling approach, a major current trend is for researchers to use a variety of machine-based deep-learning network approaches for deriving the feature space to be used as input to the cognitive-process models <ref type="bibr" target="#b1">(Annis et al., 2021;</ref><ref type="bibr" target="#b5">Battleday et al., 2020;</ref><ref type="bibr" target="#b25">Lake et al., 2015;</ref><ref type="bibr" target="#b44">Peterson et al., 2018;</ref><ref type="bibr" target="#b48">Roads &amp; Love, 2021)</ref>. The general approach here is to train deep-learning models to learn to classify large sets of items into different categories in a domain of interest. By using appropriate validation and generalization techniques to avoid overfitting the noise in the data, the respective models learn a set of weights that can classify with high accuracy both the trained items and novel items from the relevant domains. Once the training has been completed, the deep-learning model can then be used as a mechanism for embedding the items in a candidate feature space. In particular, the presentation of any given item will give rise to a set of activations of the nodes in the network. Oftentimes, some transformation of the node activations at the penultimate layer of the network, immediately prior to the final classification layer, are chosen as the candidate features.</p><p>These candidate features are then used as inputs to varieties of cognitive-process models used for predicting human performance in related tasks and domains.</p><p>Although researchers have achieved some impressive successes with these deeplearning approaches, these approaches also have potential limitations. One limitation is the lack of assurance that all stimulus features detected by the machine learning models are similarly apprehended by humans. To take a hypothetical example, suppose that a particular machine-learning model can apprehend wavelengths of light or sound that go beyond human sensitivities and that these "extra-sensory" features turn out to be highly diagnostic for purposes of classification. In that case, the candidate feature space derived from the deep-learning model would include non-human features, so the use of that feature space in combination with cognitive models could lead to misleading conclusions involving the nature of human performance. A second limitation is that there is an enormous set of highly complex nonlinear transformations that take place in translating elementary input features in the networks into the patterns of activations at the penultimate layers. Oftentimes, it is extremely difficult to place a psychological interpretation on the patterns of penultimate-layer activations. Thus, the machinelearning-based feature space that is used as an input to the cognitive models may not correspond to the types of foundational psychological building-block features that are supposed by the cognitive models in the first place.</p><p>To address the above-stated limitations, <ref type="bibr">Sanders &amp; Nosofsky (2018</ref><ref type="bibr">, 2020)</ref> proposed a hybrid two-step approach (see also <ref type="bibr" target="#b52">Rumelhart &amp; Todd, 1993;</ref><ref type="bibr" target="#b62">Steyvers &amp; Busey, 2000)</ref>. The approach was intended to combine the strengths of the similarity-scaling and deep-learning approaches to deriving psychological feature spaces for large numbers of real-world highdimensional objects. In the first step, one uses traditional similarity-scaling methods for deriving a feature-space representation for a representative subset of the objects in the domain of interest. In the second step, rather than training a deep-learning network to classify objects in this domain into categories, one instead trains the deep-learning network to reproduce the actual feature-space representation for the subset of objects that was derived from the human judgments. Again, by using appropriate validation and generalization techniques to avoid overfitting the training-item representation, the approach could allow for the embedding of an essentially infinite number of objects from the domain of interest in a high-dimensional psychological feature space.</p><p>A further advantage of the proposed two-step approach is that the same technique can be applied for positioning objects along additional dimensions not revealed by the similarityscaling techniques themselves. For example, <ref type="bibr">Sanders &amp; Nosofsky (2020)</ref> tested the proposed approach in a domain involving igneous rock categories as defined in the geologic sciences.</p><p>Using similarity-scaling techniques, <ref type="bibr" target="#b41">Nosofsky, Sanders, Meagher, et al. (2018)</ref> had previously found that an 8-dimensional multidimensional scaling (MDS) representation provided an excellent account of similarity-judgment data obtained for a set of 360 rock images. In addition, the derived MDS dimensions had natural psychological interpretations. These included the extent to which the rocks were: i) light or dark, ii) fine-or coarse-grained, iii) smooth or rough, iv) dull or shiny, v) had disorganized vs. organized textures, vi) were achromatic vs. chromatic, and vii) had a green vs. red hue. (A firm interpretation was not obtained for the eighth dimension, but it appeared to have shape-related components.) Crucially, however, <ref type="bibr">Nosofsky et al. (2020)</ref> and <ref type="bibr" target="#b55">Sanders and Nosofsky (2020)</ref> discovered that in independently conducted classification-learning experiments, observers made use of additional "supplementary" dimensions not revealed by the initial MDS analysis but that were highly diagnostic for purposes of classifying the rock images into their geologically-defined categories. One example included the extent to which the rocks possessed "porphyritic structure" -the embedding of small-sized fragments within a fine-grained groundmass. (We describe several other examples of these supplementary dimensions below.) In previous research projects, <ref type="bibr" target="#b42">Nosofsky et al. (2020)</ref> and <ref type="bibr" target="#b41">Nosofsky, Sanders, Meagher, et al. (2018)</ref> obtained direct ratings from human subjects of the positions of all the rock images along each of the MDS-derived and supplementary dimensions.</p><p>Importantly, the two-step approach proposed by <ref type="bibr" target="#b55">Sanders and Nosofsky (2020)</ref> for training deeplearning networks to reproduce MDS-derived representations also showed some preliminary success in reproducing the position of the rocks along these supplementary dimensions as well.</p><p>Despite the initial promise of the proposed two-step method, <ref type="bibr" target="#b55">Sanders and Nosofsky (2020)</ref> acknowledged that improved machine-learning technologies would likely yield more effective procedures for implementing it. Indeed, in the short time since the original proposal, an explosion of such improved technologies has emerged.</p><p>In recent years multimodal vision and language models demonstrated the ability to produce advanced image understanding. These models combine inputs from both visual and textual sources to understand and generate content that reflects a combined understanding of both modalities. Multimodal vision and language architectures such as CLIP <ref type="bibr" target="#b45">(Radford et al., 18-24 Jul 2021)</ref> demonstrate powerful zero-shot learning, surpassing humans in zero-shot, oneshot and two-shot learning on the Oxford IIT Pets image classification dataset. When embedded into larger frameworks such as Open AI's GPT4 (OpenAI et al., 2023), Google's Gemini (Gemini <ref type="bibr" target="#b11">Team et al., 2024)</ref>, or Anthropic's Claude-3 <ref type="bibr" target="#b2">(Anthropic 2024)</ref>, which all have multiple training components, including reinforcement learning from human feedback <ref type="bibr" target="#b12">(Glaese et al., 2022)</ref>, these multimodal models gain additional power and flexibility in terms of interaction with the human users. Careful design of prompts, specifically techniques such as chain-of-thought prompting, can further increase the performance of these models <ref type="bibr" target="#b67">(Wei et al., 2022;</ref><ref type="bibr" target="#b29">Lin 2024)</ref>. <ref type="bibr" target="#b22">Kouwenhoven et al. (2022)</ref> highlight the importance of developing shared vocabularies between humans and machines, proposing that natural evolution in communication could significantly improve AI interactions. Recent research investigated relationships between humans more systematically by quantifying differences across specific perceptual features <ref type="bibr" target="#b10">(Geirhos et al., 2021;</ref><ref type="bibr" target="#b61">Sheybani et al., 2024)</ref> and visual illusions <ref type="bibr" target="#b56">(Shahgir et al., 2024)</ref>, complemented by an overview by de Kleijn (2022) outlining the evolution of AI and neural networks in relation to the human brain.</p><p>The central purpose of the present work was to investigate the effectiveness of the current technology using Large Multimodal Models (LMMs) for positioning large sets of complex, real-world objects in high-dimensional feature spaces that align with human judgments.</p><p>Following <ref type="bibr" target="#b55">Sanders and Nosofsky (2020)</ref>, our example target domain involves the set of 360 rock images used in their earlier study. This domain is particularly suitable for our current investigation due to the substantial amount of previously collected psychological-scaling and direct dimension-rating data for these rock images. Consequently, the predictions generated by LMMs can be rigorously compared to the extensive existing sets of human judgments.</p><p>In the present work, we conduct an experimental investigation of the performance of LMMs that accept image and text inputs, specifically OpenAI GPT4 and Anthropic Claude-3 model family, for reproducing human dimension ratings within the rock domain<ref type="foot" target="#foot_0">foot_0</ref> . Our investigation is structured around three primary implementation conditions. In each condition, we evaluate the correspondence between the ratings produced by LMMs and human ratings along a set of 7 MDS-derived dimensions and 3 supplementary dimensions. In the first condition, for each to-be-judged image, we provide LMMs with verbal prompts that are identical to the verbal prompts that had been provided to human subjects in the previous dimensionrating studies. This first condition has certain similarities to an important investigation reported recently by <ref type="bibr" target="#b33">Marjieh et al. (2023)</ref>. Using only verbal prompts, these researchers asked GPT4 and some related machine-learning systems to make judgments of similarity for stimuli in six domains, such as tones varying in pitch or colors varying in wavelength. They then obtained MDS solutions for the GPT4-produced similarity ratings and found that the recovered MDS solutions corresponded well with ones derived in past studies in which humans provided similarity judgments of the actual perceptual stimuli. By contrast, in Condition 1 of the present study, on each trial we present to the machine-learning system an actual rock image to be judged. We then use verbal prompts to ask the machine-learning algorithm to produce its dimension ratings for the image. The system is asked to produce direct ratings along 10 different dimensions that are components of a single set of complex rock-image stimuli.</p><p>In the second condition of our investigation, we supplement the prompts with a set of images that illustrate examples with low, medium, and high values along the rated dimensions.</p><p>These "anchor images" had in fact been used in the original studies conducted by <ref type="bibr" target="#b41">Nosofsky, Sanders, Meagher, et al. (2018</ref><ref type="bibr">, 2020)</ref> in which the human dimension ratings for the rocks had originally been collected. We hypothesized that the correspondence between the multimodal machine-learning ratings and the human ratings would be significantly improved when the system was provided with the example anchor images.</p><p>Finally, we followed the investigations in these two main conditions with some further explorations on a subset of the dimensions. Although these additional explorations did not involve comprehensive tests across all the dimensions and machine-learning systems, for simplicity we will refer to them as the third condition. Specifically, we explored whether the models might yield better correspondences with the human ratings on some challenging dimensions if we queried the system with additional anchor images and also provided more detailed prompts that contained additional information about the judged dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>We used previously collected data from human participants who performed dimensionratings of 360 rock images across a set of continuous and present-absent dimensions <ref type="bibr" target="#b34">(Meagher &amp; Nosofsky, 2023;</ref><ref type="bibr" target="#b41">Nosofsky, Sanders, Meagher, et al., 2018)</ref>. The complete dataset including rock images and participant ratings is publicly available through OSF: <ref type="url" target="https://osf.io/cvwu9/">https://osf.io/cvwu9/</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stimuli</head><p>The stimuli were 360 images of rocks obtained from the web and processed to remove background objects and idiosyncratic markings such as text labels. The 360 images included 120 rocks from each of three main divisions: igneous, metamorphic, and sedimentary. Each main division included 12 rocks from ten major subtype categories (30 subtypes in total), such as granite, marble, sandstone, and so forth.</p><p>For human experiments, the stimuli were presented on a 23-in. LCD computer screen.</p><p>The stimuli were displayed on a white background. Each rock picture was approximately 2.1 in. wide and 1.7 in. tall. Subjects sat approximately 20 in. from the computer screen, so each rock picture subtended a visual angle of approximately 6.0° x 4.9°. Images were selected or digitally manipulated to have similar levels of resolution of the salient features that may be used to identify and classify the particular rock types. All of the images were photographed in a field setting and had not been modified in any way other than the removal of other portions of the original image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human dimension ratings</head><p>After excluding dimensions that did not have complete participant data, we focused our analysis on ten dimensions, seven derived from MDS analysis of similarity judgments (chromaticity, darkness/lightness, disorganized/organized, dull/shiny, fine/coarse grain, red/green, and smooth/rough) and 3 "supplementary" dimensions (conchoidal fracture, pegmatitic structure, and porphyritic texture) that were not revealed by the MDS analysis but that were added due to being highly diagnostic for purposes of classifying the rock images into their geologically-defined categories. Each of these dimensions had a continuous rating scale from 1 to 9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multimodal vision and language models</head><p>We employed state-of-the-art LMMs, GPT4 and Claude-3 model family. The Claude-3 model family includes the three models -Opus, Sonnet, and Haiku, with Opus stated to be the most intelligent model and Haiku being the least intelligent but the fastest model amongst the three models <ref type="bibr" target="#b2">(Anthropic 2024)</ref>. We conducted the analysis in three different conditions. All the models used in the experiment were prompted using API calls. In order to minimize randomness in the responses, the temperature parameter was set to 0 for all the models. Prompts to the models were identical to those provided to human subjects aside from minor modifications instructing the model to return a numerical dimension rating to the hundredths place 2 and changing plural references to describe a single trial, as each API call to the model was independent. Furthermore, in conditions without anchor images (1 and 3a), any text mentioning the presence of example images was omitted from the prompt. RGB images were used to prompt the models in conditions where anchor images were part of the prompt. All prompts are listed as part of the Supplementary Information available at <ref type="url" target="https://osf.io/za847/">https://osf.io/za847/</ref>.</p><p>In the first condition, 360 images were provided to each of the four models for each of the ten different dimensions. No anchor images were provided to the model. For GPT4 specifically, the text 'Do not respond with I'm sorry…' was added to the prompt of several features (fine/coarse grain, red/green hue, porphyritic texture, pegmatitic structure, and conchoidal fracture) in order to minimize redundant responses from the model. An example of this condition is displayed in Figure <ref type="figure" target="#fig_0">1</ref> below. In this example, the prompt for darkness/lightness is passed to the model along with the rock image to be rated on a scale of 1 to 9. The model produces one numeric value.</p><p>2 Despite the instructions for the model to return numerical dimensions to the hundredths place and to use the full array of available values, the outputs most predominantly appeared in increments of 0.05.</p><p>In the second condition, we added three anchor images to each prompt. The anchor images were combined with verbal indicators for the feature being tested (e.g., "This is an example of a dark rock"). Anchor images included examples with low, medium, and high values along the rated dimensions. We used the same anchor images that were presented to the participants in the previous human dimension-rating studies from <ref type="bibr" target="#b41">Nosofsky, Sanders, Meagher, et al. (2018</ref><ref type="bibr">, 2020)</ref>. An example of this condition with anchor images is provided in Figure <ref type="figure" target="#fig_1">2</ref> below. In this example, 3 anchor images depicting dark, medium, and light rocks are passed to the model with the darkness/lightness feature prompt and rock to be rated. The model again produces one numeric value.</p><p>In the third condition, we made several exploratory manipulations in an attempt to improve the performance of the models for two dimensions that had a relatively low correlation with human ratings, specifically organized/disorganized and pegmatitic structure. First, we modified each prompt to include what we thought to be a more precise description of each dimension (e.g., providing more information about what it means for a rock to rank high or low along the respective dimension). <ref type="foot" target="#foot_1">3</ref> We then conducted this revised evaluation without the anchor images (Condition 3a) and with anchor images (Condition 3b). Second, we provided a set of nine additional anchor images for each of the two dimensions. The anchor images spanned from low to high ratings along the two feature dimensions. The rating for each anchor image provided to the model was computed as the average rating obtained from the four authors independently (Condition 3c). An example of Condition 3c is provided in Figure <ref type="figure" target="#fig_2">3</ref> below. model along with a new disorganized/organized feature prompt and the rock to be rated. The model produces one numeric value. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preliminary summary of the main pattern of results</head><p>Before providing a detailed report of comparisons among models across the conditions, we start by presenting the results from GPT4 in Condition 1. In Condition 1, GPT4 was the best performing model overall. In addition, as will be seen, adding anchor images led to relatively small changes in GPT4's performance relative to the purely verbal prompts used in Condition 1. Claude-3 based models benefited more from adding anchor images, in some cases slightly surpassing the performance of GPT4. The initial report that we provide in this section is intended to give the reader an immediate sense of both the strengths and weaknesses of these multimodal systems in reproducing human judgments along these varied dimensions of the rock images.</p><p>We display in Figure <ref type="figure" target="#fig_3">4</ref> scatterplots of GPT4's Condition-1 results for each of the 10 dimensions. Each panel plots the mean observed human ratings for the 360 rock images against the ratings produced by GPT4. The correlation between the observed and predicted ratings is also reported. As explained later in our article, we have also constructed interactive versions of each scatterplot with more detailed information than shown here. The interactive scatterplots are available at cognlp.com.</p><p>As can be seen in Figure <ref type="figure" target="#fig_3">4</ref>, there is a good deal of variation in the quality of GPT4's predictions across the different dimensions, with the correlations ranging from r=.43 for pegmatitic structure to r=.88 for darkness/lightness. In general, GPT4 performs best in matching the human ratings for what seem to us to be "elementary" visual dimensions such as darkness/lightness, chromaticity, and red-green hue. The correlations for these elementary dimensions are reasonably high and the scatterplots illustrate that there is appropriate variation across the entire range of ratings. (In other words, the high correlations are not the result of the model predicting only very low ratings for one subset of stimuli and only very high ratings for a second set.) However, GPT4's performance declines for what seem to us to be more abstract and emergent dimensions such as organization and pegmatitic structure. In our Discussion, we consider other possible reasons for the large variation in GPT4's performance across the different dimensions. Interestingly, we will see that humans also show less agreement among themselves in their ratings of the more abstract dimensions compared to the elementary ones.</p><p>In testing these multimodal systems, our hope was that their ratings might show sufficient reliability and correspondence with the mean human ratings so as to potentially obviate the need for collecting extensive human ratings in future research. As we will see in our detailed report below, although the systems show promise, this rather ambitious goal was not achieved. Instead, we will see that the best-performing systems provide dimension ratings with reliability that is roughly in the neighborhood of a single subject. Whether or not this degree of reliability is sufficient for one's research goals will likely depend on the specific nature of one's research project.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detailed evaluation of the LMMs across the three conditions</head><p>We first evaluated each of the four models: GPT4 <ref type="bibr" target="#b43">(OpenAI et al., 2023)</ref> and Haiku, Sonnet, and Opus (three variants of the Claude-3 model from Anthropic 2024). The results of the evaluation on Condition 1 (which included only verbal prompts) and Condition 2 (which included anchor images) are shown in Table <ref type="table" target="#tab_0">1</ref>. outperformed Haiku and Opus on the remaining nine dimensions. Adding anchor images yielded mixed performance improvements across models. For GPT4, performance increased in six of ten features with decreased performance in four. For Haiku, performance increased in eight of ten features with decreases in disorganized/organized and porphyritic texture. For Sonnet, performance improved across all features except dull/shiny. For Opus, performance increased across all features except porphyritic texture. The only top score without using anchor images was GPT4 dull/shiny.</p><p>The results of modifying the prompts to include more detailed information and descriptions of dimensions that had a low correlation between human and model data (disorganized/organized and pegmatitic structure) are shown in Tables <ref type="table" target="#tab_1">2</ref> and <ref type="table" target="#tab_2">3</ref>. Table <ref type="table" target="#tab_1">2</ref> presents results without using anchor images (Condition 3a). Using the modified prompt without anchor images led to poorer performance across all models. Table <ref type="table" target="#tab_2">3</ref> presents results using anchor images (Condition 3b). Performance increased for GPT4 in the disorganized/organized dimension but decreased for all other models in all dimensions. In no case was a high correlation achieved. The results of using author-rated anchor images in combination with the longer prompts are shown in Table <ref type="table" target="#tab_3">4</ref>. Including the additional anchor images improved the GPT4 correlation for organized/disorganized from 0.42 to 0.56, but it decreased the correlation for pegmatitic structure from 0.56 to 0.47. Similarly mixed results were obtained for Haiku, and performance decreased in both dimensions for Sonnet. In no case was a very high correlation achieved for either of these complex dimensions. In general, our efforts to improve the performance of the multimodal models on these dimensions by using the modified prompts and more extended set of anchor images were not very successful. Figure <ref type="figure" target="#fig_4">5</ref> shows for each multimodal model in Conditions 1 and 2 the mean correlation across all the rock dimensions between the ratings of the human subjects and the models.</p><p>Adding anchor images increases the mean correlation and decreases the standard deviation of the correlations across all models. However, the overall improvement yielded by use of the anchors in the case of GPT4 is rather small. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Variance in human perceptual judgment data</head><p>To better understand the performance of the LMMs in relation to the ratings data obtained from human participants, we conducted two analyses to quantify the variance in the human perceptual judgment data. In the first analysis, for each dimension, we computed the correlation coefficient of the 360 ratings between each of the 20 individual subjects and the mean rating produced by the remaining 19 subjects. For example, we computed the correlation between subject 1's ratings and the mean ratings of subjects 2-20; the correlation between subject 2's ratings and the mean ratings of subjects 1 and 3-20; and so forth. We then computed the mean and standard deviation of each of these 20 individual-subject/mean-rating correlations. The results are given in the first column of Table <ref type="table" target="#tab_4">5</ref>. For ease of comparison, we also re-present the best-performing runs (i.e., with or without use of anchors) for GPT4 and Sonnet. In general, the good-fitting models' correlations are in roughly the same neighborhood as are the correlations of the individual subjects' ratings with mean ratings.</p><p>In a second analysis, we computed split-half correlations for each dimension to quantify reliability at the level of the mean ratings themselves. In this analysis, we conducted 1,000 simulations for each dimension. For each simulation, we divided the 20 subjects who gave ratings into two equally-sized random groups. We then computed the mean rating of each of the 360 rocks on that dimension for each group, and finally computed the correlation between the 360 ratings across the two groups. In Column 2 of Table <ref type="table" target="#tab_4">5</ref> we report the mean and standard deviation (across the 1000 simulations) of these split-half correlations for each of the dimensions. As can be seen, these split-half correlations are considerably higher than the ones produced by the multimodal models. In sum, the short story is that using the current technology and methods, the multimodal models appear to behave similarly to that of a single participant producing the ratings, but do not provide nearly the same reliability as is obtained when computing the ratings averaged across 20 participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interactive visualization for individual rock scores</head><p>For individual rock image results, we have developed an interactive visualization available at cognlp.com where users can view correlation plots for each feature and each condition and hover over each data point to view the specific human and model ratings for that rock. Figure <ref type="figure">6</ref> below shows an example of the interactive plots. Users select a plot for a specific condition and then select a specific feature dimension to view the correlation plot. When a specific datapoint is hovered over, rocks with that combination of scores appear, displaying the name of the specific rock image and the human and model rating for that rock.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>Recent rapid advancements in large multimodal neural networks led to the development of models that can respond to verbal prompts about image properties, demonstrating zero-shot learning. These models can also take images as a part of the input prompt, enabling one-shot and few-shot learning. It has been demonstrated that model performance can exceed human performance at many tasks, such as image classification, image captioning and visual question answering <ref type="bibr" target="#b15">(He et al., 2015;</ref><ref type="bibr" target="#b27">Li et al., 23--29</ref> Jul 2023; J. <ref type="bibr">Wang et al., 2022;</ref><ref type="bibr" target="#b65">Weihan Wang et al., 2023;</ref><ref type="bibr" target="#b66">Wenhai Wang et al., 2022;</ref><ref type="bibr" target="#b68">Yang et al., 2022)</ref>. Here we investigated whether the models are capable of mimicking humans in providing ratings of visual perceptual dimensions composing complex objects.</p><p>We compared the performance of multimodal vision and language models to human performance in a perceptual judgment task. We used an existing dataset where human participants were asked to make dimension ratings for a series of rocks along different dimensions that were found either through MDS scaling of human similarity judgment data or were deemed important for classifying the rock images into their geologically-defined categories. This dataset has been immensely valuable in cognitive science for understanding human category learning <ref type="bibr" target="#b6">(Cerone et al., 2022;</ref><ref type="bibr" target="#b31">Lu, Penney, &amp; Kang, 2021;</ref><ref type="bibr" target="#b34">Meagher &amp; Nosofsky, 2023;</ref><ref type="bibr" target="#b42">Nosofsky et al., 2020</ref><ref type="bibr" target="#b39">Nosofsky et al., , 2022;;</ref><ref type="bibr" target="#b41">Nosofsky, Sanders, Meagher, et al., 2018)</ref>. Its application with LMMs presented here constitutes a novel approach to evaluating how close the models are to human performance in terms of judging perceptual features.</p><p>Our work provides a starting tool that can be used by the research community to quantify the performance of multimodal vision and language models in communicating with human users about perceptual dimensions. Our finding that GPT4 and Sonnet models overall provided clearly better results than Haiku and Opus provides an example of the utility of this benchmark score.</p><p>Our results indicate, however, that there is room for improvement and that the benchmark is far from saturation.</p><p>human ratings for visual dimensions that could be considered elementary, such as darkness/lightness, red/green hue, chromaticity and dull/shiny (all with the correlation above 0.8). The mean correlation of individual subject ratings with mean ratings had similar values for these dimensions (Table <ref type="table" target="#tab_4">5</ref>). However, model correlations significantly dropped for more abstract and emergent dimensions such as organization and pegmatitic structure to around 0.5 and for conchoidal fracture, porphyritic texture and smoothness to around 0.7. For these more abstract and emergent dimensions, correlations of individual subject ratings with mean ratings were also lower than for more elementary dimensions. Whereas elementary dimensions like lightness or color hue might be consistent across various domains, making it easier for both humans and models to learn and transfer knowledge, more abstract dimensions might be specific to certain contexts or domains. For example, the texture of a rock might not have a direct analog in other domains that humans and models have been exposed to, leading to more variable performance in those specific dimensions.</p><p>Our results demonstrated only marginal improvement when anchor images were provided as a part of the prompt (Condition 2). For instance, the average correlation of GPT4 increased from 0.68+/-0.16 to only 0.69+/-0.13. In addition, in our explorations with two of the challenging dimensions, providing more details about the dimensions with enhanced prompts (Condition 3b) also did not lead to a notable improvement. When providing additional anchor images to GPT4 (Condition 3c) the average correlation between the two dimensions increased from only 0.49 to 0.52. This shows that improving the correlation performance for the abstract, emergent dimensions is rather challenging and that providing more information about these dimensions through enhanced prompts and additional examples does not strongly impact the results.</p><p>As technology continues to advance, and the LMMs approach human performance in perceptual judgments, they might have a transformative impact on cognitive modeling. Rigorous tests of cognitive models of memory, learning, and decision-making are often limited to simplistic stimuli spaces since modeling realistic visual inputs is rather challenging. Using the LMMs to serve as perceptual modules and extract features that a human would extract given the same visual input could enable the scaling of cognitive models to more realistic settings.</p><p>Advancements in deep neural networks have already been applied to cognitive science, leading to more general models useful in practical applications, such as decision-making in medicine <ref type="bibr" target="#b14">(Hasan et al., 2022;</ref><ref type="bibr" target="#b17">Holmes et al., 2020;</ref><ref type="bibr" target="#b47">Rahgooy et al., 2022)</ref> and economics <ref type="bibr" target="#b18">(Horton, 2023;</ref><ref type="bibr" target="#b21">Korinek, 2023)</ref>. The present research path of having the multimodal models reproduce human perceptual judgments should enhance the applications of formal cognitive models to these realworld settings even more.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Prompting pipeline for Condition 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Prompting pipeline for Condition 2.</figDesc><graphic coords="12,72.00,227.12,467.95,262.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Prompting pipeline for Condition 3c.In this example, nine anchor images with ratings of average author scores are passed to the</figDesc><graphic coords="13,72.00,223.48,467.95,262.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Scatterplots of human and GPT4 ratings in Condition 1 (without anchor images). The Pearson correlation coefficients are shown in the upper left and the regression lines are shown in red.</figDesc><graphic coords="14,72.00,97.90,468.00,200.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Mean and standard deviation of correlation between the mean rock ratings of human subjects and LMMs in conditions 1 and 2 ('_a' refers to prompts with anchor images).</figDesc><graphic coords="18,135.98,285.25,340.05,255.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="20,108.00,353.98,414.90,239.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Correlations between multimodal model ratings and averaged human ratings in Conditions 1 and 2 (with and without anchor images). For each dimension, the highest correlation among the four models in both conditions is indicated in boldface font.</figDesc><table><row><cell>dimension/model</cell><cell>GPT4</cell><cell>GPT4</cell><cell>Haiku</cell><cell>Haiku</cell><cell cols="2">Sonnet Sonnet</cell><cell>Opus</cell><cell>Opus</cell></row><row><cell></cell><cell></cell><cell>with</cell><cell></cell><cell>with</cell><cell></cell><cell>with</cell><cell></cell><cell>with</cell></row><row><cell></cell><cell></cell><cell>anchor</cell><cell></cell><cell>anchor</cell><cell></cell><cell>anchor</cell><cell></cell><cell>anchor</cell></row><row><cell>chromaticity</cell><cell>0.77</cell><cell>0.80</cell><cell>0.81</cell><cell>0.82</cell><cell>0.80</cell><cell>0.77</cell><cell>0.72</cell><cell>0.80</cell></row><row><cell>darkness/lightness</cell><cell>0.88</cell><cell>0.85</cell><cell>0.86</cell><cell>0.87</cell><cell>0.84</cell><cell>0.89</cell><cell>0.79</cell><cell>0.83</cell></row><row><cell>disorganized/organized</cell><cell>0.48</cell><cell>0.42</cell><cell>0.25</cell><cell>0.12</cell><cell>0.45</cell><cell>0.57</cell><cell>0.28</cell><cell>0.33</cell></row><row><cell>dull/shiny</cell><cell>0.81</cell><cell>0.72</cell><cell>0.40</cell><cell>0.48</cell><cell>0.75</cell><cell>0.61</cell><cell>0.33</cell><cell>0.47</cell></row><row><cell>fine/coarse grain</cell><cell>0.76</cell><cell>0.80</cell><cell>-0.02</cell><cell>0.53</cell><cell>0.61</cell><cell>0.76</cell><cell>0.25</cell><cell>0.46</cell></row><row><cell>red/green</cell><cell>0.82</cell><cell>0.78</cell><cell>0.53</cell><cell>0.71</cell><cell>0.67</cell><cell>0.83</cell><cell>0.59</cell><cell>0.82</cell></row><row><cell>smooth/rough</cell><cell>0.48</cell><cell>0.63</cell><cell>0.04</cell><cell>0.24</cell><cell>0.32</cell><cell>0.68</cell><cell>0.19</cell><cell>0.42</cell></row><row><cell>conchoidal fracture</cell><cell>0.67</cell><cell>0.71</cell><cell>0.30</cell><cell>0.53</cell><cell>0.55</cell><cell>0.70</cell><cell>0.29</cell><cell>0.43</cell></row><row><cell>pegmatitic structure</cell><cell>0.43</cell><cell>0.56</cell><cell>0.28</cell><cell>0.32</cell><cell>0.38</cell><cell>0.66</cell><cell>0.20</cell><cell>0.57</cell></row><row><cell>porphyritic texture</cell><cell>0.66</cell><cell>0.67</cell><cell>0.59</cell><cell>0.43</cell><cell>0.59</cell><cell>0.65</cell><cell>0.58</cell><cell>0.55</cell></row></table><note><p>Overall, Sonnet and GPT4 performed best, with Sonnet scoring highest on five of ten dimensions (darkness/lightness, disorganized/organized, red/green, smooth/rough, and pegmatitic structure), and GPT4 scoring highest on four of ten (dull/shiny, fine/coarse grain, conchoidal fracture, and porphyritic texture). Within the Claude-3 family, the cheapest to use model, Haiku, performed best on chromaticity, and the medium-priced model, Sonnet,</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Condition 3a: Modified prompt without anchor images.</figDesc><table><row><cell>dimension/model</cell><cell>GPT4</cell><cell>GPT4</cell><cell>Haiku</cell><cell>Haiku</cell><cell>Sonnet</cell><cell>Sonnet</cell><cell>Opus</cell><cell>Opus</cell></row><row><cell></cell><cell>original</cell><cell>longer</cell><cell>original</cell><cell>longer</cell><cell>original</cell><cell>longer</cell><cell>original</cell><cell>longer</cell></row><row><cell></cell><cell>prompt</cell><cell>prompt</cell><cell>prompt</cell><cell>prompt</cell><cell>prompt</cell><cell>prompt</cell><cell>prompt</cell><cell>prompt</cell></row><row><cell cols="2">disorganized/organized 0.48</cell><cell>0.46</cell><cell>0.25</cell><cell>0.20</cell><cell>0.45</cell><cell>0.44</cell><cell>0.28</cell><cell>0.23</cell></row><row><cell>pegmatitic structure</cell><cell>0.43</cell><cell>0.29</cell><cell>0.28</cell><cell>0.25</cell><cell>0.38</cell><cell>0.34</cell><cell>0.20</cell><cell>0.17</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Condition 3b: Longer prompt with anchor images.</figDesc><table><row><cell>dimension/model</cell><cell>GPT4</cell><cell>GPT4</cell><cell>Haiku</cell><cell>Haiku</cell><cell>Sonnet</cell><cell>Sonnet</cell><cell>Opus</cell><cell>Opus</cell></row><row><cell></cell><cell>original</cell><cell>longer</cell><cell>original</cell><cell>longer</cell><cell>original</cell><cell>longer</cell><cell>original</cell><cell>longer</cell></row><row><cell></cell><cell>prompt</cell><cell>prompt</cell><cell>prompt</cell><cell>prompt</cell><cell>prompt</cell><cell>prompt</cell><cell>prompt</cell><cell>prompt</cell></row><row><cell cols="2">disorganized/organized 0.42</cell><cell>0.47</cell><cell>0.12</cell><cell>0.10</cell><cell>0.57</cell><cell>0.55</cell><cell>0.33</cell><cell>0.32</cell></row><row><cell>pegmatitic structure</cell><cell>0.56</cell><cell>0.50</cell><cell>0.32</cell><cell>0.25</cell><cell>0.66</cell><cell>0.56</cell><cell>0.57</cell><cell>0.55</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Condition 3c: Longer prompt with author-rated images.</figDesc><table><row><cell>dimension/model</cell><cell>GPT4</cell><cell>GPT4</cell><cell>Haiku</cell><cell>Haiku</cell><cell>Sonnet</cell><cell>Sonnet</cell><cell>Opus</cell><cell>Opus</cell></row><row><cell></cell><cell>original</cell><cell>longer</cell><cell>original</cell><cell>longer</cell><cell>original</cell><cell>longer</cell><cell>original</cell><cell>longer</cell></row><row><cell></cell><cell>prompt</cell><cell>prompt</cell><cell>prompt</cell><cell>prompt</cell><cell>prompt</cell><cell>prompt</cell><cell>prompt</cell><cell>prompt</cell></row><row><cell cols="2">disorganized/organized 0.42</cell><cell>0.56</cell><cell>0.12</cell><cell>0.05</cell><cell>0.57</cell><cell>0.55</cell><cell>0.33</cell><cell>0.47</cell></row><row><cell>pegmatitic structure</cell><cell>0.56</cell><cell>0.47</cell><cell>0.32</cell><cell>0.50</cell><cell>0.66</cell><cell>0.40</cell><cell>0.57</cell><cell>0.55</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Correlation in human ratings (first two columns) and correlation between human ratings and best performing LMM runs (columns three to six).</figDesc><table><row><cell>dimension</cell><cell>Mean</cell><cell>Mean split-</cell><cell>GPT4</cell><cell>Haiku</cell><cell>Sonnet</cell><cell>Opus</cell></row><row><cell></cell><cell>individual-</cell><cell>half group-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>subject rating</cell><cell>rating</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>correlations</cell><cell>correlations</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>+/-SD</cell><cell>+/-SD</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>chromaticity</cell><cell>0.79 +/-0.07</cell><cell cols="2">0.94 +/-0.01 0.80</cell><cell>0.82</cell><cell>0.80</cell><cell>0.80</cell></row><row><cell>darkness/lightness</cell><cell>0.87 +/-0.05</cell><cell cols="2">0.97 +/-0.00 0.88</cell><cell>0.87</cell><cell>0.89</cell><cell>0.83</cell></row><row><cell cols="2">disorganized/organized 0.66 +/-0.09</cell><cell cols="2">0.89 +/-0.02 0.56</cell><cell>0.25</cell><cell>0.57</cell><cell>0.33</cell></row><row><cell>dull/shiny</cell><cell>0.80 +/-0.06</cell><cell cols="2">0.95 +/-0.01 0.81</cell><cell>0.48</cell><cell>0.75</cell><cell>0.47</cell></row><row><cell>fine/coarse grain</cell><cell>0.76 +/-0.09</cell><cell cols="2">0.94 +/-0.01 0.80</cell><cell>0.53</cell><cell>0.76</cell><cell>0.46</cell></row><row><cell>red/green</cell><cell>0.85 +/-0.03</cell><cell cols="2">0.97 +/-0.00 0.82</cell><cell>0.71</cell><cell>0.83</cell><cell>0.82</cell></row><row><cell>smooth/rough</cell><cell>0.74 +/-0.07</cell><cell cols="2">0.93 +/-0.01 0.63</cell><cell>0.24</cell><cell>0.68</cell><cell>0.42</cell></row><row><cell>conchoidal fracture</cell><cell>0.68 +/-0.10</cell><cell cols="2">0.86 +/-0.02 0.71</cell><cell>0.53</cell><cell>0.70</cell><cell>0.43</cell></row><row><cell>pegmatitic structure</cell><cell>0.67 +/-0.07</cell><cell cols="2">0.90 +/-0.02 0.56</cell><cell>0.32</cell><cell>0.66</cell><cell>0.57</cell></row><row><cell>porphyritic texture</cell><cell>0.79 +/-0.08</cell><cell cols="2">0.95 +/-0.01 0.67</cell><cell>0.59</cell><cell>0.65</cell><cell>0.58</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>In independent work conducted in parallel with the present work, Sanders (2024) has been exploring the performance of GPT4 in reproducing human judgments for a subset of 30 rock images from the current rocks-360 set. He reports results similar to those that we report in this article.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>For example, using the original prompt, GPT4 gave high ratings on the organization dimension to rocks with square crystals that were glued together in haphazard fashion, whereas humans had given low ratings to such images. Our modified prompt attempted to emphasize "global organization" rather than whether local components of the rock were well organized. As will be seen, these exploratory modifications had little effect. The wording for the modified prompts is reported in our Supplementary Materials.</p></note>
		</body>
		<back>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Availability of data and materials: Supplemental material, including all the prompts, is available at <ref type="url" target="https://osf.io/za847/">https://osf.io/za847/</ref>. The complete dataset, including rock images and participant ratings, is available at <ref type="url" target="https://osf.io/cvwu9/">https://osf.io/cvwu9/</ref>. None of the reported studies were preregistered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Code availability:</head><p>The code is available at <ref type="url" target="https://github.com/cogneuroai/multimodal-models-rock">https://github.com/cogneuroai/multimodal-models-  rock</ref> </p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A specific purpose of the present investigation was to shed light on the possibility of replacing human participants' perceptual judgment collection with ratings provided by LMMs. This line of work has recently attracted a significant amount of attention <ref type="bibr" target="#b0">(Aher et al., 2022;</ref><ref type="bibr" target="#b3">Argyle et al., 2023;</ref><ref type="bibr" target="#b8">Demszky et al., 2023;</ref><ref type="bibr" target="#b9">Dillion et al., 2023;</ref><ref type="bibr" target="#b33">Marjieh et al., 2023)</ref>. Our study is novel in that it focuses on perceptual judgments of LMMs of a specific psychological dataset composed of visual stimuli and human ratings along a large set of carefully chosen perceptual dimensions. On the one hand, our results indicated that the correlations between the goodperforming models' ratings and the mean human ratings were in roughly the same ballpark as the correlation between individual subjects' ratings and the mean human ratings. For certain research purposes, this level of performance may provide a useful starting point, such as for curating stimulus sets for use in pilot studies. However, using our current methods and technologies, the models did not produce correlations with the human data at the same level as measured in terms of split-half correlations between group means. Thus, it seems unlikely that the dimension ratings yielded by the multimodal models would provide the same predictive power of performance in independent tasks as achieved when using actual human mean ratings <ref type="bibr" target="#b34">(Meagher &amp; Nosofsky, 2023;</ref><ref type="bibr">Sanders &amp; Nosofsky, 2020)</ref>.</p><p>Inevitably, the development of LMMs will advance, and we expect their perceptual judgments to get closer to those of humans. However, our results indicate that general model capability is not a good predictor of the correlation. In particular, for Claude-3 models, previous evaluations found that Opus performs best at various benchmarks (Opus is also the most expensive of the three Claude-3 models), but in our experiments its performance was worse than that of Sonnet. However, the performance of Sonnet was better than the performance of Haiku, which is considered to be the least intelligent by Anthropic 2024. Unfortunately, because OpenAI and Anthropic did not release the exact number of parameters in their models, we were unable to perform a quantitative analysis comparing the achieved correlations and model size.</p><p>In general, the ratings obtained from multimodal models had the highest correlation with Declarations Funding: No external funding was used.</p><p>Conflicts of interest/Competing: All authors report no conflicts of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics approval: Not applicable.</head><p>Consent to participate: Not applicable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consent for publication: Not applicable.</head><p>Authors' contributions: BD, SSM, RN, and ZT contributed to study design and conceptualization, data analysis and interpretation, and drafting and editing the manuscript. BD and SSM contributed to study implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Open Practices Statement</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Using large language models to simulate multiple humans</title>
		<author>
			<persName><forename type="first">G</forename><surname>Aher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Arriaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kalai</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2208.10264</idno>
		<idno>ArXiv, abs/2208.10264</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2208.10264" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Combining convolutional neural networks and cognitive models to predict novel object recognition in humans</title>
		<author>
			<persName><forename type="first">J</forename><surname>Annis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gauthier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Palmeri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology. Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="785" to="807" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Anthropic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Out of One, Many: Using Language Models to Simulate Human Samples</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Argyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Busby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Fulda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Gubler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rytting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wingate</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political Analysis: An Annual Publication of the Methodology Section of the American Political Science Association</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="337" to="351" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Chapter One -Memorability: How what we see influences what we remember</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Bainbridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Psychology of Learning and Motivation</title>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Federmeier</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Beck</surname></persName>
		</editor>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Capturing human categorization of natural images by combining deep networks and cognitive models</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Battleday</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Software Engineering and Formal Methods</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cerone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Autili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bucaioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Graziani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Palmieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Temperini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Venture</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SEFM 2021 Collocated Workshops: CIFMA, CoSim-CPS, OpenCERT, ASYDE, Virtual Event</title>
		<imprint>
			<publisher>Springer Nature</publisher>
			<date type="published" when="2021">2022. December 6-10, 2021</date>
		</imprint>
	</monogr>
	<note>Revised Selected Papers</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Artificial Intelligence Versus Biological Intelligence: A Historical Overview</title>
		<author>
			<persName><forename type="first">R</forename><surname>De Kleijn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Law and Artificial Intelligence: Regulating AI and Applying AI in Legal Practice</title>
		<meeting><address><addrLine>The Hague</addrLine></address></meeting>
		<imprint>
			<publisher>TMC Asser Press</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="29" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Using large language models in psychology</title>
		<author>
			<persName><forename type="first">D</forename><surname>Demszky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Yeager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Clapper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chandhok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Eichstaedt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jamieson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krettek-Cobb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jonesmitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Dweck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Pennebaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Psychology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="688" to="701" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Can AI language models replace human participants</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dillion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="597" to="600" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Partial success in closing the gap between human and machine vision</title>
		<author>
			<persName><forename type="first">R</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Narayanappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mitzkus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Thieringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wichmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<idno>abs/2106.07411</idno>
		<ptr target="https://proceedings.neurips.cc/paper/2021/hash/c8877cff22082a16395a57e97232bb6f-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context</title>
		<author>
			<persName><forename type="first">Gemini</forename><surname>Team</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Teplyashin</surname></persName>
		</author>
		<author>
			<persName><surname>Dmitry</surname></persName>
		</author>
		<author>
			<persName><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Glaese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2403.05530" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>arXiv [cs.CL</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Improving alignment of dialogue agents via targeted human judgements</title>
		<author>
			<persName><forename type="first">A</forename><surname>Glaese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mcaleese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Trębacz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aslanides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Firoiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ewalds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rauh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Weidinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chadwick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Thacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Campbell-Gillingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Comanescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dathathri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Greig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<idno>arXiv [cs.LG</idno>
		<ptr target="http://arxiv.org/abs/2209.14375" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Influences of categorization on perceptual discrimination</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Goldstone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology. General</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="178" to="200" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improving medical image decision-making by leveraging metacognitive processes and representational similarity</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Eichbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Seegmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stratton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Trueblood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Topics in Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="400" to="413" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Revealing the multidimensional mental representations of natural objects underlying human similarity judgements</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Hebart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1173" to="1185" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A Joint Deep Neural Network Evidence Accumulation Modeling Approach to Human Decision-Making with Naturalistic Images</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>O'daniels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Trueblood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Brain &amp; Behavior</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Horton</surname></persName>
		</author>
		<idno type="DOI">10.3386/w31122</idno>
		<ptr target="https://doi.org/10.3386/w31122" />
		<title level="m">Large Language Models as Simulated Economic Agents: What Can We Learn from Homo Silicus</title>
		<imprint>
			<publisher>National Bureau of Economic Research</publisher>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The versatility of SpAM: A fast, efficient, spatial method of data collection for multidimensional scaling</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Hout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Goldinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Ferguson</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0028860</idno>
		<ptr target="https://doi.org/10.1037/a0028860" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">142</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="256" to="281" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multidimensional scaling: Multidimensional scaling</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Hout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Papesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Goldinger</surname></persName>
		</author>
		<idno type="DOI">10.1002/wcs.1203</idno>
		<ptr target="https://doi.org/10.1002/wcs.1203" />
	</analytic>
	<monogr>
		<title level="j">Wiley Interdisciplinary Reviews: Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="93" to="103" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generative AI for Economic Research: Use Cases and Implications for Economists</title>
		<author>
			<persName><forename type="first">A</forename><surname>Korinek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Economic Literature</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1281" to="1317" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Emerging Grounded Shared Vocabularies Between Human and Machine, Inspired by Human Language Evolution</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kouwenhoven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Verhoef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>De Kleijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Raaijmakers</surname></persName>
		</author>
		<idno type="DOI">10.3389/frai.2022.886349</idno>
		<idno type="PMID">35558168</idno>
		<idno type="PMCID">PMC9087278</idno>
	</analytic>
	<monogr>
		<title level="j">Front Artif Intell</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">886349</biblScope>
			<date type="published" when="2022-04-26">2022 Apr 26</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The features underlying the memorability of objects</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Hebart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Bainbridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science Advances</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">17</biblScope>
			<date type="published" when="2023">2023. 2981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Multidimensional Scaling</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Kruskal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wish</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978">1978</date>
			<publisher>SAGE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Determining the Dimensionality of Multidimensional Scaling Representations for Cognitive Modeling</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Psychology</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="149" to="166" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Brunskill</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Engelhardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Sabato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">&amp;</forename><forename type="middle">J</forename><surname>Scarlett</surname></persName>
		</editor>
		<meeting>the 40th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2023-07-29">23--29 Jul 2023</date>
			<biblScope unit="volume">202</biblScope>
			<biblScope unit="page" from="19730" to="19742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName><surname>Pmlr</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">How to write effective prompts for large language models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="611" to="615" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">SUSTAIN: a network model of category learning</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Love</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Medin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Gureckis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="309" to="332" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Category similarity affects study choices in self regulated learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Penney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Memory &amp; Cognition</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="67" to="82" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The universal law of generalization holds for naturalistic stimuli</title>
		<author>
			<persName><forename type="first">R</forename><surname>Marjieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jacoby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology. General</title>
		<imprint>
			<biblScope unit="volume">153</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="573" to="589" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Large language models predict human sensory judgments across six modalities</title>
		<author>
			<persName><forename type="first">R</forename><surname>Marjieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sucholutsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Van Rijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jacoby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2302.01308" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>arXiv [cs.CL</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Testing formal cognitive models of classification and old-new recognition in a real-world high-dimensional category domain</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Meagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Nosofsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="page">101596</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Attention, similarity, and the identification-categorization relationship</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Nosofsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology. General</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="57" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Tests of an exemplar model for relating perceptual classification and recognition memory</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Nosofsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology. Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="27" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Similarity scaling and cognitive process models</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Nosofsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Psychology</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="25" to="53" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Short-term memory scanning viewed as exemplar-based categorization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Nosofsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Donkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fific</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="280" to="315" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Contrasting exemplar and prototype models in a natural-science category domain</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Nosofsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Meagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology. Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1970" to="1994" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A formal psychological model of classification applied to natural-science category learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Nosofsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Sanders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Mcdaniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Directions in Psychological Science</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="135" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Toward the development of a feature-space representation for a complex natural category domain</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Nosofsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Sanders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Meagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Douglas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="530" to="556" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Search for the Missing Dimensions: Building a Feature-Space Representation for a Natural-Science Category Domain</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Nosofsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Sanders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Meagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Douglas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Brain &amp; Behavior</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="13" to="33" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">GPT4 Technical Report</title>
		<author>
			<persName><forename type="first">Achiam</forename><surname>Openai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Akkaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Aleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Altenschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Anadkat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Avila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Balcom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Baltescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Belgum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2303.08774" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>cs.CL</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Evaluating (and improving) the correspondence between deep neural networks and human representations</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Abbott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2648" to="2669" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning Transferable Visual Models From Natural Language Supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021-07-24">18-24 Jul 2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName><surname>Pmlr</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Integrating Machine Learning and Cognitive Modeling of Decision Making</title>
		<author>
			<persName><forename type="first">T</forename><surname>Rahgooy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Venable</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Trueblood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Theory of Mind for Human-Machine Teams</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="173" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Enriching ImageNet with human similarity judgments and psychological embeddings</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Roads</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Love</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr46437.2021.00355</idno>
		<ptr target="https://doi.org/10.1109/cvpr46437.2021.00355" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Nashville, TN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-06">2021, June. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Modeling Similarity and Psychological Space</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Roads</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Love</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Psychology</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="215" to="240" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Obtaining psychological embeddings through joint kernel and metric learning</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Roads</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Mozer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2180" to="2193" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A measure of stimulus similarity and errors in some paired-associate learning tasks</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Z</forename><surname>Rothkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="94" to="101" />
			<date type="published" when="1957">1957</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning and Connectionist Representations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Todd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Attention and Performance XIV</title>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Sanders</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024-05-29">2024. May 29, 2024</date>
		</imprint>
	</monogr>
	<note>Personal communication</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Using deep learning representations of complex natural stimuli as input to psychological models of classification</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Sanders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Nosofsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the Cognitive Science Society</title>
		<meeting>the 2018 Conference of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Training Deep Networks to Construct a Psychological Feature Space for a Natural-Object Category Domain</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Sanders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Nosofsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Brain &amp; Behavior</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="229" to="251" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">IllusionVQA: A Challenging Optical Illusion Dataset for Vision Language Models</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Shahgir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Sayeed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">U</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shahriyar</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2403.15952" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Stimulus and response generalization: A stochastic model relating generalization to distance in psychological space</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Shepard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="325" to="345" />
			<date type="published" when="1957">1957</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">The analysis of proximities: Multidimensional scaling with an unknown distance function</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Shepard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">I. Psychometrika</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="125" to="140" />
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Multidimensional Scaling, Tree-Fitting, and Clustering</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Shepard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">210</biblScope>
			<biblScope unit="issue">4468</biblScope>
			<biblScope unit="page" from="390" to="398" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Toward a Universal Law of Generalization for Psychological Science</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Shepard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">237</biblScope>
			<biblScope unit="issue">4820</biblScope>
			<biblScope unit="page" from="1317" to="1323" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">ModelVsBaby: A developmentally motivated benchmark of out-of-distribution object recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sheybani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">B</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tiganj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Maini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dendukuri</surname></persName>
		</author>
		<idno type="DOI">10.31234/osf.io/83gae</idno>
		<ptr target="https://doi.org/10.31234/osf.io/83gae" />
	</analytic>
	<monogr>
		<title level="m">PsyArXiv</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Predicting Similarity Ratings to faces using physical descriptions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Busey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational, Geometric, and Process Perspectives on Facial Cognition: Contexts and Challenges</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="115" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Prototype and exemplar-based information in natural language categories</title>
		<author>
			<persName><forename type="first">G</forename><surname>Storms</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>De Boeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ruts</surname></persName>
		</author>
		<idno type="DOI">10.1006/jmla.1999.2669</idno>
		<ptr target="https://doi.org/10.1006/jmla.1999.2669" />
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="51" to="73" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">GIT: A Generative Image-to-text Transformer for Vision and Language</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2205.14100" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">CogVLM: Visual Expert for Pretrained Language Models</title>
		<author>
			<persName><forename type="first">Weihan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2311.03079" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">InternImage: Exploring large-scale vision foundation models with deformable convolutions</title>
		<author>
			<persName><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings / CVPR, IEEE Computer Society Conference on Computer Vision and Pattern Recognition. IEEE Computer Society Conference on Computer and Pattern Recognition</title>
		<meeting>/ CVPR, IEEE Computer Society Conference on Computer Vision and Pattern Recognition. IEEE Computer Society Conference on Computer and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="14408" to="14419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Chain of thought prompting elicits reasoning in large language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<idno>, abs/2201.11903</idno>
		<ptr target="https://proceedings.neurips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Focal Modulation Networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2203.11926</idno>
		<idno>, abs/2203.11926</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2203.11926" />
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
