<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">OSM AND CLINICAL 1 The Open Science Movement and Clinical Psychology Training: Rigorous Science is Transparent Science</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Donald</forename><forename type="middle">R</forename><surname>Lynam</surname></persName>
							<email>dlynam@purdue.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychological Sciences</orgName>
								<orgName type="institution">Purdue University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Sbarra</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">University of Arizona</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jennifer</forename><surname>Tackett</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">Northwestern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Priscilla</forename><surname>Lui</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Riley</forename><surname>Mcdanal</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">Stony Brook University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Katherine</forename><surname>Schaumberg</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aidan</forename><forename type="middle">G C</forename><surname>Wright</surname></persName>
							<affiliation key="aff6">
								<orgName type="department" key="dep1">Department of Psychology</orgName>
								<orgName type="department" key="dep2">Department of Psychiatry</orgName>
								<orgName type="department" key="dep3">Eisenberg Family Depression Center</orgName>
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yexinyu</forename><surname>Yang</surname></persName>
							<affiliation key="aff7">
								<orgName type="department">Department of Psychology and Neuroscience</orgName>
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joshua</forename><forename type="middle">D</forename><surname>Miller</surname></persName>
							<affiliation key="aff8">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">University of Georgia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">OSM AND CLINICAL 1 The Open Science Movement and Clinical Psychology Training: Rigorous Science is Transparent Science</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9004CAA5394278266525A005762741AE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-22T20:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Clinical psychology, like other disciplines, is facing a replication/credibility crisis that undermines the evidentiary basis of our science. The Open Science Movement (OSM) offers solutions through practices such as preregistration, Registered Reports, and sharing of data, materials, and code. Clinical psychology has been slow to adopt these reforms, leaving trainees underprepared for emerging norms. This article reviews the factors that contributed to the crisis, outlines the necessary role of transparency in distinguishing rigorous from flawed research, and documents the limited uptake of open science practices in clinical journals and graduate training.</p><p>We argue that transparency is an ethical as well as methodological imperative and propose a competency-based model for embedding open science principles into doctoral education. We conclude by calling on programs, journals, and accrediting bodies to make transparency a core requirement, essential for restoring credibility and advancing a cumulative, trustworthy clinical science.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Clinical psychology, like other areas of psychology, is facing a replication or credibility crisis 1 -a recognition that the empirical literature is on shakier ground than previously thought. <ref type="bibr" target="#b28">Hengartner (2018)</ref> reviewed the state of psychotherapy research and argued that many of its published efficacy trials are riddled with biases that systematically inflate treatment effects. He concluded that the replication crisis is "no less pernicious and prevalent in clinical psychology" than in social psychology, underscoring how fragile much of the clinical evidence base may be.</p><p>These concerns about the replicability of findings and the credibility of the field gave rise to the Open Science Movement (OSM)--a broad shift toward greater transparency, rigor, and reproducibility. The OSM emphasizes structural reforms-like preregistration, Registered Reports, provision of open data, materials, and code (whenever possible) and reproducibility checks-to build more transparent, trustworthy, self-correcting, and evaluable research practices.</p><p>Unfortunately, clinical psychologists have been slow to join the OSM <ref type="bibr" target="#b35">(Korbmacher et al., 2023;</ref><ref type="bibr">Tackett et al., 2019)</ref>. This reluctance by the field of clinical psychology sets the next generation of clinical psychological scientists at a clear disadvantage.</p><p>In what follows, we explore why clinical psychology lags behind other sub-disciplines in joining the OSM, and what we can do about it. To move forward, clinical scientists must first understand the factors and practices that led to the crisis. We then outline a set of proposed solutions and discuss what they can and cannot be expected to do. Next, we propose changes for clinical psychology with a focus on doctoral training. At the heart of this proposal is this: graduate students in clinical psychology are eager for deeper training and more experience in open science <ref type="bibr">(Van Til et al., 2025)</ref>. Because transparency, which open science practices help to 1 We use the terms the replication crisis and the credibility crisis somewhat interchangeably throughout. Both refer to concerns about the trustworthiness (e.g., robustness, believability) of the published literature and of the people who produce that literature <ref type="bibr" target="#b84">(Vazire, 2018)</ref>. achieve, is a necessary condition for high-quality, ethical research, training programs that do nott address these issues, are not meeting students' needs. Finally, we consider barriers to implementing this training and outline ways these barriers might be addressed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Replication Crisis</head><p>We are approaching the 15-year anniversary of a confluence of events that catalyzed what is now termed the "replication crisis" in psychological science. The history of this crisis is well documented <ref type="bibr" target="#b51">(Nelson et al., 2018)</ref> but we revisit key moments to highlight the need for the OSM. In 2011, Bem published a series of studies seeming to indicate that precognition (i.e., the ability to predict the future) existed in one of psychology's premier journals. The fact that he could demonstrate statistically significant findings in eight of nine studies for a phenomenon most empiricists do not believe exists raised grave concerns about his use of common but problematic practices (e.g., selective reporting of studies and outcomes; outcome swapping, phacking, hypothesizing after results are known <ref type="bibr">[HARKing]</ref>; See Table <ref type="table">1</ref> for a glossary of terms used throughout this paper). Also in 2011, it was discovered that well-known social psychologist Diederik Stapel had committed extensive fraud across his publications <ref type="bibr" target="#b11">(Callaway, 2011)</ref>. Finally, in that same year, <ref type="bibr">Simmons and colleagues (2011)</ref> demonstrated the impact of a set of field-wide standard operating procedures (e.g., p-hacking) on false-positive rates. Specifically, they showed that false positive rates were strongly influenced by the researcher degrees of freedom available in most studies (e.g., decisions about when to stop data collection, whether to include covariates; selectively choosing which results to report). In 2015, the Open Science <ref type="bibr">Collaboration (2015)</ref> published a landmark report revealing alarmingly low replication rates in cognitive and social psychology. This collaborative group attempted to replicate 100 studies in which 97 had demonstrated statistically significant results in the original study. Despite generally having higher statistical power, this group found only 37 replications were statistically significant; of note, the effect sizes in the replications were roughly half that reported in the original studies.</p><p>These reports sent shockwaves through the field and forced attention on an entire suite of standard research practices and the incentive structures within the field, calling for scientific reforms to safeguard the credibility of the field.</p><p>The Four Horsemen <ref type="bibr" target="#b8">Bishop (2019)</ref> identified four key contributors to the crisis-collectively referred to as the "four horsemen of irreproducibility": low statistical power, publication bias, p-hacking, and HARKing. Although we will discuss them separately, p-hacking and HARKing fall under a broader umbrella of questionable research practices (QRPs), described in a recent paper by <ref type="bibr" target="#b50">Nagy et al. (2025)</ref>, as "… ways of producing, maintaining, sharing, analyzing, or interpreting data that are likely to produce misleading conclusions, typically in the interest of the researcher" (p. 1).</p><p>Up to 50% of researchers across scientific disciplines report engaging in QRPs, highlighting the continued urgency to address problematic research practices <ref type="bibr" target="#b50">(Nagy et al., 2025)</ref>. All four horsemen undermine the credibility of scientific conclusions and the integrity of the research process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistical Power</head><p>Statistical power, the probability of correctly rejecting the null hypothesis (H₀) when it is false in the population, is well below the recommended 80% minimum in most psychological research studies for plausible effect sizes in the field <ref type="bibr" target="#b2">(Bakker et al., 2012;</ref><ref type="bibr">Swiatkowski &amp; Dompnier, 2017</ref>). The problem with underpowered studies is not just the increased risk of Type II errors (i.e., failing to reject the null when it is false in the population); underpowered studies also distort the broader research record. Although low power does not increase the nominal Type I error rate for any given test (alpha, typically .05), it reduces the likelihood of detecting true effects. As a result, the relative proportion of false positives in the published literature increases <ref type="bibr" target="#b10">(Button et al., 2013;</ref><ref type="bibr" target="#b30">Ioannidis, 2005)</ref>. Equally important, null results from underpowered studies are uninformative-they do not provide strong evidence against the presence of an effect and cannot resolve scientific questions <ref type="bibr" target="#b41">(Maxwell, 2004)</ref>. Thus, it is difficult to publish underpowered studies that yield predominantly null findings. This often prompts additional testing and engagement in QRPs (e.g., changing outcomes, adding variables to analyses, exploring subgroups, trying alternative operationalizations) in the search for statistical significance, increasing the overall number of statistical tests being conducted. As more tests accumulatewithin a single study, across a program of research, or across the field, especially under conditions of widespread low power and publication bias-the proportion of published findings that are actually false positives rises sharply <ref type="bibr" target="#b2">(Bakker et al., 2012;</ref><ref type="bibr" target="#b30">Ioannidis, 2005;</ref><ref type="bibr" target="#b68">Simmons et al., 2011)</ref>. Finally, underpowered studies also contribute to overestimated effect sizes entering the literature, as only the inflated and statistically significant results are likely to be published <ref type="bibr" target="#b31">(Ioannidis, 2008)</ref>. <ref type="bibr" target="#b2">Bakker et al. (2012)</ref> used simulations to demonstrate how low statistical power and QRPs-such as selectively reporting significant results or flexibility in data analysis-can dramatically inflate the actual Type I error rate in psychological research. Although researchers typically assume an alpha of 0.05, their simulations showed that when multiple QRPs are used in studies with small sample sizes, the true false positive rate can easily exceed 60%. Although initial reports of power in clinical work suggest decent sample size <ref type="bibr" target="#b61">(Reardon et al., 2019)</ref>, a focus on only sample size in the context of power is incomplete as different analytic approaches have vastly different sample size requirements (e.g., main effects vs. interactive effects). In fact, the mean sample size reported by Reardon and colleagues in two premier clinical journals (Ns = 175-182) is still below the level at which basic correlations stabilize <ref type="bibr">(Schonbrodt &amp; Perugini, 2013)</ref> and far below the point where more complicated statistics like interactions stabilize <ref type="bibr">(Castillo et al., under review)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P-hacking</head><p>P-hacking involves conducting analyses on a dataset until a desired, typically statistically significant, effect is found or selectively reporting subsets of analyses that yielded desired outcomes and suppressing others that did not <ref type="bibr" target="#b68">(Simmons et al., 2011)</ref>. These practices inflate Type I error rates. In their original paper, Simmons et al. examined the impact of specific types of p-hacking-running analyses on two different dependent variables, adding observations in the face of non-significant results, controlling for gender or allowing gender to interact with treatment, and dropping (or not) one of three experimental conditions. Although each practice alone slightly inflated the probability of a Type I error above the nominal 5% level (e.g., 9.5%, 7.7%, 11.7%, and 12.6%), their combined influence led to a Type I error rate of 60.7%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hypothesizing After the Results are Known (HARKing)</head><p>HARKing <ref type="bibr" target="#b33">(Kerr, 1998)</ref> is another QRP, which occurs when a researcher present post-hoc hypotheses (i.e., a hypotheses generated after engaging with the data) as though they were a priori hypotheses. HARKing comes in many flavors including creating a hypothesis consistent with the known results, presenting one or more hypotheses known post-hoc that contradict the data (giving the false appearance of hypothesis testing), or suppressing a priori hypotheses that were not statistically significant <ref type="bibr" target="#b33">(Kerr, 1998)</ref>. All flavors allow exploratory work to masquerade as confirmatory work so as to make the work appear consistent with the hypothetico-deductive method. Unfortunately, HARKing undermines the scientific method; if hypotheses are generated from the data, rather than tested against the data, no genuine test of a theory has occurred.</p><p>HARKing also leads to Type I errors being published and translated into theory, causing the loss of valuable information related to one's original hypothesis and the presentation of biased, inaccurate models of science <ref type="bibr" target="#b49">(Murphy &amp; Aguinis, 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Publication Bias</head><p>Publication bias refers to a general bias in journals and among researchers for significant and sexy (e.g., novel; counter-intuitive) results and against null results <ref type="bibr" target="#b24">(Greenwald, 1975)</ref>.</p><p>Reluctance to publish null results from underpowered studies is understandable, but there is no reason to prevent publication of null results from high-powered studies. These biases push researchers to search for statistically significant results in their data-setting the stage for QRPs, which increase the prevalence of Type I errors in the literature. This bias also leads to the "filedrawer problem," in which researchers suppress null results themselves because they believe they will not be published, or they conflict with previously published findings <ref type="bibr" target="#b62">(Rosenthal, 1979)</ref>.</p><p>Publication bias influences the outcomes of meta-analytic work because published results may be inflated by selection bias for statistically significant findings <ref type="bibr" target="#b16">(Coursol &amp; Wagner, 1986)</ref>. These biases can lead to the expenditure of time and resources on phenomena with a façade of empirical support because null results have been suppressed (e.g., ego depletion).</p><p>The practices described above have been decried for decades. Tukey wrote about the problems with multiple comparisons 70 years ago. Cohen was writing about the problems of low statistical power in the 1960s. Greenwald wrote about bias against the null hypothesis in 1975.</p><p>And Kerr wrote about HARKing in 1998. What changed after 2011 was the growing recognition of the joint, cumulative impact of these practices which systematically distort the published literature, give rise to inflated effects, false positives, and a misleading sense of credibility.</p><p>Not only have these practices harmed the field, many of these practices (e.g., p-hacking, HARKing) are inconsistent with APA's (and others; e.g., American Statistical Association) code of ethics and should be considered unethical not just questionable <ref type="bibr" target="#b47">(Miller et al., 2025)</ref>. <ref type="bibr">Miller et al. (2025)</ref> argue that practices like HARKing and p-hacking represent clear violations of the APA Ethics Code, particularly its standards concerning integrity, scientific responsibility, and honesty. They emphasize that presenting post hoc hypotheses as if they were a priori misleads readers and reviewers, directly contravening Standard 8.10 on accurate reporting of results.</p><p>Similarly, manipulating analyses to achieve statistical significance without full disclosure violates principles of transparency and openness embedded in the general principle of Integrity.</p><p>These practices undermine trust in science, damage the cumulative knowledge base, and erode the professional obligation psychologists have to represent their work truthfully. Much as clinical psychologists work hard to avoid ethical violations in their clinical work (e.g., practicing outside one's competence; breaching confidentiality), they must work as assiduously to avoid ethical violations in their research practices. Importantly, clinical training programs must teach graduate students that such practices are unethical with the same level of attention and vigor as they instruct students about ethical practices in clinical applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Open Science Movement (OSM)</head><p>This realization that multiple practices contributed to a less robust, empirically-grounded literature in psychology catalyzed the emergence of the OSM, which advocates a set of research practices designed to enhance the transparency, reproducibility, and credibility of psychological science. These practices include preregistration (e.g., <ref type="bibr" target="#b6">Benning et al., 2019)</ref>, the use of Registered Reports (e.g., <ref type="bibr">Chambers &amp; Tzavella, 2022)</ref>, and the sharing of analytic code, statistical software output, study materials, and datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Open Science Practices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preregistration</head><p>Preregistration is among the workhorses of open science. It involves publicly documenting a study's hypotheses, methods, and analysis plans, ideally before collecting or accessing data.<ref type="foot" target="#foot_0">foot_0</ref> This process helps distinguish between confirmatory and exploratory research by creating a time-stamped, publicly accessible (or privately held) plan that can later be compared to what was actually done. Typically submitted to platforms like the Open Science Framework (OSF), a preregistration includes details such as the research question, a sample size justification, exclusion criteria, variables to be measured, statistical analyses to be used, and inferential criteria. The primary goal is to increase transparency and reduce QRPs like p-hacking and HARKing. Importantly, preregistration does not prevent exploratory work -it simply clarifies which findings were predicted in advance and which were discovered post hoc. <ref type="bibr">Benning and colleagues (2019)</ref> documented what they called the "registration continuum," noting that one can and should write registrations even while data are being collected or even after data collection is complete (but prior to analysis). Although preregistration prior to data collection is ideal, it is not always possible given the common use of archival data in clinical psychology, and there is still value to the registration of one's hypotheses and analytic plans after data have been collected. Such registrations increase transparency. Templates exist for registering studies where the data have already been collected that can help scholars navigate this process ( <ref type="bibr">Van den Akker et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Registered Reports</head><p>Registered Reports are a publishing format designed to enhance transparency and reduce publication bias by conducting peer review before data collection, or, in the case of secondary data analysis, before analyses. Researchers submit a Stage 1 manuscript that includes the study rationale, hypotheses, detailed methodology, and proposed analyses.<ref type="foot" target="#foot_1">foot_1</ref> This is reviewed by journal editors and peer reviewers, who evaluate the rigor and significance of the planned research-not the results. If accepted, the study receives in-principle acceptance (IPA), guaranteeing publication regardless of outcome, provided the researchers follow the approved protocol. After data collection and analysis, the completed manuscript (Stage 2) is submitted for final review to confirm adherence to the original plan and evaluate the interpretation of results. This formatoften referred to as a "results free" peer review-reduces publication bias and incentivizes sound research design over flashy findings. It also removes incentives for p-hacking in that authors need not worry about finding statistically significant results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Open Materials, Data, and Code</head><p>A final set of open science practices involves sharing the core components of a study-its raw data, analysis scripts, and study materials (e.g., surveys, stimuli, protocols) 4 -so others can evaluate, reproduce, and build upon the research. These materials are typically hosted on public repositories like the OSF, GitHub, or institutional archives, with appropriate documentation to ensure usability. Sharing data allows others to verify results and conduct secondary analyses; sharing code and raw output files ensures transparency in how results were derived; and sharing materials facilitates replication and adaptation in future studies. Open sharing promotes scientific credibility, accelerates progress, and aligns with growing norms and expectations in many fields and journals. Ethical considerations-such as participant privacy and intellectual property-must be addressed, but these are often manageable with de-identification, licenses, and embargoes (see <ref type="bibr" target="#b46">Meyer, 2018)</ref>. The importance of open sharing of all materials (i.e., data, code, and materials) is reflected by a recent change at the journal Psychological Science, where transparent practices, including the expectation that authors will make all primary data, original research material, and analysis scripts available in a third-party repository, are now the default <ref type="bibr" target="#b26">(Hardwicke &amp; Vazire, 2024)</ref>. These changes also involve efforts by the journal to conduct reproducibility checks, ensuring all findings reported in a paper can be independently verified. To do this well, journals will have to invest more resources to support these activities (e.g., funds for consulting editors or other positions specifically tasked with checking correspondence between preregistrations and submissions, checking reproducibility, etc.). Given the profit margins for some academic publishers, greater investment in transparency and OS practices seems entirely feasible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>What Open Science Will (and Will Not) Do</head><p>Misconceptions as to what open science practices, particularly preregistration, will and will not do abound. It is important to be clear about what the field can expect if and, hopefully, when these practices become normative. Open science practices will not necessarily improve scientific rigor or replicability; that is, they will not necessarily improve research design, increase statistical power, or prevent researchers from capitalizing on researcher degrees of freedom (although specific, detailed preregistrations can curtail this to some degree).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>What Open Science Will Not Do</head><p>Preregistration forces attention to several design issues-a priori hypotheses, power and statistical precision, decisions regarding data screening (e.g., how outliers will be treated; whether data will be screened for response validity), covariate inclusion, multiple comparisons/analyses, etcetera. Attention to these issues may improve these aspects of the design. <ref type="bibr" target="#b9">Bogdan (2025)</ref> recently reported that across the last 10 years, there has been an increase in the reporting of stronger p-values (i.e., p values &lt; .01) in every subdiscipline of psychology.</p><p>This presumably reflects an increase in statistical power and a decrease in p-hacking (or phacking to a more stringent alpha cut-off). But attention to these issues will not necessarily solve them. At best, preregistration might prevent HARKing, but this does not, by itself, lead to rigorous science. A researcher can always preregister an underpowered, poorly designed study with vague hypotheses, invalid and unreliable measures, inappropriate analyses, and non-specific inferential criteria <ref type="bibr" target="#b63">(Rubin, 2020;</ref><ref type="bibr" target="#b75">Szollosi &amp; Newell, 2020)</ref>. Preregistration is not a sufficient condition for good science.</p><p>Registered Reports, on the other hand, have a greater capacity to improve rigor <ref type="bibr">(Chamber &amp; Tzavella, 2022)</ref>. Within the Registered Reports (RRs) framework, a study's introduction, methods, and planned analyses are reviewed and a decision is reached about the manuscript. It is through the course of the revision that scientific rigor might be enhanced as the approach turns peer reviewers into informal collaborators who can guide the study authors towards more rigorous tests. Registered Reports are also the only open science practice that directly combats publication bias, as results have no bearing on acceptance. There is correlational evidence that Registered Reports are related to increased rigor <ref type="bibr" target="#b71">(Soderberg et al., 2021)</ref> and a greater prevalence of null findings--56% in RRs vs. 4% in traditional report <ref type="bibr">(Scheel et al., 2021)</ref>.</p><p>As with preregistration, making data, code, and materials available will not necessarily improve the rigor of a study. Making data and code available allows other researchers to check the code to see which analyses were actually run and to judge how appropriate these analyses were for the research questions asked. This is necessary because models are often ambiguously described and occasionally incorrect. Providing the data allows for a check on the reproducibility of the findings. Neither of these guarantees rigorous science, but they will make errors more apparent and safeguard against fraud.</p><p>Importantly, open science practices, particularly preregistration, do not prevent exploratory analyses or other methods reforms like robustness checks or multiverse analyses. In a recent critique of preregistration mandates that extends to preregistration itself, <ref type="bibr">Klonsky (2024;</ref><ref type="bibr">cf, Vize et al., 2024)</ref> suggests that preregistration might discourage or prevent exploratory analyses, multiverse analyses, robustness checks, sensitivity analyses, and so on. Klonsky writes that preregistration mandates will "establish a norm that researchers should choose one or a subset of reasonable analyses and not others" (p. 14). He further suggests that preregistration offers "a blanket reward for a selective rather than comprehensive approach to understanding data, even though the latter approach is optimal" (p. 15). Finally, he argues that if preregistration becomes the norm, it will "devalue analyses conceived after engaging with the data" (p. 15).</p><p>We believe these concerns to be overblown and that any downsides to preregistration are likely to be offset by the benefits. Moreover, we are unaware of any clinical psychology journal currently mandating preregistration, nor any proposals from the field to do so. Importantly, a number of articles have discussed how preregistration can accommodate multiverse analyses or various other robustness checks (e.g., <ref type="bibr" target="#b27">Hardwicke &amp; Wagenmakers, 2023;</ref><ref type="bibr">Chambers &amp; Tzavella, 2022;</ref><ref type="bibr">Simmons et al., 2021)</ref>. In fact, these practices are strengthened by being preregistered due to the increased transparency. Exploratory analyses and analyses conceived after engaging with the data are valid analytic approaches within the preregistration framework; they simply need to be labeled as deviations from the preregistered analytic plan<ref type="foot" target="#foot_3">foot_3</ref> (see <ref type="bibr" target="#b59">Phillips et al., 2025</ref> for clearly labelled deviations from a preregistration or O'Hara et al., 2020 for clearly labelled exploratory analyses that were not part of the a priori analytic plan). The only thing that preregistration does not allow is claiming that analyses conceived and carried out after engaging with the data were planned. There are published guidelines for how best to report deviations from preregistrations <ref type="bibr" target="#b37">(Lakens, 2024;</ref><ref type="bibr" target="#b87">Willroth &amp; Atherton, 2024)</ref>. These critiques underscore the importance of examining the principle and philosophy of open science practices rather than unnecessary reification of the practices themselves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>What Open Science Will Do</head><p>The goal of preregistration and other open science practices is to increase the transparency of research, allowing for evaluation of the severity of the test to which a given claim has been subjected <ref type="bibr">(Vize et al., 2024)</ref>. The problem in the field is not just that there is underpowered, p-hacked, and HARKed research populating our literature; the problem is that it is difficult, if not impossible, to sort the good (i.e., rigorous) from the bad research in our literature. This problem exists because we do not know how each result was arrived at.</p><p>Preregistration does not directly increase credibility; rather, it enhances transparency, which enables evaluations of credibility-specifically, whether a claim has passed a severe test. That is, transparency allows the field to see whether claims have passed severe tests, but it cannot by itself make those tests severe. It is a necessary but not sufficient condition for rigorous science.</p><p>A test is severe if it has a high probability of revealing a claim to be false when it actually is false <ref type="bibr" target="#b36">(Lakens, 2019;</ref><ref type="bibr" target="#b42">Mayo, 2018)</ref>. More broadly, the principle of severity addresses the question: When do data provide strong evidence for a claim or hypothesis? <ref type="bibr" target="#b43">(Mayo &amp; Spanos, 2011)</ref>. The short answer: more severe tests yield stronger evidence <ref type="bibr">(Vize et al., 2024)</ref>. As Mayo writes:</p><p>If data x agree with a claim C but the method used is practically guaranteed to find such agreement, and had little or no capability of finding flaws with C even if they exist, then we have bad evidence, no test (BENT)" (p. 5).</p><p>Underpowered research and practices like QRPs and HARKing undermine the severity of teststhey do not allow falsification. They lead to BENT studies where positive findings offer little to no evidential value <ref type="bibr" target="#b42">(Mayo, 2018)</ref>. Transparency allows such tests to be identified and their conclusions weighted accordingly.</p><p>The ability to transparently evaluate the severity of a test is essential, particularly because psychological theories often fail to constrain key analytic decisions (e.g., which measurement instrument to use, how to handle outliers, or which covariates to include; <ref type="bibr" target="#b36">Lakens, 2019)</ref>. Several scholars have argued that psychology's various crises-the replication crisis, the generalizability crisis, and the theory crisis-share a common root: empirical claims are frequently not based on severe tests <ref type="bibr" target="#b15">(Claesen et al., 2022)</ref>. The field might look different if researchers reported how studies were conducted, allowing others to assess the severity of the evidence provided.</p><p>We would be less inclined to give weight to papers that fished through data for significant results and retroactively wrote introductions to fit them (e.g., <ref type="bibr" target="#b4">Bem, 2002)</ref>. We would discount papers that selectively reported the only significant finding out of many tests conducted or selectively reported studies in multi-study manuscripts. We would question studies that made outlier and covariate decisions based on their influence on results. When such practices are hidden, it becomes impossible to distinguish severe tests from non-severe ones. Transparency is thus a prerequisite for progress-this is the foundation of the OSM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Slow Progress of Clinical Psychology</head><p>For a variety of reasons, clinical psychology has been slower to embrace the OSM and to adopt widespread open science practices. <ref type="bibr" target="#b79">Tackett and Miller (2019)</ref> noted that "the lack of engagement by the clinical psychology community…remains a glaring and disconcerting omission on the broader replicability landscape" (p. 487). The slow and non-urgent response may be due, in part, to the fact that some of the publications (e.g., <ref type="bibr" target="#b5">Bem, 2011)</ref> or broader debates about evidentiary value fell in other subdisciplines of psychology (e.g., ego-depletion). Although there are some features of clinical psychology that reduce some concerns (e.g., <ref type="bibr" target="#b78">Tackett et al., 2017;</ref><ref type="bibr">Tackett et al., 2019)</ref>, the aforementioned issues are present and relevant to clinical science as well (e.g., bias against the null; low power; QRPs).</p><p>In an audit of 60 leading clinical psychology journals (first and second quartiles by impact factor), Nutu and colleagues (2019) examined editorial policies across five domainspreregistration, data sharing, preprints, reporting guidelines, and conflict-of-interest (COI) disclosure. They also assessed compliance in a sample of ~200 articles from journals with supportive policies in at least four domains. While 52 journals mandated COI disclosure, only 15 cited preregistration, only a single journal mandated data sharing, and fewer than half endorsed reporting guidelines. Engagement rates were terrible: 3% of articles were preregistered, 2% shared data, only one preprint was identified, just 19% followed reporting guidelines (mostly marked non-applicable), and only about half included a COI disclosure. Although the use of Registered Reports was comparably low across journal types, personality journals showed much higher engagement in OS practices than did personality disorder journals.</p><p>The average percentage of articles sharing data in personality journals was 58.22% compared to 13.04% in personality disorder journals. A similar difference was observed in sharing analytic code; on average, 65.10% of articles in personality journals shared code compared to 13.04% in personality disorder journals. Although rates of preregistration were lower than rates of sharing data and code, the difference between journal types remained; in personality journals, 25.32% of studies were preregistered compared to 9.70% of studies in personality disorder journals. (3) member of the first two groups and the scientifically oriented Academy for Psychological Clinical Science (APCS).</p><p>Coverage in the research methods syllabi was generally thin across programs. The number of different readings on syllabi that were about open science or the replication/credibility crisis ranged from 0-17, with a median of 2 and an interquartile range of 0 to 5 (mean = 3.32; SD = 3.93). Fully 26% of programs did not have any assigned readings that fell into any of the 23 coded categories. Across all programs, the average percentage of readings dedicated to at least one of the included categories was 7.27. Disappointingly, readings criticizing open science were more frequent than readings dealing with HARKing, Registered Reports, measurement issues, fraud, incorrect reporting of results, detecting "bad science" and open access.</p><p>The accompanying graduate student survey also revealed less than optimal training. Sixty-five percent of students had not completed a preregistration. Less than 9% had been involved in a Registered Report (RR); almost 30% reported they were "not sure" what an RR was. Students also rated their exposure to various open science principles and practices using a 1</p><p>(not at all) to 5 (a great amount) scale with 3 (a moderate amount) as the midpoint, and where 77.81% and 72.18% of respondents indicated they "liked" or "strongly liked" learning about each. We are doing our future colleagues a disservice if we are not preparing them for the most rigorous and exacting standards and norms of future psychological science.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>What Has to Change</head><p>We believe that there are a variety of changes required to build a more robust and trustworthy clinical science literature, which include the adoption of various open science practices. Change is needed at all levels: student, faculty, programs, and institutions. Most of these changes are not specific to clinical psychology but apply to the entire field; however, if modifications to these approaches are needed for clinical psychology, they will only happen if clinical scientists broadly engage in these conversations and efforts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Student Training: Toward Assessing Open Science Competencies</head><p>As evidenced by the results of <ref type="bibr">Van Til et al. (2025)</ref>, exposure to the factors contributing to the replication crisis and to OS principles and techniques in graduate training is inadequate. To avoid the practices that contribute to replication failures (i.e., low power, p-hacking, HARKing, and publication bias), one must understand and be able and willing to recognize them in one's own work. One of the more insidious features of these factors is that they are often committed without malevolent intent through subtle, psychologically motivated processes <ref type="bibr" target="#b50">(Nagy et al., 2025)</ref>. Practices like HARKing or p-hacking frequently feel justifiable to researchers because of motivated reasoning <ref type="bibr">(Lund et al., 2018)</ref>, the garden of forking paths (Gelman &amp; Loken, 2014), moral blind spots (Ellemers, 2018), and/or practical considerations such as null findings in an underpowered study that preclude publication (and thus creates incentives to "find" a statistically significant result to enables publication). As a result, researchers may remain unaware that they are engaging in practices that compromise the integrity of their science <ref type="bibr" target="#b19">(Fiedler &amp; Schwarz, 2016)</ref> and may be ethical violations <ref type="bibr" target="#b47">(Miller et al., 2025)</ref>.</p><p>The fallacy of expertise/training/seniority should be familiar to clinical psychologists; much of the work on statistical versus actuarial judgment underscores the importance of maintaining a "beginner's mind" (e.g., <ref type="bibr" target="#b25">Grove et al., 2000)</ref>. Related research finds that published academics across many fields (including psychology) routinely make fundamental errors in their interpretation of p-values; whereas, undergraduate students with no formal statistical training do not make such errors <ref type="bibr" target="#b45">(McShane &amp; Gal, 2016)</ref>. These dynamics underscore the need for explicit training in research design, inference, and OS norms <ref type="bibr" target="#b48">(Munafò et al., 2017;</ref><ref type="bibr" target="#b83">Vazire, 2017)</ref>. <ref type="bibr" target="#b50">Nagy et al. (2025)</ref> recently published a bestiary of 40 consensually defined questionable research practices; familiarizing students with this taxonomy, along with a discussion of their ethical status and scientific consequences, might mitigate against these practices.</p><p>Our training must include much more attention to the issue of transparency and open science practices. These practices are often understood as a mere set of technical procedurespreregistration, data sharing, registered reports-akin to using regression or checking reliability.</p><p>But this is an incomplete understanding. Open science is not a statistical technique or a specific method; it is not merely a set of procedures. It is a commitment to a meta-principle of transparency that underlies the entire scientific enterprise. Just as falsifiability is a foundational criterion for scientific claims, transparency is foundational for evaluating the credibility of those claims. Practices like preregistration, data sharing, and code availability are instantiations of this principle, and it is essential that they be done well. These practices do not belong to a specific methodological school or type of analysis-they apply to all scientific inquiry <ref type="bibr" target="#b48">(Munafò et al., 2017;</ref><ref type="bibr" target="#b52">Nosek et al., 2015)</ref>. Even methodologically sophisticated work can be subject to bias, rationalization, and error <ref type="bibr" target="#b19">(Fiedler &amp; Schwarz, 2016;</ref><ref type="bibr">Gelman &amp; Loken, 2014)</ref>, underscoring the need for transparency in all cases. Thus, training in open science is not a niche skill-it is core scientific training. From this perspective, we find it deeply problematic that some investigators state, simply, that they "do not do open science" (in their laboratories) or they object to transparency; this position stands in direct contrast to a belief that one is engaging in high-quality science. There can be no rigorous science without transparency. These issues-both the factors contributing to the replication crisis and the principles and tools of open science-must be mastered. We suggest this will require far more than two readings in our research methods classes <ref type="bibr">(Van Til et al., 2025)</ref>. <ref type="foot" target="#foot_4">6</ref>One way to think about the training needs in this area is to consider student competencies and the explicit scientific skills that are needed to practice open science. This end-point training goal has the benefit of moving beyond knowledge alone and moving from intention to application. From this perspective, the floor of open science graduate training is a knowledgebased curriculum whereas the ceiling is determined through training-related activities and requirements that compel students to engage in increased transparency. As noted above, simply preregistering a study does not ensure its rigor. These activities will need to be completed in a mentored fashion, and all doctoral mentors have experience supervising dissertation proposals and critiquing the depth of these documents. The same degree of supervision should be applied to students' open science competencies, which also requires a commitment from faculty mentors to learn and adopt these practices. Table <ref type="table">2</ref> lists a set of core competencies that we believe are necessary and may be sufficient for training students in better open science practices. As illustrated in the table, we pair each of these competencies with a set of readings and propose program-related activities that could be used to promote and evaluate students' proficiency in the specific behavioral skills.</p><p>At the student level, implementation can be incentivized by requiring students to preregister at least one required project during their graduate training; several of the current authors have been able to implement this requirement within their areas to some modest effect.</p><p>End of semester grants/papers assignments in core courses (e.g., Research Methods; Psychopathology) can be replaced with end of semester preregistrations; two of the current authors implement this in their research methods classes. <ref type="foot" target="#foot_5">7</ref> Student awards can be established; these are already established at several programs that might serve to further motivate meaningful student engagement in open science practices (Supplemental Table 1 provides potential criteria for such an award).</p><p>Training in open science will also prepare students to navigate the recently revised Transparency and Openness Promotion (TOP) Guidelines-a framework for journals to design and implement publication standards that reflect the values of transparency, rigor, and reproducibility. The 2025 revision of the TOP Guidelines <ref type="bibr" target="#b23">(Grant et al., 2025)</ref> organizes standards into three overarching categories: research practices, verification practices, and verification studies. The research practices category, which remains the centerpiece of the framework, includes seven specific standards: citation, data transparency, code transparency, materials transparency, design and analysis transparency, study registration, and analysis plan registration.</p><p>These components reflect the key domains in which transparent reporting is expected and provide clear guidance about what openness looks like in practice. Each standard includes three levels: Level 1 requires disclosure (e.g., stating whether data are shared), Level 2 requires the behavior (e.g., actually sharing the data), and Level 3 requires verification (e.g., confirming that the data permit analytic reproducibility). Together, these levels create a progressive model that distinguishes between mere intention, execution, and verifiability-an approach that can be directly mapped onto graduate training programs committed to fostering open science behaviors.</p><p>Our experience is that the TOP Guidelines can be used as instructive training tools for students to "reverse engineer" what high-fidelity open science looks like in practice; in much the same way we may use treatment fidelity rating scales can be used to enhance psychotherapy training, the TOP Guidelines can be used for pedagogical purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation</head><p>Beyond exposure and training in courses, students need to implement these practices in their own work. These efforts will likely be of limited utility, however, if graduate students return to labs where open science is not actively supported or prioritized-and worse, opposed-by faculty. The most effective way for faculty to support open science practices among their students is to model it in their own research. At a minimum, faculty can create space for students to engage in these practices independently. Many mentors already engage with their students in behaviors that are preregistration adjacent through dissertation and thesis proposals; posting those documents to the OSF is a natural next step. Likewise, incorporating consent form language that allows for data sharing, or uploading final analysis code, are easy entry points that provide structure and practice without much cost. <ref type="bibr" target="#b32">Kathawalla et al. (2021)</ref> offer a clear, concrete guide for how students and mentors can begin incorporating these practices, with strategies tailored for low-resource labs and resistant environments. Their focus on small, feasible steps makes the process approachable-even for faculty who are just getting started or who remain unsure about how to proceed.</p><p>At the departmental and institutional levels, meaningful change will require a reorientation of incentive structures to explicitly value transparency. Currently, faculty hiring, merit evaluations, and promotion decisions tend to reward grant funding, media visibility, and publications in premier journals-often with little consideration of how the research was produced <ref type="bibr">(Nosek et al., 2012;</ref><ref type="bibr" target="#b30">Ioannidis, 2005)</ref>. Said differently, in terms of variance explained, institutional incentive structures that reward these activities are arguably the strongest drivers undermining scientific credibility (e.g., <ref type="bibr" target="#b69">Smaldino &amp; McElreath, 2016)</ref>. This dynamic sends a clear message: flashier or grant funded work is better work, regardless of its reproducibility or rigor. Transparency itself must be treated as a core academic contribution and, as mentioned above, not a niche area or specialty pursuit. Practices such as preregistration, sharing open data, code, and materials, and Registered Reports should be directly recognized in hiring and promotion decisions <ref type="bibr" target="#b48">(Munafò et al., 2017;</ref><ref type="bibr" target="#b13">Chambers, 2019)</ref>. Faculty applicants should be asked to talk about these issues in job talks-whether findings come from studies that were preregistered and followed other OS practices. Applicants who are not yet engaged in such practices can be asked about future plans in this vein.</p><p>Departments should revise evaluation rubrics to include transparent practices as evidence of research quality; with these revisions, transparency is not merely personal choice, it is a scientific job requirement. Institutions should also provide infrastructure and training to support these practices. Unless transparency is institutionally incentivized-on par with publications and funding-it will remain an individual choice rather than a collective norm. An excellent example is the University of Maryland's Toolkit for Aligning Incentives 2.0 <ref type="bibr" target="#b18">(Dougherty et al., 2024)</ref>, which provides a variety of suggestions for reforming evaluation policies in line with values like openness, equity, and rigor. The toolkit offers factsheets and concrete guidance for departments seeking to revise hiring, tenure, and promotion practices to reward behaviors that reflect the core academic mission of contributing to the public good. Among its recommendations are the inclusion of transparent, reproducible, and publicly accessible research outputs in evaluation rubrics, as well as recognition of community-engaged scholarship and value-aligned teaching.</p><p>The toolkit provides a values-to-behavior worksheet that maps institutional principles-such as accessibility, rigor, and inclusivity-onto specific, observable faculty contributions. This kind of structure helps departments operationalize values in concrete and evaluable ways. Without such reforms, faculty remain subject to limited metrics (e.g., publication counts, impact factors) that do not necessarily reflect research quality and may even actively disincentivize transparency.</p><p>Aligning incentives with core scholarly values is not just about fairness-it is essential for restoring credibility in psychological science and ensuring its contributions are both trustworthy and socially meaningful. Another relevant resource departments and universities may consider is the San Francisco Declaration on Research Assessment (see: <ref type="url" target="https://sfdora.org/">https://sfdora.org/</ref>), which provides guidelines for holistic assessments of research performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Barriers to Implementation</head><p>Presently, none of the above suggestions are implemented widely. In general, students are not being trained in the replication crisis or open science practices <ref type="bibr">(Van Til et al., 2025)</ref>. Most clinical psychology faculty are not engaging in these practices in a routine or meaningful manner. We are aware of very few programs that have incorporated open science into their hiring, merit, or promotion rubrics. Obviously, there are barriers that must be overcome.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cultural Resistance</head><p>One huge barrier to the broader adoption of open science practices is cultural resistance within the academic community, particularly among more senior scholars. These researchers often built careers under different norms-ones that rewarded (or at least allowed) flexibility in data analysis, prioritized novel findings, and rarely required transparency. For some, open science is experienced not as an invitation to improve how science is conducted, but as a rebuke of earlier practices that were once fairly normative. This resistance occasionally surfaces in public commentary, such as Fiske's (2016) editorial decrying "methodological terrorists" and characterizing early OS advocates as overly hostile critics. Senior scholars may be hesitant to question the practices and assumptions that supported their earlier success <ref type="bibr" target="#b36">(Lakens, 2019;</ref><ref type="bibr" target="#b84">Vazire, 2018)</ref>. Additionally, researchers embedded in high-prestige institutions may feel less pressure to adopt open science practices, either because their success insulates them from changing norms or because they do not see clear incentives to do so <ref type="bibr" target="#b3">(Bakker et al., 2020;</ref><ref type="bibr" target="#b53">Nosek &amp; Bar-Anan, 2012)</ref>. Until these attitudes shift or institutional structures (e.g., journals, funders, hiring and promotion committees) create clearer expectations-reform is likely to remain partial and uneven.<ref type="foot" target="#foot_6">foot_6</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Underpowered Research</head><p>A major barrier to the adoption of open science practices lies in research contexts where large samples are difficult to obtain. Studies involving rare populations-such as individuals with low-base-rate disorders-or costly methodologies (e.g., neuroimaging; biological assays)often operate with constrained sample sizes. When combined with the typically small effect sizes found in psychology <ref type="bibr" target="#b21">(Gignac &amp; Szodorai, 2016;</ref><ref type="bibr" target="#b65">Schäfer &amp; Schwarz, 2019)</ref>, these constraints result in chronically underpowered studies. As noted earlier, low statistical power not only increases the likelihood of Type II errors but also inflates effect size estimates and raises the false positive rate among significant findings <ref type="bibr" target="#b10">(Button et al., 2013;</ref><ref type="bibr" target="#b30">Ioannidis, 2005)</ref>. Recent largescale analyses have underscored the severity of this issue: <ref type="bibr" target="#b40">Marek et al. (2022)</ref>, using data from the Adolescent Brain Cognitive Development (ABCD) study, demonstrated that brain-behavior correlations in neuroimaging are often much smaller than previously assumed, and that thousands of participants are typically required to achieve adequate power in typical between subjects studies. This makes clear that most published MRI studies-often conducted in the context of single labs via R01 grant mechanisms that support samples of fewer than 100 participants-are drastically underpowered. In such contexts, researchers may be reluctant to preregister analyses, commit to sharing data, or submit to the rigor of Registered Reportsfearing that transparent methods will expose a fragility in their results. In this context, open science practices may be experienced as existential threats. It is these factors that appear to underlie the results of Bogdan (2025) who found less increase in stronger p-values at highly ranked research universities; the authors reported that two-thirds of the association could be explained by highly ranked universities preferring laborious, expensive, and subtle research topics where effect sizes are likely to be exceedingly small. Addressing this barrier requires structural solutions-such as multi-site collaborations, shared data repositories, and alternative evaluation criteria-that make transparent science feasible even under difficult constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Funding</head><p>With an entire institute devoted to funding mental health research (NIMH) and two others devoted to funding research on specific types of mental health problems (NIDA and NIAAA) the grant culture exerts a strong influence on the field, and not always for the better. Lilienfeld (2017) suggested that the grant culture is a primary contributor to the replication crisis. He was hopeful that the replication crisis might serve as a necessary reckoning rather than a disaster, marking a shift toward greater humility and methodological rigor in psychological science. But he argued that the deeper threat lies elsewhere-in the institutional pressures created by the grant culture. The relentless focus on securing funding distorts priorities, rewarding fundraising over meaningful contributions and pushing researchers toward questionable practices, narrow lines of inquiry, and safe, fundable topics. 9 It fosters confirmation bias, intellectual hyper-specialization, and overpromising, all while leaving little time for the kind of deep, reflective thinking that fuels real insight. He felt that unless psychology grapples seriously with these structural distortions, the field's self-correction will remain superficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistical Training</head><p>Applied (e.g., clinical) researchers may be weaker quantitatively and may not fully appreciate how the horsemen work to undermine the field <ref type="bibr">(Aiken et al. 1990</ref><ref type="bibr" target="#b1">(Aiken et al. , 2008))</ref>. To fully appreciate the corrosive effect the horsemen have on the literature requires an understanding of sampling variability, the probabilistic nature of our statistics, statistical power, the problems caused by multiple testing (within a study, across a program of research, or within the field), and the influence of selection (i.e., publication bias). These are not simple ideas, and most people will require multiple exposures (e.g., multiple methods/statistics classes) to fully grasp them.</p><p>Scholars at all levels of expertise and training must commit to an ongoing dialogue, in the spirit of scientific humility and self-improvement. In this way, senior scientists not only catch up so as to provide students and junior scientists the level of scientific training they require, but further serve as role models for the practice of continuous self-improvement and self-awareness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sensitive Data</head><p>Another persistent barrier to open science is the challenge of sharing sensitive data, 9 A recent news story noted that Duke University School of Medicine will begin actively reducing salaries of faculty who are not sufficiently successful at securing external funding, despite the incentives this creates for the use of problematic research practices and a political landscape where external funding is increasingly limited, especially for certain topics. <ref type="url" target="https://www.dukechronicle.com/article/2025/07/duke-university-school-of-medicine-implements-faculty-productivity-guidelines-external-grant-funding-requirements-salary-reductions">https://www.dukechronicle.com/article/2025/07/duke-university-school-of-medicine-implements- faculty-productivity-guidelines-external-grant-funding-requirements-salary-reductions</ref> .</p><p>particularly in clinical, neuropsychological, and developmental research. These datasets often contain personally identifiable or protected health information, raising legitimate concerns about participant confidentiality and regulatory compliance (e.g., HIPAA, GDPR). Even when data are de-identified, the risk of re-identification-especially in small or richly detailed samples-can be non-trivial. As a result, many researchers working with clinical populations perceive data sharing as either infeasible or ethically fraught. This has created a structural disincentive: those working in high-risk or high-stakes domains are less able to meet open data expectations and may feel unfairly penalized in a reform movement that sometimes assumes a one-size-fits-all standard.</p><p>But there are solutions. Controlled-access repositories, data use agreements, and synthetic datasets offer pathways to balance transparency with privacy <ref type="bibr" target="#b22">(Gilmore et al., 2018)</ref>. To lower barriers, institutions and journals must provide clear, realistic guidance on responsible data sharing, including flexible models tailored to sensitive contexts. Otherwise, the push for transparency risks privileging low-risk, convenience-sample research-ironically reinforcing some of the weaknesses Open Science was meant to fix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Vision for the Future</head><p>The field needs to recognize this truth: to be credible, research must be transparent. Other researchers need to have access to the decisions and processes that led to the conclusions. As noted earlier, transparency does not guarantee important or rigorous science. One can write a Registered Report on a trivial topic. One can preregister a horribly flawed study with inappropriately small samples, invalid measures, and fundamentally mismatched analytic approaches. But one cannot have credible science without transparency because to evaluate rigor-the severity of a test-one requires access to the choices the scientist makes along the way. Credible science is important in all domains of psychology but particularly so in clinical-all stakeholders must know the truth as to the reliability and validity of our findings for assessments, therapy modalities, studies of etiology, and so on. These are important issues with practical ramifications for actual people. Ethical research practices call for transparency.</p><p>We look forward to increased training in the replication crisis and open science practices.</p><p>Although it takes some training and practice to reach full competencies in such approaches, beginning the process is easy. Resources (e.g., readings, lectures, templates, examples) abound.</p><p>Preregistration does not require much additional time. Students are already completing variants of Registered Reports vis-a-vis thesis and dissertation proposals. Many faculty members are already engaged in preregistration-adjacent work in the way of grant, thesis, and dissertation proposals. Data and code already exist on our own computers and servers. What is required is a commitment to transparency because it is important to the field and the right thing to do, and because the next generation of clinical psychological scientists will only be trained and equipped to meet the demands of modern science if we do so.</p><p>We look forward to more journals following the lead of Vazire and her team at Psychological Science <ref type="bibr">(Hardwicke &amp; Vazire, 2023)</ref> who are making transparency the default and not merely an option. Top journals should not just encourage transparent practices-they should require them.</p><p>The same is true for reviewers at these journals; one needs to know how results were arrived at in order to fully evaluate the rigor of a study. If authors refuse to engage in practices that increase transparency, that should preclude publication in these journals. Publication in the top journals should be available only to research that is transparent; after all, transparency is a necessary condition for good science. There may be some issues specific to clinical psychology that make some OS practices less feasible (e.g., the sharing of sensitive data), but other OS practices are still available (i.e., preregistration, Registered Reports, sharing of code). Minimally, all journals can move to higher levels of the TOP guidelines.</p><p>We hope that psychology departments in the US in general and clinical psychology programs more specifically will pivot towards valuing transparency among faculty and graduate students. This will involve (a) encouraging evaluations of open science practices for all job candidates across the field, and (b) building OS evaluation criteria into promotion and tenure evaluations. Clinical graduate training programs should bake these practices into their training in the same way they require competencies in clinical work, research, interpersonal and professional skills-these should be evaluated regularly along with other competencies.</p><p>However, this requires a faculty body that values and incorporates these approaches into their own work and models this for early career researchers both formally and informally. As with many things, the younger generation is well ahead of us-the survey results from <ref type="bibr">Van Til (2025)</ref> indicate that our graduate students think these topics are important for science and critical to their training. Our call for change in clinical psychology is not only a plea to improve methodological rigor, but also an ethical imperative <ref type="bibr" target="#b47">(Miller et al., 2025)</ref> and a topic we have failed to address for far too long.</p><p>Finally, we call on accrediting bodies-the American Psychological Association (APA) and the Psychological Clinical Science Accreditation System (PCSAS)-to give these issues the attention they deserve. These organizations are among the most influential levers for change in the field. When accrediting bodies introduce new requirements, programs respond. By incorporating open science and transparency standards into accreditation criteria, they could accelerate the adoption of practices that strengthen the credibility and integrity of clinical psychology.</p><p>the journal system. PLOS <ref type="bibr">ONE,</ref><ref type="bibr">17(8)</ref>, e0272808. <ref type="url" target="https://doi.org/10.1371/journal.pone.0272808">https://doi.org/10.1371/journal.pone.0272808</ref> Yarkoni, T. (2018, October 2). No, it's not the incentives-it's you. [citation needed]. Retrieved August 12, 2025, from <ref type="url" target="https://talyarkoni.org/blog/2018/10/02/no-its-not-the-incentives-its-you/">https://talyarkoni.org/blog/2018/10/02/no-its-not-the-incentives-  its-you/</ref> Table 1. Glossary of Terms and Impact Term Definition Impact Low statistical power Power = probability of correctly rejecting H0 Effect sizes in the literature are overestimates; increases Type II errors; increases proportion of false positives in the literature; prompts additional testing/QRPs p-hacking conducting multiple analyses on a dataset until a desired, typically statistically significant, effect is found or selectively reporting subsets of analyses that yielded desired outcomes while suppressing others that did not Undermines scientific credibility by inflating effect sizes and increasing the likelihood of false positives. HARKing Presenting a hypothesis generated after engaging with the data as though it were an a priori hypothesis Allows exploratory work to masquerade as confirmatory work which provides unwarranted credibility; fails to provide a test of the hypothesis; pushes Type I errors into literature; loss of information about original hypothesis Publication bias General bias among editors, reviewers, and authors for statistically significant and/or novel results and against null findings Pushes researchers to find statistically significant results in their data which sets the stage for QRPs; pushes Type I errors into the literature; keeps disconfirming evidence out of the literature; undermines meta-analytic work QRPs Broad umbrella term for methods of collecting, analyzing, interpreting, or reporting data that increase the likelihood of misleading conclusions, often serving the researcher's interests. Includes specific p-hacking practices, HARKing, and other methods Increased Type I error rates, overestimates of effect size; Allows exploratory work to masquerade as confirmatory work which provides unwarranted credibility Transparency The comprehensive documentation and sharing of research procedures, data, and analytic decisions, allowing others to trace how findings were produced. Increases reproducibility; allows evaluation of the strength and credibility of the research Preregistration The process of publicly Distinguishes exploratory work from documenting a study's research questions, hypotheses, design, and planned analyses before data collection or analysis begins. confirmatory work; prevents HARKing; limits flexibility in data analysis (QRPs); increases evaluability Registered Reports A publishing format in which the study's rationale, design, and analysis plan are peer-reviewed and accepted before data collection. If accepted, the journal commits to publishing the results regardless of outcome, provided the authors follow the approved protocol. Distinguishes exploratory work from confirmatory work; prevents HARKing; limits flexibility in data analysis (QRPs); may improve rigor (including statistical power); overcomes publication bias Open materials Sharing the stimuli, surveys, codebooks, or other materials used to conduct a study evaluate, replicate, or adapt the research procedures Open data Making the raw or cleaned data from a study publicly available in a usable format, with sufficient documentation to allow others to understand and reanalyze it facilitates reproducibility; allows identification of statistical errors Open code Providing access to the analysis code (e.g., R scripts, SPSS syntax) used to process and analyze the data Facilitates reproducibility workflow; code review swap-in class activity to reproduce code with a published paper. Familiarity with FAIR data practices &amp; repository use <ref type="bibr" target="#b22">Gilmore et al., 2018;</ref><ref type="bibr" target="#b70">Soderberg, 2018;</ref><ref type="bibr" target="#b81">van Ravenzwaaij et al. (2025)</ref> Workshop to create a dataset 'FAIR checklist': metadata, codebook, licenses; deposit a de-identified demo dataset in OSF with a landing page.</p><p>Deposited dataset with README, codebook, variable dictionary, license, and identifier; passes a FAIRness spot-check rubric. Navigation of the preprint ecosystem <ref type="bibr" target="#b32">Kathawalla et al., 2021;</ref><ref type="bibr" target="#b89">Wingen et al., 2022;</ref><ref type="bibr">PsyArXiv overview</ref> Draft &amp; post a preprint with clear 'status' and versioning; add lay summary and limitations to preprint page.</p><p>Live preprint with DOI; version history; public-facing summary communicating peer-review status and caveats. Knowledge of data-sharing best practices &amp; experience sharing data <ref type="bibr" target="#b46">Meyer, 2018;</ref><ref type="bibr" target="#b22">Gilmore et al., 2018</ref> Draft data-sharing plan for an IRB protocol; generate de-identification workflow; publish an anonymized teaching dataset with usage notes. IRB-ready data-sharing section; published data package with deidentification rationale and reuse notes.</p><p>Critical evaluations of openscience claims <ref type="bibr" target="#b63">Rubin, 2020;</ref><ref type="bibr">Szollosi et al., 2020;</ref><ref type="bibr" target="#b17">Devezer et al., 2019</ref> Structured debate: advantages, limitations, and scope conditions of preregistration /RRs/replication.</p><p>Active participation in open debate; paper summarizing pros and cons. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Awareness of incentives &amp; institutional policy structures</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Measurement issues / Reliability</head><p>• Flake, J. K., &amp; Fried, E. I. (2020). Measurement schmeasurement: Questionable measurement practices and how to avoid them. Advances in methods and practices in psychological science, 3(4), 456-465.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Practices that work against error detection</head><p>• Wicherts, J. M., <ref type="bibr">Bakker, M., &amp; Molenaar, D. (2011)</ref>. Willingness to share research data is related to the strength of the evidence and the quality of reporting of statistical results.</p><p>PLoS <ref type="bibr">ONE,</ref><ref type="bibr">6(11)</ref>, e26828.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistical significance testing</head><p>• Lakens, D., Scheel, A. M., &amp; Isager, P. M. (2022). Justify Your Alpha: A Primer on Two Practical Approaches. Advances in Methods and Practices in Psychological Science, 5(2), 25152459221080396 • McShane, B. B., Gal, D., Gelman, A., Robert, C., &amp; Tackett, J. L. (2019). Abandon Statistical Significance. The American Statistician, 73(sup1), 235-245. <ref type="url" target="https://doi.org/10.1080/00031305.2018.1527253">https://doi.org/10.1080/00031305.2018.1527253</ref> Incorrect reporting of results • Nuijten, M. B., Hartgerink, C. H. J., Van Assen, M. A. L. M., Epskamp, S., &amp; Wicherts, J. M. (2016). The prevalence of statistical reporting errors in psychology (1985-2013).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc>In a recent editorial on policies and engagement in Open Science at Clinical Psychological Science,<ref type="bibr" target="#b29">Howard et al. (2024)</ref> reported on the percentages of articles receiving badges for preregistration, open data, and open materials. Importantly, this journal has been awarding badges for 7 years and instituted Open Science as an editorial policy three years ago. Following an initial uptake in badges across the first three years (from 2017 to 2019), rates have remained relatively flat for open data and materials and risen slightly for preregistration. Importantly, this plateau occurred at the start of a new editorial term during which formal badge adjudication occurred for the first time in a clinical psychology outlet. This indicates that badges alone (without adjudication) may be invalid indicators of adherence to OS practices-motivation and interest is not enough, precision and accuracy of open science practices is also needed to ensure they are promoting the intended principles. From 2020 to 2023, approximately 26% of articles included open data while 20% included open materials. Across this same period, the percentage of articles receiving badges for preregistration increased from 12% in 2020 to a high of 27% in 2022 but fell in 2023 to 16%. From 2020 to 2023, approximately 35% of articles received at least one badge. The field is not doing well when fewer than 40% of articles are receiving at least one badge at a journal that established open science practices and principles as a primary priority area for submissions. Vize and Lynam (in press) compared the use of OS practices from 2021-2023 in two personality disorder journals (Journal of Personality Disorders and Personality Disorders: Theory, Research, and Treatment) to these practices in three personality journals (Journal of Personality, Journal of Research in Personality, and European Journal of Personality).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><figDesc>This lack of engagement is reflected in the training that aspiring clinical psychologists receive as graduate students. In a recent Registered Report,Van Til et al. (2025)  examined the state of graduate training in the replication crisis and OS practices in clinical psychology doctoral programs in the United States. This study included two sources of information-a review of research methods syllabi used in clinical psychology doctoral programs and a clinical psychology graduate student survey. The authors received syllabi from 112 clinical psychology programs and were able to code 104 of these; readings were coded into 23 different categories dealing with the replication crisis or open science methods. For the graduate student survey, Van Til and colleagues attempted to collect data from 100 doctoral students in each of three bins -(1) member of an APA accredited program, (2) member of an APA accredited program and a member of the Council of University Directors of Clinical Psychology (CUDCP) programs, and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><figDesc>those exposures occurred. Regarding the latter question, students endorsed greater exposure to all investigated open science-related principles and practices in their research methods class compared to the aggregate of the other queried sources (i.e., other classes, colloquia, discussions with advisor, social media, other), indicating that the limited exposure in methods classes was not being compensated for in other places. In terms of amount of exposure to open science topics, participants' exposure ranged, on average, from "none at all" to "a moderate amount." Exposure was highest for basic methodological concepts that are not new or specific to open science such as statistical significance (mean = 3.25) and reliability/measurement (mean = 3.05) and lowest for specific OS practices such as RRs (mean = 1.44) and preregistration (mean = 2.00). Notably, most of the comparisons across the three training categories (APA membership only; APA and CUDCP membership; APA, CUDCP and APCS membership) were not significant, suggesting that more scientifically oriented training programs are not doing more of this OS training than more clinically oriented programs. The most encouraging results from the survey were in relation to attitudes towards learning about the replication crisis and open science,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fraud•</head><figDesc>Nosek et al., 2012; TOP Guidelines 2015; TOP 2025 update; DORA Policy audit: evaluate your program/department/journal against TOP/DORA; propose a feasible incentive (e.g., OS badges, RR adoption) and an implementation plan. Brief policy memo benchmarking against TOP/DORA and recommending a specific change with timeline and stakeholders; student-led self-study of program promotion criteria in light of DOR guidelines. No (0)/Yes (1) Supplemental Table 2 Potential Readings in Each Coding Category Identified by Van Til et al. (in press) generated by members of the Open Science Committee of the Academy of Psychological Clinical Science Factors Contributing to the Replication Crisis Documenting the replication crisis • Bakker, M., Van Dijk, A., &amp; Wicherts, J. M. (2012). The rules of the game called psychological science. Perspectives on Psychological Science, 7(6), 543-554. • Bem, D. J. (2011). Feeling the future: Experimental evidence for abnormal retroactive influences on cognition and affect. Journal of Personality and Social Psychology, 100, 407-425. • Bishop, D. (2019). Rein in the four horsemen of irreproducibility. Nature, 568(7753). • Giner-Sorolla, R. (2012). Science or art? How aesthetic standards grease the way through the publication bottleneck but undermine science. Perspectives on Psychological Science, 7(6), 562-571. • Miller, J. D., Phillips, N. L., &amp; Lynam, D. R. (2025). Questionable research practices violate the American Psychological Association's Code of Ethics. Journal of Psychopathology and Clinical Science. • Open Science Collaboration. (2015). Estimating the reproducibility of psychological science. Science, 349(6251), aac4716. • Reuters Staff. (2011, November 1). Dutch psychologist admits he made up research data. Reuters. Statistical power • Button, K. S., Ioannidis, J. P., Mokrysz, C., Nosek, B. A., Flint, J., Robinson, E. S., &amp; Munafò, M. R. (2013). Power failure: why small sample size undermines the reliability of neuroscience. Nature reviews neuroscience, 14(5), 365-376. • Cohen, J. (1992). A power primer. Psychological Bulletin, 112(1), 155-159. • Gervais et al. (2015). A powerful nudge? Presenting calculable consequences of underpowered research shifts incentives towards adequately powered designs. Social Psychological and Personality Science, 6, 847-854 • Higginson, A. D., &amp; Munafo, M. R. (2016). Current incentives for scientists lead to underpowered studies with erroneous conclusions. PLoS Biology, 14(11), e2000995. • Reardon, K. W., Smack, A. J., Herzhoff, K., &amp; Tackett, J. L. (2019). An N-pact factor for clinical psychological research. Journal of Abnormal Psychology, 128(6), 493. HARKing • Kerr, N. L. (1998). HARKing: Hypothesizing after the results are known. Personality • Gervais, W. M., Jewell, J. A., Najle, M. B., &amp; Ng, B. K. (2015). A powerful nudge? Presenting calculable consequences of underpowered research shifts incentives toward adequately powered designs. Social Psychological and Personality Science, 6(7), 847-854. • Higginson, A. D., &amp; Munafo, M. R. (2016). Current incentives for scientists lead to underpowered studies with erroneous conclusions. PLoS Biology, 14(11), e2000995. • Nosek, B. A., Spies, J. R., &amp; Motyl, M. (2012). Scientific Utopia: II. Restructuring incentives and practices to promote truth over publishability. Perspectives on Psychological Science, 7(6), 615-631. • Yarkoni, T. (2018, October 2). No, it's not the incentives. It's you [Blog post]. Talyarkoni. Lewis-Kraus, G. (2024, November 19). The Business-School Scandal That Just Keeps Getting Bigger. The Atlantic. • McKie, R. (2012, September 13). False positives: fraud and misconduct are threatening scientific research. The Guardian.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Although recent work recommends greater precision and a finer-grained distinction among registrations, study protocols, analysis plans, and related outputs<ref type="bibr" target="#b44">(Mayo-Wilson, Grant, Corker, &amp; Moher, 2025)</ref>, we use the term preregistration broadly to encompass advance documentation of study protocols, hypotheses, analytic plans, and statistical inference criteria (at least prior to data analysis).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>To be clear, not all journals in which clinical psychologists publish accept Registered Reports at this time. Although several premier journals do, such as Clinical Psychological Science, Journal of Clinical and Consulting Psychology, and Journal of Psychopathology and Clinical Science, others do not (e.g., Psychological Assessment). We believe and hope the adoption rate of this powerful tool will increase with time and exposure, as well as lobbying by authors, editorial board members, and even publishers.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>We do not discuss Open Access publishing here. Although it is often conceived of as part of a larger Open Science movement designed to make findings more accessible to scientists and the lay public, it is not a specific response to the replication/credibility crisis and it does not serve to make research more credible.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>Entirely exploratory studies should also be preregistered as exploratory. The preregistration of such studies safeguards against later HARKing. Syed (2024) has discussed the myth (along with two others) that preregistration is only relevant for certain kinds of studies.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>A list of potential readings covering all topics coded byVan Til et al. (2025)  is provided in the supplement.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>For students conducting research that might be perceived as being controversial in the current political environment, preregistrations might be kept private until the associated manuscript is submitted. In particularly high stakes work, preregistrations may not be made public except to share them with peer reviewers during the review process until the manuscript is set to be published online.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6"><p>Much has been written about perverse incentives and their effect on science and scientific reform. We ascribe to Yarkoni's (https://talyarkoni.org/blog/2018/10/02/no-its-not-the-incentives-its-you/) perspective on this matter. He writes, "If you find yourself unable to do your job without regularly engaging in practices that clearly devalue the very science you claim to care about, and this doesn't bother you deeply, then maybe the problem is not actually The Incentives-or at least, not The Incentives alone. Maybe the problem is You."</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Awareness of corrosive &amp; unethical research practices (QRPs) and how to avoid them <ref type="bibr" target="#b68">Simmons et al., 2011;</ref><ref type="bibr" target="#b33">Kerr, 1998;</ref><ref type="bibr" target="#b10">Button et al., 2013;</ref><ref type="bibr" target="#b30">Ioannidis, 2005</ref> Guided critique of a published paper to identify researcher dfs; power analysis workshops Marked-up critique identifying researcher dfs and potential QRPs; documented power analysis justifying sample size; revised methods section that pre-commits decision rules. Drafting, organizing, and finalizing (and posting) preregistrations; knowledge of Registered Reports <ref type="bibr">Nosek et al., 2018;</ref><ref type="bibr">Chambers &amp; Tzavella, 2022;</ref><ref type="bibr" target="#b77">Tackett, Brandes, &amp; Reardon, 2019;</ref><ref type="bibr">van den Akker et al., 2021;</ref><ref type="bibr" target="#b87">Willroth &amp; Atherton, 2024</ref> In-class OSF preregistration build;  <ref type="bibr" target="#b88">Wilson et al., 2017;</ref><ref type="bibr" target="#b64">Sandve et al., 2013;</ref><ref type="bibr">Hardwicke et al., 2018</ref> Reproducible analysis pipeline in R/Python using a template repositiories; README, environment file, and scripted Generate reproducible code that is successfully rerun by an independent peer or faculty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental Materials</head><p>Supplemental Table <ref type="table">1</ref>.</p><p>Transparency and Openness Graduate Student Paper Award Suggested Criteria 1. Is this a preregistration or a Registered Report? Neither (0)/Preregistration (.5)/Registered Report (1) 2. Were your hypotheses preregistered with a time stamp on osf.io, aspredicted.org, or similar (If no hypotheses were registered, was the study preregistered as exploratory)? No (0)/Yes (.5) 3. Was the design of your study and/or the variables preregistered with a time stamp on osf.io, aspredicted.org, or similar? No (0)/Yes (.5) 4. The target sample size and its rationale along with criteria for ending data collection were determined in advance and preregistered with a time stamp on osf.io, aspredicted.org, or similar? No (0)/Yes (.5) 5. There were no deviations from the planned sample size/termination criteria during data collection or the deviations were made transparent in the work. No (there were unnoted deviations) (0) Yes (there were no deviations or deviations were noted) (.5) 6. Analysis plan was preregistered with a time stamp on osf.io, aspredicted.org, or similar? (0 = No, .25 = Minimal detail, .5 = Substantial detail) 7. The final analysis did not reveal any deviations from the pre-registered analysis plan or the deviations were made transparent in the work. No (0)/Yes (.5) 8. Analysis code was preregistered with timestamp on osf.io, aspredicted.org, or similar? No (0)/Yes (1) 9. The raw data for this study have been published (e.g., on osf.io, PURR [Purdue University Research Repository], or similar). No (0)/Yes (1) 10. Materials (instructions, stimuli, questionnaires, description of measurement procedures, etc.) have been made available online (if possible, i.e., not copyright protected), together with all notes necessary for replication of the methodology by an independent investigator? No (0)/Yes ( <ref type="formula">1</ref>  <ref type="bibr" target="#b52">A., Alter, G., Banks, G. C., et al. (2015)</ref>. Promoting an open research culture.</p><p>Science, 348( <ref type="formula">6242</ref> • Piwowar, H., Priem, J., Larivière, V., Alperin, J. P., <ref type="bibr">Matthias, L., Norlander, B., ... &amp; Haustein, S. (2018)</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Graduate training in statistics, methodology, and measurement in psychology: A survey of PhD programs in North America</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Aiken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sechrest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Reno</surname></persName>
		</author>
		<idno type="DOI">10.1037/0003-066X.45.6.721</idno>
		<ptr target="https://doi.org/10.1037/0003-066X.45.6.721" />
	</analytic>
	<monogr>
		<title level="j">American Psychologist</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="721" to="734" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Doctoral training in statistics, measurement, and methodology in psychology: Replication and extension of Aiken, West, Sechrest, and Reno&apos;s</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Aiken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Millsap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Psychologist</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="50" />
			<date type="published" when="1990">2008. 1990</date>
		</imprint>
	</monogr>
	<note>survey of PhD programs in North America</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The rules of the game called psychological science</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Dijk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wicherts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perspectives on Psychological Science</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="543" to="554" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ensuring the quality and specificity of preregistrations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cls</forename><surname>Veldkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malm</forename><surname>Van Assen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eav</forename><surname>Crompvoets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Nosek</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pbio.3000937</idno>
		<ptr target="https://doi.org/10.1371/journal.pbio.3000937" />
	</analytic>
	<monogr>
		<title level="j">PLoS Biology</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">3000937</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Writing the Empirical Journal Article</title>
		<author>
			<persName><forename type="middle">D J</forename><surname>Bem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Compleat Academic: A Career Guide</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Darley</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Zanna</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Iii</forename><surname>Roediger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">L</forename></persName>
		</editor>
		<meeting><address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<publisher>American Psychological Association</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Feeling the future: Experimental evidence for anomalous retroactive influences on cognition and affect</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Bem</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0021524</idno>
		<ptr target="https://doi.org/10.1037/a0021524" />
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="407" to="425" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The registration continuum in clinical science: A guide toward transparent practices</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Benning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G C</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Abnormal Psychology</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">528</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The registration continuum in personality disorder studies: Theory, rationale, and template</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Benning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Personality Disorders: Theory, Research, and Treatment</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="18" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rein in the four horsemen of irreproducibility</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bishop</surname></persName>
		</author>
		<idno type="DOI">10.1038/d41586-019-01307-2</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">568</biblScope>
			<biblScope unit="issue">7753</biblScope>
			<biblScope unit="page">435</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">One decade into the replication crisis, how have psychological results changed?</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Bogdan</surname></persName>
		</author>
		<idno type="DOI">10.1177/25152459251323480</idno>
	</analytic>
	<monogr>
		<title level="j">Advances in Methods and Practices in Psychological Science</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Power failure: why small sample size undermines the reliability of neuroscience</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Button</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Ioannidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mokrysz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Nosek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Flint</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Munafò</surname></persName>
		</author>
		<idno type="DOI">10.1038/nrn3475</idno>
		<ptr target="https://doi.org/10.1038/nrn3475" />
	</analytic>
	<monogr>
		<title level="j">Nature reviews. Neuroscience</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="365" to="376" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Report finds massive fraud at Dutch universities</title>
		<author>
			<persName><forename type="first">E</forename><surname>Callaway</surname></persName>
		</author>
		<idno type="DOI">10.1038/479015a</idno>
		<ptr target="https://doi.org/10.1038/479015a" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">479</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">When do interaction/moderation effects stabilize in linear regression?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Baranger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vize</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Lynam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
	<note>Manuscript submitted for publication</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The seven deadly sins of psychology: A manifesto for reforming the culture of scientific practice</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Chambers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Princeton University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The past, present and future of Registered Reports</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tzavella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Human Behavior</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="29" to="42" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Severity and crises in science: Are we getting it right when we&apos;re right and wrong when we&apos;re wrong? PsyArXiv Preprints</title>
		<author>
			<persName><forename type="first">A</forename><surname>Claesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lakens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Vanpaemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Van Dongen</surname></persName>
		</author>
		<idno type="DOI">10.31234/osf.io/ekhc8</idno>
		<ptr target="https://doi.org/10.31234/osf.io/ekhc8" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Effect of positive findings on submission and acceptance rates: A note on meta-analysis bias</title>
		<author>
			<persName><forename type="first">A</forename><surname>Coursol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">E</forename><surname>Wagner</surname></persName>
		</author>
		<idno type="DOI">10.1037/0735-7028.17.2.136</idno>
		<ptr target="https://doi.org/10.1037/0735-7028.17.2.136" />
	</analytic>
	<monogr>
		<title level="j">Professional Psychology: Research and Practice</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="136" to="137" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Scientific discovery in a model-centric framework: Reproducibility, innovation, and epistemic diversity</title>
		<author>
			<persName><forename type="first">B</forename><surname>Devezer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Nardin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Baumgaertner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">O</forename><surname>Buzbas</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0216125</idno>
		<ptr target="https://doi.org/10.1371/journal.pone.0216125" />
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">216125</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Toolkit for Aligning Incentives 2</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Dougherty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mckiernan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tananbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Garcia</surname></persName>
		</author>
		<idno type="DOI">10.31219/osf.io/8x4e9</idno>
		<ptr target="https://doi.org/10.31219/osf.io/8x4e9" />
		<imprint>
			<date type="published" when="2024-02-21">2024. February 21</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Questionable research practices revisited</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fiedler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Schwarz</surname></persName>
		</author>
		<idno type="DOI">10.1177/1948550615612150</idno>
		<ptr target="https://doi.org/10.1177/1948550615612150" />
	</analytic>
	<monogr>
		<title level="j">Social Psychological and Personality Science</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="52" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The statistical crisis in science: Data-dependent analysis-a &quot;garden of forking paths&quot;-explains why many statistically significant comparisons don&apos;t hold up</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Fiske</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Loken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
		<ptr target="https://www.psychologicalscience.org/observer/a-call-to-change-sciences" />
	</analytic>
	<monogr>
		<title level="j">American Scientist</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="460" to="466" />
			<date type="published" when="2014">2016. 2014</date>
		</imprint>
	</monogr>
	<note>A call to change science&apos;s culture: Supplanting intellectual bullying with collaboration APS Observer</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Effect size guidelines for individual differences researchers</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Gignac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Szodorai</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.paid.2016.06.069</idno>
		<ptr target="https://doi.org/10.1016/j.paid.2016.06.069" />
	</analytic>
	<monogr>
		<title level="j">Personality and Individual Differences</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="74" to="78" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Practical solutions for sharing data and materials from psychological research</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">O</forename><surname>Gilmore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lorenzo Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Adolph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Methods and Practices in Psychological Science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="121" to="130" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Corker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Mellor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L K</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Cashin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lagisz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Nosek</surname></persName>
		</author>
		<idno type="DOI">10.31222/osf.io/nmfs6_v2</idno>
		<ptr target="https://doi.org/10.31222/osf.io/nmfs6_v2" />
		<title level="m">TOP 2025: An Update to the Transparency and Openness Promotion Guidelines</title>
		<imprint>
			<date type="published" when="2025-02-03">2025. February 3</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Consequences of prejudice against the null hypothesis</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Greenwald</surname></persName>
		</author>
		<idno type="DOI">10.1037/h0076157</idno>
		<ptr target="https://doi.org/10.1037/h0076157" />
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Clinical versus mechanical prediction: a meta-analysis</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Grove</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Zald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Lebow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Snitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Assessment</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="30" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Transparency is now the default at Psychological Science</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Hardwicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vazire</surname></persName>
		</author>
		<idno type="DOI">10.1177/09567976231221573</idno>
		<ptr target="https://doi.org/10.1177/09567976231221573" />
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="708" to="711" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Reducing bias, increasing transparency and calibrating confidence with preregistration</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Hardwicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Wagenmakers</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41562-022-01497-2</idno>
		<ptr target="https://doi.org/10.1038/s41562-022-01497-2" />
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="26" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Raising awareness for the replication crisis in clinical psychology by focusing on inconsistencies in psychotherapy research: How much can we rely on published findings from efficacy trials</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Hengartner</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2018.00256</idno>
		<ptr target="https://doi.org/10.3389/fpsyg.2018.00256" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">256</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Open science at Clinical Psychological Science: Reflections on progress, lessons learned, and suggestions for continued improvement</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M S</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">J</forename><surname>Kirtley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Urry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Tackett</surname></persName>
		</author>
		<idno type="DOI">10.1177/21677026241255882</idno>
		<ptr target="https://doi.org/10.1177/21677026241255882" />
	</analytic>
	<monogr>
		<title level="j">Clinical Psychological Science</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="195" to="204" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Why most published research findings are false</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P A</forename><surname>Ioannidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Medicine</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">124</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Why most discovered true associations are inflated</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P A</forename><surname>Ioannidis</surname></persName>
		</author>
		<idno type="DOI">10.1097/EDE.0b%20013e31818131e7</idno>
		<ptr target="https://doi.org/10.1097/EDE.0b" />
	</analytic>
	<monogr>
		<title level="j">Epidemiology</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="640" to="648" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Easing into open science: A guide for graduate students and their advisors</title>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">K</forename><surname>Kathawalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Silverstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Syed</surname></persName>
		</author>
		<idno type="DOI">10.1525/collabra.18684</idno>
		<ptr target="https://doi.org/10.1525/collabra.18684" />
	</analytic>
	<monogr>
		<title level="j">Collbara: Psychology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">18684</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">HARKing: hypothesizing after the results are known</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L</forename><surname>Kerr</surname></persName>
		</author>
		<idno type="DOI">10.1207/s15327957pspr0203</idno>
		<ptr target="https://doi.org/10.1207/s15327957pspr0203" />
	</analytic>
	<monogr>
		<title level="j">Personality and Social Psychology Reeview</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="196" to="217" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Campbell&apos;s law explains the replication crisis: Pre-registration badges are history repeating</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Klonsky</surname></persName>
		</author>
		<idno type="DOI">10.1177/10731911241253430</idno>
		<ptr target="https://doi.org/10.1177/10731911241253430" />
	</analytic>
	<monogr>
		<title level="j">Assessment</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="224" to="234" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The replication crisis has led to positive structural, procedural, and community changes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Korbmacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Azevedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Pennington</surname></persName>
		</author>
		<idno type="DOI">10.1038/s44271-023-00003-2</idno>
		<ptr target="https://doi.org/10.1038/s44271-023-00003-2" />
	</analytic>
	<monogr>
		<title level="j">Communications Psychology</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The value of preregistration for psychological science: A conceptual analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lakens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Japanese Psychological Review</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="221" to="230" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">When and how to deviate from a preregistration</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lakens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Collabra</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">117094</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Psychology&apos;s replication crisis and the grant culture: Righting the ship</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">O</forename><surname>Lilienfeld</surname></persName>
		</author>
		<idno type="DOI">10.1177/1745691616687745</idno>
		<ptr target="https://doi.org/10.1177/1745691616687745" />
	</analytic>
	<monogr>
		<title level="j">Perspectives on Psychological Science</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="660" to="664" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Motivated reasoning when assessing the effects of refugee intake</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Erlandsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Västfjäll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tinghög</surname></persName>
		</author>
		<idno type="DOI">10.1017/bpp.2018.41</idno>
		<ptr target="https://doi.org/10.1017/bpp.2018" />
	</analytic>
	<monogr>
		<title level="j">Behavioural Public Policy</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="213" to="236" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Reproducible brain-wide association studies require thousands of individuals</title>
		<author>
			<persName><forename type="first">S</forename><surname>Marek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tervo-Clemmens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Calabro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Montez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Hatoum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">U F</forename><surname>Dosenbach</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41586-022-04492-9</idno>
		<ptr target="https://doi.org/10.1038/s41586-022-04492-9" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">603</biblScope>
			<biblScope unit="page" from="654" to="660" />
			<date type="published" when="2022">2022. 7902</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The persistence of underpowered studies in psychological research: Causes, consequences, and remedies</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Maxwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Methods</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="147" to="163" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Mayo</surname></persName>
		</author>
		<title level="m">Statistical inference as severe testing: How to get beyond the statistics wars</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Error Statistics in Philosophy of Statistics , Handbook of Philosophy of Science</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Mayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Spanos</surname></persName>
		</author>
		<editor>Prasanta S. Bandyopadhyay and Malcolm R. Forster</editor>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Elsevier</publisher>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Consistent and precise description of research outputs could improve implementation of open science</title>
		<author>
			<persName><forename type="first">E</forename><surname>Mayo-Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Corker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moher</surname></persName>
		</author>
		<idno type="DOI">10.31222/osf.io/q6xwp_v4</idno>
		<ptr target="https://doi.org/10.31222/osf.io/q6xwp_v4" />
	</analytic>
	<monogr>
		<title level="j">MetaArXiv</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Blinding us to the obvious? The effect of statistical training on the evaluation of evidence</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Mcshane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1707" to="1718" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Practical tips for ethical data sharing</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Meyer</surname></persName>
		</author>
		<idno type="DOI">10.1177/2515245917747656</idno>
	</analytic>
	<monogr>
		<title level="j">Advances in Methods and Practices in Psychological Science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="131" to="144" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Questionable research practices violate the APA Code of Ethics</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Lynam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Psychopathology and Clinical Science</title>
		<imprint>
			<biblScope unit="page" from="113" to="114" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A manifesto for reproducible science</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Munafò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Nosek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V M</forename><surname>Bishop</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41562-016-0021</idno>
		<ptr target="https://doi.org/10.1038/s41562-016-0021" />
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2017">2017. 0021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">HARKing: How badly can cherry-picking and question trolling produce bias in published results</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Aguinis</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10869-017-9524-7</idno>
		<ptr target="https://doi.org/10.1007/s10869-017-9524-7" />
	</analytic>
	<monogr>
		<title level="j">Journal of Business and Psychology</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Bestiary of questionable research practices in psychology</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hergert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Elsherif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wallrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Waltzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename><surname>Rubínová</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
		<idno type="DOI">10.1177/25152459251348431</idno>
	</analytic>
	<monogr>
		<title level="j">Advances in Methods and Practices in Psychological Science</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">25152459251348431</biblScope>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Psychology&apos;s renaissance</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Simmons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Simonsohn</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev-psych-122216-011836</idno>
		<ptr target="https://doi.org/10.1146/annurev-psych-122216-011836" />
	</analytic>
	<monogr>
		<title level="j">Annual Review of Psychology</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="511" to="534" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Scientific standards. Promoting an open research culture</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Nosek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Alter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Banks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borsboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Breckler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yarkoni</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.aab2374</idno>
		<ptr target="https://doi.org/10.1126/science.aab2374" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">348</biblScope>
			<biblScope unit="issue">6242</biblScope>
			<biblScope unit="page" from="1422" to="1425" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Scientific utopia: I. Opening scientific communication</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Nosek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bar-Anan</surname></persName>
		</author>
		<idno type="DOI">10.1080/1047840X.2012.692215</idno>
		<ptr target="https://doi.org/10.1080/1047840X.2012.692215" />
	</analytic>
	<monogr>
		<title level="j">Psychological Inquiry</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="217" to="243" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">The preregistration revolution</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Nosek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Ebersole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Dehaven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Mellor</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1708274114</idno>
		<ptr target="https://doi.org/10.1073/pnas.1708274114" />
	</analytic>
	<monogr>
		<title level="j">PNAS</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2600" to="2606" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Scientific utopia II: Restructuring incentives and practices to promote truth over publishability</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Nosek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Spies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Motyl</surname></persName>
		</author>
		<idno type="DOI">10.1177/1745691612459058</idno>
		<ptr target="https://doi.org/10.1177/1745691612459058" />
	</analytic>
	<monogr>
		<title level="j">Perspectives on Psychological Science</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="615" to="631" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Open science practices in clinical psychology journals: An audit study</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nutu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gentili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Naudet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Cristea</surname></persName>
		</author>
		<idno type="DOI">10.1037/abn0000414</idno>
		<ptr target="https://doi.org/10.1037/abn0000414" />
	</analytic>
	<monogr>
		<title level="j">Journal of Abnormal Psychology</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="510" to="516" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Contct with an ex-partner is associated with psychological distress after marital separation</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>O'hara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Grinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Tackman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Mehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Sbarra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clinical Psychological Science</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="450" to="463" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Estimating the reproducibility of psychological science</title>
		<author>
			<orgName type="collaboration">Open Science Collaboration</orgName>
		</author>
		<idno type="DOI">10.1126/science.aac4716</idno>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">349</biblScope>
			<biblScope unit="page">4716</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Additive and interactive relations of personality and cognition with externalizing behaviors</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">T</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Hyatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Lynam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clinical Psychological Science</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="951" to="977" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Justifications shape ethical blind spots</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pittarello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Leib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gordon-Hecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shalvi</surname></persName>
		</author>
		<idno type="DOI">10.1177/0956797615571018</idno>
		<ptr target="https://doi.org/10.1177/0956797615571018" />
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="794" to="804" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">An N-pact factor for clinical psychological research</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Reardon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Herzhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Tackett</surname></persName>
		</author>
		<idno type="DOI">10.1037/abn0000435</idno>
		<ptr target="https://retractionwatch.com/category/diederik-stapel/" />
	</analytic>
	<monogr>
		<title level="j">Journal of Abnormal Psychology</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="493" to="499" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">The file drawer problem and tolerance for null results</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rosenthal</surname></persName>
		</author>
		<idno type="DOI">10.1037/0033-2909.86.3.638</idno>
		<ptr target="https://doi.org/10.1037/0033-2909.86.3.638" />
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="638" to="641" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Does preregistration improve the credibility of research findings</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rubin</surname></persName>
		</author>
		<idno type="DOI">10.20982/tqmp.16.4</idno>
	</analytic>
	<monogr>
		<title level="j">Quantitative Methods for Psychology</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="376" to="390" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Ten simple rules for reproducible computational research</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">K</forename><surname>Sandve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nekrutenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hovig</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pcbi.1003285</idno>
		<ptr target="https://doi.org/10.1371/journal.pcbi.1003285" />
	</analytic>
	<monogr>
		<title level="j">PLOS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">1003285</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">The meaningfulness of effect sizes in psychological research: Differences between sub-disciplines and the impact of potential biases</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Schwarz</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2019.00813</idno>
		<ptr target="https://doi.org/10.3389/fpsyg.2019.00813" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">813</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">An excess of positive results: Comparing the standard psychology literature with Registered Reports</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Scheel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R M J</forename><surname>Schijen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lakens</surname></persName>
		</author>
		<idno type="DOI">10.1177/25152459211007467</idno>
	</analytic>
	<monogr>
		<title level="j">Advances in Methods and Practices in Psychological Science</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">At what sample size do correlations stabilize</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D</forename><surname>Schönbrodt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perugini</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jrp.2013.05.009</idno>
		<ptr target="https://doi.org/10.1016/j.jrp.2013.05.009" />
	</analytic>
	<monogr>
		<title level="j">Journal of Research in Personality</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="609" to="612" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Simmons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Simonsohn</surname></persName>
		</author>
		<idno type="DOI">10.1177/0956797611417632</idno>
		<ptr target="https://doi.org/10.1177/0956797611417632" />
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1359" to="1366" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">The natural selection of bad science</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Smaldino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mcelreath</surname></persName>
		</author>
		<idno type="DOI">10.1098/rsos.160384</idno>
		<ptr target="http://dx.doi.org/10.1098/rsos.160384" />
	</analytic>
	<monogr>
		<title level="j">Royal Society Open Science</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">160384</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Using OSF to share data: A step-by-step guide</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Soderberg</surname></persName>
		</author>
		<idno type="DOI">10.1177/2515245918757689</idno>
		<ptr target="https://doi.org/10.1177/2515245918757689" />
	</analytic>
	<monogr>
		<title level="j">Advances in Methods and Practices in Psychological Science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="115" to="120" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Initial evidence of research quality of registered reports compared with the standard publishing model</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Soderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Errington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Schiavone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bottesini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Thorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vazire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Esterling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Nosek</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41562-021-01142-4</idno>
		<ptr target="https://doi.org/10.1038/s41562-021-01142-4" />
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="990" to="997" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Replicability crisis in social psychology: Looking at the past to find new pathways for the future</title>
		<author>
			<persName><forename type="first">W</forename><surname>Świątkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dompnier</surname></persName>
		</author>
		<idno type="DOI">10.5334/irsp.66</idno>
		<ptr target="https://doi.org/10.5334/irsp" />
	</analytic>
	<monogr>
		<title level="j">International Review of Social Psychology</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="111" to="124" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Three persistent myths about open science</title>
		<author>
			<persName><forename type="first">M</forename><surname>Syed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Trial &amp; Error</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Is preregistration worthwhile?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Szollosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kellen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shiffrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Van Rooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Van Zandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Donkin</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2019.11.009</idno>
		<ptr target="https://doi.org/10.1016/j.tics.2019.11.009" />
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="94" to="95" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">People as intuitive scientists: Reconsidering statistical explanations of decision making</title>
		<author>
			<persName><forename type="first">A</forename><surname>Szollosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Newell</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2020.09.005</idno>
		<ptr target="https://doi.org/10.1016/j.tics.2020.09.005" />
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1008" to="1018" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Psychology&apos;s replication crisis and clinical psychological science</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Tackett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Brandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Markon</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev-clinpsy-050718-095710</idno>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Clinical Psychology</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="579" to="604" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Leveraging the Open Science Framework in clinical psychological assessment research</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Tackett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Brandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Reardon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Assessment</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">1386</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">It&apos;s time to broaden the replicability conversation: Thoughts for and from clinical psychological science</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Tackett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">O</forename><surname>Lilienfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Oltmanns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Shrout</surname></persName>
		</author>
		<idno type="DOI">10.1177/1745691617690042</idno>
		<ptr target="https://doi.org/10.1177/1745691617690042" />
	</analytic>
	<monogr>
		<title level="j">Perspectives on Psychological Science</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="742" to="756" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Introduction to the special section on increasing replicability, transparency, and openness in clinical psychology</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Tackett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.1037/abn0000455</idno>
		<ptr target="https://doi.org/10.1037/abn0000455" />
	</analytic>
	<monogr>
		<title level="j">Journal of Abnormal Psychology</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="487" to="492" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Preregistration of secondary data analysis: A template and tutorial</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">R</forename><surname>Van Den Akker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Chopik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">I</forename><surname>Damian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Davis-Kean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Kosie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kruse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Olsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Valentine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Van 't Veer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bakker</surname></persName>
		</author>
		<idno type="DOI">10.15626/MP.2020.2625</idno>
		<ptr target="https://doi.org/10.15626/MP.2020.2625" />
	</analytic>
	<monogr>
		<title level="j">Meta-Psychology</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">2625</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">De-Identification when making data sets findable, accessible, interoperable, and reusable (FAIR): Two worked examples from the behavioral and social sciences</title>
		<author>
			<persName><forename type="first">D</forename><surname>Van Ravenzwaaij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Jong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hoekstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Scheibe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Span</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Wessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">E</forename><surname>Heininga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Methods and Practices in Psychological Science</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">25152459251336130</biblScope>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Open science training in APA-accredited clinical psychology programs: A registered report</title>
		<author>
			<persName><forename type="first">K</forename><surname>Van Til</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">V</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Lynam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clinical Psychological Science</title>
		<imprint/>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Quality uncertainty erodes trust in science</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vazire</surname></persName>
		</author>
		<idno type="DOI">10.1525/collabra.74</idno>
		<ptr target="https://doi.org/10.1525/collabra" />
	</analytic>
	<monogr>
		<title level="j">Collabra: Psychology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Implications of the credibility revolution for productivity, creativity, and progress</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vazire</surname></persName>
		</author>
		<idno type="DOI">10.1177/1745691617751884</idno>
		<ptr target="https://doi.org/10.1177/1745691617751884" />
	</analytic>
	<monogr>
		<title level="j">Perspectives on Psychological Science</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="411" to="417" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Open science practices in personality disorder journals</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Vize</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Lynam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
	<note>Manuscript submitted for publication</note>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">On the use and (mis)uses of preregistration: A reply to Klonsky</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Vize</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Lynam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Assessment</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="235" to="243" />
			<date type="published" when="2024">2025. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Best laid plans: A guide to reporting preregistration deviations</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Willroth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">E</forename><surname>Atherton</surname></persName>
		</author>
		<idno type="DOI">10.1177/25152459231213802</idno>
		<ptr target="https://doi.org/10.1177/25152459231213802" />
	</analytic>
	<monogr>
		<title level="j">Advances in Methods and Practices in Psychological Science</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">25152459231213802</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Good enough practices in scientific computing</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cranston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kitzes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nederbragt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Teal</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pcbi.1005510</idno>
		<ptr target="https://doi.org/10.1371/journal.pcbi.1005510" />
	</analytic>
	<monogr>
		<title level="j">PLOS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">1005510</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">No preprint is an island: How preprints enter Preferred reporting items for systematic reviews</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wingen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Berkessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Englich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Clinical Epidemiology</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="1006" to="1012" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">The preregistration revolution</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Nosek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Ebersole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Dehaven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Mellor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2600" to="2606" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Preregistration of secondary data analysis: A template and tutorial</title>
		<author>
			<persName><forename type="first">O</forename><surname>Van Den Akker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chopik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Damian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Davis-Kean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Meta-psychology</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">2625</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">On the use and misuses of preregistration: A reply to Klonsky</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Vize</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Lynam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Assessment</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="235" to="243" />
			<date type="published" when="2024">2025. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Registered reports. A new publishing initiative at Cortex</title>
		<author>
			<persName><forename type="first">Registered</forename><surname>Reports ; Chambers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cortex</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="609" to="610" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">The past, present, and future of Registered Reports</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tzavella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Human Behavior</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="42" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">An excess of positive results: Comparing the standard psychology literature with registered reports</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Scheel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Schijen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lakens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Methods and Practices in Psychological Science</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">25152459211007467</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Open data</note>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Enabling open-science initiatives in clinical psychology and psychiatry without sacrificing patients&apos; privacy: Current practices and future challenges</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Denny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Malin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Methods and Practices in Psychological Science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="104" to="114" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">The poor availability of psychological research data for reanalysis</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wicherts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borsboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kats</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Molenaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Psychologist</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="726" to="728" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">How open science helps researchers succeed</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Mckiernan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Bourne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kenall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename><surname>Yarkoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename></persName>
		</author>
		<idno type="DOI">10.7554/eLife.16800Openmaterials</idno>
		<ptr target="https://doi.org/10.7554/eLife.16800Openmaterials" />
	</analytic>
	<monogr>
		<title level="j">eLife</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">16800</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">The peer reviewers&apos; openness initiative: Incentivizing open research practices through peer review</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Morey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Etchells</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hoekstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lakens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename><surname>Wagenmakers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E.-J</forename></persName>
		</author>
		<idno type="DOI">10.1098/rsos.150547Openaccess</idno>
		<ptr target="https://doi.org/10.1098/rsos.150547Openaccess" />
	</analytic>
	<monogr>
		<title level="j">Advances in Methods and Practices in Psychological Science</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2016">2016. 2023</date>
		</imprint>
	</monogr>
	<note>Tutorial: Power Analyses for Interaction Effects in Cross-Sectional Regressions Royal Society Open Science</note>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Moderation effects in personality disorder research</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Vize</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A A</forename><surname>Baranger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Finsaas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Olino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Lynam</surname></persName>
		</author>
		<ptr target="thttps://www.americanscientist.org/article/open-science-isnt-always-open-to-all-scientists" />
	</analytic>
	<monogr>
		<title level="m">Open Science Isn&apos;t Always Open to All Scientists</title>
		<imprint>
			<publisher>American Scientist</publisher>
			<date type="published" when="2019">2023. 2019</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="78" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Open science and multicultural research: Some data, considerations, and recommendations</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gobrial</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Giadolor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rollock</surname></persName>
		</author>
		<ptr target="https://pubmed.ncbi.nlm.nih.gov/35404633/" />
	</analytic>
	<monogr>
		<title level="j">Cultural Diversity &amp; Ethnic Minority Psychology</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">567</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Open science, done wrong, will compound inequities</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ross-Hellauer</surname></persName>
		</author>
		<idno type="DOI">10.1038/d41586-022-00724-0</idno>
		<ptr target="https://doi.org/10.1038/d41586-022-00724-0" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">603</biblScope>
			<biblScope unit="page">363</biblScope>
			<date type="published" when="2022">2022. 7901</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
