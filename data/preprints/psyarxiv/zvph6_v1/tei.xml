<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Helping behaviors rely on explicit but not implicit inferences of affect</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Doron</forename><surname>Atias</surname></persName>
							<email>doron.atias@yale.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Maria</forename><surname>Gendron</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Helping behaviors rely on explicit but not implicit inferences of affect</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D3AC4415976E818B03185BAAEB8059CE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-22T14:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Contemporary models of emotion highlight the fundamental function of emotional expressions in eliciting adaptive social behaviors. Critically, past research testing the social effects of emotional expressions has primarily relied on simulated expressions that may not capture the complexity of realworld emotional displays. Across two studies (N=1,534), we examined how naturally diverse vocalizations influence prosocial behavior. In Study 1, authentic videos of crying individuals were synched with typical crying, atypical crying, typical laughter, or no vocalization. When participants were asked to make affective inferences (Study 1A), vocalization type influenced their perceived valence judgments. However, when participants made donation decisions without explicit inference instructions (Study 1B), vocalization type did not impact donations, except when audiovisual signals were affectively conflicting. In Study 2, we examined the role of explicit inferences of affect in facilitating the link between naturalistic expressions and prosocial behaviors. We found that when participants first made explicit inferences about others' affective states, they donated more money overall and their helping behaviors were more sensitive to vocalization types. However, when participants donated without providing explicit inferences first, their helping behaviors were less influenced by vocal cues and donation amounts were lower overall. Collectively, these findings highlight the role of explicit inferential processing in establishing the link between naturalistic emotional expressions and social behavior.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction.</head><p>From the very first moments of life, innate affective cues such as crying vocalizations function to regulate social behaviors and elicit adaptive reactions in caregivers that are crucial for infants' survival and well-being <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref> .</p><p>Although the structure of affective signals continuously develops during late infancy, childhood, and adolescence, becoming increasingly nuanced and complex <ref type="bibr" target="#b3">4</ref> , their function remains relatively stable throughout life: to coordinate everyday social interactions by shaping people's thoughts, feelings, and behavior <ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6</ref> .</p><p>A basic premise in contemporary emotion research is that emotional expressions convey valuable information about the expressers' mental states and intentions, which are readily inferred by perceivers and, in turn, influence their social behaviors <ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8</ref> . Accordingly, social-functional frameworks, like the EASI model, have identified two distinct pathways through which emotional expressions may influence perceivers' behavior: inferential processes and affective reactions <ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10</ref> . For example, vocal crying may elicit social support by signaling one's distress and suffering, which is inferred by others <ref type="bibr" target="#b10">11</ref> , and/or by eliciting affective reactions in others, like compassion and sympathy, which in turn shape their social behavior <ref type="bibr" target="#b11">12</ref> .</p><p>In line with this proposition, several studies have shown that participants were more willing to comply with requests for help when these requests were accompanied by facial expressions of disappointment/sadness, compared to anger expressions <ref type="bibr" target="#b12">13</ref> . Similarly, facial expressions of sadness increased perceivers' charity donations, compared to happy and neutral expressions <ref type="bibr" target="#b13">14</ref> , and tearful expressions elevated people's intentions to offer social support, compared to tearless expressions <ref type="bibr" target="#b14">15</ref> . In negotiation contexts, participants made larger concessions and placed lower demands when their opponents expressed anger, compared to happiness or no emotion <ref type="bibr" target="#b15">16</ref> .</p><p>Critically, in testing these effects, past work has mostly relied on explicit statements of other people's feelings (e.g., "this offer makes me really angry", in negotiation contexts), or static images of simulated cues (e.g., canonical facial expressions of anger, sadness, etc.). Notwithstanding their advantages (highly homogeneous and consensual portrayals), simulated expressions are typically overly simplified and stereotypical, and thus may fail to represent the complexity and variability of real-life emotional displays <ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref> . Since simulated and stereotypical expressions have such a salient symbolic value <ref type="bibr" target="#b19">20</ref> , they may lead perceivers to automatically draw explicit inferences of other people's emotions. In contrast, naturalistic expressions are typically highly diverse and do not necessarily fit a discrete emotion prototype, and thus it is likely that their processing and social effects diverge from more simulated and iconic expressions <ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref> .</p><p>In the current paper, we employ a more naturalistic approach to investigate the social effects of naturally diverse and multimodal emotional expressions. We focus on two principal questions: (a) are people's prosocial behaviors sensitive to the affective information conveyed by naturally diverse expressions; and (b) to what extent does deliberately inferring affect from naturalistic expressions shape people's prosocial behaviors. Across two studies, we presented participants with authentic videos of individuals crying along with a narrative describing a devastating life event (e.g., cancer relapse). After watching the videos, participants had the opportunity to donate real money to help these individuals from a provided budget. This procedure builds on existing paradigms that test the social effects of emotional expressions. Yet, in contrast to previous research that mostly used static images of stereotypical emotional expressions, here we sought to optimize ecological validity by using dynamic, multimodal, and naturally diverse emotional expressions of people (ostensibly) seeking help.</p><p>In Study 1, videos of crying individuals were synched with different types of real-life crying vocalizations. Specifically, these included typical crying vocalizations, validated for consensually signaling negative affect, and atypical crying vocalizations, validated for signaling ambiguous and incongruent affective messages (i.e., sounded positive/laughter-like to listeners overall). Synchronizing the videos with typical and atypical crying vocalizations allowed us to examine the extent to which perceivers are sensitive to the affective informational value of the vocal cues when explicitly tasked to infer affect versus when simply behaving. In Study 1A (N=216), participants watched videos of crying individuals and rated the perceived valence of the person in each video (i.e., how positive/negative the person feels). We hypothesized that when tasked to explicitly infer affect, perceivers would be sensitive to the varying affective information conveyed by the embedded crying vocalizations. Specifically, we predicted that target individuals synced with typical crying vocalizations would be perceived as feeling more negative affect, compared to target individuals synced with atypical crying vocalizations.</p><p>In Study 1B, we used the same videos to examine whether participants' helping behaviors towards the individuals in the videos are impacted by the distinct types of embedded vocalizations. An independent sample of participants (N=952) was exposed to the stories and videos of distressed individuals and decided how much money they wished to donate to help them from a provided budget. As in Study 1A, different participant groups were exposed to videos synced with different types of crying vocalizations, namely, typical or atypical crying. An additional group was exposed to videos synced with typical laughter vocalizations, to contrast with the atypical crying vocalizations condition. A final group of participants viewed silent versions of the videos to provide a baseline response (see Figure <ref type="figure" target="#fig_0">1</ref>). If helping behaviors are sensitive to the affective information conveyed by the different vocalization types, videos synched with typical crying vocalizations were expected to elicit higher donation sums, compared to those synched with atypical crying vocalizations, and videos synched with typical laughter vocalizations were expected to elicit the lowest donations.</p><p>In Study 2, we further investigated the role of explicit (as opposed to implicit) inferences of affect in facilitating the link between naturalistic expressions and prosocial behaviors. Participants (N=366) watched authentic videos of people seeking help while expressing typical crying, atypical crying, typical laughter, or no vocalization (i.e., silent videos), and were assigned to one of two task-framing conditions. In the donation-framed condition, participants were instructed to watch the video and decide how much money they would like to donate from a provided budget (as in Study 1B). Then, after placing their donation, they were asked to rate how positive or negative the person in the video feels. In the affect-framed condition, participants were instructed to watch the video and rate how positive or negative the person in the video feels (as in Study 1A). Then, after providing their affective judgments, they were asked to decide how much money they would like to donate from a provided budget. We predicted that the influence of vocalization types on participants' helping behaviors would differ as a function of the task-framing condition. Specifically, participants' helping behaviors were expected to be more sensitive to the varying vocalization types when they explicitly inferred the targets' affective state prior to placing their donations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study 1A: The Effects of Vocalization Type on Valence Judgments.</head><p>To examine the perceivers' sensitivity to diverse vocal cues when they are explicitly tasked to infer affect, we compared the perceived valence ratings of targets paired with typical and atypical crying vocalizations. As illustrated in Figure <ref type="figure" target="#fig_4">2A</ref>, the perceived valence judgments were impacted by the vocalization type, . Specifically, the target crying individuals were perceived as feeling more negative when their videos were paired with typical crying compared to silent videos , and least negative when their videos were paired with atypical crying vocalizations , all .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study 1B: The Effects of Vocalization Type on Prosocial Behavior and Empathic Responses.</head><p>In study 1B, we examined the extent to which the participants' sensitivity to the different types of vocal crying persists also when participants are tasked to behave without explicitly inferring other people's affect. Here, we focused on donation decisions as a measure of prosocial behavior, similar to previous studies on the prosocial effects of emotional expressions <ref type="bibr" target="#b12">13</ref> . We also tested perceivers' empathic responses, based on their self-reports of emotional reactions after watching the videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Donations.</head><p>Despite the influence of vocalization type on perceived valence ratings, we found no evidence for a statistically significant difference in donation sums between typical crying , atypical crying , and silent videos , all . These null results were confirmed in a subsequent Bayesian analysis, which yielded a of .009 (error % = .003), suggesting strong support for .</p><p>However, when the videos were paired with typical laughter, the donations decreased (Figure <ref type="figure" target="#fig_4">2B</ref>). This suggests that participants did not entirely ignore the vocal channel in the videos. Rather, their prosocial behaviors changed only when the expressed audiovisual signals were affectively conflicting.</p><p>Empathic concern. Empathic concern was defined as "the tenderhearted feeling of compassionate concern, feeling sorry for the other" <ref type="bibr" target="#b26">27</ref> . It was calculated as the average ratings of how sympathetic, compassionate, moved, and concerned the perceivers felt towards the crying individuals after watching their videos. As shown in Figure <ref type="figure" target="#fig_4">2C</ref>, there was a main effect for vocalization type on empathic concern scores, . Similar to the donation results, this was driven by a decrease in empathic concern for typical laughter vocalizations (. We did not find evidence that empathic concern scores in the typical crying (, atypical crying (, and silent videos ( conditions differed significantly (all ). As shown in Figure <ref type="figure" target="#fig_4">2D</ref>, people's donations and empathic concern scores were highly correlated across all vocalization types, . This suggests that people's own affective reactions after watching the targets' videos were connected to their prosocial behavioral responses, yet their affective reactions and behavioral responses were not impacted by the informational value of the crying vocal signals. Finally, the effect of vocalization type on empathic distress was not statistically significant (), suggesting that the diverse vocalizations may have elicited complementary, but not reciprocal, emotional reactions in the perceivers <ref type="bibr" target="#b9">10</ref> . Nevertheless, empathic distress scores correlated with both empathic concern () and donation sums (), suggesting that both aspects of empathic reactions are associated with prosocial behavior toward expressive targets (see Supplementary Figure <ref type="figure" target="#fig_3">S1</ref>).</p><p>Together, the results from Study 1 suggest that perceivers are sensitive to naturally diverse crying vocalizations (e.g., typical crying and atypical crying) when they are tasked to infer affect, yet this variation does not necessarily lead to distinct prosocial behaviors and empathic reactions. For example, we show that crying individuals in videos paired with typical crying vocalizations are perceived as feeling more negative, compared to videos paired with atypical crying vocalizations. Yet, when perceivers were not tasked to explicitly infer the targets' affective state, these diverse vocalizations elicited similar levels of prosocial and empathic responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study 2: The Effects of Vocalization Type and Task Framing on Valence</head><p>Judgments and Prosocial Behavior.</p><p>Valence judgments. Similar to Study 1A, vocalization type impacted the perceived valence of the individuals in the videos, . Specifically, people expressing typical crying vocalizations were perceived as feeling more negative compared to those expressing atypical crying (Figure <ref type="figure" target="#fig_2">3A</ref>). Further, targets expressing atypical crying vocalizations were judged as feeling more negative than targets paired with typical laughter in the affect-framed condition, but not in the donation-framed condition. In both task framing conditions, there was no difference in perceived valence ratings between the silent videos and the videos including typical crying (see Table <ref type="table" target="#tab_0">1</ref>). Overall, these results conceptually replicate Study 1A's findings, showing that the perceptual diversity of emotional vocalizations (e.g., typical crying and atypical crying) influence perceivers affective judgments, regardless of whether the perceivers' task framing was to donate or to infer the affective state of the person in the video. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Donations. Despite the impact of vocalization type on affective judgments,</head><p>when participants' task framing was to donate to the person in the video (donation-framed condition), vocalization type did not impact donations (all ).</p><p>These null results were confirmed in a subsequent Bayesian analysis, yielding a of .068 with error % = .003. However, when participants' task framing was to judge the feelings of the person in the video (affect-framed condition), their donation sums increased, (Figure <ref type="figure" target="#fig_2">3B</ref>). Overall, donations were higher in the affect-framed compared to the donation-framed condition , . Critically, a posthoc analysis revealed that this effect was driven by the increase in donation sums for videos including typical crying and atypical crying, compared to silent videos and videos synced with typical laughter vocalizations. judgments in both the donation-framed and affect-framed conditions. (B) Donation sums as a function of task framing and vocalization type. Donations were similar across all vocalization types in donation-framed, but not in the affect-framed conditions. Error bars reflect bootstrapped 95% confidence intervals. See the online article for the color version of this figure.</p><p>Authenticity and deception. Overall, participants in the affect-framed condition judged the targets in the videos as more authentic , , and less deceptive , , compared to targets in the donation-framed condition and , respectively. We also found a significant main effect for vocalization type on perceived authenticity, , and deception, . Specifically, videos including typical laughter vocalizations were judged as being least authentic and videos without vocalizations (i.e., silent videos) were judged as being most authentic</p><p>. Videos showing typical crying and atypical crying vocalizations did not differ significantly in their perceived authenticity and deception. Finally, the interaction between vocalization type and task framing was found nonsignificant in both authenticity and deception measurements.</p><p>Willingness to share the video post on social media. Overall, participants' willingness to share was higher for the affect-framed , compared to the donation-framed condition , . The participants' willingness to share also changed by vocalization type, , and they were more willing to share silent videos than videos with typical crying vocalizations and typical laughter vocalizations . The interaction between vocalization type and task framing was not statistically significant.</p><p>Overall, the results from Study 2 are consistent with and conceptually replicate the findings from Study 1, such that perceivers are less sensitive to vocal cues when asked to behave but not when asked to infer others' affect.</p><p>Critically, and new to Study 2, we show that when participants are primarily tasked with behaving, it is unlikely that their behavior is relying on implicit affective inferences, otherwise we would expect to see similar donation patterns across both task framing conditions. More broadly, this challenges assumptions that mental state inference is routine and necessary for individuals to behave in response to others' expressive behavior.</p><p>Interestingly, when participants were asked to infer the targets' affect after providing their donation, their affective inferences were sensitive to vocalization type, such that people expressing typical crying vocalizations were perceived as feeling more negative than those expressing atypical crying or typical laughter. This suggests that participants either implicitly inferred the affective state of the target while watching their videos, or constructed the target's affect in retrospect, when asked to provide explicitly affective inference. However, even if the participants implicitly inferred the targets' affect, these inferences did not informed their subsequent helping behavior.</p><p>We also observe different patterns of authenticity/deception judgments and willingness to share a video based on the task framing, again bolstering the interpretation that different processes are engaged based on the task frame (i.e., affective inference or prosocial behavior). We see that individuals are more likely to view a targets' expression as valid (authentic, not deceptive) and shareable when they are primarily tasked with inferring the targets' affective state.</p><p>Finally, these findings have implications for the integration of multiple affective cues from different sensory channels. Perceivers seem to be less sensitive to the vocal channel when asked to behave. This suggests that while subtle differences in vocal cues may be salient in lab tasks that call perceivers' attention to the informational value of these different signals,</p><p>when people are out in the world simply behaving on a moment-to-moment basis, they may be less sensitive to subtleties of different cues. This insight suggests a need for additional work that focuses on behavior in response to expressive cues rather than inferences, which characterizes the large majority of the literature on expressive cues such as emotional vocalizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion.</head><p>In two studies, we examined how naturally diverse emotional vocalizations influence prosocial behaviors. We found that when participants first made explicit inferences about others' affective states, they donated more money overall, and their helping behaviors were more sensitive to the expressed emotional vocalizations. However, when participants donated without providing explicit inferences first, their helping behaviors were less influenced by vocal cues, and donation amounts were lower overall.</p><p>Collectively, these findings highlight the role of explicit inferential processing in establishing the link between naturalistic emotional expressions and social behavior.</p><p>Our results suggest that drawing inferences about other people's affect when making behavior decisions is not trivial and may require more deliberate mental processing than is sometimes assumed. Past studies have stressed the automaticity of inferring emotions from expressions. For example, perceivers' accuracy in recognizing certain (putatively basic)</p><p>emotions (e.g., sadness, fear, happiness) from facial <ref type="bibr" target="#b27">28</ref> and vocal <ref type="bibr" target="#b28">29</ref> cues was uncompromised by cognitive load and remained generally high (&gt;80%) even when participants were instructed to respond as quickly as possible.</p><p>Additionally, evidence from neuroscientific and behavioral research reveals that canonical emotional expressions can be processed outside of conscious awareness <ref type="bibr" target="#b29">30</ref> and yet shape perceivers' affective inferences <ref type="bibr" target="#b30">31</ref> , emotional learning <ref type="bibr" target="#b31">32</ref> , and motor responses <ref type="bibr" target="#b32">33</ref> .</p><p>In contrast, the present pattern of results corroborates recent evidence</p><p>showing that decisions to draw explicit inferences about emotion are cognitively taxing and people prefer to avoid them when given the option <ref type="bibr" target="#b33">34</ref> .</p><p>Furthermore, these findings are consistent with previous research suggesting that motivational factors are essential for enhancing people's sensitivity to affective cues <ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36</ref> , which in turn impacts their social behavior <ref type="bibr" target="#b36">37</ref> .</p><p>Critically, although the hallmark of emotional expressions is their dynamic and spontaneous nature, the majority of past research demonstrating the automaticity of emotion decoding has relied on simulated emotional expressions. Previous accounts have argued that simulated expressions are often exaggerated and stereotypical portrayals, used as symbols to reflect broad emotion concepts (e.g., smiles for happiness, frown for anger, crying face for grief, etc.) <ref type="bibr" target="#b19">20</ref> . Thus, the impact of such iconic expressions on perceivers' inferential processing and behavior may diverge from naturalistic expressions, which are often more diverse, ambiguous and contextually malleable <ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39</ref> . Our results suggest that automatic inferences of other people's affect, even if they occur in naturalistic settings, do not readily translate into differential social behaviors without explicit inferential processing.</p><p>The key role of explicit inferential processes in shaping affective phenomena was previously emphasized by research focusing on the relationship between language and emotion perception <ref type="bibr" target="#b39">40</ref> . For example, previous work has demonstrated how deliberate access (or lack of access) to emotion words shapes emotion percepts, such that the same facial expressions are encoded differently as a function of relevant emotion word accessibility <ref type="bibr" target="#b40">41</ref> . Recent studies have extended this work from emotion perception to emotion experience, showing how accessibility to more nuanced and diverse emotion language is associated with more diversified life experiences <ref type="bibr" target="#b41">42</ref> . The evidence from this study complements this line of work by showing how affective inferences are not trivial and may have downstream consequences on people's behavior in social interactions.</p><p>By using naturally diverse and multimodal emotional expressions, we introduce a novel and more ecological approach for testing the relationship between affective cues and social behaviors. The use of naturalistic stimuli in emotion research critically enhances its ecological validity, yet it may also come at a cost of tight experimental control. First, the videos in this study were collected from online sources, therefore we could not verify the expressers' feelings from self-reports at the time of the emotional episodes.</p><p>However, all the selected videos were evaluated as capturing authentic emotional episodes by independent judges and were perceived as conveying negative affect by multiple empirical samples <ref type="bibr" target="#b42">43</ref> . Another potential limitation is the relatively low number of videos and donation-seeking contexts that were used in this study. We opted not to present participants with a large number of donation videos, since, to us, presenting a few donation requests more closely resembles how people typically make donation decisions in everyday life. Moreover, in this study, we focused primarily on donation decisions to reflect people's prosocial behaviors more broadly. Although previous research has suggested that donation decisions can be generalized to other types of prosocial behaviors (e.g., helping a stranger, cooperating in games) <ref type="bibr" target="#b12">13</ref> , future work will be necessary to test such generalization with naturalistic stimuli. Future work should also examine a broader spectrum of emotion-laden contexts to further establish the social effects of naturalistic expressions. Nevertheless, we believe the payoff in employing a more naturalistic approach, that combines naturally diverse and multimodal stimuli in research of emotion offsets the limitations. Not only because it is essential for enhancing the ecological validity of emotion research <ref type="bibr" target="#b43">[44]</ref><ref type="bibr" target="#b44">[45]</ref><ref type="bibr" target="#b45">[46]</ref> , but also because it illuminates more complex relationships between affective cues and social behavior.</p><p>Methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study 1A: The Effects of Vocalization Type on Valence Judgments.</head><p>Participants. A sample of 224 participants from the UK were recruited online via Prolific crowd-sourcing platform (<ref type="url" target="https://www.prolific.co/">https://www.prolific.co/</ref>). Four participants were excluded from analysis due to a technical failure in data acquisition, and four participants were removed during preprocessing due to extreme valence ratings. Overall, a sample of 216 participants were included for analysis (143 women, 71 men, 1 other, 1 preferred not to say; = 40.12). In all studies, sample size estimations were determined following the guidelines and simulations provided in <ref type="bibr" target="#b46">47</ref> in order to secure power of 80% to detect an effect size of d = .4 when α = 0.05. All participants reported having intact or corrected vision and hearing.</p><p>Stimuli. Four home-video clips, each presenting a crying person (3 females, 1 male), were selected from the stimuli set used in ref <ref type="bibr" target="#b42">43</ref> . Videos include individuals crying to the camera while sharing a devastating event in their life, including tragic news of cancer relapse, loss of loved ones, breakup with romantic partners, and coping with mental breakdowns. In preparation of the stimuli set, the original soundtracks in the videos were replaced with either typical crying, atypical crying, or no vocalization (i.e., silent video). The typicality of the crying vocalizations used in this study was validated in a previous study, based on the normative evaluation of these signals by naïve listeners <ref type="bibr" target="#b42">43</ref> . This resulted a total set of 12 unique video-clips. All videos were edited using Sony® Vegas® Pro 13 video editing software and converted to .MP4 format. Videos were edited such that the onset of the embedded vocalization was synced with the fitting frame in the video in which the reaction would naturally be expected. The duration of the video clips ranged from 1.43 s to 5.10 s (M = 3.16 s, SD = 1.52).</p><p>Procedure and design. All participants provided informed consent prior to their participation in the study and received standard payment for their participation. The experiment started by filling a short demographic questionnaire, followed by a voice differentiation task, designed to ensure that participants are wearing headphones before continuing to the main experiment. Participants were assigned to one of the three conditions: typical, atypical crying vocalizations, or no vocalization (i.e., silent video). In each condition, participants viewed the videos and then were asked to rate how positive or negative the person in the video felt. Valence ratings were recorded for each video using a bipolar scale ranging from -5 (most negative) to 5 (most positive), with 0 set as a neutral midpoint. No information was given about the origin of vocalizations. All stimuli were presented in a single block in randomized order. The experiment followed a one-way factorial design (vocalization type: typical crying, atypical crying vocalizations, and silent videos; between-subject) and valence ratings as the dependent variable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study 1B: The Effects of Vocalization</head><p>Type on Empathic Responses and Prosocial Behavior. Participants. A sample of 974 participants from the UK were recruited online via Prolific crowd-sourcing platform (<ref type="url" target="https://www.prolific.co/">https://www.prolific.co/</ref>). Twenty-two participants were excluded from analysis due to a technical failure in data acquisition. Overall, a sample of 952 participants were included for analysis (588 women, 354 men, 6 others, 4 preferred not to say; = 36.7). Stimuli. The same as in Study 1A, with an additional vocalization condition in which typical laughter vocalizations were paired with the videos. We included this condition to test whether participants are sensitive to vocalization types that are emotionally conflicting with the video content. This resulted in 16 unique video clips showing the same four individuals with different types of vocal expressions embedded (typical crying, atypical crying, typical laughter, and silent). The editing procedure and video durations were the same as in Study 1A.</p><p>Procedure. All participants provided informed consent prior to their participation in the study and received standard payment for their participation. The experiment started by filling a short demographic questionnaire, followed by a voice differentiation task, designed to ensure that participants were wearing headphones before continuing to the main experiment (except when the main experiment included silent videos, in which case the main experiment started immediately after the completion of the demographic questionnaire).</p><p>In the main experiment, participants were first presented with a text describing the stories of four individuals who experienced a devastating life event and are seeking monetary donations (see Supplementary for stories' transcripts). Participants were then assigned to one of four experimental conditions: typical crying, atypical crying, typical laughter or silent videos.</p><p>After each video, participants were provided a budget of £50 from which they could decide how much money they would like to donate to help the person presented in the video (ranging from £0 to £50). Prior to their donations, participants were informed that a lottery would be held among all the participants in this study, and the amount that the winning participant chose to donate will actually be donated on their behalf. After sending the donations, the remaining amount from the £50 budget will be paid to the winner as a bonus payment, producing an individual incentive against donating. This ensured donations are costly to participants, such that the more they choose to donate, the less they can win as a bonus payment.</p><p>Following each donation, participants rated their empathic responses towards the person presented in the video (see below). Finally, after watching all four video-clips, participants were asked to rate the extent to which they or someone they know have experienced a similar situation to the one described in the experiment, ranging from 1-not at all to 6-completly. Following study completion, the participants were debriefed and informed that the scenarios presented in this study were fictional and that the people presented in the videos were unrelated to the stories. We explained that the reason this information was withheld was to ensure their reaction was authentic, as emotions and donations to fictional individuals may differ from those to real ones. All participants were informed that a real lottery took place. The winner (who incidentally chose to donate the entire sum to the charity) was also debriefed, congratulated, and notified that £200 had been donated</p><p>(anonymously) on their behalf to an authorized charity organization dedicated to helping people experiencing homelessness <ref type="url" target="https://www.crisis.org.uk/get-involved/donate/">https://www.crisis.org.uk/get-involved/donate/</ref>.</p><p>Empathic responses. Following each video, participants were asked to rate their emotional response using eight emotion scales. Specifically, they were asked to rate how sympathetic, compassionate, moved, concerned, uneasy, upset, overwhelmed, and distressed they feel regarding the person presented in the video, ranging from 0-not at all to 6-very much. These scales capture two types of empathic responses: "empathic concern" and "empathic distress" <ref type="bibr" target="#b47">48</ref> . Empathic concern was defined as "the tenderhearted feeling of compassionate concern, feeling sorry for the other" <ref type="bibr" target="#b26">27</ref> , and is calculated as the average ratings of the sympathetic, compassionate, moved, and concerned emotion scales. Empathic distress was defined as "subjectively feeling the same emotion or state as others, usually for intense emotional states" <ref type="bibr" target="#b26">27</ref> , and is calculated as the average ratings of the uneasy, upset, overwhelmed and distressed emotion scales.</p><p>Analysis and design. One-way between-subjects analyses of variance were conducted using 'stat' and 'emmeans' packages <ref type="bibr" target="#b48">49</ref> in R (version 4.4.2) for each dependent variable (donation sums and empathic responses). Post-hoc analyses of significant main effects were Bonferroni corrected. The independent variable in all analyses was the vocalization type embedded in the videos (i.e., typical crying; atypical crying; typical laughter, no vocalization). This study was preregistered on Open Science Framework:</p><p>osf.io/s8tgj.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study 2: The Effects of Vocalization Type on Valence Judgments and Prosocial Behavior.</head><p>Participants. A sample of 570 US participants were recruited online via Prolific crowd-sourcing platform (<ref type="url" target="https://www.prolific.co/">https://www.prolific.co/</ref>). One Hundred and Fourty-six participants were excluded due to failure in attention checks.</p><p>Finally, 58 participants were removed during data preprocessing due to statistically extreme valence ratings (based on median absolute deviation; see ref <ref type="bibr" target="#b49">50</ref> ). Overall, a sample of 366 participants (196 women, 170 men; = 39.2)</p><p>was included for analysis.</p><p>Stimuli. The typical and atypical crying vocalizations from Study 1 were presented in their original video contexts 2 . The laughter vocalizations embedded in crying videos were same as in Study 1. We omitted one crying vocalization from the stimuli set of study 2 since its original video context did not capture a clear view of the crying person's expressive facial behavior. This resulted in a total set of 18 unique videos (3 typical crying, 3 atypical crying; 3 typical laughter; and 9 silent videos). Videos were edited using Sony® Vegas® Pro 13. Videos duration ranged from 1.02 s to 3.10 s (mean duration = 1.41 s, SD = 0.04).</p><p>Procedure. After providing informed consent and passing a voice differentiation task (as in Study 1), participants proceeded to the main task.</p><p>Participants were told that they are about to watch a video from a social media post, showing an individual crying for financial help after experiencing a devastating event in their life. Then, they were randomly assigned to a vocalization type condition (typical crying, atypical crying, typical laugher, or silent videos), and one of two task-framing conditions (donation or affect). In the donation-framed condition, participants were asked to watch the video and first decide how much money they would like to donate to that person from a provided budget of $50. After placing their donation, they then rated how positive or negative that person feels on a valence scale, ranging from -5</p><p>(extremely negative) to 5 (extremely positive) with 0 as neutral. In the affectframed condition, participants were asked to watch the video and first rate how positive or negative the person in the video feels. After submitting a given rating, participants were then asked to decide how much they would like to donate to the person in the video from a provided budget of $50. In both task framing conditions, after placing their donations and providing their affective judgments (in different order), the participants were asked to rate the emotional authenticity of the person in the video, ranging from 0completely posed/fake to 6-completely authentic/genuine; the perceived deception in the post, ranging from 0-not at all to 6-completely; their willingness to share the post in their social media, ranging from 0-not at all to 6-completely; and their willingness to receive more information about the person in the video and the situation that led them to seek help (yes/no binary response). Finally, participants provided demographic information and were debriefed before exiting the study (as in Study 1).</p><p>Analysis and design. Two-way between-subjects analyses of variance were conducted using 'stat' and 'emmeans' packages <ref type="bibr" target="#b48">49</ref> in R (version 4.4.2) for each dependent variable (donation sums, perceived valence ratings, perceived authenticity, perceived deception, and willingness to share video). In all analyses, the independent variables were task framing (donation-framed, affect-framed) and vocalization type (typical crying, atypical crying, typical laughter, or silent videos). All post-hoc analyses of significant effects were FDR corrected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2:</head><p>The effects of vocalization type on prosocial behavior -Bayesian analysis results.</p><p>Table S2.1 Model Comparison in Bayesian analysis of variance for Donation Sums in Study 1B. Models P(M) P(M| data) BF M BF 10 error % Null model 0.5 0.991 107.372 1.000 Vocalization type model 0.5 0.009 0.009 0.009 0.030</p><p>Note. The null model posits no difference between the vocalization type conditions (i.e., silent videos, congruent crying, ambiguous crying, and incongruent crying vocalizations. Alternatively, the vocalization type model posits that at least one of the four groups differs from the others (i.e., a main effect for vocalization type). Table S2.2 Model Comparison in Bayesian analysis of variance for Empathic Concern in Study 1B. Models P(M) P(M| data) BF M BF 10 error % Null model 0.5 0.952 19.633 1.000 Vocalization type model 0.5 0.048 0.051 0.051 0.017 Note. The null model posits no difference between the vocalization type conditions (i.e., silent videos, congruent crying, ambiguous crying, and incongruent crying vocalizations. Alternatively, the vocalization type model posits that at least one of the four groups differs from the others (i.e., a main effect for vocalization type). Table S2.3 Model Comparison in Bayesian analysis of variance for Donation Sums in Study 2. Models P(M) P(M| data) BF M BF 10 error % Null model 0.5 0.937 14.803 1.000 Vocalization type model 0.5 0.063 0.068 0.068 0.003 Note. The null model posits no difference between the vocalization type conditions (i.e., silent videos, congruent crying, ambiguous crying, and incongruent crying vocalizations. Alternatively, the vocalization type model posits that at least one of the four groups differs from the others (i.e., a main effect for vocalization type). Figure S2.3. Raincloud plots showing the response distributions of donation sums in Study 2 across the four vocalization type conditions (silent videos, congruent crying, ambiguous crying, and incongruent crying vocalizations).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S3: The effects of vocalization type on prosocial behavior -Scenarios.</head><p>In Study 1, participants were presented with narratives describing devastating life events followed by video clips of distressed individuals. For the purpose of this study, we generated four fictional, yet realistic scripts, describing the unfortunate circumstances that (ostensibly) led the targets in the videos to ask for help. Irrespective of any potential differences between the stories, it is noted that the main analyses in Study 1 is based on the aggregated results per experimental condition (i.e., vocalization type) across all stories' scripts. Nevertheless, a supplemental analysis of the different stories and their ability to elicit emotional responses from readers is provided below.</p><p>Scenario 1 -Medical procedure.</p><p>Jane, David, Amanda, and Kate are extremely devoted dog owners. They have their dogs for many years and consider them as part of their families.</p><p>Tragically, each of them had experienced the same dreadful event a few days ago as their beloved dogs were accidentally hit by car outside their house and are now hospitalized in critical condition. To save their lives, they must go through a series of costly surgeries followed by a long hospitalization period in the local clinic. Sadly, their financial state makes it impossible for them to cover all the medical expenses. Desperately, they decided to post a short video of themselves on social media telling their story and ask for financial assistance for saving their dog before it's too late.</p><p>Scenario 2 -Fire in the house.</p><p>The next videos present Jane, David, Amanda, and Kate who experienced a similar tragic event. They are all hardworking single parents who work in double shifts at minimum wage to provide for their young 6-year-old daughter. A few days ago, when the family was away, a fire broke out in their daughters' room. Fortunately, no one was hurt and the fire department extinguished the flames before the fire spread to the rest of the house.</p><p>However, the entire content of the child's room was destroyed. All the furniture, clothing, books and toys that they purchased over the years for their child were entirely ruined. Due to their financial state, there is simply no way they will be able to buy replacements without help. Their daughter, now sleeping in the parent's bedroom, has been crying ever since. She misses her room with all her books and toys. Desperately, they decided to post a short video of themselves on social media telling their story and ask for financial assistance in buying contents for a new children's room.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scenario 3 -Computer was stolen.</head><p>Jane, David, Amanda, and Kate are young product designers who just finished their studies and started to work as freelancers on several projects. To be able to work professionally, each of them spent his/her entire savings on purchasing a top-notch computer including state-of-the-art hardware and software required for professional designers. Unfortunately, a few days ago their apartments were burglarized, and their new computers with all the work they have done so far (and yet to be paid of) have been stolen. Due to their current financial state, they can't get more loans to buy a new computer and are at risk of losing everything they've worked for to make their first steps in the industry. Desperately, they decided to post a short video of themselves on social media telling their story and ask for financial assistance in buying a new computer to keep their work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scenario 4 -Small business in danger.</head><p>Jane, David, Amanda, and Kate are young pastry chefs who worked for years as low wage employees in chain bakeries. About a year ago, each of them decided to finally follow their dream and open a small patisserie in their city.</p><p>They invested countless hours and their entire lifesavings in the new businesses. The outcome exceeded everything they wished for -their new patisseries were well received by the local critiques, and they started to have more and more customers. Tragically, a few days ago, major flooding occurred and although no one was injured, most of their kitchen equipment and machineries were damaged beyond repair. Furthermore, their insurance companies refused to compensate them for the damages and compensation from the government, if any, could take months. They are at serious risk of losing their businesses. Desperately, they decided to post a short video of themselves on social media telling their story and ask for financial assistance in recovering at least some of the damage caused by the flooding so that they could save their beloved business.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S3: Transcripts validation experiment.</head><p>To measure the affective response that the four transcripts elicit in participants, we presented 209 participants with one transcript and measured their sadness, empathic concern, empathic distress, and arousal responses to the story. As illustrated in Fig. <ref type="figure" target="#fig_5">S3</ref>, Scenario 3 ("Computer was stolen") elicited lower sadness, empathic concern and empathic distress responses, compared to Scenario 2 ("Fire in the house"), see Table <ref type="table">S3</ref>.1.</p><p>Scenario 3 ("Computer was stolen") elicited lower arousal response compared to Scenario 4 ("Small business in danger") and Scenario 1</p><p>("Medical procedure"), see Table <ref type="table">S3</ref>.2. Importantly, these differences did not affect the main results of Study 1. Specifically, we reanalyzed the data of Study1 with both vocalization type and scenario type as IVs and found that the main effect for scenario type and the interaction between scenario type and vocalization type were non-significant (p&gt;.05). Table S3.1 Overall difference in emotional responses towards Scenario 2 and Scenario 3 (M = Scenario 2 -Scenario 3). Emotional response M 95% CI p Sadness 0.94 [0.16, 1.72] &lt;.02 Empathic Concern 0.75 [0.09, 1.41] &lt;.02 Empathic Distress 0.81 [0.17, 1.44] &lt;.01 Arousal 0.81 [-0.05, 1.66] &gt;.07</p><p>Note. M = mean. CI = confidence interval. p = adjusted p-value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table S3.2</head><p>Overall difference in arousal responses to Scenario Analysis and design. Four distinct dependent variables were measured in this validation study: felt sadness, ranging from 0 (not at all) to 6 (very much); felt arousal, ranging from 0 to 8; Empathic concern, indicating the participants' scores on the empathic concern index, ranging from 0 to 6; and empathic distress, indicating the participants' scores on the empathic distress index, ranging from 0 to 6 (see manuscript for full description of empathic concern and empathic distress scores). A one-way between-subjects analysis of variance was conducted for each dependent variable separately. All post-hoc analyses of significant main effects were Bonferroni corrected. The independent variable in all analyses was the scenario type (Scenario 1-4).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Illustration of the experimental procedures for Study 1 and Study 2.</figDesc><graphic coords="8,72.00,71.95,468.00,331.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. (A) Perceived valence ratings of the targets in the videos as a function of vocalization type. The vocalization type influenced the perceivers' affective judgments of the individuals in the videos. (B) Donation sums as a function of vocalization type. Donation sums decreased only when the targets' videos were paired with typical laughter (in yellow). (C) Empathic concern as a function of vocalization type. Empathic concern decreased only when the targets' videos were paired with typical laughter (in yellow). Error bars reflect bootstrapped 95% confidence intervals. (D) Correlation scatterplot between the donation sums and the empathic concern scores. Each data point represents a single participant colored by the vocalization type they were exposed to (silent videos in teal, typical crying in red, atypical crying in orange, and typical laughter in yellow). The shaded area represents the computed 95% confidence region. See the online article for the color version of this figure.</figDesc><graphic coords="11,72.00,71.95,468.00,315.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. (A) Perceived valence ratings of the targets in the videos as a function of task framing and vocalization type. Vocalization type influenced the perceivers' affective</figDesc><graphic coords="14,72.00,191.95,418.70,488.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure S2. 1 .</head><label>1</label><figDesc>Figure S2.1. Raincloud plots showing the response distributions of donation sums in Study 1B across the four vocalization type conditions (silent videos, congruent crying, ambiguous crying, and incongruent crying vocalizations).</figDesc><graphic coords="39,72.00,437.80,468.00,200.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure S2. 2 .</head><label>2</label><figDesc>Figure S2.2. Raincloud plots showing the response distributions of donation sums in Study 1B across the four vocalization type conditions (silent videos, congruent crying, ambiguous crying, and incongruent crying vocalizations).</figDesc><graphic coords="40,72.00,337.40,468.00,214.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. S3 .</head><label>S3</label><figDesc>Fig. S3. Violin and boxplots of experienced (a) sadness, (b) arousal, (c) empathic concern, and (d) empathic distress in response to the stories' transctipts. Scenario 3 ("Computer was stolen") elicited lower sadness, concern and distress responses compared to Scenario 2 ("Fire in the house") and lower arousal response compared to Scenario 4 ("Small business in danger") and Scenario 1 ("Medical procedure"). Central white circles and vertical white bars reflect mean and SD.</figDesc><graphic coords="46,72.00,71.95,332.30,187.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><figDesc>3 and Scenario 4 (first row: M = Scenario 3 -Scenario 4), and to Scenario 1 and Scenario 3 [0.02, 1.73] &lt;.05 Note. M = mean. CI = confidence interval. p = adjusted p-value.Methods.Participants. A sample of 224 participants from the UK were recruited online via Prolific crowd-sourcing platform (https://www.prolific.co/). 15 participants did not complete the experiment. Overall, a sample of 209 participants (129 females, 77 males, 1 other, and 2 rathered not to say; = 40.3) were included for analysis. All participants reported having intact or corrected vision.Procedure and design. All participants provided informed consent prior to their participation in the study and received standard payment for their participation. The experiment started by filling a short demographic questionnaire. Then, participants were randomly allocated to one the four scenario conditions. In each condition, participants were presented with a text describing the narrative of four individuals who experienced a devastating life event and seeking monetary donations. After reading the scenario, the participants rated their affective reaction, empathic concern, empathic distress, and felt arousal using standard rating scales (as in Study 1B).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="38,72.00,71.95,426.25,404.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="41,72.00,373.40,466.05,328.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Affective Judgments as a function of Task Framing and Vocalization Type.</figDesc><table><row><cell>Task Framing:</cell><cell cols="2">Donation</cell><cell cols="2">Affective Inference</cell></row><row><cell>Vocalization Type</cell><cell>M (SD)</cell><cell>95% CI</cell><cell>M (SD)</cell><cell>95% CI</cell></row><row><cell>Typical Crying</cell><cell>-3.51 (1.46)</cell><cell>[-3.93, -3.09]</cell><cell>-5.00 (0.00)</cell><cell>[-5.00, -5.00]</cell></row><row><cell>Atypical Crying</cell><cell>-2.25 (2.71)</cell><cell>[-2.99, -1.50]</cell><cell>-3.94 (1.14)</cell><cell>[-4.27, -3.61]</cell></row><row><cell>Typical Laughter</cell><cell>-2.15 (1.86)</cell><cell>[-2.75, -1.55]</cell><cell>-2.96 (1.59)</cell><cell>[-3.42, -2.49]</cell></row><row><cell>Silent Videos</cell><cell>-3.69 (1.52)</cell><cell>[-4.11, -3.27]</cell><cell>-5.00 (0.00)</cell><cell>[-5.00, -5.00]</cell></row></table><note><p>Note. M = Mean valence ratings; SD = Standard Deviation; CI = Confidence Interval.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>This decision was to ensure that sensory integration between the video and the voice was not responsible for any discounting of the vocal cues.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental Material</head><p>Helping behaviors rely on explicit but not implicit inferences of affect Doron Atias and Maria Gendron S1: The relationships between prosocial behavior and empathic reactions to affective cues.</p><p>The empathic distress scores significantly correlated with both empathic concern (Fig. <ref type="figure">S4c</ref>) and donation sums (Fig. <ref type="figure">S4b</ref>). The similarity in experience ratings statistically correlated with all other measurements but to a much lesser extent (Fig. <ref type="figure">S4d-S4f</ref>).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Functional flexibility of infant vocalization and the emergence of language</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Oller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc Natl Acad Sci</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page">6318</biblScope>
			<date type="published" when="2013">2013</date>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Acoustic Analyses of Developmental Changes and Emotional Expression in the Preverbal Vocalizations of Infants</title>
		<author>
			<persName><forename type="first">E</forename><surname>Scheiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hammerschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Jürgens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zwirner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Voice</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="509" to="529" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Associations between acoustic features of maternal speech and infants&apos; emotion regulation following a social stressor</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kolacz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Dasilva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">I</forename><surname>Bertenthal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Porges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Infancy</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="135" to="158" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The Development of Emotion Expression during the First Two Years of Life</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Z</forename><surname>Malatesta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Monographs of the Society for Research in Child Development</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page">i</biblScope>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The emotions</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Frijda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Studies in Emotion and Social Interaction</title>
		<imprint>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Social Functions of Emotions at Four Levels of Analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Keltner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Haidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition and Emotion</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="505" to="521" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The social roles and functions of emotions</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Frijda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mesquita</surname></persName>
		</author>
		<idno type="DOI">10.1037/10152-002</idno>
	</analytic>
	<monogr>
		<title level="m">Emotion and culture: Empirical studies of mutual influence</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Kitayama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Markus</surname></persName>
		</editor>
		<meeting><address><addrLine>Washington</addrLine></address></meeting>
		<imprint>
			<publisher>American Psychological Association</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="51" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Emotions are social</title>
		<author>
			<persName><forename type="first">B</forename><surname>Parkinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British J of Psychology</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="663" to="683" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">How emotions regulate social life: The emotions as social information (EASI) model</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Van Kleef</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current directions in psychological science</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="184" to="188" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The Social Effects of Emotions</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Van Kleef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Côté</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Psychology</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="629" to="658" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Moving the self and others to do good: The emotional underpinnings of prosocial behavior</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Van Kleef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-J</forename><surname>Lelieveld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Opinion in Psychology</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="80" to="88" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Emotion, Regulation, and Moral Development</title>
		<author>
			<persName><forename type="first">N</forename><surname>Eisenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. Rev. Psychol</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="665" to="697" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">How emotional expressions shape prosocial behavior: Interpersonal effects of anger and disappointment on compliance with requests</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Van Doorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Van Kleef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Der Pligt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Motivation and emotion</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="128" to="141" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The Face of Need: Facial Emotion Expression on Charity Advertisements</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Small</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Verrochi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Marketing Research</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="777" to="787" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Tears evoke the intention to offer social support: A systematic investigation of the interpersonal effects of emotional crying across 41 countries</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Zickfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Social Psychology</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page">104137</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The Interpersonal Effects of Anger and Happiness in Negotiations</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Van Kleef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K W</forename><surname>De Dreu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S R</forename><surname>Manstead</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="57" to="76" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Spontaneous facial behavior during intense emotional episodes: Artistic truth and optical truth</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Fernández-Dols</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-A</forename><surname>Ruiz-Belda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The psychology of facial expression</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="255" to="274" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Emotion and Expression: Naturalistic Studies</title>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Fernández-Dols</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Crivelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Emotion Review</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="24" to="29" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Do emotions result in their predicted facial expressions? A meta-analysis of studies on the co-occurrence of expression and emotion</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">I</forename><surname>Durán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Fernández-Dols</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Emotion</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1550" to="1569" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Facial expressions of emotion: What lies beyond minimal universality?</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="page" from="379" to="391" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Loud and unclear: Intense real-life vocalizations during affective situations are perceptually ambiguous and contextually malleable</title>
		<author>
			<persName><forename type="first">D</forename><surname>Atias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="page" from="1842" to="1848" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Body Cues, Not Facial Expressions, Discriminate Between Intense Positive and Negative Emotions</title>
		<author>
			<persName><forename type="first">H</forename><surname>Aviezer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Trope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Todorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">338</biblScope>
			<biblScope unit="page" from="1225" to="1229" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Real-life intense fear is communicated through context, not facial expressions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lecker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl. Acad. Sci. U.S.A</title>
		<imprint>
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="page">2414677122</biblScope>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Face and context integration in emotion inference is limited and variable across categories and individuals</title>
		<author>
			<persName><forename type="first">S</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jara-Ettinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gendron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">2443</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Universality Reconsidered: Diversity in Making Meaning of Facial Expressions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gendron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Crivelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Barrett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Curr Dir Psychol Sci</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="211" to="219" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Context in Emotion Perception</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mesquita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gendron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Curr Dir Psychol Sci</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="286" to="290" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The Many Faces of Empathy: Parsing Empathic Phenomena through a Proximate, Dynamic-Systems View of Representing the Other in the Self</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Preston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Hofelich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Emotion Review</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="24" to="33" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The automaticity of emotion recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Tracy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Robins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Emotion</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="81" to="95" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Automaticity in the recognition of nonverbal emotional vocalizations</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Lima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anikin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Castro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Emotion</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="219" to="233" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A subcortical pathway to the right amygdala mediating &quot;unseen&quot; fear</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Öhman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl. Acad. Sci. U.S.A</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="1680" to="1685" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Seeing What You Feel: Affect Drives Visual Perception of Structurally Neutral Faces</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Wormwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Quigley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Barrett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="496" to="503" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Conscious and unconscious emotional learning in the human amygdala</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Öhman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">393</biblScope>
			<biblScope unit="page" from="467" to="470" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unconscious Facial Reactions to Emotional Facial Expressions</title>
		<author>
			<persName><forename type="first">U</forename><surname>Dimberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Thunberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Elmehed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol Sci</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="86" to="89" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Empathy is hard work: People choose to avoid empathy because of its cognitive costs</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Cameron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="page" from="962" to="976" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Bernieri</surname></persName>
		</author>
		<title level="m">Interpersonal Sensitivity: Theory and Measurement</title>
		<imprint>
			<publisher>Psychology Press</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Empathy: A motivated account</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page" from="1608" to="1647" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Distress, and Compassion: Turning a Blind Eye to the Suffering of Others</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Van Kleef</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol Sci</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1315" to="1322" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Real-life and posed vocalizations to lottery wins differ fundamentally in their perceived valence</title>
		<author>
			<persName><forename type="first">D</forename><surname>Atias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Aviezer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Emotion</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1394" to="1399" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">When emotions run high: A critical role for context in the unfolding of dynamic, real-life facial affect</title>
		<author>
			<persName><forename type="first">J</forename><surname>Israelashvili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Hassin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Aviezer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Emotion</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="558" to="562" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Language as context for the perception of emotion</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Lindquist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gendron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="327" to="332" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Emotion words shape emotion percepts</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gendron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Lindquist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Barsalou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Barrett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Emotion</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="314" to="325" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Emotional Granularity is Associated with Daily Experiential Diversity</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hoemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kuppens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gendron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Affec Sci</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="291" to="306" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Context shapes emotion perception of real-life laughter and crying vocalizations regardless of their diverse perceptual profiles</title>
		<author>
			<persName><forename type="first">D</forename><surname>Atias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bamberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Peleg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Aviezer</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Manuscript in preparation</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Faking It Isn&apos;t Making It: Research Needs Spontaneous and Naturalistic Facial Expressions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dawel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Krumhuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Palermo</surname></persName>
		</author>
		<idno type="DOI">10.1007/s42761-025-00320-1</idno>
	</analytic>
	<monogr>
		<title level="j">Affec Sci</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A systematic survey of face stimuli used in psychological research 2000-2020</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dawel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Horsburgh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behav Res</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="1889" to="1901" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Empathic accuracy: Lessons from the perception of contextualized real-life emotional expressions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Atias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Aviezer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Neural Basis of Mentalizing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="171" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">How many participants do we have to include in properly powered experiments? A tutorial of power analysis with reference tables</title>
		<author>
			<persName><forename type="first">M</forename><surname>Brysbaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of cognition</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Two facets of affective empathy: concern and distress have opposite relationships to emotion recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Israelashvili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sauter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition and Emotion</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1112" to="1122" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Emmeans: Estimated Marginal Means, Aka Least-Squares Means</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lenth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">R Package Version</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">How to Classify, Detect, and Manage Univariate and Multivariate Outliers, With Emphasis on Pre-Registration</title>
		<author>
			<persName><forename type="first">C</forename><surname>Leys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Delacre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Mora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lakens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Review of Social Psychology</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
