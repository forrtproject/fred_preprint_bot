<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SOCIAL VIRTUAL REALITY ELICITS NATURAL INTERACTION BEHAVIOR 1 Social Virtual Reality Elicits Natural Interaction Behavior with Personalized and Generic Avatars</title>
				<funder>
					<orgName type="full">Swiss National Science Foundation</orgName>
				</funder>
				<funder ref="#_KssZKBy">
					<orgName type="full">SNSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Gayoung</forename><surname>Son</surname></persName>
							<email>gayoung.son@unibe.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Cognitive Psychology, Perception and Research Methods</orgName>
								<orgName type="institution">University of Bern</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">University of Bern</orgName>
								<address>
									<addrLine>Fabrikstrasse 8</addrLine>
									<postCode>3012</postCode>
									<settlement>Bern</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marius</forename><surname>Rubo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Cognitive Psychology, Perception and Research Methods</orgName>
								<orgName type="institution">University of Bern</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SOCIAL VIRTUAL REALITY ELICITS NATURAL INTERACTION BEHAVIOR 1 Social Virtual Reality Elicits Natural Interaction Behavior with Personalized and Generic Avatars</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">12F0F94DF3DF23BC3936C624B216559D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-23T06:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Social Virtual Reality</term>
					<term>Avatar Appearance</term>
					<term>Nonverbal Communication</term>
					<term>Gaze Behavior</term>
					<term>Social Interaction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Social Virtual Reality (VR) allows to interact in shared virtual environments while embodying computerized avatars which display behavior in real-time. The technique mimics real social interactions in its preservation of the spatial relatedness of social gaze and other facets of non-verbal behavior, but the extent to which people behave naturally in such artificial situations remains largely unknown. Here we show in 128 participants who interacted in dyads that the coordination of gaze and speech behavior closely follows patterns known from face-to-face interactions: eye gaze to a partner's eye region was relatively enhanced while listening compared to while speaking and at the end of a speaking turn compared to the beginning of a turn. Gaze, speech and smiling behavior were sensibly adapted to differing conversation topics (small talk, personal talk, talk about conflicting opinions). In contrast to written communication on the internet, anonymization -here realized using generic as opposed to personalized avatars -was not associated with behavioral disinhibition or any differences in subjective experience, possibly due to a closeness-generating effect of direct eye contact despite the concealment of one's own and the interaction partner's identity. Our results indicate that social VR elicits natural interaction behavior and may be used to implement anonymized face-to-face interactions without the negative side-effects often associated with anonymization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Social Virtual Reality Elicits Natural Interaction Behavior with Personalized and Generic</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Avatars</head><p>Social virtual reality (Social VR) is a form of digital communication which aims to closely mimic real-life social interactions <ref type="bibr" target="#b26">(Hennig-Thurau et al., 2023;</ref><ref type="bibr" target="#b46">Maloney et al., 2020;</ref><ref type="bibr" target="#b48">McVeigh-Schultz et al., 2019;</ref><ref type="bibr" target="#b53">Y. Pan &amp; Steed, 2017;</ref><ref type="bibr" target="#b57">Rogers et al., 2022;</ref><ref type="bibr" target="#b65">Smith &amp; Neff, 2018;</ref><ref type="bibr" target="#b70">Wei et al., 2022)</ref>: inhabiting a shared virtual environment as virtual avatars with others, people may express their non-verbal communication behavior in its natural integration in social space, including deictic gestures and gaze communication. While social VR may approximate real-life social situations in its replication of a shared environment, it might lead to a disruption of the expression of the natural interaction behavior. After all, social VR represents an artificial situation where individuals encounter each other as computerized avatars which, despite advances in visual quality <ref type="bibr" target="#b71">(Weidner et al., 2023)</ref>, cannot be mistaken for reality. Furthermore, the ability to customize avatars introduces a layer of anonymity that may affect how the interaction unfolds, as users can conceal their real identities or conversely present themselves as they appear in real-life.</p><p>Although it is commonly argued that VR offers more ecologically valid settings in which to study natural behavior <ref type="bibr" target="#b54">(Parsons, 2015)</ref>, the extent to which interactions in VR are truly naturalistic remains relatively unclear.</p><p>Over the past years, several studies aimed to empirically assess the extent to which social interaction in social VR can be seen as naturalistic. Previous studies in the field often focused on subjective experiences to assess the quality of interaction in VR, with fewer studies assessing objective behavior in participants interacting in VR environments <ref type="bibr" target="#b73">(Xenakis et al., 2022;</ref><ref type="bibr" target="#b74">Yaremych &amp; Persky, 2019)</ref>. <ref type="bibr" target="#b49">Miller et al. (2023)</ref> found that participants in social VR kept greater interpersonal distances in larger rooms, a behavior known from real-life situations. Despite only having gesture and audio cues, participants embodying a robotic avatar in social VR displayed similar speech patterns to face-to-face interactions, reflected in higher rates of interruptions compared to interactions with no visible avatars <ref type="bibr" target="#b65">(Smith &amp; Neff, 2018)</ref>. Attentional processes were sometimes assessed based on head orientation (e.g., <ref type="bibr" target="#b25">Han et al., 2022;</ref><ref type="bibr" target="#b31">Ichino et al., 2023;</ref><ref type="bibr" target="#b49">Miller et al., 2023;</ref><ref type="bibr" target="#b65">Smith &amp; Neff, 2018)</ref>, with only few studies assessing eye gaze behavior which is known to more accurately represent attention allocation <ref type="bibr" target="#b0">(Abdullah et al., 2021;</ref><ref type="bibr" target="#b45">Ma &amp; Pan, 2022)</ref>.</p><p>Overall, findings on interaction behavior in social VR remain sparse and the field has not yet fully connected with broader accounts on interaction psychology, which, mostly by analyzing videotapes of real face-to-face interactions, have compiled a rich and detailed body of knowledge on face-to-face interactions throughout the last decades <ref type="bibr" target="#b6">(Aron et al., 1997;</ref><ref type="bibr">J. A. Hall &amp; Knapp, 2013;</ref><ref type="bibr" target="#b23">J. A. Hall et al., 2019)</ref>. A comparison of the patterns of interaction behavior in social VR with patterns previously observed real-life situations may help to assess the extent to which social VR fosters realistic interactions.</p><p>A robust behavioral pattern observed in real-life interactions is an increased time spent gazing at a partner's eyes while listening to the partner compared to while speaking (e.g., <ref type="bibr" target="#b13">Degutyte &amp; Astell, 2021)</ref>, which was also observed in social VR, albeit in a triadic interaction <ref type="bibr" target="#b0">(Abdullah et al., 2021)</ref>. Additionally, people in real-life interactions tend to gaze more frequently towards their partner's eyes at the end of speaking turns compared to the beginning of speaking turns <ref type="bibr" target="#b29">(Hessels et al., 2019;</ref><ref type="bibr" target="#b30">Ho et al., 2015;</ref><ref type="bibr" target="#b35">Kendon, 1967)</ref> in order to manage turn-taking. This mechanism was replicated in principle in an interaction with a computer agent shown on a computer screen where the avatar's gaze avoidance at the beginning of turns was shown to influence turn-taking <ref type="bibr" target="#b3">(Andrist et al., 2013)</ref>, but little is known about the presence of this pattern in people interacting with each other through social VR.</p><p>People may additionally vary their interaction behavior depending on the type of conversation in which they are involved. For example, <ref type="bibr" target="#b59">Rutter et al. (1978)</ref> found that participants gazed more at the end of a turn during a competitive conversation (in which participants were instructed to persuade the partner) compared to a cooperative conversation (in which participants discussed shared opinions). On the other hand, participants tend to divert their gaze when responding to personal <ref type="bibr" target="#b16">(Exline et al., 1965)</ref> or challenging questions <ref type="bibr" target="#b19">(Glenberg et al., 1998)</ref>.</p><p>Further, people may modulate the extent of their speaking to express dominance over a partner (J. A. <ref type="bibr" target="#b22">Hall et al., 2005)</ref> and may smile to express positive affect, interest or trust <ref type="bibr">(DeWall et al., SOCIAL VIRTUAL REALITY ELICITS NATURAL INTERACTION BEHAVIOR 5 2009;</ref><ref type="bibr" target="#b39">Krumhuber et al., 2007)</ref>. While it was observed that computer characters which showed a smiling expression were similarly perceived as more positive <ref type="bibr" target="#b51">(Oh et al., 2016)</ref>, little is known about the use of smiling behavior in social VR.</p><p>Social VR setups additionally allow manipulation of social interaction properties in ways that have no direct equivalent in real-life interactions. A manipulation which attracted attention in VR research is the design of virtual avatars, which may visually resemble participants' physical appearance (described as personalized) but may likewise be given any other physical appearance (described as generic). Studies overall observed positive effects of avatar personalization on self-reported presence, social presence or embodiment <ref type="bibr" target="#b69">(Waltemate et al., 2018;</ref><ref type="bibr" target="#b71">Weidner et al., 2023)</ref>, although not all studies observed such an effect (e.g., <ref type="bibr" target="#b41">Latoschik et al., 2017;</ref><ref type="bibr" target="#b45">Ma &amp; Pan, 2022;</ref><ref type="bibr" target="#b61">Salagean et al., 2023;</ref><ref type="bibr" target="#b66">Suk &amp; Laine, 2023)</ref>.</p><p>While the mentioned studies on effects of avatar personalization were conducted on participants who entered a virtual environment in isolation, one previous study investigating group behavior in social VR <ref type="bibr" target="#b25">(Han et al., 2022)</ref> observed increased self-reported presence but decreased enjoyment in participants using personalized as compared to generic avatars in interactions with others. To the best of our knowledge, no studies have yet directly investigated effects of avatar personalization on fine-grained gaze and speech behavior in social VR. The embodiment of generic avatars, which conceal a person's identity relative to personalized avatars, may be expected to exert a disinhibiting influence on social interaction behavior, similar to disinhibition effects observed in anonymous written communication on the internet <ref type="bibr" target="#b67">(Suler, 2004)</ref>.</p><p>Such effects are thought to originate from a detachment of social information processing from regulatory processes as they naturally unfold in the presence of others. While disinhibition in written communication may be expressed in a more extreme choice of words, behavior in face-to-face interactions can be more subtly regulated in the balancing of inhibition and disinhibition. For instance, social inhibition was found to be associated with an aversion of eye contact <ref type="bibr" target="#b11">(Chen &amp; Clarke, 2017)</ref>, whereas disinhibition was linked to louder speaking (J. A. <ref type="bibr" target="#b22">Hall et al., 2005)</ref> and more interrupting behavior <ref type="bibr" target="#b34">(Keltner et al., 2003)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Present Study</head><p>The present study investigates whether established behavioral patterns known from face-to-face interactions are likewise traceable in a social VR environment, indicating the extent to which interactions in social VR may be seen as natural or lifelike, and whether the use of generic avatars leads to disinhibited social behavior. We implemented a comprehensive display and recording of behavioral information (with visible head and eye movements, hand and finger movements, facial expressions and lip synchronization) as recommended by (X. <ref type="bibr" target="#b52">Pan &amp; Hamilton, 2018;</ref><ref type="bibr" target="#b70">Wei et al., 2022;</ref><ref type="bibr" target="#b71">Weidner et al., 2023)</ref>, and focused on an in-depth analysis of interaction behavior. A key variable of interest was eye gaze behavior which was described as a particularly important communication channel in navigating dyadic interactions <ref type="bibr" target="#b4">(Argyle &amp; Cook, 1976;</ref><ref type="bibr" target="#b13">Degutyte &amp; Astell, 2021</ref>), but we also assessed speaking and smiling behavior.</p><p>Participants engaged in dyadic interactions on different conversational topics, either embodying personalized or generic avatars. We hypothesized that participants would show behavioral patterns known from face-to-face interactions (more gaze at a partner's eye region while listening compared to while speaking and at the end of a turn compared to the beginning of a turn), and that participants embodying generic as compared to personalized avatars would show signs of disinhibition (more gaze to the eye region, louder speaking, more interrupting) and report lower presence, social presence and body ownership. Analyses testing these hypotheses were complemented with a presentation of several social behavioral measurements within and across conversation topics to allow for a comprehensive characterization of behavior in social VR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Materials and Methods</head><p>Participants 128 participants (105 female, 22 male, 1 diverse, aged 18 to 51 years, M = 22.60, SD = 3.57) interacted in dyads in social VR. The sample size was predefined to allow for the detection of between-group effects of d = 0.5 with a probability of 80% with alpha set to .05. The majority (122 students, 95.31%) were psychology students enrolled at the University of Bern, at the Bachelor level. Exclusion criteria included neurological illnesses, use of centrally acting agents and strong eye sight problems which require glasses that do not fit under a VR equipment. The study was approved by the ethics committee of the University of Bern and was preregistered (<ref type="url" target="https://osf.io/p53fu">https://osf.io/p53fu</ref>). Data and scripts are available at (<ref type="url" target="https://osf.io/fyr2q/">https://osf.io/fyr2q/</ref>). Before the preregistered sample size was reached, we excluded one dyad which reported that they spoke English in the interaction, another dyad in which the computers lost connection during the interaction, and seven dyads in which the system failed to transmit facial expressions. Participants reported various degrees of prior experience with VR. 55 participants had no experience with VR (42.97%), 43 had one prior experience (33.59%), 23 reported two to three experiences (17.97%) and 7 participants disclosed 3 or more experiences with VR (5.47%). No participants reported regular VR use.</p><p>Each dyad was randomly assigned to either a condition where both participants embodied either personalized or generic avatars. All dyads conversed on three topics in a randomized order (small talk, personal talk and opinion talk; see section Tasks). The entire procedure and data preprocessing were pre-registered. Pre-registered hypotheses were complemented with novel hypotheses (e.g., a more in-depth comparison of gaze to the eye region at the beginning and end of a turn) to provide a more comprehensive picture of participant behavior while also controlling for multiple comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Apparatus and Software</head><p>In each testing, two desktop laptops (Windows 11, NVIDIA GeForce RTX 4070 GPU, Intel Core i9-13900HX CPU, 32 GB RAM) were used to run a custom social VR software developed in Unity 3D (<ref type="url" target="https://unity.com">https://unity.com</ref>). The computers were connected via Ethernet and communicated in a shared local area network (LAN). Network communication was developed with Fish-Net networking (<ref type="url" target="https://github.com/FirstGearGames/FishNet/">https://github.com/FirstGearGames/FishNet/</ref>). Network tick rate was set to 120 Hz. We monitored round-trip-times (RTT) for signals throughout all sessions. RTT was defined as the total time elapsed from when a signal was processed in a client's FixedUpdate-Loop, sent to the server, returned to and received by the client. Across all sessions, the average RTT was 47.85 ms, with 95% of measured RTT values falling between 27.50 ms and 71.00 ms. To visualize the VR environment, Meta Quest Pro (Model: DK94EC; <ref type="url" target="https://www.meta.com">https://www.meta.com</ref>) head-mounted displays (HMDs) were used. These HMDs feature sensors to record eye movements, facial expressions, hand gestures and voice data, which was collected at 100 Hz and rendered onto the participant's avatar concurrently. Audio communication was realized using Dissonance Voice Chat (<ref type="url" target="https://placeholder-software.co.uk/">https://placeholder-software.co.uk/</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Avatar Creation</head><p>Both personalized and generic avatars were created using Character Creator 4 (CC4; <ref type="url" target="https://www.reallusion.com/character-creator/">https://www.reallusion.com/character-creator/</ref>) and the Headshot plugin. This tool generates a realistic 3D model from a single portrait photograph. For each avatar, we manually adjusted facial details such as eye color and blendshapes as well as hair and body size. The avatars were dressed identically. Only the head, torso, and white silhouettes of the hand were visible in the virtual environment, which is a common configuration in social VR environments and is often preferred over full-body visualizations <ref type="bibr" target="#b71">(Weidner et al., 2023)</ref>. In the personalized condition, we used the participant photo to generate a look-alike avatar. In the generic condition, twelve different generic avatars (6 female, 6 male) were created based on artificially created portraits of non-existent individuals <ref type="bibr">(Karras et al., 2018, https:</ref>//this-person-does-not-exist.com/en) and were selected randomly for each participant with only gender matched. Avatars were exported as FBX (Filmbox) files incorporating wrinkle textures, the CC4 extended expression profile and 15 visemes (1:1 Direct system; <ref type="url" target="https://manual.reallusion.com/Character-Creator-4">https://manual.reallusion.com/Character-Creator-4</ref>). The CC4 extended expression profile includes 140 blendshapes to control nuanced facial expressions. See Figure <ref type="figure" target="#fig_0">1</ref> for examples of personalized avatars generated from portraits and generic avatars.</p><p>Head movements, eye movements, hand and finger movements as well as facial expressions were directly mapped onto avatars based on live streamed data from Meta's API (<ref type="url" target="https://developer.oculus.com/">https://developer.oculus.com/</ref>). In addition, lip synchronization was realized based on audio streams from the microphone using Salsa Lip-Sync (<ref type="url" target="https://crazyminnowstudio.com">https://crazyminnowstudio.com</ref>). Since delay of audio transmission often exceeds delay of transmitting individual values in digital networks Close-up portrait photographs of two experimenters (first column), personalized avatars generated from these photographs (second column) and two generic avatars (third column). and lip synchronization likewise has a delay of several hundred milliseconds, each computer performed lip synchronization computations for data from its own microphone and transmitted resulting phonemes to the other client for direct implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure</head><p>Participants received an information email beforehand asking them not to wear excessive make-up, which could interfere with the avatar creation process and were asked to bring contact lenses if they needed eyesight correction. The two participants of a dyad did not interact, see or hear each other physically at any point in the experiment but were located in different rooms while connected via social VR. Additionally, participants arrived and departed at staggered times to further reduce the possibility of encountering each other. In the current study, most participants SOCIAL VIRTUAL REALITY ELICITS NATURAL INTERACTION BEHAVIOR 10 did not recognize each other: Only five out of 128 participants answered that they recognized their partner (3.91%). Two dyads, both in the personalized condition, answered they were previously acquainted with their respective partners. One participant, who was in the generic condition, did not specify their relationship.</p><p>On arrival to the lab, participants were informed about the experiment and signed the consent form. Participants in the personalized condition were additionally informed that they would be embodying an individualized avatar generated from their own photos. We then took a portrait photo of these participants and created a personalized avatar as the participants were filling out questionnaires. All participants were fitted with a heart rate sensor, answered a short demographic survey and completed a battery of questionnaires on individual traits which are not subject to the present analysis. After answering the questionnaires, participants were introduced to the VR headset. They were guided on how to fit the headset and adjust the interpupillary distance (IPD) setting. They then performed a 9-point eye-tracking calibration and checked the calibration outcomes. Once participants were comfortable with the headset, they were introduced to the virtual environment where they first adjusted their seating height and orientation so that a table in the virtual environment was aligned in orientation to the physical table in front of them.</p><p>Located on the virtual table, a mirror in the virtual environment allowed participants to see their own avatar and its replication of their head, eye, hand and finger movements along with their facial expressions. Based on participant feedback, an experimenter adjusted their avatar's body size, head pitch, re-adjusted the eye tracking if necessary and gave them time to look around and get accustomed to the situation. Note that incorporating a wide range of behavioral modalities is essential for creating a realistic and engaging social VR experience <ref type="bibr" target="#b73">(Xenakis et al., 2022)</ref> and that previous research reported on reduced social presence when facial expression displays were omitted in social VR <ref type="bibr" target="#b38">(Kimmel et al., 2023;</ref><ref type="bibr" target="#b68">Tarnec et al., 2023)</ref>.</p><p>Before connecting the two participants, experimenters briefly outlined the various conversation topics and assured participants that no video or audio recordings would be obtained.</p><p>The experimenters then initiated the connection with the other participant in the social VR program and exited the room. After connection, the mirror on the table was moved to another side of the room and participants could see their partner's avatar located on the other side of the table.</p><p>Participants were now able to interact with each another (see Figure <ref type="figure">2</ref> and Figure <ref type="figure" target="#fig_1">3</ref>). Instructions were delivered through a computerized female voice. Following a brief introduction in which the participants were reminded not to reveal their own names to their partner, the dyads were instructed to talk about each topic for 10 minutes. The topics were (a) small talk involving topics on everyday subjects, (b) personal talk including closeness-generating topics and (c) opinion talk including a discussion on conflicting opinions. The rationale behind asking participants to engage in differing conversations was to assess modulations of behavior according to conversation types (e.g., speaking amplitude) but also to test the robustness of general behavioral markers such as the interaction of speech and gaze across contexts For each conversation topic, a computerized voice presented the topic and recited example questions. A keyword for each question was displayed on a small screen on the virtual table which also showed the remaining time for each topic. For the small talk and personal talk topics, example questions were adapted from <ref type="bibr" target="#b6">Aron et al. (1997)</ref>. For opinion talk, participants were directed to choose an issue on which they held divergent views and to attempt to persuade their partner of their own perspective. We selected the three issues with the highest variance in response distributions from a pilot survey of university students to increase the probability that partners would hold divergent views to at least one of the proposed topics. These discussions were informed by research showing that opinion discrepancies among group members can induce discomfort and stress, as evidenced in studies where distress arises from perceived disagreement on controversial topics <ref type="bibr" target="#b47">(Matz &amp; Wood, 2005)</ref>. The questions of each topic in English and in the experimental language are listed in Supplementary Table <ref type="table" target="#tab_0">S1</ref>. After all topics were discussed, participants were asked to say goodbye to each other and were disconnected in the social VR setup, resulting in the partner's avatar disappearing from the scene for each participant. They then viewed a set of images in the virtual scene for about three minutes which are not covered here, before removing the headset and completing a final battery of questionnaires (see section Self-report Measures below). The entire study lasted around 1.5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2</head><p>Interaction in social VR as seen from a participant's perspective.</p><p>hours for both conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Behavioral Measures</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Behavioral variables</head><p>During the VR interactions, we continuously recorded markers of verbal and non-verbal behavior as they were described in research on human interactions in face-to-face settings, i.e., with no use of VR technology <ref type="bibr" target="#b13">(Degutyte &amp; Astell, 2021;</ref><ref type="bibr" target="#b29">Hessels et al., 2019)</ref>. Table <ref type="table" target="#tab_0">1 in sec.</ref> Results summarizes the variables of interest in this study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Preprocessing</head><p>For gaze behavior, we were primarily interested in the proportion of time spent looking towards the partner's eye region, which was defined as an area of 2 degrees around the center of either eye. To categorize gaze as being directed towards the partner's eye region or elsewhere at each moment in time, we first removed data during blinks (which accounted for an average 3.85% (SD = 2.81) of the time across participants, averaged across both eyes for each participant) as well as 100ms before and after each blink. Data was interpolated using Piecewise Cubic Hermite Interpolating Polynomial (PCHIP) and a 2nd order Butterworth filter with a cutoff frequency of 10Hz was applied. Gaze data were transposed to represented angular deviations from the center of each of the partner's eyes. To ensure measurement sensitivity towards this relatively small area of interest for a wide range of participants including those with strong eye dominance or alignment issues (e.g., strabismus), we considered at each moment in time the gaze ray from either eye or a combined gaze ray with the lowest angular deviation towards either of the partner's eyes.</p><p>Categorization of time points to phases where a participant was speaking or not speaking was based on the sound amplitude received at the HMD's built-in microphone on a scale from 0 (minimum amplitude) to 100 (maximum amplitude). The signal was first smoothed using a moving average filter with a width of 100ms. Time points with an amplitude of 2 or above and time points within 100ms of these time points were provisionally labelled as speaking. Pauses between two speaking phases of up to 2 seconds were considered to represent the same speaking turn and were thus also labelled as speaking. Speech phases lasting shorter than 1 second were then considered to be backchanneling vocalizations (e.g., "hmm" or "uh-huh") and were labelled as not speaking. Moments when a person was not speaking but the partner was speaking were labelled as listening. Moments when both partners were speaking were considered interruptions, with the participant who was listening directly prior to the interruption being the person interrupting their partner.</p><p>Smiling was determined based on data acquired through the HMD's face tracking feature which uses action units based on the Facial Action Coding System (FACS: <ref type="url" target="https://developer.oculus.com/documentation/unity/move-face-tracking/">https://developer.oculus.com/documentation/unity/move-face-tracking/</ref>). Extent of smiling on both sides of the mouth was averaged to one value between 0 (no smiling expression) and 100 (maximum smiling expression).</p><p>Measurements were averaged within each conversation in each participant. Outliers beyond 3 standard deviations from the mean were set to 3 standard deviations from the mean. The percentage of corrected outliers in behavioral variables ranged from 0% to 2.08% (see</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VR Experience</head><p>Self presence was assessed with the General Presence and Spatial Presence subscales subscales of the IGroup Presence Questionnaire (IPQ; <ref type="bibr" target="#b63">Schubert et al., 2001)</ref>. The questionnaire is comprised of six items on a 7-point Likert scale ranging from 1 (fully disagree) to 7 (fully agree), for example "In the computer generated world I had a sense of being there" and "I felt present in the virtual space". Internal consistency of this questionnaire was acceptable (Cronbach's α= .64).</p><p>Social presence was assessed with the Social Presence Scale (SPS; <ref type="bibr" target="#b28">Herrera et al., 2018)</ref>, which included five items using a 7-point Likert scale ranging from 1 (strongly disagree) to 7 (strongly agree), such as "I felt like I was face-to-face with my partner" and "I felt like my partner was watching me". Internal consistency of this scale was good (Cronbach's α= .79).</p><p>Embodiment was evaluated using the Body Ownership subscale of the Embodiment Questionnaire <ref type="bibr" target="#b20">(Gonzalez-Franco &amp; Peck, 2018)</ref>. This questionnaire assesses the participant's sense of identification with their avatar's body with five items on a 7-point Likert scale ranging from 1 (strongly disagree) to 7 (strongly agree). Examples are "I felt as if the body I saw in the virtual world might be my body" and "I felt as if the virtual body I saw when looking in the mirror was my own body". Internal consistency of this scale was good (Cronbach's α= .73).</p><p>We additionally included a questionnaire on motion sickness in VR. Participants who answered yes to a screening question ("Did you feel any discomfort in VR?") were given the VR Sickness Questionnaire (VRSQ; <ref type="bibr" target="#b37">Kim et al., 2018)</ref>. Participants were asked to report on nine common cybersickness symptoms on a 4-point Likert scale ranging from 0 (not at all) to 3 (very much), which reflect two components of motion sickness (oculomotor and disorientation). The total score is calculated as an average of the two components, ranging from 0 to 100. Internal consistency of the total scale was good (Cronbach's α= .74). 23 out of 128 (18%) participants replied that they felt some discomfort in VR. Mean cybersickness score of these participants were low (M = 26.89, SD = 13.67, ranging from 6.67 to 66.89), indicating that the VR experience was well-tolerated by the majority of participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Analysis</head><p>The influence of conversational topics and experimental condition on behavioral measures were examined using linear mixed-effects models. The model looked at the fixed effect of the independent variable of interest on the behavioral variables while accounting for the variability of the conversation topic within individuals, as reflected by random intercepts for each participant ID and random slopes for conversational topics nested within ID. Significance of the effects were determined with an F-Test with an α of 5%. We additionally report adjusted p-value computed using the <ref type="bibr" target="#b9">Benjamini and Hochberg (1995)</ref> procedure, which controls for the False Discovery Rate (FDR). Effects of conversation topics were followed up using post-hoc pairwise comparisons using estimated marginal means (EMM). Reported p-values were adjusted for multiple comparisons using the Tukey's HSD (Honestly Significant Difference). The effect of condition on self-report measures were inspected using linear regression models. Data were analysed using R (version 4.3.2, R Core Team, 2024). We used the nlme package <ref type="bibr" target="#b56">(Pinheiro et al., 2023)</ref> to fit mixed-effects models with nested random effects and the emmeans package <ref type="bibr" target="#b42">(Lenth, 2024)</ref> for pairwise comparisons. All dependent variables were standardized and centered prior to regression in order to facilitate comparison of effects. For effect sizes of differences between conversation topics, Cohen's d was calculated with the mean difference between paired observations, divided by the adjusted standard deviation (taking the variances of the measures and their correlation into account). For effect sizes of differences between conditions, Cohen's d was calculated for independent groups, where the mean difference was divided by the pooled standard deviation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coordination of Speech and Gaze across Participants</head><p>Descriptive results of the behavioral variables aggregated over all three conversation topics are shown in Table <ref type="table" target="#tab_0">1</ref>. Participants gazed at their partner significantly less during speaking than listening (β = -26.41, 95% CI <ref type="bibr">[-27.71, -25</ref>.10], t(639) = -39.78, p &lt; .001, d = 1.80, 95% CI <ref type="bibr">[-1.97, -1.63]</ref>), indicating that participants spent 26.41% less time gazing at their partner during speaking compared to listening. Additionally, participants were more likely to gaze at their partner at the end of a speaking turn compared to the beginning of a turn <ref type="bibr">(β = 17.24,</ref><ref type="bibr">95% CI [15.99,</ref><ref type="bibr">18</ref>.49], t(639) = 27.07, p &lt; .001, d = 1.28, 95% CI [-1.44, -1.12]), showing that participants gazed at their partner 17.24% more at the end of a speaking turn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Influence of Conversation Topics on Interaction Behavior</head><p>Analyses on the quality of interaction indicated that on average, participants reported to have engaged effectively with their partners on each topic (for details see Supplementary Results sec. Quality of Interaction). Linear mixed models examining the effect of conversation topic on each behavioral measure were adjusted for multiple testing and were significant for all measures.</p><p>Based on the significance of models, post hoc pairwise comparisons were conducted between the conversation topics for each measure. Figure <ref type="figure" target="#fig_2">4</ref> presents the means and standard errors of the measures for each conversation topic (for details see Supplementary Table <ref type="table">S5</ref>).</p><p>We found a significant effect of conversation topic on gaze while speaking (F(2, 254) = 6.70, p = .002, p ad j = .002). Participants gazed significantly more at their partner's eye region while speaking in small talk compared to opinion talk (p = .007, d = 0.32, 95% CI [0.14, 0.50]).</p><p>The difference in gaze between small talk and personal talk was marginally significant (p = .056, d = 0.20, 95% CI [0.03, 0.38]). There was no significant difference in gaze between personal talk and opinion talk (p = .174, d = 0.16, 95% CI [-0.33, 0.02]).</p><p>We observed a significant effect of conversation topic on gaze while listening (F(2, 254) = 8.96, p &lt; .001, p ad j &lt; .001). Participants gazed significantly more at their partner while listening in personal talk compared to small talk (p = .007, d = 0.27, 95% CI [-0.45, -0.09]) or opinion talk (p = .001, d = 0.31, 95% CI [-0.49, -0.13]). There was no significant difference in gaze between small talk and opinion talk (p = .835, d = 0.05, 95% CI [-0.12, 0.22]).</p><p>The effect of conversation topic on gaze at the beginning of a turn was significant (F(2, 254) = 38.80, p &lt; .001, p ad j &lt; .001). Post hoc pairwise comparisons were all significant, such that participants gazed significantly more at their partner in the first second of their turn in small talk compared to personal talk (p &lt; .001, d = 0.33, 95% CI [0.15, 0.51]) or opinion talk (p &lt; .001, d =  Note: Error bars represent standard errors. SOCIAL VIRTUAL REALITY ELICITS NATURAL INTERACTION BEHAVIOR 21 0.78, 95% CI [0.58, 0.98]), and in personal talk compared to opinion talk (p &lt; .001, d = 0.38, 95% CI [-0.56, -0.20]).</p><p>The effect of conversation topic on gaze at the end of a turn was significant (F(2, 254) = 9.04, p &lt; .001, p ad j &lt; .001). In the last second of their turn, participants gazed significantly less at their partner in opinion talk compared to small talk (p &lt; .001, d = 0.37, 95% CI [0.19, 0.55]) or personal talk (p = .007, d = 0.27, 95% CI [-0.45, -0.09]). There was no difference between small talk and personal talk (p = .220, d = 0.15, 95% CI [-0.03, 0.32]).</p><p>The effect of conversation topic on the amount of smiling was significant (F(2, 254) = 94.12, p &lt; .001, p ad j &lt; .001). Post hoc pairwise comparisons were all significant, such that participants smiled more in small talk compared to personal talk (p &lt; .</p><p>001, d = 0.94, 95% CI [0.73, 1.15]) or opinion talk (p &lt; .001, d = 1.19, 95% CI [0.96, 1.42]), and in personal talk compared to opinion talk (p &lt; .001, d = 0.46, 95% CI [-0.64, -0.27]). We found a significant effect of conversation topic in predicting interrupting (F(2, 254) = 64.93, p &lt; .001, p ad j &lt; .001). Participants interrupted significantly more in small talk compared to personal talk (p &lt; .001, d = 0.99, 95% CI [0.78, 1.21]) or opinion talk (p = .018, d = 0.83, 95% CI [0.63, 1.03]). There was no difference between personal talk and opinion talk (p = .608, d = 0.08, 95% CI [-0.09, 0.26]). We found a significant effect of conversation topic in predicting turn duration (F(2, 254) = 129.49, p &lt; .001, p ad j &lt; .001). Compared to small talk, participants had longer turns in personal talk (p &lt; .001, d = 1.23, 95% CI [-1.46, -1.00]) and opinion talk (p &lt; .001, d = 1.10, 95% CI [-1.32, -0.88]). Analyses revealed no significant differences between personal talk and opinion talk (p = .971, d = 0.02, 95% CI [-0.19, 0.15]). We found a significant effect of conversation topic on gaps (F(2, 250) = 22.86, p &lt; .001, p ad j &lt; .001). Compared to small talk, participants took longer to reply in personal talk (p &lt; .001, d = 0.49, 95% CI [-0.67, -0.30]) and in opinion talk (p &lt; .001, d = 0.50, 95% CI [-0.68, -0.31]).</p><p>The difference between personal talk and opinion talk was non-significant (p = .538, d = 0.09, 95% CI [-0.08, 0.27]).</p><p>The overall effect of conversation topic on loudness was significant (F(2, 254) = 7.47, p &lt; .001, p ad j = .001). Participants spoke significantly louder in opinion talk compared to personal talk (p &lt; .001, d = 0.34, 95% CI [0.16, 0.52]). Differences between other topics were non-significant (small talk vs personal talk, p = .257, d = 0.14, 95% CI [-0.04, 0.31]; small talk vs opinion talk, p = .160, d = 0.16, 95% CI [-0.34, 0.01]).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison Between Personalized and Generic Avatar</head><p>Comparison of the quality of interaction between participants embodying generic or personalized avatars showed no differences between the two conditions (see Supplementary</p><p>Results sec. Quality of Interaction). We then examined the efficacy of the manipulation of the avatars for the personalized and generic conditions. When embodying personalized as compared to generic avatars, participants reported higher values on avatar similarity Compared to participants embodying generic avatars, participants embodying personalized avatars reported that the avatar appearance was more similar to their own physical appearance;</p><p>reported that a friend would be more likely to recognize them; and stated that they would be able to recognize the interaction partner if they met her again outside of social VR, indicating that the manipulation of the conditions were overall successful.</p><p>Analyses on the experience of VR revealed no significant effect of condition, such that participants embodying a personal or generic avatar did not differ in experiences of presence (p = .693, p ad j = .693, d = 0.07, 95% CI [-0.28, 0.42]), social presence (p = .180, p ad j = .539, d = 0.24, 95% CI [-0.11, 0.59]) nor body ownership (p = .283, p ad j = .565, d = 0.19, 95% CI [-0.16, 0.54]) (for details see Supplementary Figure <ref type="figure" target="#fig_2">S4</ref>).</p><p>The influence of condition on behavioral measures were examined using linear mixed-effects models. Figure <ref type="figure" target="#fig_4">5</ref> display the means and standard errors of the behavioral variables between the conditions. Initial analyses found a significant effect of condition on turn duration with longer turns in participants embodying personalized avatars, but this effect did not withstand</p><p>Benjamini-Hochberg's corrections for multiple testing (p = .029, p ad j = .319), indicating that the difference is negligible. No other difference between the two conditions were statistically significant, even when not correcting for multiple testing. Supplementary Figure <ref type="figure" target="#fig_4">S5</ref> shows the means and standard errors of the measures for each conversation topic in both conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>The present study examined verbal and nonverbal behavior in real-time dyadic interactions in social VR while participants embodied either personalized or generic avatars.</p><p>Interaction behavior shared several similarities with face-to-face dyadic interactions <ref type="bibr" target="#b13">(Degutyte &amp; Astell, 2021;</ref><ref type="bibr" target="#b29">Hessels et al., 2019;</ref><ref type="bibr" target="#b30">Ho et al., 2015;</ref><ref type="bibr" target="#b72">Whittaker, 2003)</ref>: participants gazed at their interaction partners' eye region more while listening compared to while speaking, and when speaking, participants gazed at their interaction partners' eye region more at the end of a speaking turn compared to the beginning of a turn. This coordination of speech and gaze behavior, which is known to help people regulate turn-taking in conversations <ref type="bibr" target="#b29">(Hessels et al., 2019;</ref><ref type="bibr" target="#b30">Ho et al., 2015;</ref><ref type="bibr" target="#b35">Kendon, 1967)</ref>, was observed as statistically large effect in each of the three conversation topics and in both participant groups. These observed patterns of gaze behavior in social VR are consistent with those found in face-to-face interactions. Multiple previous studies observed that individuals typically spend about half as much time gazing at their partner during speaking compared to listening in face-to-face interactions, typically ranging from 20-65% while speaking to 30-80% while listening <ref type="bibr" target="#b4">(Argyle &amp; Cook, 1976;</ref><ref type="bibr" target="#b35">Kendon, 1967;</ref><ref type="bibr" target="#b43">Levine &amp; Sutton-Smith, 1973)</ref>.</p><p>Our results align with these findings, demonstrating a similar reduction in gaze during speaking turns and an increase at the end of speaking turns, suggesting that social VR can effectively replicate the natural coordination of speech and gaze observed in face-to-face communication.</p><p>Moreover, our study supports the findings of <ref type="bibr" target="#b0">Abdullah et al. (2021)</ref>, where participants Means and standard errors of gaze and speech behavior in the two conditions.</p><p>Note: Error bars represent standard errors.</p><p>similarly showed decreased gaze towards interaction partners while speaking in triad interactions in social VR (28% compared to 23% in the current study). These results suggest that the function of eye gaze remains constant for conversations regardless of the interaction occurring entirely in a virtual environment and highlight the importance of integrating nonverbal behavior in virtual social interactions. Similarly, as in real-life interactions, users of advanced social VR setups can view each others' head and eye movements, hand and finger movements as well as facial expressions and lip movements. The situation may be contrasted with communication via videoconferencing (i.e. using Zoom), where non-verbal communication is detached from its natural integration in social space, sometimes disrupting the communication flow and causing irritation and fatigue <ref type="bibr" target="#b8">(Bailenson, 2021)</ref>. Note that while people using social VR may easily monitor who is looking at and attending to them similarly as in real life, videoconferencing setups give the visual impression that others are constantly looking in one's direction (as they are facing their own computer monitor). When communicating via videoconferencing as compared to social VR, Abdullah and colleagues (2021) found that participants spent more time looking at others' faces, used less deictic gestures, had longer conversational turns and showed more backchannelling behavior, possibly suggesting the need for active efforts to maintain the social connection and a less natural and lively communication in videoconferencing. In a study by <ref type="bibr" target="#b58">Rubo and Gamer (2021)</ref>, participants more specifically reacted to an agent's smiling in VR compared to when viewing the same scene on a computer monitor, similarly suggesting a more natural sense of social involvement in a shared social space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Differences between conversation topics</head><p>Participants showed a robust and similar coordination of speech and gaze across conversations, but also exhibited behavioral adaptations to specific conversation topics. In small-talk conversations, where participants were instructed to discuss everyday topics, participants spent significantly more time speaking, interrupted each other more, had shorter turn durations, responded faster, smiled more, and gazed at their conversation partners' eye region more at the beginning of turns compared to all other conversation topics. These findings show that the relatively fast-paced and dynamic exchange in face-to-face small-talk conversations is similarly actualized when people interact with each other in social VR <ref type="bibr" target="#b15">(Eggins &amp; Slade, 2004)</ref>.</p><p>In contrast, conversations on more personal topics appeared to foster more reflective communication behavior, as indicated through longer turn durations and increased gaps before replying. In these conversations, participants spent the longest time gazing at their partners' eye region while listening, which may indicate heightened attentiveness and engagement, likely due to the personal and meaningful nature of the topic. Participants furthermore smiled less compared to small-talk conversations. During opinion talk, where participants were asked to convince their interaction partner on controversial issues, participants spent the shortest amount of time gazing at their partner's eyes during the beginning and end of turns and showed the least smiling of the three conversations. Similar to personal talk, participants spoke less frequently, interrupted less often, and had longer turns and gaps compared to small talk, indicating a more contemplative and thoughtful communication style. These findings align with the idea that discussions about differing opinions may lack the fluency of other conversations, but also disagree with an observation by <ref type="bibr" target="#b59">Rutter et al. (1978)</ref> where participants gazed more at the end of a turn during a competitive conversation compared to a cooperative one. However, direct comparison is difficult as we did not include a cooperative condition where participants discussed common ground. The observed relative gaze avoidance at the beginning and end of turns when discussing differing opinions may indicate increased shyness or discomfort among participants or may suggest a need for increased effort to manage the conversational flow in such a confrontational context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effects of Avatar Personalization</head><p>Participants showed overall similar verbal and non-verbal behavior when embodying generic avatars as when embodying personalized avatars. In particular, participants embodying generic avatars did not show prolonged gaze to the partner's eye region, louder speaking or more interrupting behavior, which were hypothesized signs of disinhibition. Furthermore, there were no differences in reported social presence, self-presence or body ownership between the two conditions. These observations contrast both previous observations from single-user VR studies where participants reported on higher levels of presence and body ownership when embodying personalized avatars (e.g., <ref type="bibr" target="#b50">Mottelson et al., 2023;</ref><ref type="bibr" target="#b52">X. Pan &amp; Hamilton, 2018;</ref><ref type="bibr" target="#b70">Wei et al., 2022;</ref><ref type="bibr" target="#b71">Weidner et al., 2023)</ref> and our hypothesis that generic avatars would be associated with signs of social disinhibition due to their relative concealment of participants' identities in our social VR setup <ref type="bibr" target="#b67">(Suler, 2004)</ref>.</p><p>Several factors may explain these discrepancies. With regards to self-reported levels of presence, previous studies typically examined effects of avatar appearance in within-subject designs where the participants repeatedly acted in isolation while embodying varying avatars (e.g., <ref type="bibr" target="#b41">Latoschik et al., 2017;</ref><ref type="bibr" target="#b61">Salagean et al., 2023;</ref><ref type="bibr" target="#b69">Waltemate et al., 2018)</ref>. While such setups can exhibit high statistical power, their drawing of participants' attention to a single varying factortheir own appearance in a virtual mirror -may also introduce a risk of expectancy effects, where participants may relatively prefer the avatar which was associated with the highest production effort. The between-subject design used in the present study alleviates the risk for expectancy effects at the cost of statistical power. In addition, participants in dyadic interactions may be less strongly influenced by their own appearance compared to participants in single-user VR environments: although participants in our setup spent a short period of time looking at their own avatar in a virtual mirror prior to the interaction, their sense of presence and social presence may then have been determined by the vividness of the interaction with their partner which may not strongly depend on the personalization of avatars. Consistent with our findings, other studies likewise did not observe differences in presence or social presence between personalized and generic avatars when participants were engaged in a social interaction. In a study by <ref type="bibr" target="#b66">Suk and Laine (2023)</ref>, participants interacting with a virtual agent reported higher embodiment but not presence when embodying personalized avatars. In a study by <ref type="bibr" target="#b25">Han et al. (2022)</ref>, the use of personalized avatars was associated with only a slight increase in self-presence and no differences in social or spatial presence. Similarly, visual appearance of the avatar may be less crucial for the sense of body ownership. Instead, high movement fidelity may suffice in fostering a sense of body ownership as suggested by recent meta-analytical findings <ref type="bibr" target="#b50">(Mottelson et al., 2023)</ref>.</p><p>With regards to behavioral disinhibition due to a relative anonymity when embodying generic avatars, effects may have been reduced in our setup since most participants were drawn from the same student population (albeit consisting of several cohorts with a total of more than one thousand potential interaction partners). Recognition was in principle possible since voices</p><p>were not distorted during the interaction, but as participants in the condition with generic avatars rated their similarity and recognizability, but also their partner's identifiability substantially lower compared with participants in the condition with personalized avatars, the use of generic avatars did entail a relative anonymization of the situation. Nonetheless, a sense of belonging to a common overall group may have contributed to a diminishing effect of anonymity seen in communication with strangers on the internet. Alternatively, it is possible that effects of disinhibition known from anonymous written communication are substantially alleviated in social VR due to the situation's inherent naturalness and intimacy. Note that in social VR, even anonymous interaction partners engage in eye-contact with each other, potentially eliciting a sense of familiarity and affiliation which is associated with eye-contact in face-to-face interactions (e.g., <ref type="bibr" target="#b4">Argyle &amp; Cook, 1976;</ref><ref type="bibr" target="#b10">Broz et al., 2012;</ref><ref type="bibr" target="#b33">Kellerman et al., 1989)</ref>. In a study by <ref type="bibr" target="#b40">Lapidot-Lefler and Barak (2012)</ref>, eye contact reduced subsequent negative behavior in written communication, further highlighting the impact of eye contact on social behavior.</p><p>An absence of disinhibition effects of anonymous communication would allow to use social VR for anonymous interactions without risking negative side effects. Note that while some future use cases of social VR may require the implementation of personalized avatars, other situations may profit from being able to communicate via generic avatars. For instance, the use of generic avatars may more easily comply with stricter privacy needs, allowing users to interact with a full range of nonverbal communication while maintaining privacy, which is particularly crucial for marginalized users <ref type="bibr" target="#b44">(Lin &amp; Latoschik, 2022;</ref><ref type="bibr" target="#b46">Maloney et al., 2020)</ref>. In recruiting interviews, using social VR with generic avatars may allow for fairer, less biased assessment processes. For example, a study using Swedish data demonstrated that anonymous application procedures increased the chances of women and individuals of non-Western origin advancing to the interview SOCIAL VIRTUAL REALITY ELICITS NATURAL INTERACTION BEHAVIOR 29 stage and, for women, improved job offer rates <ref type="bibr" target="#b7">(Åslund &amp; Skans, 2012)</ref>. In therapeutic settings, vulnerable participants may benefit from communicating anonymously with a therapist or with each other, or may be more willing to engage in therapy in the first place. Previous studies examining social media communities have found that anonymity associated with online written</p><p>communication can facilitate open and honest discussions about mental health, thereby improving access to social support and reducing stigma <ref type="bibr" target="#b12">(De Choudhury &amp; De, 2014;</ref><ref type="bibr" target="#b17">Eysenbach et al., 2004)</ref>. Using generic avatars in social VR for therapeutic purposes could similarly promote a safe environment for self-disclosure and support, encouraging more participants to engage in therapy.</p><p>The naturalistic interaction patterns observed in social VR holds multiple significant implications. These findings show that social VR can provide a naturalistic environment for researching social behavior, enabling more accurate observations of how individuals interact in realistic scenarios. The capacity of VR to automatically collect rich behavioral data will allow researchers to capture complex interactions that have remained often difficult and time-consuming to analyze with conventional methods <ref type="bibr" target="#b74">(Yaremych &amp; Persky, 2019)</ref>. As such, the study of social VR holds significant promise to deepen our understanding of social dynamics in ecologically valid situations, making it a powerful new measurement tool. However, it is crucial for social VR users to be aware of the fine-grained behavioral data which is generated in VR.</p><p>Such data should be managed with strict adherence to privacy protocols, requiring informed consent from users before any collection or analysis occurs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>While the present study provides detailed new insights into verbal and non-verbal behavior of individuals interacting in social VR, several factors limit the generalizability of our results. Firstly, participants were primarily drawn from a student population in Switzerland. As many aspects of social behavior are culturally learned, applicability of our results may decrease with increasing cultural distance of a population to the one tested in the present study <ref type="bibr" target="#b27">(Henrich et al., 2010)</ref>. For instance, maintaining eye contact may be more important and socially encouraged in Western cultures compared to East Asian cultures <ref type="bibr" target="#b4">(Argyle &amp; Cook, 1976;</ref><ref type="bibr" target="#b5">Argyle et al., 1986)</ref>, which have been shown in gaze avoidance in Japanese individuals (e.g., Japanese vs Finnish; <ref type="bibr" target="#b1">Akechi et al., 2013;</ref><ref type="bibr" target="#b64">Senju et al., 2013)</ref>. These results highlight the complexity of cultural influences on non-verbal communication and indicate the importance of taking cultural background into consideration.</p><p>Additionally, more women than men participated in the present study. Since gender composition in dyads were observed to modulate behavioral markers such as behavioral synchrony <ref type="bibr" target="#b18">(Fujiwara et al., 2019)</ref> and gaze behavior can be influenced by the level of attraction to an interaction partner (C. <ref type="bibr" target="#b21">Hall et al., 2010)</ref>, future research should systematically vary the gender composition of dyads, as well as to account for sexual orientation of the participants. Recruiting participants from a wider population and implementing assessment techniques in social VR environments as people use them at home or at work -outside of a scientific laboratory -may additionally allow to elicit stronger senses of anonymity in participants embodying generic avatars.</p><p>Participants in the present study only interacted with one other participant. While behavioral markers such as the overall coordination of speech and gaze may likely generalize in principle across interactions, it is noteworthy that dyadic conversations, in contrast to reactions to pre-defined stimuli such as images, are by nature interactive and include reactions to a partner's behavior <ref type="bibr" target="#b55">(Patterson, 2019)</ref>. Previous studies utilizing the social relations model have indicated that individuals show moderate consistency in their behavior across various interaction partners, however, distinct responses may be elicited depending on their partner <ref type="bibr" target="#b36">(Kenny et al., 2001)</ref>. To further investigate the stability of a person's behavior across interactions (e.g., the amount of gaze towards the partner's eye region) as well as the extent to which behavior is influenced by the partner, future research should assess interaction behavior in participants across multiple interactions with different partners.</p><p>The comparison of behavior between participants embodying generic or personalized avatars may have been compromised by limited quality of the personalization process. Although our manipulation check indicated a substantial difference between the two conditions -with personalized avatars rated higher in similarity, recognizability, and re-identifiability -participants embodying personalized avatars may have shown different behavior if their avatars had more closely resembled their physical appearance. Additionally, ratings on avatar satisfaction and attractiveness were moderate. While the present study used a state-of the art tool for avatar personalization, future studies should make use of technical improvements to allow participants to embody even more closely personalized avatars. Emerging research is focused on developing highly realistic avatars, capturing fine features such as hair strands and skin pores (e.g., <ref type="bibr" target="#b60">Saito et al., 2024)</ref>. Such advancements will significantly improve the realism and effectiveness of social VR interactions and may soon be accessible for academic use.</p><p>Similarly, the visual quality and fidelity of replicating participants' facial expressions on their avatars may be improved. In particular, we employed face-tracking and avatar technology which allowed to distinguish a range of different facial movements such as a pulling of each corner of the mouth in creating a smile, but facial expression replications may not have accounted properly for inter-individual variation. A real person's characteristic smile may not only be defined by the composition of muscle contraction intensities, but also by idiosyncrasies in how her skin is distorted or the tip of the nose is pulled upwards <ref type="bibr" target="#b62">(Schmidt &amp; Cohn, 2001)</ref>.</p><p>Improvements in facial expression replication should be used in future studies and may likely influence interaction behavior. While we focused on smiling behavior as a common and particularly salient facial expression used in communication, a more detailed assessment may allow to investigate how different types of smiles (e.g., polite or embarrassed smiles; Ambadar Overall, our findings suggest that social VR elicits natural interaction behavior and may be used for anonymous interactions without negative side-effects known from other technology-mediated forms of communication. As communication in social VR become more widespread, it is increasingly important to understand the patterns of interaction behavior in social VR and to examine how different factors such as avatar appearance and different topics of conversation may influence behavior. These results also highlight the prospect of utilizing social VR as an ecologically valid tool in the investigation of complex multi-modal interaction situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Questionnaires Table S2</head><p>Measures on Avatar Appearance and Interaction in English and German</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Variable Item</head><p>Avatar Appearance* 1. I think my virtual avatar looked similar to me.</p><p>Ich finde, mein virtueller Avatar sah ähnlich aus als ich.</p><p>2. I think my friends would recognize that my avatar is supposed to represent me.</p><p>Ich denke, meine Freunde würden erkennen, dass mein Avatar mich darstellen soll.</p><p>3. I am satisfied with the appearance of my avatar.</p><p>Ich bin zufrieden mit dem Aussehen meines Avatars.</p><p>4. I think my avatar looked more attractive than me.</p><p>Ich finde, mein Avatar sah attraktiver aus als ich.</p><p>5. I think my avatar looked less attractive than me.</p><p>Ich finde, mein Avatar sah weniger attraktiver aus als ich.</p><p>6. If I saw the person I interacted with again (e.g., in the hallway of this building or in the city), I would recognize them.</p><p>Wenn ich die Person, mit der ich interagiert habe, wieder sehen würde (z.B. im</p><p>Gang dieses Gebäudes oder in der Stadt), dann würde ich sie wiedererkennen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">I could hear my partner well.</head><p>Ich konnte mein Gegenüber akustisch gut verstehen. Table <ref type="table">S3</ref> Measures on Quality of Interaction in English and German</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topic Item</head><p>Small Talk</p><p>In one phase of the conversation, you were asked to talk about personal topics. Ist es Ihnen als Zweiergruppe gelungen, sich zu diesem Thema auszutauschen?</p><p>2. Were you able to contribute to this topic?</p><p>Konnten Sie zu diesem Thema etwas beitragen?</p><p>3. Was your partner able to contribute to this topic?</p><p>Konnte Ihr Partner zu diesem Thema etwas beitragen?</p><p>4. Did you feel that the personal conversation was more intimate than the small talk conversation?</p><p>Hatten Sie das Gefühl, dass die persönliche Gesprächssituation intimer war als die alltägliche Gesprächssituation?</p><p>Table <ref type="table">S3</ref> Measures on Quality of Interaction in English and German (continued)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topic Item</head><p>Opinion Talk</p><p>In one phase of the conversation, you were asked to talk about personal topics. Ist es Ihnen als Zweiergruppe gelungen, sich zu diesem Thema auszutauschen?</p><p>2. Were you able to contribute to this topic?</p><p>Konnten Sie zu diesem Thema etwas beitragen?</p><p>3. Was your partner able to contribute to this topic?</p><p>Konnte Ihr Partner zu diesem Thema etwas beitragen?</p><p>4. Who was better able to convince the other of their opinion? (Myself-Partner)</p><p>Wer konnte den anderen besser von ihrer oder seiner Meinung überzeugen? (Ich-Gegenüber)</p><p>Note.: Items in small talk and personal talk were adapted from <ref type="bibr" target="#b6">Aron et al. (1997)</ref>. Items in opinion talk was selected from a pilot survey of university students, which showed the highest variance in response distributions, in order to increase the probability that the dyad would hold divergent opinions to at least one of the proposed topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quality of Interaction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison Between Conversation Topics</head><p>Table <ref type="table">S6</ref> show the means, standard deviations, 95% confidence intervals of ratings on the quality of interaction for each conversation topic. Mean ratings were high on all three measures, indicating that on average, participants engaged effectively with each other in each conversation topic. We further examined differences in ratings between topics through linear mixed models.</p><p>The effect of conversation topic on each behavioral measure were significant for all measures.</p><p>Based on the significance of models, post hoc pairwise comparisons were conducted (adjusted for multiple testing using Tukey's HSD method) between the conversation topics for each measure.</p><p>Figure <ref type="figure">S2</ref> present the mean and standard errors of the ratings on the quality of interaction.</p><p>Table <ref type="table">S7</ref> show the effect sizes of pairwise comparisons as Cohen's d and 95% confidence intervals. There was a significant effect of conversation topic on talk success (F(2, 254) = 25.45, p &lt; .001, p ad j &lt; .001), self contribution (F(2, 254) = 16.02, p &lt; .001, p ad j &lt; .001) and partner contribution (F(2, 254) = 18.05, p &lt; .001, p ad j &lt; .001). Participants rated the success of the discussion, their personal contribution to the topic, and their partner's contribution significantly lower in opinion talk compared to personal talk or small talk (p &lt; .001 for mentioned comparisons). The difference between small talk and personal talk was not significant in all three ratings (talk success, p = .207; self contribution, p = .720; partner contribution, p = .794).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison Between Personalised and Generic Avatar</head><p>We examined the effect of condition on the quality of interaction. The linear regression did not indicate any significant differences between conditions for talk success (p = .186, p ad j = .372, d = 0.17, 95% CI [-0.08, 0.41]), self contribution (p = .127, p ad j = .372, d = 0.19, 95% CI [-0.44, 0.06]) or partner contribution (p = .827, p ad j = .827, d = 0.03, 95% CI <ref type="bibr">[-0.27, 0.22]</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1</head><label>1</label><figDesc>Figure 1</figDesc><graphic coords="9,87.60,148.37,421.21,280.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3</head><label>3</label><figDesc>Figure 3 Two individuals engaging in a conversation in social VR (top image) while being located in different rooms (bottom images). Photos are taken from a third-person perspective but individuals in social VR view the scene from their own first-person perspective as in real-life situations.</figDesc><graphic coords="13,95.40,172.28,421.21,376.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4</head><label>4</label><figDesc>Figure 4Means and standard errors of gaze and speech behavior in individual conversation topics.</figDesc><graphic coords="20,72.00,124.46,468.02,425.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><figDesc>(F(1, 125) = 30.74, p &lt; .001, p ad j &lt; .001, d = 0.98, 95% CI [0.61, 1.36]), avatar recognizability (F(1, 125) = 62.39, p &lt; .001, p ad j &lt; .001, d = 1.40, 95% CI [1.01, 1.79]) and avatar re-identification (F(1, 125) = 26.32, p &lt; .001, p ad j &lt; .001, d = 0.91, 95% CI [0.54, 1.28]). There were no significant differences for avatar satisfaction (p = .271, p ad j = .590, d = 0.20, 95% CI [-0.16, 0.55]), attractiveness (p = .295, p ad j = .590, d = 0.19, 95% CI [-0.54, 0.17]), unattractiveness (p = .070, p ad j = .280, d = 0.32, 95% CI [-0.03, 0.68]) and audibility (p = .745, p ad j = .745, d = 0.06, 95% CI[-0.29, 0.41]).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5</head><label>5</label><figDesc>Figure 5</figDesc><graphic coords="24,72.00,124.46,468.02,425.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><figDesc>et al., 2008) can convey different emotional nuances in social VR communication.ConclusionThis present study examined verbal and non-verbal behavior in a dyadic conversation in embodied social VR across three different conversation topics. Participants robustly showed a fine-grained coordination of speech and gaze behavior known from real face-to-face interactionswith more gaze to the partner's eyes while listening compared to while speaking and at the end of a speaking turn compared to the beginning of a speaking turn -indicating an overall natural SOCIAL VIRTUAL REALITY ELICITS NATURAL INTERACTION BEHAVIOR 32 interaction behavior in social VR. Characteristic modulations in interaction behavior depending on conversation topic further corroborate this view. Contrary to our expectation, the use of generic as compared to personalized avatars did not result in disinhibited social behavior known from anonymous online communication. The possibility for direct eye-contact, which is known to regulate interaction behavior, may have alleviated effects of social disinhibition due to anonymity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Relationship 1 .</head><label>1</label><figDesc>I know the person I interacted with. (Yes, No) Ich kenne die Person, mit der ich interagiert habe. (Ja, Nein) 2. How well do you know the person? (Acquaintance, Colleague, Friend, Other) Wie gut kennst du die Person? (Bekannte, Kolleg*in, Freund*in, Andere) Note: * = Answer options ranged from 1 = Not at all (Überhaupt nicht) to 5 = Completely (Vollkommen).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>( 1 =</head><label>1</label><figDesc>Not at all, 5 = Completely) In einer Phase des Gesprächs wurden Sie gebeten, sich über persönliche Themen auszutauschen. (1 = Trifft überhaupt nicht zu, 5 = Trifft von und ganz zu) 1. Were you successful in discussing this topic together? Ist es Ihnen als Zweiergruppe gelungen, sich zu diesem Thema auszutauschen? 2. Were you able to contribute to this topic? Konnten Sie zu diesem Thema etwas beitragen? 3. Was your partner able to contribute to this topic? Konnte Ihr Partner zu diesem Thema etwas beitragen? Personal Talk In one phase of the conversation, you were asked to talk about personal topics. (1 = Not at all, 5 = Completely) In einer Phase des Gesprächs wurden Sie gebeten, sich über persönliche Themen auszutauschen. (1 = Trifft überhaupt nicht zu, 5 = Trifft von und ganz zu) 1. Were you successful in discussing this topic together?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>( 1 =</head><label>1</label><figDesc>Not at all, 5 = Completely) In einer Phase des Gesprächs wurden Sie gebeten, sich über persönliche Themen auszutauschen. (1 = Trifft überhaupt nicht zu, 5 = Trifft von und ganz zu) 1. Were you successful in discussing this topic together?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="12,87.60,124.47,421.20,234.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="48,72.00,154.40,468.02,425.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="52,72.00,124.47,468.02,212.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="53,72.00,156.68,468.02,297.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="54,129.00,451.53,362.44,207.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="55,72.00,180.59,468.02,425.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Means and Standard Deviations For Behavioral Variables    </figDesc><table><row><cell>Variable</cell><cell>Description</cell><cell>M</cell><cell>SD</cell><cell>95% CI</cell></row><row><cell></cell><cell>Proportion of one's own speaking time</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Gaze While Speaking</cell><cell>which is spent gazing towards the</cell><cell cols="3">23.05 11.96 [21.84, 24.25]</cell></row><row><cell></cell><cell>partner's eye region</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Proportion of one's own listening time</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Gaze While Listening</cell><cell>which is spent gazing towards the</cell><cell cols="3">49.45 16.96 [47.75, 51.15]</cell></row><row><cell></cell><cell>partner's eye region</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Proportion of the first second of one's own</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Gaze Beginning of Turn</cell><cell>speaking turn which is spent gazing</cell><cell cols="3">16.01 10.49 [14.95, 17.06]</cell></row><row><cell></cell><cell>towards the partner's eye region</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Proportion of the last second of one's own</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Gaze End of Turn</cell><cell>speaking turn which is spent gazing</cell><cell cols="3">33.23 15.93 [31.63, 34.83]</cell></row><row><cell></cell><cell>towards the partner's eye region</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Average magnitude of smiling facial</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Smile</cell><cell></cell><cell cols="3">13.40 7.99 [12.60, 14.20]</cell></row><row><cell></cell><cell>expression over time</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Speaking</cell><cell>Proportion of time spent speaking</cell><cell cols="3">50.73 9.90 [49.74, 51.72]</cell></row><row><cell></cell><cell>Proportion of time spent interrupting the</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Interrupting</cell><cell></cell><cell>4.82</cell><cell>3.79</cell><cell>[4.44, 5.20]</cell></row><row><cell></cell><cell>partner's speech</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Average length of one's speech turn</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Turn Duration</cell><cell></cell><cell cols="3">11.58 5.61 [11.02, 12.14]</cell></row><row><cell></cell><cell>duration in seconds</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Average length of pause before responding</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Gaps</cell><cell></cell><cell>1.36</cell><cell>0.70</cell><cell>[1.29, 1.43]</cell></row><row><cell></cell><cell>in seconds</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Average amplitude of speech relative to</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Loudness</cell><cell></cell><cell>2.70</cell><cell>0.77</cell><cell>[2.63, 2.78]</cell></row><row><cell></cell><cell>microphone maximum recording capacity</cell><cell></cell><cell></cell><cell></cell></row></table><note><p>Note. M = mean. SD = standard deviation.</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We thank <rs type="person">Luca Panico</rs>, <rs type="person">Susanne Felder</rs>, <rs type="person">Michelle Bürki</rs>, <rs type="person">Ana Rita Calçada da Silva</rs> and <rs type="person">Michael Mozhacherry</rs> their help in data collection.</p></div>
			</div>
			<div type="funding">
<div><p>Funding: This research was supported by <rs type="funder">Swiss National Science Foundation</rs> to <rs type="person">Marius Rubo</rs> (<rs type="funder">SNSF</rs>, Grant Number <rs type="grantNumber">PZ00P1_208909</rs>). This study's sample size, obtained measures and analysis approach were preregistered (<ref type="url" target="https://osf.io/p53fu">https://osf.io/p53fu</ref>). The study conformed to the principles expressed in the Declaration of Helsinki and was approved by the local ethics committee at the <rs type="institution">University of Bern</rs> (Ref-No. 2023-09-01). Data that support the findings of this study are available at <ref type="url" target="https://osf.io/fyr2q/">https://osf.io/fyr2q/</ref> SOCIAL VIRTUAL REALITY <rs type="projectName">ELICITS NATURAL INTERACTION BEHAVIOR</rs></p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_KssZKBy">
					<idno type="grant-number">PZ00P1_208909</idno>
					<orgName type="project" subtype="full">ELICITS NATURAL INTERACTION BEHAVIOR</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Supplementary Table <ref type="table">S4</ref> for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-report Measures</head><p>After the interaction, the participants answered questions on the quality of the interaction, avatar appearance and their VR experience. Additional questions on the perception of their partner's traits are described in a separate report. For questions related to the quality of the interaction and the avatar appearance, we developed a set a of questions which are detailed in Supplementary Materials (see sec. Questionnaires).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quality of Interaction</head><p>For each of the three topics, participants evaluated their success as a dyad in adhering to the specific topic, their own contribution to the topic and their partner's contribution using a 5-point Likert scale ranging from 1 (not at all) to 5 (extremely). To further assess adherence to the instructions, we assessed the intimacy level of personal topic conversations compared to small-talk conversations and participants rated who was more convincing in discussions involving differing opinions on the same scale. An open question was included to gather participants' perceptions of the transitions between topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Avatar Appearance</head><p>In order to assess whether the manipulation of the avatar appearance was successful, participants responded to eight items regarding their interaction with and perceptions of their virtual avatars. All items were rated on a 5-point Likert scale from 1 (not at all) to 5 (completely).</p><p>The questions assessed the avatar's visual resemblance with the participant's own physical appearance (similarity); satisfaction with the avatar's appearance (satisfaction); perceptions of the avatar being more or less attractive than oneself (attractiveness, unattractiveness); whether friends would be able to recognize the avatar as a representation of oneself (recognizability) and assumed ability to re-identify the person they interacted with if encountered again (re-identification). Additionally, the clarity of auditory communication with their interaction partner was assessed to inquire if sound transmission functioned properly (audibility). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Research Transparency Statement</head><p>Conflicts of interest: The authors declare no conflict of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Materials Conversation Topics</head><p>Table <ref type="table">S1</ref> List of Conversation Topics and Questions in English and German</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topic Question</head><p>Small Talk 1. How has today been for you so far?</p><p>Wie ist der heutige Tag für Sie bisher verlaufen?</p><p>2. What do you think of the weather today?</p><p>Wie finden Sie das Wetter heute?</p><p>3. How did you arrive for this examination?</p><p>Wie sind Sie zu dieser Untersuchung angereist?</p><p>Personal Talk</p><p>1. What do you feel most grateful for in life?</p><p>Wofür fühlen Sie sich im Leben am meisten dankbar?</p><p>2. What does friendship mean to you?</p><p>Was bedeutet Freundschaft für Sie?</p><p>3. If you could change anything about your childhood, what would it be?</p><p>Wenn Sie etwas an Ihrer Erziehung als Kind ändern könnten, was wäre es?</p><p>Opinion Talk 1. Should cannabis be legal?</p><p>Sollte Cannabis legal gekauft werden können?</p><p>2. Should organs be allowed to be removed from the deceased if the person has not objected?</p><p>Sollten Organe von Verstorbenen entnommen werden dürfen, wenn die Person dem nicht widersprochen hat?</p><p>3. Should you eat animals? And if so, where is the line between animals that can or cannot be eaten?</p><p>Darf man Tiere essen? Und wenn ja, wo verläuft die Grenze zwischen Tieren, die man essen oder nicht essen darf?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Results</head><p>Outlier Correction      </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Videoconference and embodied vr: Communication patterns across task and medium</title>
		<author>
			<persName><forename type="first">A</forename><surname>Abdullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kolkmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neff</surname></persName>
		</author>
		<idno type="DOI">10.1145/3479597</idno>
		<ptr target="https://doi.org/10.1145/3479597" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM on Human-Computer Interaction</title>
		<meeting>the ACM on Human-Computer Interaction</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Attention to eye contact in the west and east: Autonomic responses and evaluative ratings (A. Mesoudi</title>
		<author>
			<persName><forename type="first">H</forename><surname>Akechi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Uibo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kikuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hasegawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Hietanen</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0059312</idno>
		<ptr target="https://doi.org/10.1371/journal.pone.0059312" />
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">59312</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">All smiles are not created equal: Morphology and timing of smiles perceived as amused, polite, and embarrassed/nervous</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ambadar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">I</forename><surname>Reed</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10919-008-0059-5</idno>
		<ptr target="https://doi.org/10.1007/s10919-008-0059-5" />
	</analytic>
	<monogr>
		<title level="j">Journal of Nonverbal Behavior</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="34" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Conversational gaze aversion for virtual agents</title>
		<author>
			<persName><forename type="first">S</forename><surname>Andrist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-40415-3_22</idno>
		<ptr target="https://doi.org/10.1007/978-3-642-40415-3_22" />
	</analytic>
	<monogr>
		<title level="s">Lecture notes in computer science</title>
		<imprint>
			<biblScope unit="page" from="249" to="262" />
			<date type="published" when="2013">2013</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Gaze and mutual gaze</title>
		<author>
			<persName><forename type="first">M</forename><surname>Argyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cross-cultural variations in relationship rules</title>
		<author>
			<persName><forename type="first">M</forename><surname>Argyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Iizuka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Contarello</surname></persName>
		</author>
		<idno type="DOI">10.1080/00207598608247591</idno>
		<ptr target="https://doi.org/10.1080/00207598608247591" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Psychology</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="287" to="315" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The experimental generation of interpersonal closeness: A procedure and some preliminary findings</title>
		<author>
			<persName><forename type="first">A</forename><surname>Aron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Melinat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">N</forename><surname>Aron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Vallone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Bator</surname></persName>
		</author>
		<idno type="DOI">10.1177/0146167297234003</idno>
		<ptr target="https://doi.org/10.1177/0146167297234003" />
	</analytic>
	<monogr>
		<title level="j">Personality and Social Psychology Bulletin</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="363" to="377" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Do anonymous job application procedures level the playing field?</title>
		<author>
			<persName><forename type="first">O</forename><surname>Åslund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">N</forename><surname>Skans</surname></persName>
		</author>
		<idno type="DOI">10.1177/001979391206500105</idno>
		<ptr target="https://doi.org/10.1177/001979391206500105" />
	</analytic>
	<monogr>
		<title level="j">ILR Review</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="82" to="107" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Nonverbal overload: A theoretical argument for the causes of zoom fatigue</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Bailenson</surname></persName>
		</author>
		<idno type="DOI">10.1037/tmb0000030</idno>
		<ptr target="https://doi.org/10.1037/tmb0000030" />
	</analytic>
	<monogr>
		<title level="j">Technology, Mind, and Behavior</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Controlling the false discovery rate: A practical and powerful approach to multiple testing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Benjamini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hochberg</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.2517-6161.1995.tb02031.x</idno>
		<ptr target="https://doi.org/10.1111/j.2517-6161.1995.tb02031.x" />
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society Series B: Statistical Methodology</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="289" to="300" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mutual gaze, personality, and familiarity: Dual eye-tracking during conversation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Broz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Nehaniv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dautenhahn</surname></persName>
		</author>
		<idno type="DOI">10.1109/roman.2012.6343859</idno>
		<ptr target="https://doi.org/10.1109/roman.2012.6343859" />
	</analytic>
	<monogr>
		<title level="j">IEEE RO-MAN: The 21st IEEE International Symposium on Robot and Human Interactive Communication</title>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Gaze-based assessments of vigilance and avoidance in social anxiety: A review</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Clarke</surname></persName>
		</author>
		<idno type="DOI">10.1007/S11920-017-0808-4/METRICS</idno>
		<ptr target="https://doi.org/10.1007/S11920-017-0808-4/METRICS" />
	</analytic>
	<monogr>
		<title level="j">Current Psychiatry Reports</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mental health discourse on reddit: Self-disclosure, social support, and anonymity</title>
		<author>
			<persName><forename type="first">M</forename><surname>De Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>De</surname></persName>
		</author>
		<idno type="DOI">10.1609/icwsm.v8i1.14526</idno>
		<ptr target="https://doi.org/10.1609/icwsm.v8i1.14526" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International AAAI Conference on Web and Social Media</title>
		<meeting>the International AAAI Conference on Web and Social Media</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="71" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The role of eye gaze in regulating turn taking in conversations: A systematized review of methods and findings</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Degutyte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Astell</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2021.616471</idno>
		<ptr target="https://doi.org/10.3389/fpsyg.2021.616471" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">616471</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Social exclusion and early-stage interpersonal perception: Selective attention to signs of acceptance</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Dewall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Maner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Rouby</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0014634</idno>
		<ptr target="https://doi.org/10.1037/a0014634" />
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="729" to="741" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Eggins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Slade</surname></persName>
		</author>
		<title level="m">Analysing casual conversation</title>
		<imprint>
			<publisher>Equinox Publishing Ltd</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Visual behavior in a dyad as affected by interview content and sex of respondent</title>
		<author>
			<persName><forename type="first">R</forename><surname>Exline</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuette</surname></persName>
		</author>
		<idno type="DOI">10.1037/h0021865</idno>
		<ptr target="https://doi.org/10.1037/h0021865" />
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="201" to="209" />
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Health related virtual communities and electronic support groups: Systematic review of the effects of online peer to peer interactions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Eysenbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Englesakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rizo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stern</surname></persName>
		</author>
		<idno type="DOI">10.1136/bmj.328.7449.1166</idno>
		<ptr target="https://doi.org/10.1136/bmj.328.7449.1166" />
	</analytic>
	<monogr>
		<title level="j">BMJ</title>
		<imprint>
			<biblScope unit="volume">328</biblScope>
			<biblScope unit="issue">7449</biblScope>
			<biblScope unit="page">1166</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gender differences in synchrony: Females in sync during unstructured dyadic conversation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fujiwara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Daibo</surname></persName>
		</author>
		<idno type="DOI">10.1002/ejsp.2587</idno>
		<ptr target="https://doi.org/10.1002/ejsp.2587" />
	</analytic>
	<monogr>
		<title level="j">European Journal of Social Psychology</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1042" to="1054" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Averting the gaze disengages the environment and facilitates remembering</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Glenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Robertson</surname></persName>
		</author>
		<idno type="DOI">10.3758/bf03211385</idno>
		<ptr target="https://doi.org/10.3758/bf03211385" />
	</analytic>
	<monogr>
		<title level="j">Memory I&amp; Cognition</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="651" to="658" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Avatar embodiment. towards a standardized questionnaire</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gonzalez-Franco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Peck</surname></persName>
		</author>
		<idno type="DOI">10.3389/frobt.2018.00074</idno>
		<ptr target="https://doi.org/10.3389/frobt.2018.00074" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Robotics and AI</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">343504</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Differential gaze behavior towards sexually preferred and non-preferred human figures</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hogue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Guo</surname></persName>
		</author>
		<idno type="DOI">10.1080/00224499.2010.521899</idno>
		<ptr target="https://doi.org/10.1080/00224499.2010.521899" />
	</analytic>
	<monogr>
		<title level="j">The Journal of Sex Research</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="461" to="469" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Nonverbal behavior and the vertical dimension of social relations: A meta-analysis</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Coats</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Lebeau</surname></persName>
		</author>
		<idno type="DOI">10.1037/0033-2909.131.6.898</idno>
		<ptr target="https://doi.org/10.1037/0033-2909.131.6.898" />
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">131</biblScope>
			<biblScope unit="page" from="898" to="924" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Nonverbal communication</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev-psych-010418-103145</idno>
		<ptr target="https://doi.org/10.1146/annurev-psych-010418-103145" />
	</analytic>
	<monogr>
		<title level="j">Annual Review of Psychology</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="271" to="294" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Nonverbal communication</title>
		<idno type="DOI">10.1515/9783110238150</idno>
		<ptr target="https://doi.org/doi:10.1515/9783110238150" />
		<editor>Hall, J. A., &amp; Knapp, M. L.</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>De Gruyter Mouton</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Understanding group behavior in virtual reality: A large-scale, longitudinal study in the metaverse</title>
		<author>
			<persName><forename type="first">E</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Bailenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">72nd Annual International Communication Association Conference</title>
		<meeting><address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Social interactions in the metaverse: Framework, initial evidence, and research roadmap</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hennig-Thurau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Aliman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Herting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Cziehso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Linder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Kübler</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11747-022-00908-0</idno>
		<ptr target="https://doi.org/10.1007/s11747-022-00908-0" />
	</analytic>
	<monogr>
		<title level="j">Journal of the Academy of Marketing Science</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="889" to="913" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The weirdest people in the world?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Henrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Heine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Norenzayan</surname></persName>
		</author>
		<idno type="DOI">10.1017/s0140525x0999152x</idno>
		<ptr target="https://doi.org/10.1017/s0140525x0999152x" />
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="61" to="83" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Effect of behavioral realism on social interactions inside collaborative virtual environments</title>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Bailenson</surname></persName>
		</author>
		<idno type="DOI">10.1162/pres_a_00324</idno>
		<ptr target="https://doi.org/10.1162/pres_a_00324" />
	</analytic>
	<monogr>
		<title level="j">Presence: Teleoperators and Virtual Environments</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="163" to="182" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Gaze allocation in face-to-face communication is affected primarily by task structure and social context, not stimulus-driven factors</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Hessels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Holleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kingstone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">T</forename><surname>Hooge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kemner</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cognition.2018.12.005</idno>
		<ptr target="https://doi.org/10.1016/j.cognition.2018.12.005" />
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">184</biblScope>
			<biblScope unit="page" from="28" to="43" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Speaking and listening with the eyes: Gaze signaling during dyadic interactions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Foulsham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kingstone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Guo</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0136905</idno>
		<ptr target="https://doi.org/10.1371/journal.pone.0136905" />
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">136905</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">How gaze visualization facilitates initiation of informal communication in 3d virtual spaces</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ichino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yoshiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yokoyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Miyachi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Okabe</surname></persName>
		</author>
		<idno type="DOI">10.1145/3617368</idno>
		<ptr target="https://doi.org/10.1145/3617368" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Comput.-Hum. Interact</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2020.2970919</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2020.2970919" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="4217" to="4228" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Looking and loving: The effects of mutual gaze on feelings of romantic love</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kellerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Laird</surname></persName>
		</author>
		<idno type="DOI">10.1016/0092-6566(89)90020-2</idno>
		<ptr target="https://doi.org/10.1016/0092-6566(89)90020-2" />
	</analytic>
	<monogr>
		<title level="j">Journal of Research in Personality</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="145" to="161" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Power, approach, and inhibition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Keltner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Gruenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Anderson</surname></persName>
		</author>
		<idno type="DOI">10.1037/0033-295x.110.2.265</idno>
		<ptr target="https://doi.org/10.1037/0033-295x.110.2.265" />
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="265" to="284" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Some functions of gaze-direction in social interaction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kendon</surname></persName>
		</author>
		<idno type="DOI">10.1016/0001-6918(67)90005-4</idno>
		<ptr target="https://doi.org/10.1016/0001-6918(67)90005-4" />
	</analytic>
	<monogr>
		<title level="j">Acta Psychologica</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="22" to="63" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A social relations variance partitioning of dyadic behavior</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Mohr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Levesque</surname></persName>
		</author>
		<idno type="DOI">10.1037/0033-2909.127.1.128</idno>
		<ptr target="https://doi.org/10.1037/0033-2909.127.1.128" />
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="128" to="141" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Virtual reality sickness questionnaire (vrsq): Motion sickness measurement index in a virtual reality environment</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Choe</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.apergo.2017.12.016</idno>
		<ptr target="https://doi.org/10.1016/j.apergo.2017.12.016" />
	</analytic>
	<monogr>
		<title level="j">Applied Ergonomics</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="66" to="73" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Let&apos;s face it: Influence of facial expressions on social presence in collaborative virtual reality</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kimmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Matviienko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Heuten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boll</surname></persName>
		</author>
		<idno type="DOI">10.1145/3544548.3580707</idno>
		<ptr target="https://doi.org/10.1145/3544548.3580707" />
	</analytic>
	<monogr>
		<title level="m">Conference on Human Factors in Computing Systems -Proceedings</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Facial dynamics as indicators of trustworthiness and cooperative behavior</title>
		<author>
			<persName><forename type="first">E</forename><surname>Krumhuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S R</forename><surname>Manstead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cosker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Rosin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kappas</surname></persName>
		</author>
		<idno type="DOI">10.1037/1528-3542.7.4.730</idno>
		<ptr target="https://doi.org/10.1037/1528-3542.7.4.730" />
	</analytic>
	<monogr>
		<title level="j">Emotion</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="730" to="735" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Effects of anonymity, invisibility, and lack of eye-contact on toxic online disinhibition</title>
		<author>
			<persName><forename type="first">N</forename><surname>Lapidot-Lefler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barak</surname></persName>
		</author>
		<idno type="DOI">10.1016/J.CHB.2011.10.014</idno>
		<ptr target="https://doi.org/10.1016/J.CHB.2011.10.014" />
	</analytic>
	<monogr>
		<title level="j">Computers in Human Behavior</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="434" to="443" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The effect of avatar realism in immersive social virtual realities</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Latoschik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Achenbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Waltemate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Botsch</surname></persName>
		</author>
		<idno type="DOI">10.1145/3139131.3139156</idno>
		<ptr target="https://doi.org/10.1145/3139131.3139156" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST, Part F</title>
		<meeting>the ACM Symposium on Virtual Reality Software and Technology, VRST, Part F</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">131944</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Emmeans: Estimated marginal means, aka least-squares means [R package version 1</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Lenth</surname></persName>
		</author>
		<ptr target="https://CRAN.R-project.org/package=emmeans" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Effects of age, sex, and task on visual behavior during dyadic interaction</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sutton-Smith</surname></persName>
		</author>
		<idno type="DOI">10.1037/h0034929</idno>
		<ptr target="https://doi.org/10.1037/h0034929" />
	</analytic>
	<monogr>
		<title level="j">Developmental Psychology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="400" to="405" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Digital body, identity and privacy in social virtual reality: A systematic review</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Latoschik</surname></persName>
		</author>
		<idno type="DOI">10.3389/FRVIR.2022.974652/BIBTEX</idno>
		<ptr target="https://doi.org/10.3389/FRVIR.2022.974652/BIBTEX" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Virtual Reality</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">974652</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Visual fidelity effects on expressive self-avatar in virtual reality: First impressions matter</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<idno type="DOI">10.1109/vr51125.2022.00023</idno>
		<ptr target="https://doi.org/10.1109/vr51125.2022.00023" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Virtual Reality and 3D User Interfaces (VR)</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">talking without a voice</title>
		<author>
			<persName><forename type="first">D</forename><surname>Maloney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Y</forename><surname>Wohn</surname></persName>
		</author>
		<idno type="DOI">10.1145/3415246</idno>
		<ptr target="https://doi.org/10.1145/3415246" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM on Human-Computer Interaction</title>
		<meeting>the ACM on Human-Computer Interaction</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Cognitive dissonance in groups: The consequences of disagreement</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Matz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wood</surname></persName>
		</author>
		<idno type="DOI">10.1037/0022-3514.88.1.22</idno>
		<ptr target="https://doi.org/10.1037/0022-3514.88.1.22" />
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="22" to="37" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Shaping pro-social interaction in vr an emerging design framework</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mcveigh-Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Isbister</surname></persName>
		</author>
		<idno type="DOI">10.1145/3290605.3300794</idno>
		<ptr target="https://doi.org/10.1145/3290605.3300794" />
	</analytic>
	<monogr>
		<title level="m">Conference on Human Factors in Computing Systems -Proceedings</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A large-scale study of proxemics and gaze in groups</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Deveaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Bailenson</surname></persName>
		</author>
		<idno type="DOI">10.1109/VR55154.2023.00056</idno>
		<ptr target="https://doi.org/10.1109/VR55154.2023.00056" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference Virtual Reality and 3D User Interfaces (VR)</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
			<biblScope unit="page" from="409" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A systematic review and meta-analysis of the effectiveness of body ownership illusions in virtual reality</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mottelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Muresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hornbaek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Makransky</surname></persName>
		</author>
		<idno type="DOI">10.1145/3590767</idno>
		<ptr target="https://doi.org/10.1145/3590767" />
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer-Human Interaction</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">76</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Let the avatar brighten your smile: Effects of enhancing facial expressions in virtual environments</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bailenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Krämer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1371/JOURNAL.PONE.0161794</idno>
		<ptr target="https://doi.org/10.1371/JOURNAL.PONE.0161794" />
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">161794</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Why and how to use virtual reality to study human social interaction: The challenges of exploring a new research landscape</title>
		<author>
			<persName><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F C</forename><surname>Hamilton</surname></persName>
		</author>
		<idno type="DOI">10.1111/bjop.12290</idno>
		<ptr target="https://doi.org/10.1111/bjop.12290" />
	</analytic>
	<monogr>
		<title level="j">British Journal of Psychology</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="395" to="417" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">The impact of self-avatars on trust and collaboration in shared virtual environments</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Steed</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0189078</idno>
		<ptr target="https://doi.org/10.1371/journal.pone.0189078" />
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Lappe</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">189078</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Virtual reality for enhanced ecological validity and experimental control in the clinical, affective and social neurosciences</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Parsons</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnhum.2015.00660</idno>
		<ptr target="https://doi.org/10.3389/fnhum.2015.00660" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Human Neuroscience</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A systems model of dyadic nonverbal interaction</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Patterson</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10919-018-00292-w</idno>
		<ptr target="https://doi.org/10.1007/s10919-018-00292-w" />
	</analytic>
	<monogr>
		<title level="j">Journal of Nonverbal Behavior</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="111" to="132" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><surname>&amp; R Core</surname></persName>
		</author>
		<author>
			<persName><surname>Team</surname></persName>
		</author>
		<ptr target="https://www.R-project.org/" />
		<title level="m">Nlme: Linear and nonlinear mixed effects models [R package version 3</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023. 2024</date>
			<biblScope unit="page" from="1" to="164" />
		</imprint>
	</monogr>
	<note>R: A language and environment for statistical computing R Foundation for Statistical Computing</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Realistic motion avatars are the future for social interaction in virtual reality</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Broadbent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Speelman</surname></persName>
		</author>
		<idno type="DOI">10.3389/FRVIR.2021.750729/BIBTEX</idno>
		<ptr target="https://doi.org/10.3389/FRVIR.2021.750729/BIBTEX" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Virtual Reality</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">750729</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Stronger reactivity to social gaze in virtual reality compared to a classical laboratory environment</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rubo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gamer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British Journal of Psychology</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="301" to="314" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The timing of looks in dyadic conversation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Rutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Stephenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ayling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>White</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.2044-8260.1978.tb00890.x</idno>
		<ptr target="https://doi.org/10.1111/j.2044-8260.1978.tb00890.x" />
	</analytic>
	<monogr>
		<title level="j">British Journal of Social and Clinical Psychology</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="21" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Relightable gaussian codec avatars</title>
		<author>
			<persName><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Nam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="130" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Meeting your virtual twin: Effects of photorealism and personalization on embodiment, self-identification and perception of self-avatars in virtual reality</title>
		<author>
			<persName><forename type="first">A</forename><surname>Salagean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Crellin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Parsons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cosker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Fraser</surname></persName>
		</author>
		<idno type="DOI">10.1145/3544548.3581182</idno>
		<ptr target="https://doi.org/10.1145/3544548.3581182" />
	</analytic>
	<monogr>
		<title level="m">Conference on Human Factors in Computing Systems -Proceedings</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Dynamics of facial expression: Normative characteristics and individual differences</title>
		<author>
			<persName><forename type="first">K</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<idno type="DOI">10.1109/icme.2001.1237778</idno>
		<idno>ICME 2001</idno>
		<ptr target="https://doi.org/10.1109/icme.2001.1237778" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo</title>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">The experience of presence: Factor analytic insights</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Friedmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Regenbrecht</surname></persName>
		</author>
		<idno type="DOI">10.1162/105474601300343603</idno>
		<ptr target="https://doi.org/10.1162/105474601300343603" />
	</analytic>
	<monogr>
		<title level="j">Presence: Teleoperators and Virtual Environments</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="266" to="281" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Cultural background modulates how we look at other persons&apos; gaze</title>
		<author>
			<persName><forename type="first">A</forename><surname>Senju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vernetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kikuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Akechi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hasegawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="DOI">10.1177/0165025412465360</idno>
		<ptr target="https://doi.org/10.1177/0165025412465360" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Behavioral Development</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="131" to="136" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Communication behavior in embodied virtual reality</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neff</surname></persName>
		</author>
		<idno type="DOI">10.1145/3173574.3173863</idno>
		<ptr target="https://doi.org/10.1145/3173574.3173863" />
	</analytic>
	<monogr>
		<title level="m">Conference on Human Factors in Computing Systems -Proceedings</title>
		<imprint>
			<date type="published" when="2018-04">2018. 2018-April</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Influence of avatar facial appearance on users&apos; perceived embodiment and presence in immersive virtual reality</title>
		<author>
			<persName><forename type="first">H</forename><surname>Suk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Laine</surname></persName>
		</author>
		<idno type="DOI">10.3390/electronics12030583</idno>
		<ptr target="https://doi.org/10.3390/electronics12030583" />
	</analytic>
	<monogr>
		<title level="j">Electronics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">583</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">The online disinhibition effect</title>
		<author>
			<persName><forename type="first">J</forename><surname>Suler</surname></persName>
		</author>
		<idno type="DOI">10.1089/1094931041291295</idno>
		<ptr target="https://doi.org/10.1089/1094931041291295" />
	</analytic>
	<monogr>
		<title level="j">CyberPsychology I&amp; Behavior</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="321" to="326" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Effect of avatar facial expressiveness on team collaboration in virtual reality</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Tarnec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bevacqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Augereau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Loor</surname></persName>
		</author>
		<idno type="DOI">10.1145/3570945.3607330</idno>
		<ptr target="https://doi.org/10.1145/3570945.3607330" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM International Conference on Intelligent Virtual Agents</title>
		<meeting>the 23rd ACM International Conference on Intelligent Virtual Agents</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">The impact of avatar personalization and immersion on virtual body ownership, presence, and emotional response</title>
		<author>
			<persName><forename type="first">T</forename><surname>Waltemate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Botsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Latoschik</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2018.2794629</idno>
		<ptr target="https://doi.org/10.1109/TVCG.2018.2794629" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1643" to="1652" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Communication in immersive social virtual reality: A systematic review of 10 years&apos; studies</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fan</surname></persName>
		</author>
		<idno type="DOI">10.1145/3565698.3565767</idno>
		<ptr target="https://doi.org/10.1145/3565698.3565767" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Symposium of Chinese CHI</title>
		<meeting>the Tenth International Symposium of Chinese CHI</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="27" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">A systematic review on the visualization of avatars and SOCIAL VIRTUAL REALITY ELICITS NATURAL INTERACTION BEHAVIOR 41 agents in ar &amp; vr displayed using head-mounted displays</title>
		<author>
			<persName><forename type="first">F</forename><surname>Weidner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Boettcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Arboleda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sinani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kunert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gerhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Broll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raake</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2023.3247072</idno>
		<ptr target="https://doi.org/10.1109/TVCG.2023.3247072" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2596" to="2606" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Theories and methods in mediated communication</title>
		<author>
			<persName><forename type="first">S</forename><surname>Whittaker</surname></persName>
		</author>
		<idno type="DOI">10.4324/9781410607348</idno>
		<ptr target="https://doi.org/10.4324/9781410607348" />
		<editor>A. C. Graesser, M. A. Gernsbacher, &amp; S. R. Goldman</editor>
		<imprint>
			<date type="published" when="2003-04">2003, April</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Nonverbal communication in immersive virtual reality through the lens of presence: A critical review</title>
		<author>
			<persName><forename type="first">I</forename><surname>Xenakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gavalas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kasapakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dzardanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vosinakis</surname></persName>
		</author>
		<idno type="DOI">10.1162/pres_a_00387</idno>
		<ptr target="https://doi.org/10.1162/pres_a_00387" />
	</analytic>
	<monogr>
		<title level="j">PRESENCE: Virtual and Augmented Reality</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="147" to="187" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Tracing physical behavior in virtual reality: A narrative review of applications to social psychology</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Yaremych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Persky</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jesp.2019.103845</idno>
		<ptr target="https://doi.org/10.1016/j.jesp.2019.103845" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Social Psychology</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page">103845</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Table S</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Standard Deviations and 95% Confidence Intervals of Behavior in Individual Topics Measure Topic M SD 95% CI Gaze while Speaking Small Talk</title>
		<author>
			<persName><surname>Means</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">19.93, 23.90</date>
			<biblScope unit="volume">24</biblScope>
		</imprint>
	</monogr>
	<note>Personal Talk 22.93 12 Opinion Talk 21.92 11.35 Personal Talk 51.43 16 Opinion Talk 48.15 16.48 [45.26 Gaze Beginning of Speech Small Talk 18.92 10.08 [17.16, 20.69] Personal Talk 16.25 11.46 [14.25, 18.26] Opinion Talk 12.84 8.94 [11.28 Gaze End of Speech Small Talk 35.53 15.29 [32.86, 38.21] Personal Talk 33.79 17.21 [30.78, 36.80 Opinion Talk 30.36 14.89 [27.75 Smile Small Talk 16.91 7.99 [15.51, 18.31] Personal Talk 12.60 7.83 [11.23, 13.97] Opinion Talk 10.69 6.85 [9.49, 11.89] Speaking Opinion Talk 50.11 10.04 [48.36, 51.87] Personal Talk 49.64 8.97 [48.07, 51.20] Small Talk 52.44 10.48 [50.61, 54.28] Interrupting Small Talk 6.75 4.27 [6.00, 7.49] Personal Talk 3.76 2.95 [3.24, 4.28] Opinion Talk 3.96 3.27 [3.39, 4.53] Turn Duration Small Talk 7.68 2.92 [7.17, 8.19] Personal Talk 13.59 5.70 [12.59, 14.59] Opinion Talk 13.47 5.55 [12.50, 14.44] Gaps Small Talk 1.14 0.52 [1.05, 1.23] Personal Talk 1.44 0.71 [1.31, 1.56] Opinion Talk 1.51 0.79 [1.38, 1.65</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
