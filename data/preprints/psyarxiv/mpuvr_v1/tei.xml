<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Vocal pitch enables differential motor learning of speech segments</title>
				<funder ref="#_EAmKrMw">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
				<funder ref="#_9vFvKpc">
					<orgName type="full">National Institute on Deafness and Other Communication Disorders</orgName>
				</funder>
				<funder ref="#_2JZKb6c">
					<orgName type="full">National Institute of Child Health and Human Development</orgName>
				</funder>
				<funder>
					<orgName type="full">Waisman Center</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Robin</forename><surname>Karlin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Speech</orgName>
								<orgName type="department" key="dep2">Language and Hearing Sciences</orgName>
								<orgName type="institution">University of Missouri</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Waisman Center</orgName>
								<orgName type="institution">University of Wisconsin</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Emily</forename><surname>Tesch</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Speech and Hearing Science</orgName>
								<orgName type="institution">Arizona State University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Waisman Center</orgName>
								<orgName type="institution">University of Wisconsin</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ding-Lan</forename><surname>Tang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Learning, and Development</orgName>
								<orgName type="institution">Hong Kong University - Academic Unit of Human Communication</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Waisman Center</orgName>
								<orgName type="institution">University of Wisconsin</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuyu</forename><surname>Zeng</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Waisman Center</orgName>
								<orgName type="institution">University of Wisconsin</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Caroline</forename><forename type="middle">A</forename><surname>Niziolek°4</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Waisman Center</orgName>
								<orgName type="institution">University of Wisconsin</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Department of Communication Sciences</orgName>
								<orgName type="institution">University of Wisconsin</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Benjamin</forename><surname>Parrell°4</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Waisman Center</orgName>
								<orgName type="institution">University of Wisconsin</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Department of Communication Sciences</orgName>
								<orgName type="institution">University of Wisconsin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Vocal pitch enables differential motor learning of speech segments</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">34A64BB252C3CDE225A90F9D956611EB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-22T20:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sensory feedback is crucial for maintaining accurate motor control. One process of movement correction is sensorimotor adaptation, or motor learning in response to perceived sensory errors.</p><p>Recent studies have demonstrated that people can simultaneously adapt to opposing errors on a single movement (e.g., leftward and rightward errors on a reach) given some context that differentiates when each error occurs. In speech production, linguistic structure (e.g., the same vowel in different words) has been shown to provide sufficient context for adapting to opposing errors, but it is not clear whether this is restricted to the same effectors (i.e. lips, tongue, jaw in the oral cavity) or also includes movements of other effectors used in speech (i.e., the vocal folds in the larynx). While manual reaching studies have shown that contextual movements need not be produced with the same effector as the learning target, they have thus far only tested left-right effector pairs. We present the results of three simultaneous adaptation experiments in speech that examine whether laryngeal movements for pitch control can provide context for oral articulatory movements for vowels. In each experiment, the resonances that correlate with articulator position during vowels were perturbed in three different directions that were predictable given a pitch context. First, Mandarin speakers differentially adapted given pitch contexts that signaled differences in word meaning, suggesting that lexical uses of pitch provide context for vowels.</p><p>Second, English speakers differentially adapted given different arbitrary pitch matching contexts on the word "head", suggesting that even non-meaningful pitch movements provide context for vowels. Third, English speakers were unable to differentially adapt when simply listening to a contextual pitch, indicating that mere auditory input of pitch is insufficient. Together, these results indicate that sensorimotor context for learning can be provided by different effectors than the learning target.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Extensive research has shown that sensory feedback plays a crucial role in maintaining accurate motor control, including speech motor control <ref type="bibr">(Houde &amp; Jordan, 2002b</ref><ref type="bibr">, 2002a;</ref><ref type="bibr" target="#b18">J. A. Jones &amp; Munhall, 2000;</ref><ref type="bibr" target="#b39">Tourville et al., 2008)</ref>. Motor behavior produces sensory feedback (e.g., visual, somatosensory, auditory feedback) that the central nervous system uses for both online control and to alter motor plans for future movement. External perturbations of this feedback have been used extensively to examine these processes, with a particular focus on mechanisms of feedbackdriven updates to future movement plans, generally referred to as sensorimotor adaptation. For example, visual feedback of reaching movements can be altered such that participants see their hand further to the right or left than reality <ref type="bibr" target="#b8">(Ghahramani et al., 1996;</ref><ref type="bibr" target="#b20">Krakauer et al., 1999;</ref><ref type="bibr" target="#b37">Simani et al., 2007)</ref>; typical studies in speech apply perturbations to the auditory feedback that speakers hear of their own voice, e.g., lowering the first resonant frequency (F1) of the vowel in head /hɛd/ to result in a token sounding more like hid /hɪd/ <ref type="bibr" target="#b25">(Munhall et al., 2009;</ref><ref type="bibr" target="#b40">Villacorta et al., 2007)</ref>. Historically, perturbations have been applied consistently to either a single movement, or to all movements in a study. As such, the scope of sensorimotor adaptation is unclear. That is, it is not known the extent to which people are learning about movement in general vs. more specific learning of the movements targeted by the experiment-and if it is specific, what defines a specific movement.</p><p>A growing literature has begun to examine what conditions allow for separate, contextdependent adaptation of one movement <ref type="bibr" target="#b9">(Gippert et al., 2023;</ref><ref type="bibr" target="#b16">Howard et al., 2010</ref><ref type="bibr" target="#b15">Howard et al., , 2012;;</ref><ref type="bibr" target="#b36">Sheahan et al., 2016)</ref>. In these simultaneous adaptation experiments, a single target movement is perturbed in two opposing directions during a single experimental phase. The direction of perturbation is consistently associated with a secondary contextual cue. For example, a forward reach may be perturbed to the right when associated with a particular color or when followed by a rightward reach, and perturbed to the left when associated with a distinct color or when followed by a leftward reach. These studies have shown that only cues that involve the motor system in some way (such as being paired with different reaches) enable differential learning, while purely sensory cues (such as different colored lights) do not <ref type="bibr" target="#b7">(Gandolfo et al., 1996;</ref><ref type="bibr" target="#b15">Howard et al., 2012</ref><ref type="bibr" target="#b17">Howard et al., , 2013;;</ref><ref type="bibr" target="#b36">Sheahan et al., 2016)</ref>. However, studies that test pairs of movements have been limited in that they have only tested context-target pairs that use either the same effector (e.g., two reaches with one hand) or contralateral effector pairs (e.g., a reach with the left hand providing context for a reach with the right hand). Thus, it is unclear if movements from a different type of effector can provide sensorimotor context for a target movement.</p><p>There has recently been a handful of studies on context-dependent adaptation in speech, which established the use of opposing auditory perturbations as a tool to investigate the scope of learning in speech. The organization of language poses a particularly interesting arena for investigation, as there are many theoretical units that could be the target of sensorimotor adaptation, such as a single word, a phoneme, or even an entire class of speech sounds. However, similarly to manual reaching studies, examination of specific learning thus far has been limited to word contexts that recruit contextual movements of the same articulators, such as simultaneous perturbation of the same vowel in different words with different preceding sounds (e.g. "head" vs. "Ted", <ref type="bibr" target="#b32">Rochet-Capellan &amp; Ostry, 2011)</ref>, or simultaneous perturbation of the same syllable in different words (e.g. "pedigree" vs. "pedicure", <ref type="bibr" target="#b47">Zeng et al., 2023)</ref>. Similar to the work in the reaching literature, both of these studies showed that speakers were able to implement different adaptations to different words at the same time, indicating that sensorimotor adaptation can apply to a more specific target than one movement (i.e., a word rather than a phoneme or syllable). However, in both of these studies, either lexical information or segmental kinematics could be providing the context.</p><p>Here, we investigate the extent to which vocal pitch-i.e., the fundamental frequency (f0) of a spoken word-can serve as the context for differential sensorimotor adaptation. F0 provides crucial insight on the question of specific sensorimotor adaptation because it creates different motor contexts without directly involving the articulators targeted by the auditory perturbations.</p><p>In Experiment 1, we test the hypothesis that f0 as lexical tone in Mandarin can serve as context for differential adaptation segments by perturbing the vowel ei in minimal pairs that consist of identical segments and differ only in tone. We show that Mandarin speakers can adapt F1 production to simultaneous opposing perturbations that are cued by lexical tone, indicating that differential adaptation is not reliant on differing kinematics of the target articulator. We also conduct two follow-up studies to pinpoint the source of motor context from lexical tone production. First, we examine whether these results are unique to lexical tone, or whether non-lexical f0 can similarly facilitate simultaneous adaptation to opposing formant perturbations of a single target syllable by having English-speaking participants imitate distinct auditory tones (Experiment 2). Second, we test a control condition that includes the same auditory pitch cue as in Experiment 2 but without imitation in production (Experiment 3), which we do not expect to enable adaptation as it is a non-motor cue. These follow-up studies show that pitch imitation, but not pitch cuing alone, facilitates adaptation to simultaneous opposing formant perturbations, indicating that speech motor learning can be specific to a vocal pitch context, even when the pitch confers no linguistic meaning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 1: Lexical tone in Mandarin Chinese</head><p>Mandarin Chinese (Mandarin) is a lexical tone language with four lexical tones. Tone has a high functional load in the language <ref type="bibr" target="#b28">(Oh et al., 2015)</ref>; minimal pairs where tone is the sole difference are common. In this experiment, we test whether lexical tone can serve as motor context for sensorimotor learning in vowels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>Twenty native speakers of Mandarin (16 women, 3 men, 1 non-binary), ranging in age from 18 to 31 years (median: 23, sd: 3.8), participated in Experiment 1. A sample size of 20 participants provides 80% power to detect an effect size of d = 0.58, which is a much smaller effect size than those reported in previous simultaneous adaptation studies <ref type="bibr" target="#b32">(Rochet-Capellan &amp; Ostry, 2011)</ref>. No participant reported any history of hearing, speech, or neurological disorders. In addition, all participants passed an automated Hughson-Westlake hearing screening (pure-tone thresholds ≤ 25 dB HL in both ears at <ref type="bibr">250, 500, 1000, 2000, and</ref><ref type="bibr">4000 Hz)</ref> 飞 fēi "fly" (tone 1; high tone); 肥 féi "fat" (tone 2; rising tone), and 费 fèi "cost" (tone 4; falling tone) (Figure <ref type="figure" target="#fig_1">1</ref>, left). Words were presented to the participant on a computer monitor using simplified Chinese characters. In each trial, the target word was on the screen for 1.5 seconds, and there was 1.25 seconds between the end of one trial and the beginning of the next trial, with a random jitter of up to 250 ms in either direction (i.e. 1-1.5 seconds between stimuli). The target words were pseudorandomly ordered within each phase (see below) such that no two sequential trials had the same target.</p><p>The vowel ei in each word received one of three perturbations to F1 (first resonant frequency of the vowel): F1 up, F1 down, or no perturbation. Formant frequency alterations were applied in mels, a logarithmic transformation of f0 where equal differences in mels are judged by listeners to correspond to equal changes in pitch. The perturbation received by each word was counterbalanced across participants to the extent possible (each of the possible six permutations assigned to three or four participants). Participants spoke words as they appeared on the screen into a desk-mounted microphone (Sennheiser MKE 600), and received auditory feedback through over-ear headphones (Beyerdynamic DT 770 PRO) at ~80 dB SPL mixed with masking noise at ~60 dB SPL to limit potential bone-or air-conducted perception of unperturbed speech.</p><p>Speech was recorded, processed, perturbed (on some trials), and played back to participants using Audapter <ref type="bibr" target="#b3">(Cai et al., 2008)</ref>. The measured latency of this system was ~19 ms.</p><p>The experiment had four phases (Figure <ref type="figure" target="#fig_1">1</ref>, bottom): a baseline phase with veridical feedback (30 trials each of 3 words; 90 total trials); a ramp phase where the perturbations were gradually introduced up to a maximum of 125 mels (30 trials each of 3 words; 90 total trials); a hold phase with constant perturbation of 125 mels (90 trials each of 3 words; 270 total trials); and a washout phase with veridical feedback (30 trials each of 3 words; 90 total trials). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data processing</head><p>Formant tracking was performed with wave_viewer <ref type="bibr" target="#b27">(Niziolek &amp; Houde, 2015)</ref>, a MATLAB-based GUI that uses the Praat formant tracking algorithm <ref type="bibr" target="#b2">(Boersma &amp; Weenink, 2017)</ref>. Vowel onset and offset were set automatically using a participant-specific amplitude threshold. Errors in vowel onset and offset were corrected by hand-marking the location of the vowel using the spectrogram and waveform of the speech sample. Vowel onset was identified by the presence of F1 and F2 on the spectrogram and periodicity on the waveform. Vowel offset was identified when F1 and F2 were no longer visible on the spectrogram. Within this marked time range, formant values were tracked based on specific parameters set for each participant (LPC order and pre-emphasis); these parameters were adjusted on a per-trial basis if there were formant tracking errors. Trials with unresolvable formant tracking errors or with production errors (such as saying the wrong word, yawning during production, etc.) were excluded (1.4%, 0-3.5% across participants). In order to focus on changes in F1 due to sensorimotor adaptation and avoid effects of formant transitions and online compensation, which is typically measurable ~100-150 ms after the onset of F1 perturbation <ref type="bibr" target="#b23">(Larson et al., 2008;</ref><ref type="bibr" target="#b39">Tourville et al., 2008)</ref>, a single mean F1 value for each trial was calculated from a window 25-100 ms after vowel onset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistical analysis</head><p>Statistical analyses were conducted on the change in F1, in mels, in each phase compared to baseline productions. The baseline F1 value was calculated as the mean F1 value of the last 10 productions in the baseline phase of each word. The statistical model includes the last 10 trials of each word from the baseline phase, the last 10 trials of each word from the hold phase (as a measure of maximum adaptation, taken when participants have the maximum exposure), and the first 10 trials of each word from the washout phase (as a measure of persistence of adaptation, taken before learning is washed out by the return to veridical feedback).</p><p>Linear mixed-effects models were performed in R using the lme4 package <ref type="bibr" target="#b1">(Bates et al., 2014;</ref><ref type="bibr" target="#b30">R Core Team, 2019)</ref>. Models were built incrementally with maximum likelihood comparisons using the anova function in the lmerTest package <ref type="bibr" target="#b21">(Kuznetsova et al., 2015)</ref> to determine which fixed effects remain in the model. Potential fixed effects included the formant shift applied (levels: F1 up, F1 down, no shift), phase of the experiment (levels: baseline, hold, washout), and the interaction between shift and phase. Models also included participant as a random effect. Post-hoc tests were performed with the emmeans package in R <ref type="bibr" target="#b24">(Lenth, 2019)</ref>, using the Tukey adjustment for multiple comparisons. Reported means are estimated means, plus or minus standard error, in distance from baseline in mels. Effect sizes were calculated using the function eff_size in the emmeans package, based on the final model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The results from Experiment 1 show that, as a group, Mandarin speakers learn simultaneous, opposing adaptations of the same segmental content using tone as context (Figure <ref type="figure" target="#fig_2">2</ref>). During the hold phase, participants overall adapted their F1 up in opposition to a downward shift (26.8 ± 9.3 mels, p &lt; 0.0001 compared to baseline), and adapted their F1 down in opposition to an upward shift (-41.9 ± 9.2 mels, p &lt; 0.0001), but did not change their production of the unshifted word (3.8 ± 9.2 mels, p = 1.00).</p><p>The adaptive responses remained in early washout: participants maintained higher F1 in the shift down condition (27.5 ± 9.3 mels, p &lt; 0.0001 compared to baseline), lower F1 in the shift up condition (-44.7 ± 9.0 mels, p &lt; 0.0001), and showed no change in the no shift condition (-1.6 ± 9.2 mels, p = 1.00). There was no significant change between hold and early washout in any shift condition (p &gt; 0.98 for all shifts). Crucially, all shift conditions were significantly different from each other in the expected direction during both the hold and washout phases (all p &lt; 0.0005); the difference between shift up and shift down was large in both the hold (Cohen's d = 1.33) and washout phases (Cohen's d = 1.39). These results indicate that Mandarin speakers learned three different adaptations on three different words that were differentiated by lexical tone alone, and support the hypothesis that the lexical tone in Mandarin can provide context for sensorimotor learning of segments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 2: Arbitrary pitch in English</head><p>Experiment 1 provides support for the idea that lexical f0 can serve as context to differentiate between segmentally identical motor plans. However, it is unclear whether the ability of f0 to enable this learning is restricted to lexical pitch, or whether f0 is universally planned with the segmental content of speech. In Experiment 2, we test this by extending the simultaneous adaptation paradigm used in Experiment 1 to English speakers producing the word head with three different arbitrary (non-lexical) f0 levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>Twenty native speakers of American English (16 women, 4 men), ranging in age from 18 to 30 years (median: 20, sd: 3.9), participated in Experiment 2. Participants underwent the same screening procedures for neurological, speech, and hearing disorders as Experiment 1.</p><p>Participants were compensated for their participation either monetarily or through extra credit in a course in the University of Wisconsin-Madison Communication Sciences and Disorders Department. All participants gave informed consent. All procedures were approved by the institutional review board at the University of Wisconsin-Madison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task</head><p>In Experiment 2, there was only one target word, head (Figure <ref type="figure" target="#fig_1">1</ref>, center). On each trial, participants heard a 300 ms pure tone (high, mid, or low; for more information on how the frequencies of the pure tones were determined, see Section 3.1.3 below). There was a 50 ms gap before the orthographic stimulus head appeared on the computer screen. The orthographic stimulus was on the screen for 1.5 seconds, and there was 1.25 seconds between the end of one trial and the beginning of the next trial, with a random jitter of up to 250 ms in either direction (i.e. 1-1.5 seconds between stimuli). Participants were instructed to match the pitch of their production of the word head to the pitch cue. Participants were instructed to speak the word as normally as possible, rather than singing. Pure tones were pseudorandomly presented such that no two adjacent trials had the same target pitch, as in Experiment 1.</p><p>The vowel /ɛ/ in head received one of three perturbations, with a maximum perturbation of 125 mels: F1 up, F1 down, or no perturbation. The perturbation received by each word was counterbalanced across participants to the extent possible (each of the possible six permutations assigned to three or four participants). The experimental phases <ref type="bibr">(baseline, ramp, hold, washout)</ref> were the same as in Experiment 1. All audio recording, perturbation, and playback equipment was similarly the same as in Experiment 1.</p><p>After the experiment was complete, participants completed an abbreviated version of the Edinburgh Lifetime Music Experience Questionnaire (ELMEQ, <ref type="bibr" target="#b29">Okely et al., 2021)</ref> to collect information on instrumental and voice training.</p><p>Stimuli: Participant-specific pitch cues For each participant, the frequencies of the pure tone targets were determined at the beginning of the experiment, based on their habitual pitch. In a pretest phase, participants read the phrases "My lion is yellow." "Our llama ran away!" and "Does Mary owe you money?" three times each, in random order. These phrases are highly sonorant and use a wide intonational range, thus providing an approximation of their habitual pitch. F0 tracks were automatically extracted using wave_viewer <ref type="bibr" target="#b26">(Niziolek, 2021)</ref>. To avoid undue influence from mistracked samples, f0 values were excluded 1) first if they were beyond minimum and maximum acceptable boundaries (lower than 50 Hz, or higher than 500 Hz), and 2) then if they were more than 3 standard deviations beyond the median pitch after the removal of values outside the acceptable boundaries.</p><p>The median f0 value of the cleaned f0 tracks was then used as the baseline for the low tone. The experimenter confirmed that the baseline f0 was a likely candidate (not based on mistracked pitch) based on their impression of the participant's voice and gender-related differences, using a general guideline of ~100 Hz for male participants and ~200 Hz for female participants. This value was then matched to the closest canonical musical note to produce the low tone. Canonical musical notes were used in case there were participants with perfect pitch that might be bothered by f0 values that were slightly off from canonical notes. The mid tone was 3 semitones higher than the low tone, and the high tone was 3 semitones higher than the mid tone, for an overall difference of 6 semitones between high and low. This value was based on the range of the Mandarin falling tone (tone 4) in pilot data from Experiment 1, such that speakers in both experiments used a similar pitch range. Pilot testing showed that this range was generally comfortable; three semitones is also well above pitch discrimination thresholds for both typical listeners and "tone-deaf" listeners (J. L. <ref type="bibr" target="#b19">Jones et al., 2009)</ref>.</p><p>Pure tones (sine waves) were then generated for each pitch, with a duration of 300 ms.</p><p>This duration was based on typical durations of the vowel in head in previous experiments <ref type="bibr" target="#b10">(Hantzsch et al., 2022)</ref>, and was chosen to promote a spoken production of the word, rather than a sung production. To ensure equal loudness percepts between the tones, the amplitude of each tone was set based on the 80-phon curve for each frequency <ref type="bibr">(ISO 226: 2003(E)</ref>: <ref type="bibr">Acoustics-Normal Equal-Loudness-Level Contours, 2003;</ref><ref type="bibr" target="#b38">Takeshima et al., 2003)</ref>.</p><p>After the tone values were determined, participants completed a practice phase with nine trials (three trials per pitch cue) that were identical in procedure to the remainder of the experiment, with no perturbation to the vowel formants. During the practice phase, the experimenter assessed whether the participant was speaking the words (as opposed to "singing" them), and if they were reliably producing pitch differences. If either of these criteria was not met, the practice phase was repeated. Participants were also able to repeat practice on request.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data processing</head><p>The data processing and analysis used in Experiment 2 were identical to those used in Experiment 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistical analysis</head><p>The general statistical analysis for Experiment 2 was identical to that of Experiment 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Results from Experiment 2 show that, as a group, English speakers learn simultaneous, opposing adaptations with arbitrary produced f0 as context (Figure <ref type="figure" target="#fig_3">3</ref>). During the hold phase, participants overall adapted their F1 up in opposition to a downward shift (26.4 ± 11.6 mels, p = 0.003 compared to baseline); however, the change in F1 in opposition to an upward shift, while numerically in the expected direction, did not reach statistical significance (-18.6 ± 11.6 mels, p = 0.06). Participants also did not change their production of the unshifted word (-4.3 ± 11.6 mels, p = 1.00).</p><p>During the washout phase, participants retained the adaptation to the downward shift (27.2 ± 11.6 mels, p = 0.002 compared to baseline); for the upward shift condition, change in F1 was numerically in the expected direction, but did not reach statistical significance, similar to the hold phase (-18.6 ± 11.6 mels, p = 0.14). Participants also did not change their production of the unshifted word (8.5 ± 11.6 mels, p = 0.95). There was no significant change between the hold and washout phases in any shift condition (all p &gt; 0.6).</p><p>Crucially, upward shift and downward shift significantly differed from each other in the expected direction during both the hold and washout phases (both p &lt; 0.0001). The difference between shift up and shift down was large in both the hold (Cohen's d = 1.01) and washout phases (Cohen's d = 1.06), but less so than for Mandarin speakers. These results suggest that speakers were able to learn simultaneous opposing adaptations when cued by matching different pitches, but not as robustly as Mandarin speakers given the lack of any significant change in the shift up condition relative to baseline. Post-hoc analysis: Ability to produce distinct pitch categories Although the pitch cues were three semitones apart, we observed that some participants had difficulty matching pitch. In particular, most participants had relatively clear high and low categories that aligned with the f0 of the pure tone cue, but for several participants the mid tone was not distinct, with many trials produced identically to either high or low tones (see Figure <ref type="figure">4a</ref> and <ref type="figure">4b</ref>). This suggests that for these speakers, the problem lay in the perception of the mid tone as a distinct category, rather than difficulty in achieving high or low f0 values. That is, these speakers may have been perceiving the mid tone as "higher" or "lower" compared to the previous tone that they heard and producing a high or low pitch accordingly. As we hypothesized that distinct motor plans are crucial to learn simultaneous opposing adaptations, an inability to distinguish the mid tone as a distinct category might impair learning. To test this post-hoc hypothesis, we conducted a further analysis comparing adaptation in participants who were able to consistently produce three distinct pitch categories and participants who were not.</p><p>To determine which participants were able to produce distinct pitches, we first extracted the median f0 from vowel onset to vowel offset in each trial. We iterated a k-means clustering algorithm 1000 times for each participant, with a target of three clusters (k =3). On each run of the algorithm, we compared the assigned cluster for each trial (low, middle, or high f0) to the target tone for the trial. We then extracted the mean percent correct classification overall, as well as the mean percent correct for the mid tone (mid tone trials classified as the middle cluster).</p><p>There was a large separation between groups in percent mid tone correct (see Figure <ref type="figure">4c</ref>); nonmatchers (n = 8) were defined as speakers who had less than 60% of the mid tones classified correctly.</p><p>As musical training may have played a role in the ability to match pitch (and potentially a role in the likelihood of treating pitch and segmental plans as a cohesive unit), we compared musical background between matchers and non-matchers using results from the post-experiment musical experience survey (ELMEQ). Pitch-matching ability was related to experience with vocal music: Of the 12 matchers, eight had vocal training or experience singing in a choir (of which five also had instrumental experience), and four had experience with an instrument but no voice or choir experience. Of the eight non-matchers, none had vocal training or experience singing in a choir; five had experience with an instrument. Figure <ref type="figure">4</ref>: A: Distribution of produced f0 for a participant who was very successful at matching. B: Distribution of produced f0 for a participant who did not consistently differentiate the mid tone. C: Comparison of participants who were successful at matching and those who were not.</p><p>To determine whether differential adaptation was contingent on pitch-matching ability, we compared matchers and non-matchers in their magnitude of differential adaptation, calculated for each participant as the mean difference between the shift up and shift down conditions, for both the hold and washout phases (Figure <ref type="figure" target="#fig_4">5</ref>). Given the small dataset, one-tailed Welch's t-tests were used to assess differences between the groups in each phase, where the predicted direction is that matchers would have greater separation between shift conditions than non-matchers. While the mean difference between matchers and non-matchers was numerically different (hold: 49.92 mels for matchers, 42.37 for non-matchers; washout: 52.67 mels for matchers, 34.53 mels for non-matchers), there was no statistically significant difference between matchers and nonmatchers in either hold (t(14.544) = 0.26, p = 0.40) or in washout (t(12.647) = 0.66, p = 0.26).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Post-hoc analysis: Comparing magnitude of adaptation to Mandarin speakers</head><p>The less consistent results for adaptation seen in English speakers in Experiment 2 compared to Mandarin speakers in Experiment 1 suggests that lexical tone and arbitrary pitch may differ in the extent to which they enable differential sensorimotor adaptation to vowel formant perturbations. To directly compare the magnitude of simultaneous opposing adaptation in these two contexts, we also conducted a one-tailed t-test comparing the two experiments on differential adaptation, defined for each speaker as the mean difference between the shift up and shift down conditions, for both the hold and washout phases. Here, the predicted direction is that Mandarin speakers in Experiment 1 would adapt more than English speakers in Experiment 2 given that pitch has lexical value in the former task and is entirely arbitrary in the latter.</p><p>Mean differential adaptation (Figure <ref type="figure" target="#fig_5">6</ref>) was numerically larger in Mandarin than in English in both the hold (Mandarin: M = 69.02 mels, SD = 68.3 mels; English: M = 46.90 mels, SD = 61.98 mels) and washout phases (Mandarin: M = 72.01 mels, SD = 55.78 mels; English: M = 45.41 mels, SD = 56.29 mels). However, this difference was not statistically significant in either phase (hold: t(37.65) = -1.07, p = 0.15; washout: t(37.997) = -1.50, p = 0.07).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 3: Pitch as an external signal to adaptation direction</head><p>The results from Experiment 2 indicate that even arbitrary f0 can provide context for sensorimotor adaptation of segments, suggesting that lexical relevance is not a necessary condition for context-dependent adaptation in speech. However, unlike in Experiment 1, the speakers in Experiment 2 also heard a pitch cue prior to the trial, in addition to producing a different pitch. In this experiment, we test whether merely hearing a distinct pitch cue, without subsequent planning and production, provides sufficient context to anchor simultaneous opposing adaptation in English speakers. In studies of upper limb control, there is a large body of evidence showing that arbitrary cues unrelated to motor planning are insufficient for motor learning <ref type="bibr" target="#b7">(Gandolfo et al., 1996;</ref><ref type="bibr" target="#b16">Howard et al., 2010</ref><ref type="bibr" target="#b15">Howard et al., , 2012</ref><ref type="bibr" target="#b17">Howard et al., , 2013))</ref>. Thus, we predict that participants will not be able to adapt their speech according to external pitch cues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Participants 21 native speakers of American English (14 women, 7 men), ranging in age from 18 to 43 years (median: 23, sd: 6.3), participated in Experiment 3. No participant who participated in Experiment 2 participated in Experiment 3. Data from two participants was excluded due to an error in the procedure that led to pitch cues that were not calibrated to their habitual pitch.</p><p>Participants underwent the same screening procedures for neurological, speech, and hearing disorders as Experiments 1 and 2. Participants were compensated for their participation monetarily. All participants gave informed consent. All procedures were approved by the Institutional Review Board at the University of Wisconsin-Madison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task</head><p>Experiment 3 was conducted using the same procedure as Experiment 2, in which a participantspecific high, mid, or low auditory pitch cue preceded the orthographic stimulus "head", but participants were instructed to read the word aloud normally and to not match the pitch they heard (Figure <ref type="figure" target="#fig_1">1</ref>, right). If participants started to anticipate the presentation of the word head and start speaking before the tone was finished, they were reminded to wait until after the tone was done. The experimenter also monitored the participants' productions for influence from the preceding pitch cue, using a visual tracker that displayed values from Matlab's built-in pitch tracking function; no participants demonstrated a tendency to inadvertently match pitch. The perturbation received by each word was counterbalanced across participants to the extent possible (each of the possible six permutations assigned to three or four participants).</p><p>In order to compare participants here to the subgroups in Experiment 2 who could and could not accurately match the target pitch, participants also completed a short section after the main experiment where they were asked to match pitch, with 10 trials for each of the high, mid, and low pitch cues. Matchers vs. non-matchers were determined from this task using the same procedure as Experiment 2; results from this analysis are included in Supplement A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data processing and analysis</head><p>Data was processed using the same procedures as Experiments 1 and 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data analysis</head><p>Data was analyzed using the same procedures as Experiments 1 and 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The results from Experiment 3 show that English speakers cannot learn simultaneous, opposing adaptations of the same segmental content when they only receive an auditory cue (Figure <ref type="figure">4</ref>). During the hold phase, participants overall produced a lower F1 than in baseline for all shifts (no shift: -34.3 ± 7.8 mels; shift down: -37.7 ± 7.8 mels; shift up: -39.6 ± 7.8 mels; all shifts significantly different from baseline, p &lt; 0.0001). There were no significant differences between shifts (all p &gt; 0.86).</p><p>In the washout phase, F1 remained lower than in the baseline phase for all shifts (no shift:</p><p>-38.8 ± 7.8 mels; shift down: -37.3 ± 7.8 mels; shift up: -43.3 ± 7.8 mels; all shifts significantly different from baseline, p &lt; 0.0001). As during the hold phase, there were no significant differences between shifts (all p &gt; 0.92). Thus, although participants overall changed their produced F1, participants did not change to oppose the perturbations tied to the different pitch cues that they heard.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>In this study, we tested the efficacy of f0 as a contextual cue for simultaneous adaptation to opposing vowel formant perturbations. In Experiment 1, we showed that Mandarin speakers can learn simultaneous, opposing adaptations to the same segmental content when the direction of the perturbation is consistently cued by a lexical tone. In Experiment 2, we showed that English speakers who heard and then matched an arbitrary pitch that carried no linguistic meaning were similarly able to adapt to the opposing perturbations tied to the separate pitches. Together, these experiments indicate that f0 in general provides sufficient sensorimotor context for simultaneous, opposing adaptation of vowels, regardless of lexical information. Finally, in Experiment 3, we showed that English speakers that simply listened to a pitch cue before producing a word could not learn simultaneous opposing adaptations, even though this cue perfectly predicted the upcoming perturbation. This result reinforces findings from reaching that cues that are unrelated to the movement cannot serve as context for simultaneous adaptation to opposing sensory perturbation <ref type="bibr" target="#b7">(Gandolfo et al., 1996;</ref><ref type="bibr" target="#b15">Howard et al., 2012;</ref><ref type="bibr" target="#b36">Sheahan et al., 2016)</ref>, expanding this finding to speech articulation.</p><p>Due to the arbitrary nature of the pitch matching, Experiment 2 is particularly informative because it isolates f0 as the context for adaptation. In Experiment 1, there were multiple possible paths to learning simultaneous opposing adaptations. First, the three words used in Experiment 1 are three distinct, yet segmentally identical lexical entries in Mandarin: it is possible that different lexical entries simply have different motor plans, as opposed to one shared plan for segmental articulation <ref type="bibr" target="#b31">(Ran et al., 2023)</ref>. Second, although the segments are phonologically the same, there are consistent phonetic differences between the words (e.g. shorter duration in tone 4 words; <ref type="bibr" target="#b45">Wu et al., 2023)</ref> that could also suggest the segmental parts of these words have distinct motor plans. In either of these scenarios, this experiment would not involve simultaneous adaptation of the "same" movement, since the [ei] in each word would be a distinct unit.</p><p>However, in Experiment 2, participants were saying the same word (head) on every trial, with no differences in lexical status or intrinsic duration, isolating f0 planning as the motor context for sensorimotor adaptation. Furthermore, the success of arbitrary f0 in enabling contextual learning suggests that all speech-related f0 can provide context for segments, specifically including intonation. However, it is unclear if the same pattern would hold for naturally planned, nonlexical intonation contours, which often stretch over many segments or even many words.</p><p>Additional studies that examine intonation as motor context would provide valuable insight on this issue.</p><p>It is possible that the aforementioned additional distinctive characteristics of Mandarin words contributed to the numerically larger adaptation response in Mandarin speakers. Although the difference in adaptation magnitude between Mandarin speakers in Experiment 1 and English speakers in Experiment 2 did not reach statistical significance, there was a fairly substantial numerical difference between Experiment 1 (69 mels during hold, 72 mels during washout) and Experiment 2 (47 mels during hold, 45 mels during washout), as well as a difference in effect size (Mandarin Cohen's d = 1.33, English Cohen's d = 1.01, both during hold). One possible reason for the lack of statistical difference is that our sample sizes were too small to adequately power an analysis of adaptation magnitude between the two groups; this seems like a plausible path given the difference in effect sizes. It may also be the case that differential adaptation in Mandarin was somewhat hindered by overlap between the contextual movements: the three Mandarin tones used in this study are traditionally analyzed as H (Tone 1), LH (Tone 2), and HL (Tone 4). Although the overall contour of each tone is quite distinct from the others, they all contain some movement towards high tone. This contrasts with English pitch matching, which were essentially three level tones at H, M, and L, and thus did not have phonetic or phonological overlap. Further studies with larger sample sizes or that examine the effects of contextual overlap may shed light on this issue. Finally, although these experiments have demonstrated that neither linguistic information nor movement of the same anatomical structure is required for contextual differentiation of speech sounds, it is still unclear exactly what permits differential adaptation. Previous studies in reaching have shown that simultaneous adaptation is also possible when the contextual movement is merely planned alongside the target movement-ultimately even without actual execution of the contextual movement <ref type="bibr" target="#b16">(Howard et al., 2010</ref><ref type="bibr" target="#b15">(Howard et al., , 2012;;</ref><ref type="bibr" target="#b36">Sheahan et al., 2016)</ref>. This indicates that planning alone can generate distinct sensorimotor neural states. As a result, <ref type="bibr" target="#b36">Sheahan et al. (2016)</ref> proposed that differential adaptation is licensed not by contextual movement per se, but by the generation of separate sensorimotor neural states at the moment of executing the target movement. Under this framing, one could conclude that f0 is planned either before or simultaneously with segments. This runs contrary to models that posit that tone planning occurs after segmental planning (J.-Y. <ref type="bibr" target="#b4">Chen, 1999;</ref><ref type="bibr" target="#b33">Roelofs, 1997</ref><ref type="bibr" target="#b34">Roelofs, , 2015))</ref>, and lends some support to models that posit that segments and f0 are planned in parallel streams <ref type="bibr" target="#b0">(Alderete et al., 2019;</ref><ref type="bibr" target="#b11">Hickok et al., 2023;</ref><ref type="bibr" target="#b41">Wan &amp; Jaeger, 1998;</ref><ref type="bibr" target="#b42">Weerathunge et al., 2022;</ref><ref type="bibr" target="#b46">Zeng, 2022)</ref>.</p><p>However, it could be the case that mere simultaneous execution-with no need for simultaneous planning whatsoever-also generates different sensorimotor states. That is, while the differential adaptation of segments with distinct f0 could reflect the relative time course of segmental vs. f0 planning, it could also simply reflect the fact that f0 and segments are executed at the same time. A key question, then, is how task-relevant the contextual movement has to be in order to affect the relevant sensorimotor state. More concretely: is context-specific adaptation only possible when the contextual movement is sufficiently relevant to the target movement, or is the sensorimotor state of the entire body taken into consideration? The current study expands on previous work that has shown that the contextual movement does not need to use the same effector as the target movement: Reaching studies have found that the movement of one hand can provide context for the movement of the other hand, both when the hands are moving at the same time <ref type="bibr" target="#b16">(Howard et al., 2010)</ref> and when the contextual hand precedes the target hand <ref type="bibr" target="#b9">(Gippert et al., 2023)</ref>. However, humans frequently use both hands in concert for a single task, which may promote any manual task as relevant for another manual task. Similarly, although arbitrary f0 is not relevant to language in the same way that lexical tone is, speakers constantly plan and control f0 alongside segments when they are speaking. Thus, f0 control in general may be more likely to be regarded as relevant for articulatory control, and thus be included in the sensorimotor context for speech segments. A recent study found that adaptation in upper limb control can be modulated by different speech contexts <ref type="bibr" target="#b22">(Lametti et al., 2023)</ref>, which may suggest that a similar relationship exists between speech and manual control due to the frequency of co-speech gesture.</p><p>Studies that test pairings of contextual and target movements that vary on a spectrum of relevance, e.g. pairing speech with other speech movements, hand movements, and foot tapping, could shed light on this question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In sum, this series of three studies provides evidence that f0 movements can be used as motor context for the movements for segments, even though they are not produced by the same set of articulators. This is the case for both lexical tone and for arbitrary pitch matching, suggesting that linguistic content is not necessary for f0 to inform segmental control. Future work examining other uses of f0, such as intonation, would provide additional insight on the generality of f0 as motor context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix: Supplemental analyses (Experiments 1 and 2)</head><p>Previous work has shown that the supraglottal articulation of both monophthongs and diphthongs is influenced by the tone in Mandarin <ref type="bibr" target="#b6">(Erickson et al., 2004;</ref><ref type="bibr" target="#b12">Hoole &amp; Hu, 2004;</ref><ref type="bibr" target="#b24">Li et al., 2023)</ref>. Specifically, high tones are associated with higher (lower F1) and more front (higher F2) vowels, while lower tones are associated with lower (higher F1) and more back (lower F2) vowels. This parallels the relationship described by intrinsic f0, where vowels articulated higher in the mouth tend to be produced with a higher f0 and vice versa (W.-R. <ref type="bibr" target="#b5">Chen et al., 2021;</ref><ref type="bibr" target="#b35">Shadle, 1985;</ref><ref type="bibr">D. Whalen et al., 1995;</ref><ref type="bibr" target="#b43">D. H. Whalen &amp; Levitt, 1995)</ref>. These relationships may affect both the baseline articulation and formant adaptation of the target vowels in experiments 1 and 2, which both used different f0 targets. We thus further separated the analysis by target tone, examining both baseline F1 productions for each tone, as well as how each tone responded to different F1 shifts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Experiment 1: Mandarin</head><p>Results are illustrated in Figure <ref type="figure" target="#fig_1">A1</ref>. Production during the baseline phase will only be affected by intrinsic f0-vowel height relationships. Here we examine the same vowel window as in the main analysis (25-100 ms). Based on the results of previous studies on the relationship between f0 and F1 in Mandarin, we would expect that Tone 1 (high level tone) and Tone 4 (falling tone)</p><p>would have the lowest F1 during this interval, as they have they highest f0, while Tone 2 (midrising tone) would have the highest F1, as it has the lowest f0. During the baseline phase, there is a significant effect of lexical tone on F1 during baseline trials (p &lt; 0.0001). Contrary to prediction, F1 in tone 4 (falling; 696 ± 23.8 mels) is significantly higher than F1 in either tone 1 (high level; 664 ± 23.8 mels) or tone 2 (rising; 660 ± 23.8 mels).</p><p>The pairing of lexical tone and shift direction was counterbalanced (to the extent possible) in this study; as such, there would be no overall interference effect of adaptation direction and intrinsic F1. However, we can still examine the magnitude of change in F1 as a function of both shift direction and tone. Production during the hold and washout phases, on the other hand, will also include effects of learning. It may be the case that shifting F1 will have different effects on vowel adaptation, depending on if the shift counters the relationship between F1 and tone, or enhances it.</p><p>There is a significant three-way interaction between phase, shift direction, and lexical tone identity (p &lt; 0.0001). Specifically, speakers did not adapt F1 in Tone 1 words at all, but showed significant adaptation in both Tone 2 and Tone 4 words. In Tone 1, there is no statistically significant difference between baseline and hold in either the shift up or shift down conditions (both p &gt; 0.83). For Tone 2, there is a significant difference between baseline and hold for both shift up and shift down (both p &lt; 0.0001); both changes oppose the perturbation.</p><p>Tone 4 patterns similarly, showing significant difference between baseline and hold for both shift up and shift down (both p &lt; 0.001). Tones 2 and 4 are also significantly different from Tone 1 during hold for both shift up (both p &lt; 0.0001); only Tone 2 is significantly different from Tone 1 for shift down (p &lt; 0.0001). This pattern counters any prediction of adaptation being influenced by enhancing or reducing intrinsic f0-F1 relationships.</p><p>Thus, in Experiment 1, the overall adaptation response to the upward and downward shifts were driven by Tones 2 and 4, while T1 remained stable throughout. This suggests that T1 has some resistance to change, but not one that can be explained by formant shifts enhancing or reducing an intrinsic f0-F1 relationship. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2</head><p>Experiment 2: English (Match)</p><p>Results from this analysis are illustrated in Figure <ref type="figure" target="#fig_2">A2</ref>. There is a significant effect of pitch cue on F1 during the baseline phase (p &lt; 0.0001). Contrary to what would be predicted by intrinsic f0-F1 relationships, head with the high pitch cue has the highest F1 (740 ± 32.4 Hz), followed by the mid pitch cue (694 ± 32.5 Hz), and the low pitch cue with the lowest F1 (667 ± 32.4 Hz). All pitch cues are significantly different from each other (p ≤ 0.0001).</p><p>The three-way interaction between shift direction, phase, and pitch cue significantly improves the fit of the model for change in F1. Specifically, the mid pitch cue did not show adaptation in any shift condition, but both the high and low pitch cues do. During the hold phase of the downward shift condition, change in F1 for the high pitch cue is numerically greater (52.9</p><p>± 13.1 mels) than the low pitch cue (22.5 ± 14.0 mels). However, this difference is not statistically significant (p = 0.29). In addition, the high pitch cue shows significantly more change than the mid pitch cue (p = 0.0002), but the low pitch cue does not (p = 0.49).</p><p>In contrast, during the hold phase of the upward shift condition, change in F1 for the low pitch cue is numerically lower (-37.8 ± 13.6 mels) than the high pitch cue (-14.4 ± 14.0). Only the low pitch cue shows significant difference between hold and baseline (p = 0.004, compared to p = 0.99 for both mid and high pitch cues); however, there are no significant differences between any of the pitch cues (all p &gt; 0.38).</p><p>Together, these results indicate that the pitch cues did not show uniform adaptation in English, and may suggest some interaction of intrinsic f0-F1 relationships and shift direction.</p><p>That only the low pitch cue seems to have triggered adaptation in the upward shift condition provides insight into the overall result for this study, which was that there was no statistically significant adaptation in the upward shift condition. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Discussion</head><p>Interestingly, adaptation did not appear to be uniform across different lexical tone or arbitrary pitch cue. In Mandarin, ei did not show adaptation to either upward or downward shifts when produced with a high level tone (T1), but did adapt, regardless of shift direction, when produced with a rising tone (T2) and falling tone (T4). Similarly, head produced with the mid pitch cue in Experiment 2 did not show adaptation to either upward or downward shifts, but both high and low pitch cues did. One possible explanation for the mid tone results in Experiment 2 is that the mid pitch cue had the most variable reproduction. As previously noted, differential adaptation depends on the use of distinct motor plans; if some participants did not develop a distinct motor plan for the mid pitch cue, then we would expect that this pitch in particular would have highly impaired adaptation. Any observed differences between low and high pitch cues could then be related to intrinsic f0-F1 relationships. This explanation does not work for the Mandarin results, however, as participants reliably produced all target tones. These results also come with a major caveat, which is that it is possible that some of the patterns would change with additional participants. Due to the counterbalancing of tone/pitch cue and shift direction, there are either six or seven participants per unique tone-shift combination; this is likely insufficient to detect any true effect of intrinsic f0 or lexical tone on formant adaptation. Future studies are needed to examine the effects of f0 on the adaptation of other aspects of production.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc>. Participants were compensated for their participation either monetarily or through extra credit in a course in the University of Wisconsin-Madison Communication Sciences and Disorders Department. All participants gave informed consent. All procedures were approved by the Institutional Review Board at the University of Wisconsin-Madison. Task There were three target words in this study, differing only in lexical tone:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Schematic of all three experiments. Gray shaded areas indicate windows of analysis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Difference from baseline F1 for each shift condition (Experiment 1).</figDesc><graphic coords="8,72.00,278.98,468.00,260.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Change from baseline F1 for each shift condition (Experiment 2).</figDesc><graphic coords="12,72.00,258.28,468.00,260.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Change from baseline F1 for each shift condition (Experiment 3, listening only).</figDesc><graphic coords="17,72.00,237.59,468.00,260.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Difference between shift down and shift up for hold and washout phases, all three experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure</head><figDesc>Figure A1. A: Baseline F1 of each lexical tone. B: changes in F1 compared to baseline, separated by lexical tone.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure</head><figDesc>Figure A2. A: Mean F1 of the vowel during baseline, separated by match pitch. B: changes in F1 from baseline, separated by match pitch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="6,72.00,72.00,479.48,276.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="14,72.00,72.00,468.00,232.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="18,72.00,72.00,352.47,264.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="24,72.00,72.00,303.08,259.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="24,72.00,338.75,468.00,260.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="26,72.00,72.00,291.80,208.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="26,72.00,287.00,414.00,230.10" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>Funding information This work was supported by grants to <rs type="person">Caroline A. Niziolek</rs> and <rs type="person">Benjamin Parrell</rs> from the <rs type="funder">National Science Foundation</rs> (<rs type="grantNumber">BCS 2120506</rs>) and <rs type="funder">National Institute on Deafness and Other Communication Disorders</rs> (<rs type="grantNumber">R01 DC019134</rs>) as well as a core grant to the <rs type="funder">Waisman Center</rs> from the <rs type="funder">National Institute of Child Health and Human Development</rs> (<rs type="grantNumber">P50 HD105353</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_EAmKrMw">
					<idno type="grant-number">BCS 2120506</idno>
				</org>
				<org type="funding" xml:id="_9vFvKpc">
					<idno type="grant-number">R01 DC019134</idno>
				</org>
				<org type="funding" xml:id="_2JZKb6c">
					<idno type="grant-number">P50 HD105353</idno>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data availability statement</head><p>Data, experimental scripts, and analysis scripts are available on OSF (<ref type="url" target="https://osf.io/v7zaf/">https://osf.io/v7zaf/</ref>).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Author contribution statement</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tone slips in Cantonese: Evidence for early phonological encoding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Alderete</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">191</biblScope>
			<biblScope unit="page">103952</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">lme4: Linear mixed-effects models using Eigen and S4</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maechler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bolker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">R Package Version</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Praat: Doing phonetics by computer (Version 6.0.26</title>
		<author>
			<persName><forename type="first">P</forename><surname>Boersma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weenink</surname></persName>
		</author>
		<ptr target="http://www.fon.hum.uva.nl/praat/" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Computer software</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A system for online dynamic perturbation of formant trajectories and results from perturbations of the Mandarin triphthong /iau</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Boucek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">H</forename><surname>Guenther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Perkell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th ISSP</title>
		<meeting>the 8th ISSP</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="65" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The representation and processing of tone in Mandarin Chinese: Evidence from slips of the tongue</title>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Psycholinguistics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="289" to="301" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A dual mechanism for intrinsic f0</title>
		<author>
			<persName><forename type="first">W.-R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Whalen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Tiede</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Phonetics</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page">101063</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Effect of tone height on jaw and tongue articulation in Mandarin Chinese</title>
		<author>
			<persName><forename type="first">D</forename><surname>Erickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Iwata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Endo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fujino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Tonal Aspects of Languages: With Emphasis on Tone Languages</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Motor learning by field approximation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Gandolfo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Mussa-Ivaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bizzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3843" to="3846" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generalization to local remappings of the visuomotor coordinate transformation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Wolpert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page" from="7085" to="7096" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Prior movement of one arm facilitates motor adaptation in the other</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gippert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Leupold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Heed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Villringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">V</forename><surname>Nikulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sehm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="4341" to="4351" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A single exposure to altered auditory feedback causes observable sensorimotor adaptation in speech</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hantzsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Parrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Niziolek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Elife</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">73694</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Beyond Broca: Neural architecture and evolution of a dual motor speech coordination system</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hickok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Venezia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Teghipco</surname></persName>
		</author>
		<idno type="DOI">10.1093/brain/awac454</idno>
		<ptr target="https://doi.org/10.1093/brain/awac454" />
	</analytic>
	<monogr>
		<title level="j">Brain</title>
		<imprint>
			<biblScope unit="volume">146</biblScope>
			<biblScope unit="page" from="1775" to="1790" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Tone-vowel interaction in standard Chinese</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hoole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Tonal Aspects of Languages: With Emphasis on Tone Languages</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sensorimotor adaptation of speech I</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Houde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Speech, Language, and Hearing Research</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sensorimotor adaptation of speech I: Compensation and adaptation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Houde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Speech, Language, and Hearing Research</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="295" to="310" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gone in 0.6 seconds: The encoding of motor memories depends on recent sensorimotor states</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Ingram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Wolpert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">37</biblScope>
			<biblScope unit="page" from="12756" to="12768" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Context-dependent partitioning of motor learning in bimanual movements</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Ingram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Wolpert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neurophysiology</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2082" to="2091" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The effect of contextual cues on the encoding of motor memories</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Wolpert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Franklin</surname></persName>
		</author>
		<idno>ISO 226: 2003</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Neurophysiology</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2632" to="2644" />
			<date type="published" when="2003">2013. 2003</date>
			<publisher>Acoustics-Normal Equal-Loudness-Level Contours</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Perceptual calibration of F0 production: Evidence from feedback perturbation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Munhall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1246" to="1251" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Widespread auditory deficits in tune deafness</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Brewer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Drayna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ear and Hearing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">63</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Independent learning of internal models for kinematic and dynamic control of reaching</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Krakauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-F</forename><surname>Ghilardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ghez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1026" to="1031" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Package &apos;lmerTest</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Brockhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H B</forename><surname>Christensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">R Package Version</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">0</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Memories of hand movements are tied to speech through learning</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Lametti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Vaillancourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Whitman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">I</forename><surname>Skipper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Interactions between auditory and somatosensory feedback for voice F 0 control</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Hain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Experimental Brain Research</title>
		<imprint>
			<biblScope unit="volume">187</biblScope>
			<biblScope unit="page" from="613" to="621" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">TONE AS A FACTOR INFLUENCING THE DYNAMICS OF DIPHTHONG REALIZATIONS IN STANDARD MANDARIN</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lenth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Al-Tamimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<ptr target="https://CRAN.R-project" />
	</analytic>
	<monogr>
		<title level="m">20th International Congress of Phonetic Sciences (ICPhS)</title>
		<imprint>
			<date type="published" when="2019">2019. 2023</date>
		</imprint>
	</monogr>
	<note>emmeans: Estimated Marginal Means, aka Least-Squares Means</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Talkers alter vowel production in response to real-time formant perturbation even when instructed not to compensate</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Munhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">N</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Byrne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Johnsrude</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="384" to="390" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">wave_viewer: First release (Version 1.1)</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Niziolek</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.593003</idno>
		<ptr target="https://doi.org/10.5281/zenodo.593003" />
	</analytic>
	<monogr>
		<title level="j">Computer software</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Wave_Viewer: First Release</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Niziolek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Houde</surname></persName>
		</author>
		<idno type="DOI">10.5281/ZENODO.13839</idno>
		<ptr target="https://doi.org/10.5281/ZENODO.13839" />
	</analytic>
	<monogr>
		<title level="j">Computer software</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bridging phonological system and lexicon: Insights from a corpus study of functional load</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Coupé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Marsico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pellegrino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Phonetics</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="153" to="176" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The Edinburgh lifetime musical experience questionnaire (ELMEQ): Responses and non-musical correlates in the lothian birth cohort 1936</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Okely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Deary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Overy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Plos One</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">254176</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">R: A Language and Environment for Statistical Computing</title>
		<author>
			<persName><forename type="first">Team</forename><surname>Core</surname></persName>
		</author>
		<ptr target="https://www.R-project.org/" />
	</analytic>
	<monogr>
		<title level="m">R Foundation for Statistical Computing</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Phonetic differences between nouns and verbs in their typical syntactic positions in a tonal language: Evidence from disyllabic noun-verb ambiguous words in Standard Mandarin Chinese</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Ran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wichmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Phonetics</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page">101241</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Simultaneous acquisition of multiple auditory-motor transformations in speech</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rochet-Capellan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Ostry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2657" to="2662" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The WEAVER model of word-form encoding in speech production</title>
		<author>
			<persName><forename type="first">A</forename><surname>Roelofs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="249" to="284" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Modeling of phonological encoding in spoken word production: From G ermanic languages to M andarin C hinese and J apanese</title>
		<author>
			<persName><forename type="first">A</forename><surname>Roelofs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Japanese Psychological Research</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="37" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Intrinsic fundamental frequency of vowels in sentence context</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Shadle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1562" to="1567" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Motor planning, not execution, separates motor memories</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Wolpert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="773" to="779" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Visual-shift adaptation is composed of separable sensory and task-dependent effects</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Simani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Mcguire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Sabes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neurophysiology</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2827" to="2841" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Comparison of loudness functions suitable for drawing equal-loudness-level contours</title>
		<author>
			<persName><forename type="first">H</forename><surname>Takeshima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ozawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kumagai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acoustical Science and Technology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="61" to="68" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Neural mechanisms underlying auditory feedback control of speech</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Tourville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Reilly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">H</forename><surname>Guenther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1429" to="1443" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Sensorimotor adaptation to feedback perturbations of vowel acoustics and its relation to perception</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Villacorta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Perkell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">H</forename><surname>Guenther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2306" to="2319" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Speech errors and the representation of tone in Mandarin Chinese</title>
		<author>
			<persName><forename type="first">I.-P</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jaeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phonology</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="417" to="461" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Auditory and somatosensory feedback mechanisms of laryngeal and articulatory speech motor control</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Weerathunge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Voon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tardif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cilento</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Stepp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Experimental Brain Research</title>
		<imprint>
			<biblScope unit="volume">240</biblScope>
			<biblScope unit="issue">7-8</biblScope>
			<biblScope unit="page" from="2155" to="2173" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The universality of intrinsic F0 of vowels</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Whalen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Levitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Phonetics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="349" to="366" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Intrinsic F 0 of vowels in the babbling of 6-, 9-, and 12-month-old French-and English-learning infants</title>
		<author>
			<persName><forename type="first">D</forename><surname>Whalen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Levitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-L</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Smorodinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2533" to="2539" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Mandarin lexical tone duration: Impact of speech style, word length, syllable position and prosodic position</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Adda-Decker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lamel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">146</biblScope>
			<biblScope unit="page" from="45" to="52" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Spoken Word Production of Mandarin Monosyllabic Words: From Lexical Selection to Form Encoding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
		<respStmt>
			<orgName>University of Kansas</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD Thesis</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Simultaneous acquisition of multiple auditorymotor transformations reveals supra-syllabic motor planning in speech production</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Niziolek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Parrell</surname></persName>
		</author>
		<idno type="DOI">10.31234/osf.io/ceqan</idno>
		<ptr target="https://doi.org/10.31234/osf.io/ceqan" />
	</analytic>
	<monogr>
		<title level="j">OSF</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
