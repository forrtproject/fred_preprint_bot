<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Teachers’ Vocal Expressions and Student Engagement in Asynchronous Video Learning</title>
				<funder ref="#_4CnCpVX">
					<orgName type="full">National Science and Technology Council, Taiwan</orgName>
				</funder>
				<funder>
					<orgName type="full">Featured Areas Research Center</orgName>
				</funder>
				<funder ref="#_qyuX5Z9">
					<orgName type="full">Advanced Institute of Manufacturing with High-tech Innovations</orgName>
					<orgName type="abbreviated">HI</orgName>
				</funder>
				<funder ref="#_uyvmVnd">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Informa UK Limited</publisher>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-03-11">2025-03-11</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hung-Yue</forename><surname>Suen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Technology Application and Human Resource Development</orgName>
								<orgName type="institution">National Taiwan Normal University</orgName>
								<address>
									<settlement>Taipei City</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu-Sheng</forename><surname>Su</surname></persName>
						</author>
						<title level="a" type="main">Teachers’ Vocal Expressions and Student Engagement in Asynchronous Video Learning</title>
					</analytic>
					<monogr>
						<title level="j" type="main">International Journal of Human–Computer Interaction</title>
						<title level="j" type="abbrev">International Journal of Human–Computer Interaction</title>
						<idno type="ISSN">1044-7318</idno>
						<idno type="eISSN">1532-7590</idno>
						<imprint>
							<publisher>Informa UK Limited</publisher>
							<biblScope unit="page" from="1" to="12"/>
							<date type="published" when="2025-03-11" />
						</imprint>
					</monogr>
					<idno type="MD5">C8200678E50B14C79DBA6FF6002C712D</idno>
					<idno type="DOI">10.1080/10447318.2025.2474469</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-22T20:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Acoustic Analysis</term>
					<term>Natural Language Processing (NLP)</term>
					<term>Machine Learning</term>
					<term>Pedagogy</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Asynchronous video learning, including massive open online courses (MOOCs), offers flexibility but often lacks students' affective engagement. This study examines how teachers' verbal and nonverbal vocal emotive expressions influence students' self-reported affective engagement. Using computational acoustic and sentiment analysis, valence and arousal scores were extracted from teachers' verbal vocal expressions, and nonverbal vocal emotions were classified into six categories: anger, fear, happiness, neutral, sadness, and surprise. Data from 210 video lectures across four MOOC platforms and feedback from 738 students collected after class were analyzed. Results revealed that teachers' verbal emotive expressions, even with positive valence and high arousal, did not significantly impact engagement. Conversely, vocal expressions with positive valence and high arousal (e.g., happiness, surprise) enhanced engagement, while negative high-arousal emotions (e.g., anger) reduced it. These findings offer practical insights for instructional video creators, teachers, and influencers to foster emotional engagement in asynchronous video learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Asynchronous video learning, a method of learning content via prerecorded videos, has emerged as a cornerstone of modern education, particularly in the context of distance learning and massive open online courses <ref type="bibr">(MOOCs)</ref>. While this mode of learning offers unparalleled flexibility and accessibility, it presents significant challenges in fostering student engagement, particularly affective engagement, due to the lack of real-time interaction between teachers and students. This study aims to explore how teachers' verbal and nonverbal vocal emotive expressions can establish a social presence and, in turn, enhance students' affective engagement in MOOCs.</p><p>Asynchronous video learning empowers students with flexibility and control over their learning journey, driving its widespread adoption in corporate training, adult education, and higher education <ref type="bibr" target="#b97">(Zeng &amp; Luo, 2023)</ref>. Reflecting this paradigm shift, the MOOC market is projected to grow from USD 14.75 billion in 2022 to USD 149.42 billion by 2029 <ref type="bibr">(Maximize Market Research, 2023)</ref>. Despite its advantages, this learning mode lacks interactivity between teachers and students, posing challenges in educational practice and research <ref type="bibr" target="#b12">(Alemayehu &amp; Chen, 2023;</ref><ref type="bibr" target="#b31">Garcia &amp; Yousef, 2023)</ref>. Teachers in asynchronous settings struggle to monitor and respond to students' reactions in real time, making it difficult to adjust communication strategies accordingly <ref type="bibr" target="#b58">(Mershad &amp; Said, 2022)</ref>. This limitation highlights the pressing need for research on strategies to effectively engage students in asynchronous video learning, particularly through prerecorded video lectures <ref type="bibr" target="#b54">(Ma et al., 2023)</ref>.</p><p>Student engagement, a critical element in asynchronous video learning, is closely linked to persistent learning interest and learning outcomes <ref type="bibr" target="#b51">(Lin et al., 2019;</ref><ref type="bibr" target="#b99">Zhao &amp; Khan, 2022)</ref>. Among the three dimensions of learning engagement delineated by <ref type="bibr" target="#b88">Walker and Koralesky (2021)</ref>affective, behavioral, and cognitive-affective engagement poses unique challenges in words spoken by teachers, while nonverbal vocal expressions include tone, pitch, and intonation.</p><p>Both help convey emotions and engage students in live and asynchronous learning <ref type="bibr" target="#b91">(Wang et al., 2022)</ref>.</p><p>Past studies indicate that teachers' emotional expressions significantly influence student affective engagement <ref type="bibr" target="#b25">(Dixson, 2010;</ref><ref type="bibr" target="#b90">Wang, 2022)</ref>. This finding is supported by the emotional response theory <ref type="bibr" target="#b60">(Mottet et al., 2006)</ref>, which posits that teachers' emotional expressions, observable in their demeanor, are detectable by students and thus trigger corresponding emotional responses among students <ref type="bibr" target="#b52">(Liu et al., 2019)</ref>. This observation resonates with the concept of emotion contagion, which posits that emotions can spread automatically between people, prompting similar reactions <ref type="bibr" target="#b33">(Hatfield et al., 1993)</ref>. These reactions are characterized by two key dimensions: "valence" (the pleasantness or unpleasantness of the feeling) and "arousal" (ranging from alertness and responsiveness to a state of calmness) <ref type="bibr" target="#b60">(Mottet et al., 2006)</ref>.</p><p>Research has shown that speakers' verbal and nonverbal vocal expressions with positive and greater arousal emotions induce the greatest emotion contagion effect among all vocal expressions <ref type="bibr" target="#b67">(Russell, 2003)</ref>. When applied to education, teachers' expressions often predict specific student behaviors more accurately than students' emotional responses, thus influencing attention, memory, motivation <ref type="bibr" target="#b29">(Fanselow, 2018)</ref>, satisfaction, and performance <ref type="bibr" target="#b41">(Iskrenovic-Momcilovic, 2018)</ref>.</p><p>A critical question remains unanswered: Do the insights into teacher-student interactions extend to the rapidly growing field of asynchronous video learning? <ref type="bibr">Perry and colleagues (2024)</ref> highlight this issue, calling for more research into how instructors are presented in videos and how this affects student reactions and learning-an area that remains largely unexplored. This gap underscores the urgent need to investigate human-computer interaction in educational contexts further. Recent studies have echoed this call, emphasizing the importance of addressing these unanswered questions <ref type="bibr" target="#b21">(Chiu, 2022;</ref><ref type="bibr" target="#b97">Zeng &amp; Luo, 2023)</ref>. Responding to this need, the current study explores the dynamics of teachers' verbal and nonverbal vocal emotive expressions in prerecorded video instruction, aiming to bridge this gap in the literature.</p><p>As shown in Table <ref type="table" target="#tab_0">1</ref>, it explored verbal expressions characterized by valence (positive or negative tone) and arousal (energy level) and six nonverbal vocal expressions: happiness and surprise (positive, high activation), anger (negative, high activation), fear and sadness (negative, moderate activation), and neutral (neutral, moderate activation). Affective engagement, the dependent variable, was assessed through students' self-reported emotional connection to and involvement with the instructional video. Suen, H. Y., &amp; Su, Y. S. (2025). Teachers' Vocal Expressions and Student Engagement in Asynchronous Video Learning. International Journal of Human-Computer Interaction, 1-12. <ref type="url" target="https://doi.org/10.1080/10447318.2025.2474469">https://doi.org/10.1080/10447318.2025.2474469</ref> </p><p>MOOCs. The findings are expected to provide actionable insights for designing asynchronous video-based instruction and inform best practices in this rapidly evolving domain of humancomputer interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">LITERATURE REVIEW</head><p>To understand how verbal and vocal emotive expressions can establish a social presence and, in turn, enhance learners' affective engagement, we build on the abovementioned four research questions and extensively review the relevant literature, focusing on the role of teachers' vocal emotive expressions in asynchronous video learning environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Teachers' verbal vocal emotive expression and students' affective engagement</head><p>Five experiments conducted by <ref type="bibr" target="#b45">Kraus (2017)</ref> demonstrated that vocal cues provide a greater degree of accuracy in perceiving others' emotions compared to facial expressions, attributing this finding to cognitive limitations. Furthermore, individuals can discern emotions from both verbal content (what is said) and nonverbal vocal cues (how it is said), independent of facial expressions <ref type="bibr" target="#b45">(Kraus, 2017;</ref><ref type="bibr" target="#b76">Simon-Thomas et al., 2009)</ref>.</p><p>Speech is the most rapid and natural medium through which to convey verbal content and nonverbal cues in human communication <ref type="bibr" target="#b26">(El Ayadi et al., 2011)</ref>. Spoken words in speech convey not only the cognitive messages and behavioral intentions of the speaker but also various emotional states, inducing listeners' emotional valence and arousal <ref type="bibr" target="#b16">(Beukeboom &amp; Semin, 2006;</ref><ref type="bibr" target="#b50">Liebenthal et al., 2016)</ref>.</p><p>Affective events theory suggests that emotions play a central role in individuals' reactions to experiences, with verbal expressions acting as significant affective events that influence students' emotional responses and, in turn, their engagement levels <ref type="bibr" target="#b92">(Weiss &amp; Cropanzano, 1996)</ref>. Moreover, linguistic theory highlights the influence of language in shaping emotional responses. Accordingly, teachers' choice of words can greatly impact students' perceptions and emotional engagement, thus creating a learning environment that is either supportive or detrimental (Immordino-Yang &amp; Damasio, 2007) <ref type="bibr" target="#b68">Russell's (1980)</ref> circumplex model of affect offers a useful framework for understanding emotional perception, categorizing it along two key dimensions: valence and arousal. Valence distinguishes emotions based on their pleasantness (or unpleasantness), while arousal captures the spectrum from calm to excited states. <ref type="bibr" target="#b67">Russell (2003)</ref> states this two-dimensional model has shown remarkable adaptability across various cultural settings, highlighting its broad applicability. It indicates that vocal emotions characterized by positive valence and high arousal often leave a stronger emotional impact.</p><p>This idea finds support in communication research. Listeners demonstrate an attentional reflex, known as an orienting response, toward positive words, a reaction that is typically diminished or absent for neutral or negative words <ref type="bibr" target="#b48">(Lee &amp; Potter, 2020)</ref>. Furthermore, positive words are not only more likely to capture attention but are also encoded more effectively. <ref type="bibr" target="#b48">Lee and Potter (2020)</ref> observed that positive words are processed more efficiently than neutral words, with negative words being the least effectively encoded.</p><p>This phenomenon is not limited to English-language contexts <ref type="bibr" target="#b77">(Stevenson et al., 2007)</ref>. Ho et al.</p><p>(2015) examined 160 Chinese words and observed that these words elicited different levels of emotional valence among adolescents in Hong Kong and Mainland China, illustrating the subtle cultural differences in emotional expression. In Taiwan, <ref type="bibr" target="#b47">Lee et al. (2022)</ref> introduced the Chinese EmoBank, a significant tool for researchers studying emotional language. Their work involved a comprehensive analysis combining semantic insights with affective computing, evaluating a vast collection of Chinese linguistic units, each rated for valence and arousal.</p><p>According to <ref type="bibr" target="#b19">Cavanagh et al. (2014)</ref>, students' assessments of preservice teachers' presentations were significantly affected by the teachers' word choices in recorded videos. Based on these findings, we propose that teachers' spoken words characterized by more positive valence and higher arousal in asynchronous video learning can enhance students' affective engagement. Therefore, we suggest the following hypothesis:</p><p>H1: The frequency of teachers' verbal vocal expressions with positive valence and high arousal positively influences students' affective engagement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Teachers' nonverbal vocal emotive expression and student affective engagement</head><p>Speakers convey a complex array of emotions that extends beyond the literal meaning of their words. This emotional content is expressed through various nonverbal vocal cues, including changes in tone, volume, speech pace, and timbre <ref type="bibr" target="#b43">(Juslin &amp; Laukka, 2003;</ref><ref type="bibr" target="#b46">Laukka, 2017)</ref>. These nuanced vocal variations elicit distinct emotional responses in listeners, encompassing a spectrum of emotions such as happiness, surprise, anger, fear, sadness, and neutrality. Specifically, happiness and surprise are associated with positive emotional states and heightened physiological arousal, whereas anger corresponds to negative emotions but is characterized by a similarly elevated level of arousal <ref type="bibr" target="#b32">(Gunes et al., 2011)</ref>.</p><p>Several theoretical perspectives help explain how a teacher's nonverbal vocal cues shape student affective engagement in educational contexts. For instance, paralanguage theory highlights the powerful role of nonverbal speech characteristics, such as tone, pitch, and rhythm, in shaping listener perceptions and emotions <ref type="bibr" target="#b86">(Trager, 1958)</ref>. This perspective suggests that subtle vocal variations by a teacher can foster a more dynamic and engaging learning atmosphere. Similarly, self-determination theory posits that an emotionally warm and empathetic vocal tone can strengthen the connection between teachers and students, thereby enhancing students' engagement and motivation to learn <ref type="bibr" target="#b69">(Ryan &amp; Deci, 2000)</ref>.</p><p>Furthermore, the theory of voice and emotion posits that nonverbal vocal cues convey emotional states, affecting listeners' feelings <ref type="bibr" target="#b43">(Juslin &amp; Laukka, 2003)</ref>. Thus, when emotionally attuned, a teacher's nonverbal vocal cues can create a conducive emotional environment for learning, promoting student engagement. <ref type="bibr" target="#b20">Chew (2022)</ref>, in a qualitative analysis, highlighted the importance of teachers' nonverbal vocal expressions as essential pedagogical tools for engaging students in real-time online classes.</p><p>Moreover, <ref type="bibr" target="#b96">Yuan et al. (2021)</ref> provided empirical evidence that these expressions can benefit students' social presence and enjoyment and alleviate boredom in asynchronous learning contexts. <ref type="bibr" target="#b67">Russell (2003)</ref> observed that the interplay of valence and arousal in nonverbal vocal expressions significantly influences their role in emotional contagion. As Tursunov et al. ( <ref type="formula">2019</ref>) explain, vocal emotions can be positioned along a valence spectrum, ranging from positive emotions like happiness and surprise to negative ones like fear, sadness, and anger, with neutrality at the midpoint.</p><p>These emotions also differ in their level of arousal, with happiness, anger, and surprise representing the high end of the spectrum, followed by fear, neutral, and then sadness at the low end. It is worth noting that happiness, anger, and surprise exhibit the most distinct valence and the highest levels of arousal.</p><p>For example, vocal expressions of happiness and surprise, with their positive valence and increased arousal, correlate with heightened affective engagement. In contrast, vocal expressions of anger, marked by negative valence and high arousal, can lead to decreased affective engagement.</p><p>Therefore, asynchronous video learning materials featuring teachers expressing positive valence with greater arousal through nonverbal cues can enhance emotional connectivity. Conversely, the perception of negative valence with higher arousal in a teacher's tone can hinder such engagement.</p><p>On the basis of these insights, the subsequent hypotheses are suggested to guide future investigations in asynchronous video learning:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H2: The frequency of teachers' nonverbal vocal expressions with positive valence and high arousal</head><p>positively influences students' affective engagement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H3: The frequency of teachers' nonverbal vocal expressions with negative valence and high arousal</head><p>negatively influences students' affective engagement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Effects of Teachers' verbal vs. nonverbal vocal expressions on student affective engagement</head><p>Dual coding theory <ref type="bibr" target="#b64">(Paivio, 1971)</ref> suggests that emotional expressions are processed via the following two distinct pathways: a cognitive route for verbal content and an automatic, affective route for nonverbal cues. This theory posits that nonverbal vocal cues are processed more directly than verbal cues, with the former thus having an immediate impact on affective engagement.</p><p>Nonverbal vocal cues, which require less cognitive interpretation, convey emotions more efficiently and intuitively than do verbal vocal cues. These cues are processed by the brain's limbic system, are responsible for emotional responses, and thus have a more direct and profound effect on listeners than does the cognitive processing involved with verbal cues <ref type="bibr" target="#b38">(Horan et al., 2012;</ref><ref type="bibr" target="#b73">Scherer, 1986</ref>). <ref type="bibr" target="#b57">Mehrabian's (1971)</ref> communication rule indicates that the impact of a message comes 7% from words, 38% from nonverbal vocal elements, and 55% from other nonverbal elements, emphasizing the importance of nonverbal communication in face-to-face interactions. However, in the context of one-way communication, which is typical of video instruction-where teachers may not be visible, and students may not constantly watch the screen-the emotional impact of nonverbal vocal elements likely exceeds that suggested by Mehrabian's rule, overshadowing verbal elements. Hsu and colleagues (2021) demonstrated that a speaker's nonverbal vocal cues have a significant emotional impact, offering a direct and universally understandable channel for emotional communication that surpasses the expressive limitations of verbal language, thus enhancing audience affective engagement with video content. The immediacy of nonverbal cue processing highlights the critical role of such cues in defining a video's emotional tone, rendering these cues essential for speakers aiming to communicate deeper emotional states.</p><p>In asynchronous video learning, where learner-instructor interactions are limited to fewer communication channels, learners may rely predominantly on the instructor's vocal cues and presented materials <ref type="bibr" target="#b28">(Fabriz et al., 2021)</ref>. As such, a teacher's nonverbal vocal cues in asynchronous online learning contexts may significantly influence students' affective engagement more than do spoken words. On the basis of these insights, we hypothesize the following in asynchronous video learning:</p><p>H4: Teachers' nonverbal vocal expressions have a greater influence on students' affective engagement than do their verbal vocal expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">METHODS</head><p>This study examined the impact of teachers' verbal and nonverbal vocal emotive expressions in asynchronous video learning content on adult students' affective engagement. In collaboration with four prominent MOOC platforms through industry-academic partnerships, we collected prerecorded course videos, all de-identified by the platforms, to protect the instructors' privacy.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data collection</head><p>Four Taiwan-based MOOC platforms-Hahow, Kvalley, TibaMe, and TKB-offer a variety of courses for adult learners and those seeking higher education using primarily prerecorded videos.</p><p>Courses were categorized into sciences and social sciences, with video quality above 1280×720 resolution. In some videos, teachers' faces appeared for up to one-quarter of the video's length, whereas other videos did not show the instructors' faces at all, with varying durations.</p><p>In this study, 210 valid video samples from unique teachers, defined as videos accurately transcribed and sentiment-analyzed using DL, were collected and analyzed. The sample included 152 male (72%) and 58 female (28%) teachers. Most courses (179, 85%) were in the social sciences, with the rest (31, 15%) being in science. Lecture durations ranged from 10 to 698 minutes, averaging 131 minutes. Teachers appeared on screen in 103 videos (49%) and were absent in 107 (51%).</p><p>To evaluate students' affective engagement, the platforms invited students who had completed each video lecture to participate in a post-class survey. Each video was assessed by at least three different students, ensuring diverse perspectives. In total, 738 unique students participated in the surveys, providing an average of 3.51 evaluations per video.</p><p>Participants were aged 18 to 45 years, mainly in the 26-30 (33%) and 31-35 (27%) year brackets.</p><p>Female respondents made up 58% of the survey population, whereas male respondents accounted for 42%. Most respondents held bachelor's degrees (65%), with the remaining 35% holding master's degrees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data measures</head><p>This research employed regression analysis to assess the proposed hypotheses. The independent variables included emotive Verbal_Valence, Verbal_Arousal, and Verbal_Aalence*Arousal from spoken words to represent vocal emotive contagion scores, along with the following six distinct nonverbal vocal expressions: happiness, anger, surprise, fear, sadness, and neutral. The dependent variable was students' self-reported affective engagement, which was averaged for each video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Teachers' verbal vocal emotive expression</head><p>An automatic speech recognition (ASR) system, "Whisper," developed by OpenAI (2022) and integrated with natural language processing (NLP), was used to convert spoken language from video feeds into Chinese text. Empirical tests indicated that in addition to minor issues with traditional-simplified Chinese conversions (e.g., the pronunciation of "zh"), its error rate was lower than that of other Chinese ASR solutions <ref type="bibr" target="#b34">(Heikinheimo, 2023)</ref>.</p><p>After transcribing each video, we used Jieba-tw (2023) for word segmentation and matched these words with the emotion-inducing word database from Chinese EmoBank <ref type="bibr" target="#b47">(Lee et al., 2022)</ref> using Python. This approach allowed us to extract emotional Verbal_Valence (0.92 to 9.00) and</p><p>Verbal_Arousal (1.00 to 9.00) scores for each word via sentiment analysis. We then multiplied the valence and arousal scores to derive the emotional contagion score for each word. These scores were summed and divided by the total number of words analyzed from the video to quantify the intensity of emotional engagement elicited by teachers' spoken words.</p><p>As an example, imagine a video where two words influenced emotional contagion. One word had a valence rating of 2 and an arousal rating of 3, resulting in a score of 6 (the product of valence and arousal). Another word in the same video had a valence of 5 and an arousal of 4, producing a score of 20. The video's overall emotional contagion score was then calculated by summing these individual scores (26) and dividing by the number of contributing words (2), resulting in a final score of 13.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Teachers' nonverbal vocal emotive expression</head><p>To analyze teachers' vocal expressions of emotion, we employed a deep learning model combining convolutional neural networks (CNNs) and long short-term memory (LSTM) networks, drawing on the approach outlined by <ref type="bibr" target="#b93">Ye et al. (2022)</ref>. This model was trained using the CASIA dataset from the Chinese Academy of Sciences, Institute of Automation <ref type="bibr" target="#b15">(Bao et al., 2014)</ref>. Our approach to emotion detection involved extracting a comprehensive set of 193 acoustic features, including Chroma features, mel-scaled spectrograms, mel-frequency cepstral coefficients (MFCCs), spectral contrast, and Tonnetz representations <ref type="bibr" target="#b70">(Sharanyaa et al., 2023;</ref><ref type="bibr" target="#b83">Su et al., 2020;</ref><ref type="bibr">Su et al., 2021a;</ref><ref type="bibr" target="#b82">Su et al., 2021b;</ref><ref type="bibr" target="#b79">Su et al., 2022)</ref>, capturing frequencies from 20 Hz to 20 kHz. To enhance the signalto-noise ratio, audio segments were processed in five-second chunks with a four-second overlap, and segments exhibiting confidence levels below the 0.7 threshold were omitted <ref type="bibr" target="#b13">(Allison et al., 2022)</ref>.</p><p>The CASIA, featuring vocal recordings from Chinese actors in various emotional states, was allocated into training and testing portions with an 80/20 distribution. This dataset includes six primary emotions-Vocal_Happiness, Vocal_Anger, Vocal_Surprise, Vocal_Fear, Vocal_Sadness, and Vocal_Neutral-rated on a 0 to 100% intensity scale. Our testing results achieved an accuracy (ACC) rate of 81%, approaching the 84% ACC cited by <ref type="bibr" target="#b93">Ye et al. (2022)</ref> for video-based emotion recognition using the CASIA dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Student affective engagement</head><p>We employed <ref type="bibr" target="#b25">Dixson's (2010)</ref> emotional engagement scale to assess affective engagement.</p><p>Participants rated the following items on a 5-point Likert scale, with endpoints of "Not at all characteristic of me" (1) and "Very characteristic of me" (5):</p><p>"I put forth effort while engaging with the video."</p><p>"I found ways to make the materials relevant to me."</p><p>"I applied the materials to my life."</p><p>"I found ways to make the materials interesting."</p><p>"I really desired to learn the content."</p><p>This scale was selected for its high reliability and strong relevance to online learning contexts, as noted by <ref type="bibr" target="#b37">Henrie et al. (2015)</ref>. It has demonstrated high reliability, with Cronbach's alpha ranging from 0.86 to 0.95 across different sample tests, and has been validated through its significant correlation with applied learning behaviors (r = .48, p &lt; .01) <ref type="bibr" target="#b24">(Dixson, 2015)</ref>.</p><p>Its effectiveness and broad applicability across diverse online learning environments have been demonstrated by numerous studies, further supporting its use for measuring engagement in MOOCs and other forms of asynchronous video instruction (e.g., <ref type="bibr" target="#b30">Farrell &amp; Brunton, 2020;</ref><ref type="bibr" target="#b95">Vo &amp; Ho, 2024)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Data analysis</head><p>We analyzed the relationship between teacher vocal cues and student affective engagement using a two-part approach. First, we conducted correlation analysis to determine which variables to include in our subsequent regression models. Then, we used hierarchical linear regression, entering</p><p>Verbal_Valence and Verbal_Arousal in the first block, the interaction term (Verbal_Valence*Arousal) in the second, and the frequencies of six vocal expressions (Vocal_Happiness, Vocal_Anger, Vocal_Surprise, Vocal_Fear, Vocal_Sadness, Vocal_Neutral) in the final block. Student affective engagement was the dependent variable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Reliability analysis</head><p>To gauge student affective engagement, we used post-class surveys administered immediately after each video lecture within a MOOC unit. These surveys typically included assessments of content mastery, such as quizzes or tests, alongside questionnaires designed to gather feedback on the overall learning experience. For this particular study, we incorporated a five-item questionnaire specifically targeting affective engagement into this standard post-class survey.</p><p>This scale demonstrated strong internal consistency, with a Cronbach's alpha (ɑ) of 0.83. With multiple students evaluating each teacher, the average intraclass correlation coefficient (ICC) was 0.66, indicating substantial consistency and reliability.</p><p>Independent variables were derived from biographical data and assessed through DL models, which achieved an ACC of 81% in recognizing perceived vocal emotions on the basis of the CASIA dataset, thus satisfying the benchmark ACC of 70-80% <ref type="bibr" target="#b74">(Schuller et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Correlation analysis</head><p>Research indicates that the on-screen visibility of teachers affects student engagement in online classes <ref type="bibr" target="#b20">(Chew, 2022;</ref><ref type="bibr" target="#b35">Henderson &amp; Schroeder, 2021)</ref>. Additionally, teacher gender may influence students' emotional perceptions <ref type="bibr" target="#b49">(Leung et al., 2018)</ref>. Course subject and length also impact student affective engagement, as variations in these factors can affect engagement levels <ref type="bibr" target="#b14">(Akiha et al., 2018)</ref>. Therefore, this study tested whether the above four variables influence student affective engagement. If significant correlations were found, then these variables were included as control variables in the regression model to test our hypotheses.</p><p>To understand the relationships among all the variables, we first conducted a correlation analysis, as shown in Table <ref type="table" target="#tab_4">2</ref>. The correlation matrix revealed that teacher visibility, course subject, video length, and teacher gender did not affect students' affective engagement. Consequently, these variables were not included in the hypothesis testing because of their lack of a significant relationship with the dependent variable. Correlation analysis indicated that teachers expressing vocal emotions of happiness and surprise were perceived by students as eliciting greater affective engagement, whereas those displaying anger had the opposite effect. The calculation of verbal contagion by multiplying valence and arousal implied a high degree of correlation between these levels. Additionally, there was no significant correlation between the valence and arousal of spoken words or among different vocal emotions, except for vocal surprise, where a very low-level correlation was noted, indicating the distinctiveness of these variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Linear regression analysis</head><p>To test the hypotheses, we used hierarchical linear regression. Initially, we introduced Verbal_Valence and Verbal_Arousal as independent variables in the first layer (Model A) and added the interaction term Verbal_Valence*Arousal in the second layer (Model B). In the last layer (Model C), we added six types of nonverbal vocal emotive expressions to examine changes in explained variance.</p><p>As detailed in Table <ref type="table" target="#tab_5">3</ref>, the analysis revealed that Model A, which focuses on the valence and arousal of teachers' spoken words, did not significantly contribute to affective engagement (p =.271 and 720), explaining only 0.3% of the variance according to the adjusted R² value. Adding the interaction of valence and arousal (Verbal_Valence*Arousal) in Model B increased the adjusted R² value by approximately 0.2%, with higher standardized coefficients for verbal valence and arousal. The coefficients of Verbal_Valence and Verbal_Arousal changed from positive to negative in Model B because of the altered interpretation of these main effects in the presence of the interaction. However, this interaction did not have a significant effect on affective engagement; thus, H1 was not supported. Conversely, incorporating six nonverbal vocal emotive expressions in Model C showed that teachers' nonverbal cues, particularly those with positive valence and higher arousal, such as happiness and surprise, significantly increased student affective engagement. A negative valence with higher arousal (anger) negatively impacted engagement. Nonverbal vocal expressions with less distinct valence and arousal, such as fear, sadness, and neutral expressions, did not significantly affect engagement, thus supporting H2 and H3.</p><p>Comparing Models B and C, including nonverbal vocal expressions augmented the explanatory power for affective engagement by 25.6% over verbal emotive expressions alone, according to the adjusted R² value, thus affirming H4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">DISCUSSION</head><p>With the rise in popularity of MOOCs, asynchronous video learning formats, and hybrid models blending face-to-face or synchronous live sessions in adult learning and higher education, the major criticism has been their lack of interactivity and emotional depth <ref type="bibr" target="#b12">(Alemayehu &amp; Chen, 2023)</ref>. This gap often results in diminished student affective engagement. Many scholars have noted that the vocal emotive expressions of teachers in MOOCs or instructional videos can enhance student affective engagement and should thus be a focus of future research (e.g., <ref type="bibr" target="#b21">Chiu et al., 2022;</ref><ref type="bibr" target="#b54">Ma et al., 2023;</ref><ref type="bibr" target="#b97">Zeng &amp; Luo, 2023)</ref>.</p><p>This study explores and examines whether verbal and nonverbal vocal emotive expressions by teachers can significantly enhance students' affective engagement in asynchronous video learning.</p><p>Through hypothesis testing using hierarchical linear regression, this study obtains several key findings and insights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Teachers' verbal vocal emotive expressions do not influence student affective engagement</head><p>Contrary to the initial assumption in H1, this study reveals that teachers' verbal emotive expressions (i.e., choice of words) in videos have a negligible effect on student affective engagement. This finding aligns with dual coding theory, which posits that verbal cues engage cognitive processes, whereas nonverbal cues trigger emotional reactions <ref type="bibr" target="#b64">(Paivio, 1971)</ref>. Thus, spoken words barely influence affective engagement in videos with both cue types, highlighting the profound impact of nonverbal vocal expressions <ref type="bibr" target="#b73">(Scherer, 1986)</ref>. Hence, the key to increasing the level of student affective engagement in asynchronous video learning lies in leveraging nonverbal vocal expressions rather than leveraging mere verbal content, marking a significant pivot in instructional video design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Teachers' nonverbal vocal emotive expressions influence student affective engagement</head><p>In support of H2 and H3, our study reveals that teachers' nonverbal vocal expressions of happiness and surprise, while mitigating displays of anger, significantly increase student affective engagement in instructional videos. These findings align with theories from paralanguage <ref type="bibr" target="#b86">(Trager, 1958)</ref>, self-determination <ref type="bibr" target="#b69">(Ryan &amp; Deci, 2000)</ref>, and voice and emotion <ref type="bibr" target="#b43">(Juslin &amp; Laukka, 2003)</ref>, emphasizing that teachers can influence students' emotions through nonverbal vocal cues in asynchronous video learning. Our study reveals that emotional contagion is driven more by emotional arousal than by valence alone. Therefore, nonverbal vocal expressions with distinct valence and greater arousal, such as happiness, anger, and surprise, are most effective in influencing student affective engagement, with surprise having the greatest impact, aligning with the circumplex model of affect <ref type="bibr" target="#b68">(Russell, 1980)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Teachers' nonverbal vocal emotive expressions influence student affective engagement more than do verbal vocal emotive expressions</head><p>As in H4, linear regressions indicate that these three types of nonverbal vocal emotive expressions have significantly greater explanatory power for student affective engagement than do verbal emotive expressions, according to dual coding theory <ref type="bibr" target="#b64">(Paivio, 1971</ref>) and <ref type="bibr" target="#b57">Mehrabian's (1971)</ref> rule.</p><p>Aside from textual and visual content, students are less likely to focus on the teacher's spoken words but are more influenced by the teacher's nonverbal vocal emotions in this learning mode.</p><p>The importance of social presence elements in the CoI model <ref type="bibr" target="#b17">(Borup et al., 2012)</ref> supports this finding, suggesting that effective nonverbal cues enhance the sense of connection in digital learning. Additionally, the circumplex model of affect <ref type="bibr" target="#b68">(Russell, 1980)</ref> highlights the role of emotional arousal in engagement. Teachers should vary their tone to convey enthusiasm through vocal expressions of surprise and happiness, modulate their pitch to emphasize key points, and use varied rhythms to maintain student interest. Reducing the degree of vocal anger is crucial to fostering an affectively engaged learning environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Research limitations and future opportunities</head><p>Despite offering numerous new findings and insights, this study acknowledges several research limitations that must be addressed. First, our sample leans toward female Chinese speakers and social science teachers. Future work could diversify the sample and subjects more broadly to improve generalizability.</p><p>Second, affective engagement is measured via self-reports, which are prone to bias. Future research could use computer-based emotional recognition tools for more objective assessments (see <ref type="bibr" target="#b61">Mutawa &amp; Sruthi, 2023)</ref>.</p><p>Finally, we analyze the emotive valence and arousal in teachers' speech using the Chinese EmoBank. Future research could implement more sophisticated sentiment analysis techniques, preferring segment analysis to bags of words <ref type="bibr" target="#b78">(Su et al., 2024;</ref><ref type="bibr" target="#b85">Suresh et al., 2024;</ref><ref type="bibr" target="#b100">Zhou &amp; Gao, 2023)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS</head><p>While acknowledging its limitations, this study contributes to the field of Human-Computer Interaction, highlighting the critical role of nonverbal vocal emotive expressions-such as happiness, surprise, and anger-in enhancing user affective engagement in asynchronous video environments. The findings provide theoretical insights and practical applications by shifting the focus from verbal content to emotional cues and emphasizing the importance of emotional arousal over valence. The study integrates established theories and frameworks to advance the understanding of how vocal expressions influence social presence and emotional engagement in one-way video-based instruction and communication mediums. Practically, it offers actionable recommendations for leveraging technology and designing interactive systems that optimize vocal emotive expressions, enabling instructors to create more engaging and emotionally resonant learning experiences. These insights address critical gaps in asynchronous learning, paving the way for more emotionally connected and interactive online education.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Research implications</head><p>Previous research has greatly enhanced our understanding of affective engagement in MOOCs, focusing on aspects such as student profiles <ref type="bibr" target="#b75">(Shi et al., 2024)</ref>, feedback mechanisms <ref type="bibr" target="#b94">(Vilkova &amp; Shcheglova, 2021)</ref>, motivational strategies <ref type="bibr" target="#b44">(Karabatak &amp; Polat, 2020)</ref>, and accessibility <ref type="bibr" target="#b59">(Mohd Ashril et al., 2024)</ref>.</p><p>This study extends the work of <ref type="bibr" target="#b84">Suen and Hung (2024)</ref>, who investigated the impact of teachers' facial and paraverbal expressions on students' affective engagement in MOOCs. Expanding their scope, our research focuses specifically on the influence of teachers' verbal and nonverbal vocal emotive expressions in asynchronous video learning environments. By examining verbal elements such as valence (emotional tone) and arousal (intensity) alongside nonverbal vocal emotions like happiness, surprise, and anger, this study aims to provide a more detailed understanding of how these vocal expressions affect students' emotional engagement.</p><p>Additionally, we address existing research gaps and align with future directions suggested by many scholars (e.g. <ref type="bibr" target="#b21">Chiu et al., 2022;</ref><ref type="bibr" target="#b54">Ma et al., 2023;</ref><ref type="bibr" target="#b97">Zeng &amp; Luo, 2023)</ref>, offering new insights into optimizing teacher-student interaction and engagement in asynchronous instructional settings.</p><p>In addition to employing emotional response theory <ref type="bibr" target="#b60">(Mottet et al., 2006)</ref> and emotion contagion theory <ref type="bibr" target="#b33">(Hatfield et al., 1993)</ref> to highlight the importance of teacher vocal emotive expressions in classrooms and synchronous online learning environments, this study applies dual coding theory <ref type="bibr" target="#b64">(Paivio, 1971</ref>) and <ref type="bibr">Mehrabian's rule (1971)</ref> to emphasize the significant impact of teachers' nonverbal vocal emotive expressions by valence and arousal dimensions in prerecorded educational videos. This application provides a novel theoretical basis for examining the relative importance of vocal emotional cues in asynchronous video learning.</p><p>To further broaden our theoretical perspective, we incorporate insights from paralanguage <ref type="bibr" target="#b86">(Trager, 1958)</ref>, self-determination theory <ref type="bibr" target="#b69">(Ryan &amp; Deci, 2000)</ref>, and voice and emotion theory <ref type="bibr" target="#b43">(Juslin &amp; Laukka, 2003)</ref>. This integrated approach allows us to identify the key nonverbal vocal emotions-surprise, happiness, and anger-significantly influencing student affective engagement in this context. Moreover, our multifaceted theoretical framework integrates the social presence elements of the CoI framework <ref type="bibr" target="#b17">(Borup et al., 2012)</ref> and the circumplex model of affect <ref type="bibr" target="#b68">(Russell, 1980)</ref> to enhance the understanding of how teacher vocal expressions impact student affective engagement in this setting. This contribution extends the literature on the affective dimensions of prerecorded video instruction to encompass the unique characteristics of teacher emotional expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Practical implications</head><p>Our study suggests a pivotal shift in focus toward optimizing teachers' vocal emotive expressions in videos to enhance student affective engagement. To begin, video designers must prioritize adaptive training programs. These programs should equip instructors and social media influencers with the skills to modulate their voices effectively. The focus should be on conveying emotions like happiness and surprise while minimizing expressions of anger. This training could involve workshops or coaching sessions focused on vocal techniques that make video content more engaging and emotionally resonant with students and audiences.</p><p>Second, video content development should more heavily consider the emotional impact of these nonverbal cues. This might include emphasizing these aspects through postproduction techniques or generative artificial intelligence generated speech and providing guidelines for teachers and speakers-or even replacing their voices-to present videos that maximize positive emotional contagion.</p><p>Finally, video content developers might explore new features that enhance the emotional expressiveness of instructors or influencers on video platforms. For example, AI-driven feedback tools could analyze a video's vocal qualities and suggest improvements to teachers or speakers, helping them adjust their delivery for greater emotional impact based on their expressive styles <ref type="bibr" target="#b100">(Zhou &amp; Gao, 2023)</ref>.</p><p>Program within the framework of the Higher Education Sprout Project by the Ministry of Education (MOE) in Taiwan.</p><p>We also extend our gratitude to Chin-Chia Yeh, Chia-Fan Chu, Jing-Rui Gu, Kuo-En Hung, Yan-Ming Huang, Yi-Chen Lin, Yu-Cheng Tseng, and Yung-Sian Fang from National Taiwan Normal University for their assistance in data collection, data cleaning, and pilot testing for this study.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc>Figure 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure</head><figDesc>Figure 1. Research Design</figDesc><graphic coords="12,54.70,172.65,476.90,222.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Variables and Measurement Tools</figDesc><table><row><cell>Variables</cell><cell>Measures</cell><cell>Tools</cell></row><row><cell>Independent variables</cell><cell></cell><cell></cell></row><row><cell cols="2">Verbal Emotive Expressions Valence Scores (1-9)</cell><cell></cell></row><row><cell cols="2">(e.g. spoken words) Arousal Scores (1-9)</cell><cell>Sentiment Analysis</cell></row><row><cell></cell><cell>Valence*Arousal Scores (1-81)</cell><cell></cell></row><row><cell cols="2">Vocal Emotive Expressions Anger probability (0-100%)</cell><cell></cell></row><row><cell cols="2">(e.g. tone, pitch, and intonation) Fear probability (0-100%)</cell><cell></cell></row><row><cell></cell><cell>Happiness probability (0-100%) Neutral probability (0-100%)</cell><cell>Acoustic Analysis</cell></row><row><cell></cell><cell>Sadness probability (0-100%)</cell><cell></cell></row><row><cell></cell><cell>Suprise probability (0-100%)</cell><cell></cell></row><row><cell>Dependent variables</cell><cell>Affective Engagement (1-5)</cell><cell>Self-reported Survey</cell></row><row><cell>affective engagement?</cell><cell></cell><cell></cell></row><row><cell cols="3">2. Do teachers' nonverbal vocal expressions with high positive valence and arousal increase</cell></row><row><cell>students' affective engagement?</cell><cell></cell><cell></cell></row><row><cell cols="3">3. Do teachers' nonverbal vocal expressions with high negative valence and arousal decrease</cell></row><row><cell>students' affective engagement?</cell><cell></cell><cell></cell></row><row><cell cols="3">4. Which type of vocal expression impacts students' affective engagement more-verbal or</cell></row><row><cell>nonverbal?</cell><cell></cell><cell></cell></row><row><cell cols="3">By addressing these questions, this study aims to enhance our understanding of effective teachers'</cell></row></table><note><p>To this end, the study seeks to answer the following research questions:</p><p>1. Do teachers' verbal vocal expressions with high positive valence and arousal increase students' verbal and vocal emotive expressions in asynchronous video learning environments, including</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Correlation Analysis</figDesc><table><row><cell>Variable</cell><cell>Mean</cell><cell>SD</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>12</cell><cell>13</cell></row><row><cell>1. Visibility a</cell><cell cols="2">1.49 0.501</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2. Gender b</cell><cell cols="3">1.724 0.448 -.012</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3. Subject c</cell><cell cols="5">1.852 0.356 .167 * -.197 ** -</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4. Length</cell><cell cols="6">130.633 100.152 -.186 ** .102 -.021 -</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>5. Verbal_Valence</cell><cell cols="7">5.53 0.345 -.096 -.077 -.130 -.014 -</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>6. Verba_Arousal</cell><cell cols="7">4.469 0.231 .051 -.125 .136 * -.154 * .119</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>7. Verbal_Valence*Arousal</cell><cell cols="9">24.729 2.224 -.039 -.135 -.012 -.098 .818 *** .665 *** -</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>8. Vocal_Happiness</cell><cell cols="9">0.159 0.053 -.088 .081 .055 .046 .036 -.034 .016</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>9. Vocal_Anger</cell><cell cols="10">0.072 0.028 -.091 .042 -.031 -.011 .054 -.053 .009 -.041</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="14">10. Vocal_Surprise 0.2 0.096 .007 -.025 .106 .013 .120 .092 .151 11. Vocal_Fear 0.138 0.074 .020 .006 -.004 -.016 -.042 -.012 -.034 -.128 .057 -.039 -</cell><cell></cell><cell></cell></row><row><cell>12. Vocal_Sadness</cell><cell cols="13">0.131 0.085 .078 -.018 -.084 -.035 -.009 .021 .011 -.075 -.039 .075 .097</cell><cell>-</cell><cell></cell></row><row><cell>13. Vocal_Neutral</cell><cell cols="15">0.63 0.142 -.038 .018 -.118 .089 -.043 -.027 -.043 -.063 -.049 -.008 .007 .167 * -</cell></row><row><cell>14. Affective Engagement</cell><cell cols="15">4.082 0.361 -.052 -.106 .058 .074 .080 .034 .083 .328 *** -.332 *** .377 *** -.045 .026 -.062</cell></row></table><note><p>* .148 * -.194 * -* p&lt;.05, ** p&lt;.01, and *** p&lt;.001. a 1 =Off-screen, 2=On-screen. b 1 =Female, 2=Male. c 1 =Science, 2=Social Sciences.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Hierarchical Regression Analysis</figDesc><table><row><cell cols="3">Dependent Variable: Affective Engagement</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>Variable</cell><cell>Standardized Coefficient</cell><cell>t</cell><cell>p</cell><cell cols="2">95% CI Lower Upper</cell><cell cols="2">R 2 Adjusted R 2</cell></row><row><cell>A</cell><cell>Verbal_Valence Verbal_Arousal</cell><cell>0.077 0.025</cell><cell>1.104 0.359</cell><cell>0.271 0.720</cell><cell>-0.063 -0.176</cell><cell>0.225 0.254</cell><cell>0.007</cell><cell>0.003</cell></row><row><cell></cell><cell>Verbal_Valence</cell><cell>-0.429</cell><cell>-0.558</cell><cell>0.577</cell><cell>-2.038</cell><cell>1.139</cell><cell></cell><cell></cell></row><row><cell>B</cell><cell>Verbal_Arousal</cell><cell>-0.364</cell><cell>-0.614</cell><cell>0.540</cell><cell>-2.391</cell><cell>1.255</cell><cell>0.009</cell><cell>0.005</cell></row><row><cell></cell><cell>Verbal_Valence*Arousal</cell><cell>0.675</cell><cell>0.661</cell><cell>0.509</cell><cell>-0.217</cell><cell>0.437</cell><cell></cell><cell></cell></row><row><cell>C</cell><cell>Verbal_Valence</cell><cell>0.331</cell><cell>0.492</cell><cell>0.623</cell><cell>-1.045</cell><cell cols="2">1.739 0.293</cell><cell>0.261</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This study was supported by the <rs type="funder">National Science and Technology Council, Taiwan</rs>, under grant <rs type="grantNumber">NSTC 113-2622-H-003 -003 -</rs>, <rs type="grantNumber">NSTC 112-2410-H-003-102-MY2</rs>, and <rs type="grantNumber">NSTC 111-2410-H-019-006-MY3</rs>. This work was financially/partially supported by the <rs type="funder">Advanced Institute of Manufacturing with High-tech Innovations (AIM-HI)</rs> from the <rs type="funder">Featured Areas Research Center</rs></p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_4CnCpVX">
					<idno type="grant-number">NSTC 113-2622-H-003 -003 -</idno>
				</org>
				<org type="funding" xml:id="_uyvmVnd">
					<idno type="grant-number">NSTC 112-2410-H-003-102-MY2</idno>
				</org>
				<org type="funding" xml:id="_qyuX5Z9">
					<idno type="grant-number">NSTC 111-2410-H-019-006-MY3</idno>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"> Suen, H. Y., &amp; Su, Y. S. (2025)<p>. Teachers' Vocal Expressions and Student Engagement in Asynchronous Video Learning. International Journal of Human-Computer Interaction, 1-12. <ref type="url" target="https://doi.org/10.1080/10447318.2025.2474469">https://doi.org/10.1080/10447318.2025.2474469</ref> </p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Teachers&apos; Vocal Expressions and Student Engagement in Asynchronous Video Learning</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Su</surname></persName>
		</author>
		<idno type="DOI">10.1080/10447318.2025.2474469</idno>
		<ptr target="https://doi.org/10.1080/10447318.2025.2474469" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Teachers&apos; Vocal Expressions and Student Engagement in Asynchronous Video Learning</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Su</surname></persName>
		</author>
		<idno type="DOI">10.1080/10447318.2025.2474469</idno>
		<ptr target="https://doi.org/10.1080/10447318.2025.2474469" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Teachers&apos; Vocal Expressions and Student Engagement in Asynchronous Video Learning</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Su</surname></persName>
		</author>
		<idno type="DOI">10.1080/10447318.2025.2474469</idno>
		<ptr target="https://doi.org/10.1080/10447318.2025.2474469" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Teachers&apos; Vocal Expressions and Student Engagement in Asynchronous Video Learning</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Su</surname></persName>
		</author>
		<idno type="DOI">10.1080/10447318.2025.2474469</idno>
		<ptr target="https://doi.org/10.1080/10447318.2025.2474469" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Teachers&apos; Vocal Expressions and Student Engagement in Asynchronous Video Learning</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Su</surname></persName>
		</author>
		<idno type="DOI">10.1080/10447318.2025.2474469</idno>
		<ptr target="https://doi.org/10.1080/10447318.2025.2474469" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Teachers&apos; Vocal Expressions and Student Engagement in Asynchronous Video Learning</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Su</surname></persName>
		</author>
		<idno type="DOI">10.1080/10447318.2025.2474469</idno>
		<ptr target="https://doi.org/10.1080/10447318.2025.2474469" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Teachers&apos; Vocal Expressions and Student Engagement in Asynchronous Video Learning</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Su</surname></persName>
		</author>
		<idno type="DOI">10.1080/10447318.2025.2474469</idno>
		<ptr target="https://doi.org/10.1080/10447318.2025.2474469" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Teachers&apos; Vocal Expressions and Student Engagement in Asynchronous Video Learning</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Su</surname></persName>
		</author>
		<idno type="DOI">10.1080/10447318.2025.2474469</idno>
		<ptr target="https://doi.org/10.1080/10447318.2025.2474469" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Teachers&apos; Vocal Expressions and Student Engagement in Asynchronous Video Learning</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Su</surname></persName>
		</author>
		<idno type="DOI">10.1080/10447318.2025.2474469</idno>
		<ptr target="https://doi.org/10.1080/10447318.2025.2474469" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Teachers&apos; Vocal Expressions and Student Engagement in Asynchronous Video Learning</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Su</surname></persName>
		</author>
		<idno type="DOI">10.1080/10447318.2025.2474469</idno>
		<ptr target="https://doi.org/10.1080/10447318.2025.2474469" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Teachers&apos; Vocal Expressions and Student Engagement in Asynchronous Video Learning</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Su</surname></persName>
		</author>
		<idno type="DOI">10.1080/10447318.2025.2474469</idno>
		<ptr target="https://doi.org/10.1080/10447318.2025.2474469" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Teachers&apos; Vocal Expressions and Student Engagement in Asynchronous Video Learning</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Su</surname></persName>
		</author>
		<idno type="DOI">10.1080/10447318.2025.2474469References</idno>
		<ptr target="https://doi.org/10.1080/10447318.2025.2474469References" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learner and instructor-related challenges for learners&apos; engagement in MOOCs: A review of 2014-2020 publications in selected</title>
		<author>
			<persName><forename type="first">L</forename><surname>Alemayehu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-L</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1080/10494820.2021.1920430</idno>
		<ptr target="https://doi.org/10.1080/10494820.2021.1920430" />
	</analytic>
	<monogr>
		<title level="j">SSCI indexed journals. Interactive Learning Environments</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="3172" to="3194" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Can you hear me now? Engendering passion and preparedness perceptions with vocal expressions in crowdfunding pitches</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Allison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Warnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Cardon</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jbusvent.2022.106193</idno>
		<ptr target="https://doi.org/10.1016/j.jbusvent.2022.106193" />
	</analytic>
	<monogr>
		<title level="j">Journal of Business Venturing</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">106193</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">What types of instructional shifts do students experience? Investigating active learning in science, technology, engineering, and math classes across key transition points from middle school to the university level</title>
		<author>
			<persName><forename type="first">K</forename><surname>Akiha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brigham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Couch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lewin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stains</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Stetzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Vinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.3389/feduc.2017.00068</idno>
		<ptr target="https://doi.org/10.3389/feduc.2017.00068" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Education</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">68</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Building a chinese natural emotional audio-visual database</title>
		<author>
			<persName><forename type="first">W</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICOSP.2014.7015071</idno>
		<ptr target="https://doi.org/10.1109/ICOSP.2014.7015071" />
	</analytic>
	<monogr>
		<title level="m">2014 12th International Conference on Signal Processing (ICSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="583" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">How mood turns on language</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Beukeboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Semin</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jesp.2005.09.005</idno>
		<ptr target="https://doi.org/10.1016/j.jesp.2005.09.005" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Social Psychology</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="553" to="566" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving online social presence through asynchronous video</title>
		<author>
			<persName><forename type="first">J</forename><surname>Borup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Graham</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.iheduc.2011.11.001</idno>
		<ptr target="https://doi.org/10.1016/j.iheduc.2011.11.001" />
	</analytic>
	<monogr>
		<title level="j">Internet and Higher Education</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="195" to="203" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Teachers&apos; Vocal Expressions and Student Engagement in Asynchronous Video Learning</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Su</surname></persName>
		</author>
		<idno type="DOI">10.1080/10447318.2025.2474469</idno>
		<ptr target="https://doi.org/10.1080/10447318.2025.2474469" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The effect over time of a video-based reflection system on preservice teachers&apos; oral presentations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cavanagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bower</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Moloney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sweller</surname></persName>
		</author>
		<idno type="DOI">10.14221/ajte.2014v39n6</idno>
		<ptr target="https://doi.org/10.14221/ajte.2014v39n6" />
	</analytic>
	<monogr>
		<title level="j">Australian Journal of Teacher Education</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Performing presence with the Teaching-body via videoconferencing: A postdigital study of the Teacher&apos;s face and Voice</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Chew</surname></persName>
		</author>
		<idno type="DOI">10.1007/s42438-022-00288-2</idno>
		<ptr target="https://doi.org/10.1007/s42438-022-00288-2" />
	</analytic>
	<monogr>
		<title level="j">Postdigital Science and Education</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="394" to="436" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Applying the self-determination theory (SDT) to explain student engagement in online learning during the COVID-19 pandemic</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K F</forename><surname>Chiu</surname></persName>
		</author>
		<idno type="DOI">10.1080/15391523.2021.1891998</idno>
		<ptr target="https://doi.org/10.1080/15391523.2021.1891998" />
	</analytic>
	<monogr>
		<title level="j">Journal of Research on Technology in Education</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="14" to="30" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Affective engagement of higher education students in an online course</title>
		<author>
			<persName><forename type="first">W</forename><surname>Daher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sabbah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abuzant</surname></persName>
		</author>
		<idno type="DOI">10.28991/esj-2021-01296</idno>
		<ptr target="https://doi.org/10.28991/esj-2021-01296" />
	</analytic>
	<monogr>
		<title level="j">Emerging Science Journal</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="545" to="558" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Understanding continuance intention among MOOC participants: The role of habit and MOOC performance</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Teo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Rappa</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.chb.2020.106455</idno>
		<ptr target="https://doi.org/10.1016/j.chb.2020.106455" />
	</analytic>
	<monogr>
		<title level="j">Computers in Human Behavior</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page">106455</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Measuring student engagement in the online course: The Online Student Engagement scale (OSE)</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Dixson</surname></persName>
		</author>
		<idno type="DOI">10.24059/olj.v19i4.561</idno>
		<ptr target="https://doi.org/10.24059/olj.v19i4.561" />
	</analytic>
	<monogr>
		<title level="j">Online Learning</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">165</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Creating effective student engagement in online courses: What do students find engaging</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Dixson</surname></persName>
		</author>
		<ptr target="https://scholarworks.iu.edu/journals/index.php/josotl/article/view/1744" />
	</analytic>
	<monogr>
		<title level="j">Journal of the Scholarship of Teaching and Learning</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Survey on speech emotion recognition: Features, classification schemes, and databases</title>
		<author>
			<persName><forename type="first">M</forename><surname>El Ayadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Kamel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Karray</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2010.09.020</idno>
		<ptr target="https://doi.org/10.1016/j.patcog.2010.09.020" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="572" to="587" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Teachers&apos; Vocal Expressions and Student Engagement in Asynchronous Video Learning</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Su</surname></persName>
		</author>
		<idno type="DOI">10.1080/10447318.2025.2474469</idno>
		<ptr target="https://doi.org/10.1080/10447318.2025.2474469" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Impact of synchronous and asynchronous settings of online teaching and learning in higher education on students&apos; learning experience during COVID-19</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fabriz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mendzheritskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stehle</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2021.733554</idno>
		<ptr target="https://doi.org/10.3389/fpsyg.2021.733554" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">733554</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Emotion, motivation and function</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Fanselow</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cobeha.2017.12.013</idno>
		<ptr target="https://doi.org/10.1016/j.cobeha.2017.12.013" />
	</analytic>
	<monogr>
		<title level="j">Current Opinion in Behavioral Sciences</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="105" to="109" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A balancing act: A window into online student engagement experiences</title>
		<author>
			<persName><forename type="first">O</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Brunton</surname></persName>
		</author>
		<idno type="DOI">10.1186/s41239-020-00199-x</idno>
		<ptr target="https://doi.org/10.1186/s41239-020-00199-x" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Educational Technology in Higher Education</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">25</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cognitive and affective effects of teachers&apos; annotations and talking heads on asynchronous video lectures in a web development course</title>
		<author>
			<persName><forename type="first">M</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yousef</surname></persName>
		</author>
		<idno type="DOI">10.58459/rptel.2023.18020</idno>
		<ptr target="https://doi.org/10.58459/rptel.2023.18020" />
	</analytic>
	<monogr>
		<title level="j">Research and Practice in Technology Enhanced Learning</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Emotion representation, analysis and synthesis in continuous space: A survey</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gunes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
		<idno type="DOI">10.1109/FG.2011.5771357</idno>
		<ptr target="https://doi.org/10.1109/FG.2011.5771357" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG)</title>
		<meeting>the 2011 IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG)<address><addrLine>Santa Barbara, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="827" to="834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Emotional contagion</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hatfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Cacioppo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Rapson</surname></persName>
		</author>
		<idno type="DOI">10.1111/1467-8721</idno>
		<ptr target="https://doi.org/10.1111/1467-8721" />
	</analytic>
	<monogr>
		<title level="j">Current directions in psychological science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="96" to="100" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Heikinheimo</surname></persName>
		</author>
		<ptr target="https://www.speechly.com/blog/analyzing-open-ais-whisper-asr-models-word-error-rates-across-languages" />
		<title level="m">Analyzing open AI&apos;s whisper ASR accuracy: Word error rates across languages and model sizes</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A Systematic review of instructor presence in instructional videos: Effects on learning and affect</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L</forename><surname>Schroeder</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.caeo.2021.100059</idno>
		<ptr target="https://doi.org/10.1016/j.caeo.2021.100059" />
	</analytic>
	<monogr>
		<title level="j">Computers and Education Open</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">100059</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Teachers&apos; Vocal Expressions and Student Engagement in Asynchronous Video Learning</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Su</surname></persName>
		</author>
		<idno type="DOI">10.1080/10447318.2025.2474469</idno>
		<ptr target="https://doi.org/10.1080/10447318.2025.2474469" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Measuring student engagement in technology-mediated learning: A review</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Henrie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Halverson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Graham</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.compedu.2015.09.005</idno>
		<ptr target="https://doi.org/10.1016/j.compedu.2015.09.005" />
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Education</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="36" to="53" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Understanding emotional response theory: The role of instructor power and justice messages</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Horan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Weber</surname></persName>
		</author>
		<idno type="DOI">10.1080/01463373.2012.669323</idno>
		<ptr target="https://doi.org/10.1080/01463373.2012.669323" />
	</analytic>
	<monogr>
		<title level="j">Communication Quarterly</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="210" to="233" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Speech emotion recognition considering nonverbal vocalization in affective conversations</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1109/TASLP.2021.3076364</idno>
		<ptr target="https://doi.org/10.1109/TASLP.2021.3076364" />
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1675" to="1686" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">We feel, therefore we learn: The relevance of affective and social neuroscience to education</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Immordino-Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Damasio</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1751-228X.2007.00004.x</idno>
		<ptr target="https://doi.org/10.1111/j.1751-228X.2007.00004.x" />
	</analytic>
	<monogr>
		<title level="j">Mind, Brain, and Education</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="10" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning a programming language</title>
		<author>
			<persName><forename type="first">O</forename><surname>Iskrenovic-Momcilovic</surname></persName>
		</author>
		<idno type="DOI">10.1177/0020720918773975</idno>
		<ptr target="https://doi.org/10.1177/0020720918773975" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Electrical Engineering Education</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="324" to="333" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<ptr target="https://github.com/APCLab/jieba-tw" />
		<title level="m">Jieba Chinese word segmentation Taiwan traditional Chinese version</title>
		<imprint>
			<publisher>Jieba-tw</publisher>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Communication of emotion in vocal expression and music performance: Different channels, same code?</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Juslin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laukka</surname></persName>
		</author>
		<idno type="DOI">10.1037/0033-2909.129.5.770</idno>
		<ptr target="https://doi.org/10.1037/0033-2909.129.5.770" />
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="770" to="814" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The effects of the flipped classroom model designed according to the ARCS motivation strategies on the students&apos; motivation and academic achievement levels</title>
		<author>
			<persName><forename type="first">S</forename><surname>Karabatak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Polat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename></persName>
		</author>
		<idno type="DOI">10.1007/s10639</idno>
		<ptr target="https://doi.org/10.1080/10447318.2025.2474469" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2020">2020. 2025</date>
		</imprint>
	</monogr>
	<note>Teachers&apos; Vocal Expressions and Student Engagement in Asynchronous Video Learning Education and Information Technologies</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Voice-only communication enhances empathic accuracy</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Kraus</surname></persName>
		</author>
		<idno type="DOI">10.1037/amp0000147</idno>
		<ptr target="https://doi.org/10.1037/amp0000147" />
	</analytic>
	<monogr>
		<title level="j">American Psychologist</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="644" to="654" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Vocal communication of emotion</title>
		<author>
			<persName><forename type="first">P</forename><surname>Laukka</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-28099-8_562-1</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-28099-8_562-1" />
	</analytic>
	<monogr>
		<title level="m">Encyclopedia of Personality and Individual Differences</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Zeigler-Hill</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">&amp;</forename><forename type="middle">T</forename><surname>Shackelford</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Chinese EmoBank: Building valence-arousal resources for dimensional sentiment analysis</title>
		<author>
			<persName><forename type="first">L.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3489141</idno>
		<ptr target="https://doi.org/10.1145/3489141" />
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Asian and Low-Resource Language Information Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The impact of emotional words on listeners&apos; emotional and cognitive responses in the context of advertisements</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Potter</surname></persName>
		</author>
		<idno type="DOI">10.1177/0093650218765523</idno>
		<ptr target="https://doi.org/10.1177/0093650218765523" />
	</analytic>
	<monogr>
		<title level="j">Communication Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="1155" to="1180" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Voice, articulation, and prosody contribute to listener perceptions of speaker gender: A systematic review and meta-analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Oates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chan</surname></persName>
		</author>
		<idno type="DOI">10.1044/2017_JSLHR-S-17-0067</idno>
		<ptr target="https://doi.org/10.1044/2017_JSLHR-S-17-0067" />
	</analytic>
	<monogr>
		<title level="j">Journal of speech, language, and hearing research</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="266" to="297" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The language, tone and prosody of emotions: neural substrates and dynamics of spoken-word emotion perception</title>
		<author>
			<persName><forename type="first">E</forename><surname>Liebenthal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Silbersweig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Stern</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnins.2016.00506</idno>
		<ptr target="https://doi.org/10.3389/fnins.2016.00506" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neuroscience</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">The impact of student engagement on learning outcomes in a cyber-flipped course</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kinshuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11423-019-09698-9</idno>
		<ptr target="https://doi.org/10.1007/s11423-019-09698-9" />
	</analytic>
	<monogr>
		<title level="j">Education Technology Research Development</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="1573" to="1591" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Emotion cycles in services: Emotional contagion and emotional labor effects</title>
		<author>
			<persName><forename type="first">X.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N.-W</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Gremler</surname></persName>
		</author>
		<idno type="DOI">10.1177/1094670519835309</idno>
		<ptr target="https://doi.org/10.1177/1094670519835309" />
	</analytic>
	<monogr>
		<title level="j">Journal of Service Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="285" to="300" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Teachers&apos; Vocal Expressions and Student Engagement in Asynchronous Video Learning</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Su</surname></persName>
		</author>
		<idno type="DOI">10.1080/10447318.2025.2474469</idno>
		<ptr target="https://doi.org/10.1080/10447318.2025.2474469" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">The comparison of two automated feedback approaches based on automated analysis of the online asynchronous interaction: A case of massive online teacher training</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<idno type="DOI">10.1080/10494820.2023.2191252</idno>
		<ptr target="https://doi.org/10.1080/10494820.2023.2191252" />
	</analytic>
	<monogr>
		<title level="j">Interactive Learning Environments</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Engagement matters: Student perceptions on the importance of engagement strategies in the online learning environment</title>
		<author>
			<persName><forename type="first">F</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">U</forename><surname>Bolliger</surname></persName>
		</author>
		<idno type="DOI">10.24059/olj.v22i1.1092</idno>
		<ptr target="https://doi.org/10.24059/olj.v22i1.1092" />
	</analytic>
	<monogr>
		<title level="j">Online Learning</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="205" to="222" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<ptr target="https://www.maximizemarketresearch.com/market-report/mooc-market/120455/" />
		<title level="m">MOOC market -global industry analysis and forecast</title>
		<imprint>
			<publisher>Maximize Market Research</publisher>
			<date type="published" when="2023">2023. 2023-2029</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Silent messages</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mehrabian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971">1971</date>
			<publisher>Wadsworth</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">DIAMOND: A tool for monitoring the participation of students in online lectures</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mershad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Said</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10639-021-10801-y</idno>
		<ptr target="https://doi.org/10.1007/s10639-021-10801-y" />
	</analytic>
	<monogr>
		<title level="j">Education and Information Technologies</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Barriers, strategies and accessibility: Enhancing engagement and retention of learners with disabilities in MOOCs -A systematic literature review (SLR)</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A N</forename><surname>Mohd Ashril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Chee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yahaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Abdul Razak</surname></persName>
		</author>
		<idno type="DOI">10.1080/10447318.2024.2414892</idno>
		<ptr target="https://doi.org/10.1080/10447318.2024.2414892" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Handbook of instructional communication: Rhetorical and relational perspectives</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Mottet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Richmond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Mccroskey</surname></persName>
		</author>
		<idno type="DOI">10.4324/9781315189864</idno>
		<ptr target="https://doi.org/10.4324/9781315189864" />
	</analytic>
	<monogr>
		<title level="j">Allyn &amp; Bacon</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Enhancing human-computer interaction in online education: A machine learning approach to predicting student emotion and satisfaction</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Mutawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sruthi</surname></persName>
		</author>
		<idno type="DOI">10.1080/10447318.2023.2291611</idno>
		<ptr target="https://doi.org/10.1080/10447318.2023.2291611" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page" from="8827" to="8843" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Teachers&apos; Vocal Expressions and Student Engagement in Asynchronous Video Learning</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Su</surname></persName>
		</author>
		<idno type="DOI">10.1080/10447318.2025.2474469</idno>
		<ptr target="https://doi.org/10.1080/10447318.2025.2474469" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<ptr target="https://openai.com/research/whisper" />
		<title level="m">Introducing Whisper</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Imagery and verbal processes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paivio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971">1971</date>
			<publisher>Holt, Rinehart, and Winston</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Learning from online instructional videos considering video presentation modes, technological comfort, and student characteristics</title>
		<author>
			<persName><forename type="first">M</forename><surname>Perry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F L</forename><surname>Azevedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Henricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Crues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhat</surname></persName>
		</author>
		<idno type="DOI">10.1080/10447318.2024.232891</idno>
		<ptr target="https://doi.org/10.1080/10447318.2024.232891" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Instructor social presence: Learners&apos; needs and a neglected component of the community of inquiry framework</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lowenthal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Social presence in online learning: Multiple perspectives on practice and research</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Whiteside</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Garrett</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">&amp;</forename><forename type="middle">K</forename><surname>Dikkers</surname></persName>
		</editor>
		<editor>
			<persName><surname>Swan</surname></persName>
		</editor>
		<imprint>
			<publisher>Stylus</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="86" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Core affect and the psychological construction of emotion</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Russell</surname></persName>
		</author>
		<idno type="DOI">10.1037/0033-295X.110.1.145</idno>
		<ptr target="https://doi.org/10.1037/0033-295X.110.1.145" />
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="145" to="172" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">A circumplex model of affect</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Russell</surname></persName>
		</author>
		<idno type="DOI">10.1037/h0077714</idno>
		<ptr target="https://doi.org/10.1037/h0077714" />
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1161" to="1178" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Self-determination theory and the facilitation of intrinsic motivation, social development, and well-being</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Deci</surname></persName>
		</author>
		<idno type="DOI">10.1037/0003-066X.55.1.68</idno>
		<ptr target="https://doi.org/10.1037/0003-066X.55.1.68" />
	</analytic>
	<monogr>
		<title level="j">American Psychologist</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="68" to="78" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Emotion recognition using speech processing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sharanyaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Mercy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">G</forename></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<idno type="DOI">10.1109/CONIT59222.2023.10205935</idno>
		<ptr target="https://doi.org/10.1109/CONIT59222.2023.10205935" />
	</analytic>
	<monogr>
		<title level="m">2023 3rd International Conference on Intelligent Technologies (CONIT)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Perceptual cues in nonverbal vocal expressions of emotion</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Sauter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Eisner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Scott</surname></persName>
		</author>
		<idno type="DOI">10.1080/17470211003721642</idno>
		<ptr target="https://doi.org/10.1080/17470211003721642" />
	</analytic>
	<monogr>
		<title level="j">The Quarterly Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="2251" to="2272" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Teachers&apos; Vocal Expressions and Student Engagement in Asynchronous Video Learning</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Su</surname></persName>
		</author>
		<idno type="DOI">10.1080/10447318.2025.2474469</idno>
		<ptr target="https://doi.org/10.1080/10447318.2025.2474469" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Vocal affect expression: A review and a model for future research</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
		<idno type="DOI">10.1037/0033-2909.99.2.143</idno>
		<ptr target="https://doi.org/10.1037/0033-2909.99.2.143" />
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="143" to="165" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">The interspeech 2018 computational paralinguistics challenge: Atypical &amp; self-assessed affect, crying &amp; heart beats</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Batliner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Steidl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Seppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename><surname>Sethu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2018-1548</idno>
		<ptr target="https://doi.org/10.21437/Interspeech.2018-1548" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech 2018</title>
		<meeting>Interspeech 2018</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="122" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">From unsuccessful to successful learning: Profiling behavior patterns and student clusters in Massive Open Online Courses</title>
		<author>
			<persName><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Dennen</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10639-023-12010-1</idno>
		<ptr target="https://doi.org/10.1007/s10639-023-12010-1" />
	</analytic>
	<monogr>
		<title level="j">Education and Information Technologies</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="5509" to="5540" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">The voice conveys specific emotions: Evidence from vocal burst displays</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Simon-Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Keltner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sauter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sinicropi-Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abramson</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0017810</idno>
		<ptr target="http://dx.doi.org/10.1037/a0017810" />
	</analytic>
	<monogr>
		<title level="j">Emotion</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="838" to="846" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Characterization of the affective norms for English words by discrete emotional categories</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Stevenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Mikels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>James</surname></persName>
		</author>
		<idno type="DOI">10.3758/bf03192999</idno>
		<ptr target="https://doi.org/10.3758/bf03192999" />
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1020" to="1024" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Evaluating the impact of pumping on groundwater level prediction in the chuoshui river alluvial fan using artificial intelligence techniques</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Lo</surname></persName>
		</author>
		<idno type="DOI">10.9781/ijimai.2024.04.002</idno>
		<ptr target="https://doi.org/10.9781/ijimai.2024.04.002" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Interactive Multimedia and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="28" to="37" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Applying machine learning technologies to explore students&apos; learning features and performance prediction</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Q</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnins.2022.1018005_Su</idno>
		<ptr target="https://doi.org/10.3389/fnins.2022.1018005_Su" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neuroscience</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">1018005</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Teachers&apos; Vocal Expressions and Student Engagement in Asynchronous Video Learning</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Su</surname></persName>
		</author>
		<idno type="DOI">10.1080/10447318.2025.2474469</idno>
		<ptr target="https://doi.org/10.1080/10447318.2025.2474469" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Deep Learning Methods in Internet of Medical Things for Valvular Heart Disease Screening System</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1109/JIOT.2021.3053420</idno>
		<ptr target="https://doi.org/10.1109/JIOT.2021.3053420" />
	</analytic>
	<monogr>
		<title level="j">IEEE Internet of Things Journal</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="16921" to="16932" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Predicting behavioral competencies automatically from facial expressions in real-time video-recorded interviews</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Hung</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11554-021-01071-5</idno>
		<ptr target="https://doi.org/10.1007/s11554-021-01071-5" />
	</analytic>
	<monogr>
		<title level="j">Journal of Real-Time Image Processing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1011" to="1021" />
			<date type="published" when="2021">2021b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Performance analysis of multiple aggregated acoustic features for environment sound classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Madani</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.apacoust.2019.107050</idno>
		<ptr target="https://doi.org/10.1016/j.apacoust.2019.107050" />
	</analytic>
	<monogr>
		<title level="j">Applied Acoustics</title>
		<imprint>
			<biblScope unit="volume">158</biblScope>
			<biblScope unit="page">107050</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Enhancing learner affective engagement: The impact of instructor emotional expressions and vocal charisma in asynchronous video-based online learning</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Hung</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10639-024-12956-w</idno>
		<ptr target="https://doi.org/10.1007/s10639-024-12956-w" />
	</analytic>
	<monogr>
		<title level="j">Education and Information Technologies</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">A comprehensive understanding of code-mixed language semantics using hierarchical transformer</title>
		<author>
			<persName><forename type="first">T</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chakraborty</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCSS.2024.3360378</idno>
		<ptr target="https://doi.org/10.1109/TCSS.2024.3360378" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Social Systems</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="4139" to="4148" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Paralanguage: A first approximation</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Trager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Studies in Linguistics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="1958">1958</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Discriminating emotions in the valence dimension from speech using timbre features</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tursunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Pang</surname></persName>
		</author>
		<idno type="DOI">10.3390/app9122470</idno>
		<ptr target="https://doi.org/10.3390/app9122470" />
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">2470</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Student and instructor perceptions of engagement after the rapid online transition of teaching due to COVID-19</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Koralesky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Sciences Education</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Teachers&apos; Vocal Expressions and Student Engagement in Asynchronous Video Learning</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Su</surname></persName>
		</author>
		<idno type="DOI">10.1080/10447318.2025.2474469</idno>
		<ptr target="https://doi.org/10.1080/10447318.2025.2474469" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">To be expressive or not: the role of teachers&apos; emotions in students&apos; learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2021.737310</idno>
		<ptr target="https://doi.org/10.3389/fpsyg.2021.737310" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">737310</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Instructors&apos; expressive nonverbal behavior hinders learning when learners&apos; prior knowledge is low</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2022.810451</idno>
		<ptr target="https://doi.org/10.3389/fpsyg.2022.810451" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Affective events theory: A theoretical discussion of the structure, causes and consequences of affective experiences at work</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cropanzano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Research in organizational behavior: An annual series of analytical essays and critical reviews</title>
		<editor>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Staw</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Cummings</surname></persName>
		</editor>
		<imprint>
			<publisher>Elsevier Science/JAI Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">A Novel Speech Emotion Model Based on CNN and LSTM Networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACAIT56212.2022.10137926</idno>
		<ptr target="https://doi.org/10.1109/ACAIT56212.2022.10137926" />
	</analytic>
	<monogr>
		<title level="m">2022 6th Asian Conference on Artificial Intelligence Technology (ACAIT)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Deconstructing self-regulated learning in MOOCs: In search of help-seeking mechanisms</title>
		<author>
			<persName><forename type="first">K</forename><surname>Vilkova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shcheglova</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10639-020-10244-x</idno>
		<ptr target="https://doi.org/10.1007/s10639-020-10244-x" />
	</analytic>
	<monogr>
		<title level="j">Education and Information Technologies</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="17" to="33" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Online learning environment and student engagement: The mediating role of expectancy and task value beliefs</title>
		<author>
			<persName><forename type="first">H</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ho</surname></persName>
		</author>
		<idno type="DOI">10.1007/s13384-024-00689-1</idno>
		<ptr target="https://doi.org/10.1007/s13384-024-00689-1" />
	</analytic>
	<monogr>
		<title level="j">Australian Educational Researcher</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Would it be better if instructors technically adjust their image or voice in online courses? Impact of the way of instructor presence on online learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2021.746857</idno>
		<ptr target="https://doi.org/10.3389/fpsyg.2021.746857" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">746857</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Effectiveness of synchronous and asynchronous online learning: A metaanalysis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<idno type="DOI">10.1080/10494820.2023.2197953</idno>
		<ptr target="https://doi.org/10.1080/10494820.2023.2197953" />
	</analytic>
	<monogr>
		<title level="j">Interactive Learning Environments</title>
		<imprint>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Teachers&apos; Vocal Expressions and Student Engagement in Asynchronous Video Learning</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Su</surname></persName>
		</author>
		<idno type="DOI">10.1080/10447318.2025.2474469</idno>
		<ptr target="https://doi.org/10.1080/10447318.2025.2474469" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">The students&apos; flow experience with the continuous intention of using online English platforms</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khan</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2021.807084</idno>
		<ptr target="https;//doi.org/10.3389/fpsyg.2021.807084" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">807084</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Construction and application of English-Chinese multimodal emotional corpus based on artificial intelligence</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.1080/10447318.2023.2169526</idno>
		<ptr target="https://doi.org/10.1080/10447318.2023.2169526" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Teachers&apos; Vocal Expressions and Student Engagement in Asynchronous Video Learning</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Su</surname></persName>
		</author>
		<idno type="DOI">10.1080/10447318.2025.2474469</idno>
		<ptr target="https://doi.org/10.1080/10447318.2025.2474469" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
