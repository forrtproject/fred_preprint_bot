<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Running Head: IMAGE METRICS FOR GLOSS DISCRIMINATION Gloss discrimination: Towards an image-based perceptual model</title>
				<funder ref="#_C5DTmjs">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_SYhTBpw">
					<orgName type="full">Deutsche Forschungsgemeinschaft (DFG, German Research Foundation</orgName>
				</funder>
				<funder ref="#_cQd8Cwy #_RR4FVvD">
					<orgName type="full">European Research Council</orgName>
					<orgName type="abbreviated">ERC</orgName>
				</funder>
				<funder ref="#_vu8JZdJ">
					<orgName type="full">H2020 Marie Skłodowska-Curie Actions</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Jacob</forename><forename type="middle">R</forename><surname>Cheeseman</surname></persName>
							<email>jacob.cheeseman@psychol.uni-giessen.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Experimental Psychology</orgName>
								<orgName type="institution">Justus Liebig University Giessen</orgName>
								<address>
									<addrLine>Otto-Behaghel-Str. 10F</addrLine>
									<postCode>35394</postCode>
									<settlement>Giessen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">James</forename><forename type="middle">A</forename><surname>Ferwerda</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Carlson Center for Imaging Science</orgName>
								<orgName type="institution">Rochester Institute of Technology</orgName>
								<address>
									<addrLine>54 Lomb Memorial Dr</addrLine>
									<postCode>14623</postCode>
									<settlement>Rochester</settlement>
									<region>New York</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Takuma</forename><surname>Morimoto</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Experimental Psychology</orgName>
								<orgName type="institution">Justus Liebig University Giessen</orgName>
								<address>
									<addrLine>Otto-Behaghel-Str. 10F</addrLine>
									<postCode>35394</postCode>
									<settlement>Giessen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Roland</forename><forename type="middle">W</forename><surname>Fleming</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Experimental Psychology</orgName>
								<orgName type="institution">Justus Liebig University Giessen</orgName>
								<address>
									<addrLine>Otto-Behaghel-Str. 10F</addrLine>
									<postCode>35394</postCode>
									<settlement>Giessen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Center for Mind, Brain and Behavior (CMBB)</orgName>
								<address>
									<addrLine>Hans-Meerwein-Str. 6</addrLine>
									<postCode>35032</postCode>
									<settlement>Marburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Running Head: IMAGE METRICS FOR GLOSS DISCRIMINATION Gloss discrimination: Towards an image-based perceptual model</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">791601E9BD541782F2B3A07E9BCA60E2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-10-22T06:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>material perception</term>
					<term>surface reflectance</term>
					<term>thresholds</term>
					<term>JNDs</term>
					<term>MacAdam&apos;s ellipses</term>
					<term>color constancy</term>
					<term>texture</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Gloss is typically considered the perceptual counterpart of a surface's specular reflectance characteristics, much as color is the perceptual counterpart of a surface's diffuse reflectance spectrum. In many contexts, it is tempting to ask how discriminable two surfaces are on the basis of their reflectance properties. Yet, as we argue here, this is a poorly-posed question, as factors other than reflectance (e.g., lighting, shape, viewpoint) can have substantial effects on how discriminable two images of glossy surfaces are to human participants. This fundamental difficulty with predicting gloss discrimination, whether from a physical measurement or from proximal image data, has so far hobbled efforts to establish a rigorously defined perceptual standard for surface gloss, similar to those that exist for color. Here, we propose an experimental framework for making this problem tractable, starting from the premise that any perceptual standard of gloss discrimination must account for how distal scene variables influence the statistics of proximal image data. With this goal in mind, we rendered a large set of images in which shape, illumination, viewpoint, and surface roughness were varied. For each combination of viewing conditions, a fixed difference in surface roughness was used to create a pair of images showing the same object (from the same viewpoint and under the same lighting) with high and low gloss. Human participants (N=150) completed a paired comparisons task in which they were required to select image pairs with the largest apparent gloss difference.</p><p>Importantly, rankings of the scenes derived from these judgments represent differences in perceived gloss independent of physical reflectance. We find that these rankings are remarkably consistent across participants, and are well predicted by a straightforward Visual Differences Predictor <ref type="bibr" target="#b12">(Daly, 1992;</ref><ref type="bibr" target="#b41">Mantiuk et al., 2023)</ref>. This allows us to estimate reasonable bounds on visual discriminability for a given surface across a wide range of viewing conditions. This has potential applications in both vision science, computer graphics and industrial contexts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Determining visual thresholds for proximal stimulus variables-such as luminance <ref type="bibr" target="#b49">(Nachmias &amp; Kocher, 1970)</ref>, wavelength <ref type="bibr" target="#b58">(Pokorny &amp; Smith, 1970)</ref>, contrast <ref type="bibr" target="#b8">(Campbell &amp; Robson, 1968)</ref>, orientation <ref type="bibr" target="#b2">(Appelle, 1972)</ref> or spatial frequency <ref type="bibr" target="#b7">(Campbell et al., 1970)</ref>-is conceptually straightforward, with well-defined psychophysical methods underpinned by signal-detection theory <ref type="bibr" target="#b29">(Green &amp; Swets, 1966</ref>). Yet, it also often happens that we want to know how well participants can distinguish between stimuli that differ in some distal physical property, such as surface gloss. For example, in the pigment and paint industry, it is often necessary to manufacture parts with matching surface appearance, which would require differences in appearance to be 'within tolerance', i.e., below threshold (for a recent review and commentary, see European Coatings Dossier on <ref type="bibr">Testing and Measuring, 2019)</ref>. Both R&amp;D and quality control require some means to establish whether two samples are perceptually indistinguishable in terms of their gloss. Ideally, it should be possible to do this on the basis of a physical measurement applied to the surfaces. Similarly, computer graphics researchers often need to know how sensitive participants are to reflectance parameters, to determine, for example, whether a given approximation is acceptable <ref type="bibr" target="#b30">(Greenberg et al., 1997;</ref><ref type="bibr" target="#b57">Pellacini et al., 2000)</ref>. And in vision research, establishing discrimination thresholds for reflectance properties would also be useful for characterizing human perceptual abilities and constraining theories of gloss perception.</p><p>However, although the idea of measuring discrimination thresholds for gloss seems intuitive enough, there is a fundamental challenge, due to the fact that surface reflectance is a distal scene property, rather than a proximal stimulus variable like luminance or cone excitation ratios. The images that form the basis of any threshold measurements are the result of complex interactions between multiple distal scene factors in addition to the reflectance: the illumination striking the surface, the surface's shape and the observer's viewpoint. It is not possible to 'leave out' any of these factors; designating values for each factor is a prerequisite for creating the images required for the experiment. Nonetheless, lighting, shape and viewpoint can have potentially enormous effects on the measured thresholds. Under one set of conditions, a given difference in surface reflectance can significantly alter many pixels in the image, yielding very low threshold estimates (Figure <ref type="figure" target="#fig_0">1A</ref>). Yet under other view conditions, the exact same difference in reflectance could have little to no effect on the image, and therefore yield infinite threshold estimates (Figure <ref type="figure" target="#fig_0">1B</ref>). Thus, although we can experimentally determine whether any two images of surfaces are perceptually distinguishable, we do not know how the results will Running Head: IMAGE METRICS FOR GLOSS DISCRIMINATION generalize to other conditions. In concrete terms: gloss thresholds measured under one illumination may be useless for determining whether two surfaces are perceptually distinguishable under a different illumination. The same holds for changes in shape or even viewpoint. Here, we seek to provide an approach to circumvent this challenge to yield 'reasonable bounds' on discrimination thresholds for gloss. The practical needs of industry have driven the development of numerous color spaces over the past century. One of the most well-known, the CIE 1931 XYZ color space, was computed from simple color-matching experiments, where participants adjusted lights of different wavelengths to have identical luminosity. Later in 1942, David MacAdam published the results of similar experiments that showed how sensitivity to differences in chromaticity vary within the 1931 CIE XYZ color space. Errors in color-matching performance were found to vary systematically within the space, thus indicating that equal increments within the space do not correspond to equal differences in perceived color. This motivated later researchers to propose color spaces that partially correct for such distortions (e.g., CIELAB, CIECAMO2, CIECAM16). A perceptually-uniform space for gloss would be especially useful for industrial applications, where there is a need to maintain a consistent material appearance throughout the manufacturing process. However, such a space has remained elusive, owing to the multidimensional nature of gloss, and a lack of agreement about which dimensions of gloss are relevant for particular applications. The six dimensions of perceived gloss set out by <ref type="bibr" target="#b36">Hunter and Harold (1987)</ref> have been highly influential, and other results suggest that two to four dimensions account for nearly all variance in subjective comparisons of gloss, at least for the range of surfaces that were considered <ref type="bibr" target="#b37">(Kildau, 2016;</ref><ref type="bibr" target="#b57">Pellacini et al., 2000;</ref><ref type="bibr" target="#b60">Prokott, 2016;</ref><ref type="bibr" target="#b70">Toscani et al., 2020)</ref>. However, the number and nature of these dimensions will depend on the set of appearances chosen for testing, and the intended application. While much attention has been given to understanding biases in gloss perception (i.e., influences of lighting or shape on the overall level of gloss; see <ref type="bibr" target="#b23">(Fleming et al., 2003;</ref><ref type="bibr" target="#b46">Motoyoshi &amp; Matoba, 2012;</ref><ref type="bibr" target="#b51">Nishida &amp; Shinya, 1998;</ref><ref type="bibr" target="#b68">te Pas &amp; Pont, 2005;</ref><ref type="bibr" target="#b71">Vangorp et al., 2007)</ref>, here we seek to define conditions for measuring sensitivity to changes in surface reflectance, paving the way for standards that could serve both industry and vision researchers.</p><p>Previous researchers have attempted to characterize gloss perception using a variety of experimental and analytical frameworks, traditionally with real surfaces in controlled lighting environments. For example, <ref type="bibr" target="#b54">Obein et al. (2004)</ref> assessed the relationship between the perceived gloss of real surfaces and instrumental measurements of specular reflection using Maximum Likelihood Difference Scaling (MLDS; <ref type="bibr" target="#b40">Maloney &amp; Yang, 2003)</ref>. Although they did not find statistical evidence that a single scale could be used for both of the incident angles tested, one of their central claims is that participants' judgments of gloss exhibit constancy under changes in viewing angle. However, the evidence for gloss constancy is rather mixed <ref type="bibr" target="#b9">(Chadwick &amp; Kentridge, 2015;</ref><ref type="bibr">Doerschner, Boyaci, et al., 2010;</ref><ref type="bibr" target="#b17">Faul, 2019;</ref><ref type="bibr" target="#b23">Fleming et al., 2003;</ref><ref type="bibr" target="#b55">Olkkonen &amp; Brainard, 2011)</ref>, and it is not obvious how instrumental measurements can possibly generalize much beyond the original scene configuration, especially when shape and illumination are varied in addition to changes in viewpoint. It has long been known within the field that measuring the proportion of reflected light at a sparse sampling of incident angles is an unreliable predictor of perceived gloss <ref type="bibr" target="#b32">(Harrison, 1945)</ref>. Nevertheless, despite well-documented shortcomings, 'gloss meters' based on this principle remain the industry standard, in part because such measurements can be collected quickly, and better methods are not widely available. On the other extreme, one can measure reflected light at many more incident angles, covering the entire hemisphere above the surface plane, and use this data to estimate a bidirectional reflectance distribution function (BRDF; <ref type="bibr" target="#b50">Nicodemus et al., 1977)</ref>. Until very recently, measuring BRDFs has been too costly and inefficient for widespread practical application <ref type="bibr" target="#b21">(Filip &amp; Kolafová, 2019)</ref>. Despite these recent technical advances, however, it is unlikely that our perceptions of gloss are based on a BRDF-like representation of surface reflectance. Indeed, we have argued that the brain generates heuristic representations, or 'statistical appearance models' of gloss appearance over a range of typical viewing conditions <ref type="bibr" target="#b22">(Fleming, 2014;</ref><ref type="bibr" target="#b24">Fleming &amp; Storrs, 2019)</ref>.</p><p>Advances in computer graphics simulation over the previous three decades have allowed vision researchers to apply these technologies to the study of gloss perception. For example, the study by <ref type="bibr" target="#b57">Pellacini et al. (2000)</ref> is notable for its application of Multidimensional Scaling <ref type="bibr">(MDS;</ref><ref type="bibr">Borg &amp; Groenen, 2005)</ref> to judgments of glossy spheres shown in simulated illumination.</p><p>With this data, they constructed a perceptually-uniform gloss space consisting of two dimensions (contrast and distinctness of the reflected image), which they later used to derive just-noticeable differences (JNDs) in gloss <ref type="bibr" target="#b20">(Ferwerda et al., 2001)</ref>. While these authors were the first to apply this approach to understand gloss perception, the generalizability of their results is limited to the set of appearances used to create the space <ref type="bibr" target="#b25">(Fores et al., 2014)</ref>. Given that shape and illumination strongly influence material appearance <ref type="bibr" target="#b71">(Vangorp et al., 2007)</ref>, what is a sufficiently-diverse set of conditions for the purpose of characterizing gloss sensitivity? In the limit, iteratively rendering many combinations of illumination, shape, viewpoint, and surface reflectance will yield a set of images that includes the 'typical' appearance of glossy surfaces across multiple material categories. However, in an industrial manufacturing context (e.g., quality control for surface coatings), often the goal is to measure appearance changes between multiple copies of a single material formulation. Our previous study, <ref type="bibr" target="#b10">Cheeseman et al. (2020)</ref>, investigated gloss perception in such 'symmetric' viewing conditions, where we measured sensitivity to differences along a single perceptually-uniform dimension (specular reflectance),</p><p>showing that even with all other variables held constant, sensitivity varies significantly with stimulus magnitude. Here, we pursue a complimentary approach -holding surface reflectance constant while varying illumination, shape and viewpoint -in order to identify viewing conditions where estimates of sensitivity to differences in surface reflectance have optimal generalizability.</p><p>The current study therefore seeks to establish a framework for characterizing sensitivity to gloss per se, and perhaps, to other qualities of material appearance. To anticipate, we show that under symmetric conditions-when all scene parameters except reflectance are held constant-gloss discrimination reduces to an image discrimination task that can be well predicted by extant image-discrimination models. As a result, we can predict the variations in gloss discrimination that occur as various scene parameters are altered. This provides a route into defining 'reasonable bounds' on gloss discrimination across viewing conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 1: Predicting apparent gloss differences across viewing conditions</head><p>Many studies have assessed gloss perception by varying the reflectance of surfaces under different viewing conditions. Here, we present participants with a fixed difference in surface reflectance while varying illumination, shape and viewpoint, with the intent of identifying an image metric that can predict perceived differences of gloss across viewing conditions. Importantly, because the difference in reflectance is identical across conditions, any visible differences in gloss are due to extrinsic distal variables that are independent of intrinsic surface reflectance. If an image metric can predict which viewing conditions tend to accentuate or obscure apparent gloss, this could provide a principled basis for establishing tolerances on gloss sensitivity in real world conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>One-hundred-fifty adults (79 males and 71 females; age range: 18 to 68 years; M = 27 years, SD = 8 years) with normal or corrected-to-normal visual acuity participated in the experiment and were paid €10 per hour. Participants were recruited online using Prolific (prolific.co); they were required to have native fluency in English, and a desktop or laptop computer. All experimental procedures were approved by the Justus Liebig University Giessen Psychology Department Ethics Board and conformed with the guidelines of the American Psychological Association (Version 2017) and the Declaration of Helsinki (Version 2013, excluding pre-registration). Informed consent was obtained from all participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stimuli</head><p>Stimulus images were created with the Mitsuba physically-based renderer (Jakob, 2010).</p><p>High dynamic range, linear RGB renderings were tone-mapped to low dynamic range sRGB images using the method described in <ref type="bibr" target="#b63">Reinhard et al. (2002)</ref>. Parameters controlling the overall luminance (key) and clipping of highlights (burn) in the image were set to the Mitsuba-default values of 0.18 and 0, respectively. The 720 x 720 pixel images subtend 26 degrees of visual angle at a viewing distance of 50 cm, although viewing distance was not controlled in the online experiment. Instead, participants were instructed to place a credit card (or another card of equivalent size) on their display screen, and adjust the length and width of a rectangle to match the size of the card. This measurement was used to calibrate the size of the images such that  The basic scene (e.g., see Figure <ref type="figure" target="#fig_2">2D</ref>) includes a central target object seated on a marble-textured pedestal under natural illumination. A set of 10 target objects (Figure <ref type="figure" target="#fig_2">2A</ref>) was selected that span a variety of surface features that are more or less likely to accentuate gloss appearance.. For example, some objects featured smoothly curved surfaces (e.g., car or turtle), while others featured rough or discontinuous surfaces (e.g., cabbage or plant). All objects in the scene were rendered with an improved version of the Ward BRDF model that obeys energy conservation and has better physical accuracy at grazing angles <ref type="bibr" target="#b28">(Geisler-Moroder &amp; Dür, 2010)</ref>.</p><p>The model has three parameters that control the specular reflectance (ρ s ), diffuse reflectance (ρ d ), and roughness (α) of a surface. The apparent glossiness of the target object was varied with two levels of surface roughness (α = 0.01, 0.19) while specular and diffuse reflectance were fixed (ρ s = 0.066, ρ d = 0.1, 0.3, 0.1). A set of 10 high dynamic range environment maps (Figure <ref type="figure" target="#fig_2">2C</ref>) was selected that featured a variety of indoor and outdoor illumination conditions. For example, environments with direct lighting can lead to bright, distinct highlights on a reflecting surface, whereas environments with diffuse lighting usually do not. Similarly, 10</p><p>Running Head: IMAGE METRICS FOR GLOSS DISCRIMINATION evenly-distributed viewpoints (Figure <ref type="figure" target="#fig_2">2B</ref>) were sampled from the vertices of a hemi-icosphere positioned above the target object, thus providing a variety of high and low viewing angles for each target object. The combination of 10 shapes, 10 illuminations, and 10 viewpoints produced a set of 1000 scenes. A subset of 100 scenes was randomly sampled (without replacement) from this larger set for use in Experiment 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure</head><p>The experiment was created in PsychoPy v2021.2.3 <ref type="bibr" target="#b56">(Peirce &amp; Macaskill, 2018)</ref> and run on the Pavlovia experiment hosting platform (pavlovia.org). To avoid requiring participants to pre-load a large set of images at the start of the experiment, 100 scenes were further divided into 10 subsets of 10 scenes. Separate groups of 15 participants were recruited to judge each subset of scenes; these can also be understood as 10 independent experiments with separate groups of participants and stimuli. The first stage of the experiment required participants to complete 20 practice trials in a simplified version of the task using luminance patches rather than rendered images. For each practice trial, participants were instructed to select the left or right pair of images that showed the larger difference in luminance, inspired by the Method of Quadruplets from Maximum Likelihood Difference Scaling (MLDS). Seven participants failed to correctly judge these exaggerated suprathreshold differences in luminance with at least 90% accuracy during the practice phase, and were excluded from the analysis, as it was assumed that they either did not understand the task instructions, or a technical problem impeded their performance. The task remained the same during the experimental trials, except that participants selected the left or right pair of images (rather than luminance patches) in which there is a larger difference in apparent gloss of the target object. Additionally, to account for outliers, participants who completed the experimental trials were excluded if their responses produced an extremely low correlation with other participants' judgments of the same set of images. Frequencies representing how often each scene is chosen were calculated for each participant, and compared across participants. If the average correlation between one participant's frequencies and those of the other participants exceeds the Interquartile Range of these average correlations (multiplied by 1.5), this was considered an outlier, and the participant's data was excluded from the analysis. Twenty-two outliers were excluded in total. In summary, separate groups of 15 participants judged separate sets of 10 scenes. For each set of scenes, 45 unique scene pairs were presented in a random order across 6 repetitions.</p><p>One-hundred-fifty participants collectively completed a total of 40,500 trials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image metrics</head><p>HDR-VDP-3 <ref type="bibr" target="#b41">(Mantiuk et al., 2023</ref>) is a popular metric for predicting the visibility of image differences and assessing the impacts of compression or other image processing operations on image quality. In our analysis, HDR-VDP-3 was used to predict perceived differences between test and reference images. The metric was applied in a 'side-by-side' task mode, which is appropriate for comparing two images displayed adjacent to each other. Input images were encoded in 'sRGB-display' format to correspond with standard color images displayed on an sRGB monitor, with peak luminance calibrated to 100 cd/m 2 and a black level at 1 cd/m 2 . The images were processed with a high angular resolution of 120 pixels per visual degree, appropriate for a close viewing distance or high-resolution display. For the modulation transfer function (MTF), which models the scattering of light in the eye's optics-referred to as glare-we chose to bypass this step by setting the 'mtf' option to 'none'. This decision was made because the glare effect, while significant for high-contrast HDR images, adds computational complexity that was not essential for our purposes. The output of HDR-VDP-3 provided us with a probability map of detection for each pixel (P_map), with values ranging from 0 to 1. We computed the mean of this probability map to represent the visibility metric for each stimulus condition, allowing us to assess the average detectability of image differences across the entire image.</p><p>Although HDR-VDP-3 also provides a single valued probability of detection (P_det) for the whole image, we found that the average of the probability map (P_map) was a better predictor of the human data.</p><p>Unlike HDR-VDP-3, which can predict visible differences from full sRGB images, our measurements of contrast, coverage, sharpness, and skewness (similar to previous studies; see <ref type="bibr" target="#b43">(Marlow et al., 2012;</ref><ref type="bibr" target="#b47">Motoyoshi et al., 2007)</ref> were derived by first converting the sRGB images into luminance images (calibrated to cd/m 2 ). We then eliminated the diffuse component, thus ensuring the metrics were computed only from specular reflections. Subsequently, to remove reflections from within the object or from the pedestal, we thresholded the specular image.</p><p>Pixels exceeding a certain intensity threshold-determined as a percentage (k%) of the highest intensity, with k values set at 0, 1, 3, 5, 10, 20, 30, and 40-were retained. The k values were selected to evaluate a range of intensity thresholds. We then decomposed the thresholded highlight image into eight sub-band images through Gaussian band-pass filtering across a range of frequencies. This allowed us to capture the effects of spatial frequency modulation on the perception of gloss <ref type="bibr" target="#b5">(Boyadzhiev et al., 2015)</ref>. The contrast for each frequency band, as well as for the combined frequency image, was determined by calculating the root-mean-square-error (RMSE) of the pixel intensities. Alongside contrast, we also evaluated metrics for highlight coverage and sharpness, which were calculated from the thresholded highlight images.</p><p>Coverage is the proportion of the object area that is covered by specular reflections, providing an indication of the extent of gloss across a surface. Sharpness is defined by the rate of change in luminance, measured using the slope of the local magnitude spectrum and local maximum total variation (TV) to emphasize areas of the object where intensity transitions are most pronounced <ref type="bibr" target="#b73">(Vu et al., 2012)</ref>. The values of k (for contrast, coverage, and sharpness) and spatial frequency bands (for contrast only) were varied to determine values that produced the best correlation with the human data. All of these image metrics were calculated with the scene background masked, leaving only the object region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The scene that most participants judged to depict the largest visible difference in apparent gloss is shown in Figure <ref type="figure" target="#fig_5">3</ref>  stimuli. Apparent differences in gloss caused by variations in lighting, shape, and viewpoint were well predicted by this image metric, which produced a correlation of .81 with the behavioral data (i.e., how often each scene was selected for showing a larger gloss difference). Other image metrics were also evaluated for their ability to predict this data, including sub-band contrast, highlight coverage, highlight sharpness, and the skewness of the pixel intensity histogram.</p><p>These metrics have all been shown to correlate with gloss appearance in experimental conditions <ref type="bibr" target="#b1">(Anderson &amp; Kim, 2009;</ref><ref type="bibr" target="#b38">Kim &amp; Anderson, 2010;</ref><ref type="bibr" target="#b45">Morimoto et al., 2023;</ref><ref type="bibr" target="#b47">Motoyoshi et al., 2007;</ref><ref type="bibr" target="#b65">Schmid et al., 2023)</ref>. A Generalized Linear Model (GLM) was used to assess the relationship between these metrics and human judgments. The model included HDR-VDP-3, contrast, coverage, sharpness, and skewness as predictor variables. The analysis revealed that only HDR-VDP-3 (coefficient = 0.1215, standard error = 0.013, z = 9.603, p &lt; 0.0001) and</p><p>Contrast (coefficient = 0.2645, standard error = 0.054, z = 4.942, p &lt; 0.0001) were significantly correlated with human judgments (see Figure <ref type="figure" target="#fig_6">4</ref>). Although HDR-VDP-3 is optimized for the prediction of psychophysical data and is not meant to be a biologically plausible model of the   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 2: Image metric validation in a lab-based control experiment</head><p>We have a metric (HDR-VDP-3) that predicts human judgments of gloss differences across variations in lighting, shape, and viewpoint. However, these judgments were collected from participants over the internet in uncontrolled viewing conditions with unknown display characteristics, viewing distance, and ambient illumination, which may have influenced our results (e.g., see <ref type="bibr" target="#b31">Haghiri et al., 2019)</ref>. The purpose of this second experiment was therefore to test whether the predictions of the model are also valid for controlled laboratory conditions using scenes selected from Experiment 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>Twenty-two adults (7 males and 15 females; age range: 20 to 39 years; M = 27 years, SD = 5 years) with normal or corrected-to-normal visual acuity participated in the experiment and were paid €12 per hour. Participants were recruited from the university student population.</p><p>All experimental procedures were approved by the Justus Liebig University Giessen Psychology Department Ethics Board and conformed with the guidelines of the American Psychological Association (Version 2017) and the Declaration of Helsinki (Version 2013, excluding pre-registration). Informed consent was obtained from all participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stimuli</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Running Head: IMAGE METRICS FOR GLOSS DISCRIMINATION</head><p>The distribution of VDP predictions for the full set of images is shown in Figure <ref type="figure" target="#fig_8">5</ref>. As previously mentioned, we used a subset of 100 scenes for the first experiment (red bars). Now, for the second experiment, we selected two new scenes from the full 1,000 scenes (dark blue bars). Specifically, we chose two scenes where gloss sensitivity is predicted to be low or high, corresponding to the 10th and 90th percentile values of the distribution, respectively. For each scene, we rendered new images with finer differences in surface roughness using the log-spaced values illustrated in Figure <ref type="figure" target="#fig_9">6</ref>    white point and a gamma setting of 2.2. During each trial, participants were presented with two images arranged side-by-side, and their task was to select the (left or right) image that showed a green target object with a higher degree of gloss. Other than this difference in the apparent gloss of the target object, all other scene variables were identical between the two images. This two-alternative forced-choice (2AFC) task replicates the procedure used in our previous work <ref type="bibr" target="#b11">(Cheeseman et al., 2021)</ref>. The first stage of the experiment required participants to complete 10 practice trials with a pair of images (selected from Experiment 1) showing a clearly visible gloss difference. Feedback was provided during the practice trials to indicate whether the object in the chosen image had a higher gloss (i.e., lower roughness). All participants were able to complete the practice trials without difficulty and were allowed to proceed to the next phase of the experiment. In this second phase, participants performed the same 2AFC task with images from two scenes that had not been shown in Experiment 1. Image pairs from each scene were presented repeatedly (40 times) in random order, following the Method of Constant Stimuli; that is, for each scene, a standard image was presented alongside one of ten comparison images Running Head: IMAGE METRICS FOR GLOSS DISCRIMINATION (see Stimuli section above). On average, the experiment lasted about 40 minutes, with a rest period at the halfway point. Collectively, our 22 participants completed 17,600 trials of this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Figure <ref type="figure">7</ref> shows the minimum and maximum reflectance images for the 10th and 90th percentile scenes, along with their corresponding psychometric functions calculated from pooled observer data using psignifit 4 <ref type="bibr" target="#b66">(Schütt et al., 2016)</ref>. HDR-VDP-3 predicts lower sensitivity for the 10th percentile scene, and higher sensitivity for the 90th percentile scene, respectively. Note that in Experiment 1, a fixed difference in roughness was predicted to be more visible with the combination of lighting, shape, and viewpoint shown in the 90th percentile scene. In the current experiment, smaller differences in roughness were used to measure discrimination performance in controlled laboratory conditions. The significantly steeper slope of the psychometric function Given that HDR-VDP-3 has been validated in controlled laboratory conditions, the model predictions can be used to search for combinations of lighting, shape, and viewpoint that should yield the highest sensitivity, shown in Figure <ref type="figure">8</ref>. To evaluate the relative effect of lighting, shape, and viewpoint on the model predictions, we used a random forest model <ref type="bibr" target="#b6">(Breiman, 2001;</ref><ref type="bibr" target="#b33">Hastie et al., 2009)</ref>. Random forests are particularly suited for this analysis because they do not assume linear relationships and are better able to predict continuous values using categorical variables. In this context, the target variable (mean P_map predicted by HDR-VDP-3) is estimated based on paths taken through a series of decision trees constructed from the categorical variables. The random forest algorithm considers every possible division of the VDP prediction values for levels of each categorical variable, and calculates which path will result in the largest decrease in variance of these values. Feature importance scores from a random forest model indicate which categorical variables contribute most to estimating the target variable across all the decision trees. Figure <ref type="figure">9</ref> shows importance scores for individual lighting maps, object shapes, and viewpoints, and the overall importance of each variable. Furthermore, if we calculate descriptive statistics on the predictions of HDR-VDP-3, as shown in Figure <ref type="figure" target="#fig_0">10</ref>, these can be used to estimate upper and lower bounds of predicted sensitivity for specific shapes (across lighting and viewpoint), or any other combination of these variables. scenes that included the lighting, shape, or viewpoint that produced the highest mean VDP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>General Discussion</head><p>In Experiment 1, we rendered a set of images with a fixed difference in surface reflectance (roughness) for a variety of different lighting conditions, object shapes, and viewpoints. We collected human judgments of these images in an online experiment, finding that participants were highly consistent in their ranking of gloss differences across these viewing</p><p>conditions. An existing model that predicts the visibility of image differences, HDR-VDP-3, was able to predict the human ranking of the gloss differences in our image set to a surprising degree -in fact, well within the range of inter-participant correlations (see Figure <ref type="figure" target="#fig_6">4</ref>). This</p><p>indicates that HDR-VDP-3 performed as well as any image-computable model could, given the variance in our data. Interestingly, in this case, similar performance can also be achieved by measuring the contrast of the specular term. In Experiment 2, HDR-VDP-3 was used to select two scenes from our full image set, representing combinations of lighting, shape, and viewpoint Running Head: IMAGE METRICS FOR GLOSS DISCRIMINATION that represent opposing predictions of the model, leading to lower or higher sensitivity to the same difference in physical reflectance. The model predictions were validated in controlled laboratory conditions, evidenced by a significant difference in gloss sensitivity (see Figure <ref type="figure">7</ref>).</p><p>These model predictions were then used to estimate the relative contribution of specific viewing conditions to gloss sensitivity. This provides a first step towards characterizing the impact of viewing factors on gloss discrimination, so that 'reasonable bounds' on JNDs can be established.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Towards 'reasonable bounds' on JNDs for surface reflectance</head><p>Our study suggests that HDR-VDP-3 can predict how gloss discriminability varies across a range of viewing conditions-at least, about as well as individual participants predict one another. This lays the foundations for automatically establishing JNDs 'within reasonable bounds' for materials with particular appearances (e.g., coatings with particular formulations).</p><p>Here we outline the approach and describe some of the additional open research questions that would need to be resolved to develop a working system.</p><p>The basic logic of the approach runs as follows. The JND for a given surface reflectance characteristic is defined as the smallest magnitude change in the physical BRDF of the surface that can be perceptually detected. As noted throughout this study, this value can vary due to extrinsic factors-lighting, object shape and viewpoint. Our goal is to predict the range of values the JND can take across 'reasonable' changes in viewing conditions.</p><p>As there are potentially an infinite number of possible changes to the BRDF, let us limit ourselves to the case where we wish to evaluate the JND for a specific kind of reflectance change. A simple case would be when varying a single parameter of an analytic BRDF model (as in our experiments), although any change that can be summarized with a single number suffices. For example, suppose a paint manufacturer wishes to determine tolerance bounds on a particular parameter of the paint formulation or manufacture process, such as the temperature at which a coating should be applied, or the duration of grinding of a particular ingredient in the paint. As long as the parameter leads to a smooth and systematic change in the BRDF-e.g., by changing the specular lobe-in a predictable way, then we can use images of samples with different parameter values viewed under constant conditions to estimate a JND with HDR-VDP-3 (or some other image-computable image-difference metric).</p><p>To be more precise, the assumption is that (small) changes in the manufacture parameter shift the BRDF along a specific vector in the high-dimensional space of all BRDFs (e.g., making the specular lobe broader in a particular way). The goal of determining the tolerance for the parameter then becomes the goal of determining the magnitude of that vector for which two samples can just be discriminated.</p><p>In the unusual circumstances that the BRDF will be seen exclusively under fixed viewing conditions (i.e., a single, specific shape, under fixed specific lighting, from a specific viewpoint), then it should be sufficient to image samples (e.g., render or photograph) with a few values of the parameter under those viewing conditions, and run the resulting images through the image difference predictor. The JND will be inversely proportional to the change in the image difference metric caused by a given change in the reflectance. Although in this study we used only a single pair of values of reflectance properties to estimate the impact on the image metric, in practice, using multiple samples with different values would give more robust estimates of the impact of the reflectance parameter on the image differences and therefore a more reliable estimate of the JND.</p><p>However, more typically the extrinsic view parameters (shape, lighting and viewpoint) are free to change. As extrinsic variables change, the impact of a given change in reflectance on the image also changes-making larger, more detectable image changes in some conditions, and smaller, less detectable ones in other conditions. As a result, there will be a distribution of values for the JND across extrinsic parameters. A route to estimating this distribution is to change the shape, lighting and viewpoint across a 'representative' range of conditions, and for each one, image the surface with a range of reflectance parameters. Again, by passing the resulting images through HDR-VDP-3 (or other image-difference metric), it should be possible to predict the JND for each particular combination of extrinsic parameters.</p><p>Given a distribution of JND values, an empirically informed decision can then be made about 'reasonable bounds' for the JND. For example, one might select the 95 th percentile of the distribution of values, meaning that two materials within the tolerance for that reflectance-determining parameter would look indistinguishable in at least 95% of conditions.</p><p>In general, the greater the strictness of the tolerance requirements, the more different lighting, shape and viewpoint conditions would need to be evaluated to estimate the tail of the JND distribution. An alternative approach to sample the tail of the distribution more efficiently than random sampling would be to seek out 'adversarial' combinations of lighting, shape and viewpoint that make the given differences in reflectance especially salient in the image <ref type="bibr" target="#b4">(Bousseau et al., 2011)</ref>. A particularly efficient way of achieving this in computer graphics contexts, would be to use differentiable rendering to optimize predicted visible difference between surfaces by varying lighting, shape and viewpoint. This would aid selecting tolerances based on 'worst case' scenarios. However, it is worth remembering that it is almost always possible to construct a particularly problematic combination of shape, lighting and viewpoint, and such non-generic worst-case conditions may essentially never be encountered in the real world <ref type="bibr" target="#b26">(Freeman, 1994)</ref>. Depending on the derivatives of the scene parameters that lead to very small JNDs, the 'worst case' may require extremely precise alignment of the viewpoint with the surface and light sources, for example, which are unlikely to occur except under carefully contrived circumstances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations and Future Work</head><p>We have outlined a general approach to determining 'reasonable bounds' on JNDs for surface reflectance properties, however there are many open research topics, and additional steps to convert this outline into a working and validated system suitable for critical applications.</p><p>Here we illustrated the ability of HDR-VDP-3 to predict the relative discriminability of a change in roughness (distinctness-of-image gloss) across changes in lighting, shape and view angle. Future work should confirm that a similar approach is effective for other reflectance parameters. It would also be important to demonstrate that the approach generalizes beyond computer graphics to real-world conditions.</p><p>Previous studies have identified many factors that affect gloss constancy, including environmental factors such as illumination <ref type="bibr" target="#b0">(Adams et al., 2018;</ref><ref type="bibr" target="#b23">Fleming et al., 2003;</ref><ref type="bibr" target="#b27">Ged et al., 2020;</ref><ref type="bibr" target="#b34">Ho et al., 2006;</ref><ref type="bibr" target="#b45">Morimoto et al., 2023;</ref><ref type="bibr" target="#b46">Motoyoshi &amp; Matoba, 2012;</ref><ref type="bibr" target="#b55">Olkkonen &amp; Brainard, 2011;</ref><ref type="bibr" target="#b59">Pont &amp; te Pas, 2006;</ref><ref type="bibr" target="#b74">Wendt &amp; Faul, 2017)</ref>, viewpoint <ref type="bibr" target="#b35">(Ho et al., 2007)</ref>, and intrinsic surface factors such as shape <ref type="bibr" target="#b3">(Berzhanskaya et al., 2005;</ref><ref type="bibr" target="#b45">Morimoto et al., 2023;</ref><ref type="bibr" target="#b51">Nishida &amp; Shinya, 1998;</ref><ref type="bibr" target="#b55">Olkkonen &amp; Brainard, 2011;</ref><ref type="bibr" target="#b69">Tiedemann, 2018)</ref>, and diffuse reflectance <ref type="bibr" target="#b45">(Morimoto et al., 2023;</ref><ref type="bibr" target="#b72">Vladusich, 2013;</ref><ref type="bibr" target="#b76">Wendt et al., 2010;</ref><ref type="bibr" target="#b75">Wendt &amp; Faul, 2018)</ref>. Our approach to investigating these factors was, like most previous studies, to sample a rather arbitrary selection of shapes and illuminations and a limited range of view angles. A more thorough and systematic exploration of the impact of shape, lighting and viewpoint would be beneficial. This is challenging as the space of possible shapes and illuminations is practically infinite. One approach would be to consider parametric spaces of lighting and shape, for example using spherical harmonics decompositions <ref type="bibr" target="#b44">(Mazzarella et al., 2014;</ref><ref type="bibr" target="#b48">Mury et al., 2009;</ref><ref type="bibr" target="#b53">Norman et al., 2020;</ref><ref type="bibr" target="#b61">Ramamoorthi &amp; Hanrahan, 2001)</ref>.</p><p>Additionally, gloss constancy is affected by how surfaces are presented to participants;</p><p>for example, studies have demonstrated that the presence of dynamic motion <ref type="bibr" target="#b14">(Doerschner et al., 2011;</ref><ref type="bibr" target="#b19">Ferwerda &amp; Padhye, 2021;</ref><ref type="bibr" target="#b67">Shiwen et al., 2023;</ref><ref type="bibr" target="#b76">Wendt et al., 2010;</ref><ref type="bibr" target="#b75">Wendt &amp; Faul, 2018)</ref>, Fresnel effects <ref type="bibr" target="#b17">(Faul, 2019</ref><ref type="bibr" target="#b18">(Faul, , 2021))</ref>, disparity <ref type="bibr" target="#b76">(Wendt et al., 2010)</ref>, dynamic range Running Head: IMAGE METRICS FOR GLOSS DISCRIMINATION <ref type="bibr">(Doerschner, Maloney, et al., 2010)</ref>, and the particular tone mapping operator used in the rendering process <ref type="bibr" target="#b0">(Adams et al., 2018)</ref> are also important factors. These various factors also deserve consideration, and future work could explore the extent to which they impact upon the predictions of HDR-VDP-3, following a similar experimental framework.</p><p>Rather than assess how viewing conditions affect many differences in reflectance, as is typically done in studies of gloss constancy, here we assessed how viewing conditions affect a single, fixed difference in reflectance. This approach ensured that the observed differences in gloss perception were not confounded by variations in physical reflectance. <ref type="bibr" target="#b42">Marlow and Anderson (2013)</ref> took a similar approach, manipulating surface geometry and the structure of the light field to assess their relative contributions to perceived gloss for a single value of physical reflectance. However, future work should test the assumption that changes in a reflectance parameter lead to proportional changes in the detectability of image differences (as predicted by HDR-VDP-3). It could be that for some reflectance characteristics, there is a nonlinear relationship between changes in the parameter value and changes in the image. The key assumption here is that for small changes, i.e., close to the JND, image changes are approximately linearly related to the reflectance characteristics. While this assumption seems reasonable, it should be tested. Moreover, while we have shown that there is a systematic relationship between HDR-VDP-3 and gloss discrimination, this falls short of explicitly estimating a specific value for the JND from the image difference metric. Additional work is necessary to identify quantitative mappings from HDR-VDP-3 to variations in reflectance parameters, so that the JND can be expressed in terms of units of change of the reflectance parameter. Another important limitation of our study is that it considers discrimination across distinct images of the same object. For many practical applications, however, the key question is whether two juxtaposed surfaces (e.g., two doors of a car), or two neighboring parts of the same surface have the same appearance. Future studies should investigate the detectability of abrupt spatial transitions in reflectance (as in Figure <ref type="figure" target="#fig_0">1</ref>), as the JNDs for these may be substantially lower than suggested by our findings.</p><p>Finally, in the long run, it will also be necessary to generalize our approach to asymmetric comparisons (perhaps involving dynamic scenes and physical surfaces), where the difference in reflectance is confounded by differences in viewing conditions or surface color.</p><p>Under these conditions, it is clear that mere image difference metrics will not capture differences in surface appearance. An image-computable model that can evaluate visual equivalence would be a useful starting point for overcoming this limitation (e.g., see <ref type="bibr" target="#b62">Ramanarayanan et al., 2007)</ref>.</p><p>Our recent work <ref type="bibr" target="#b45">(Morimoto et al., 2023)</ref> explored how object shape and lighting environment Running Head: IMAGE METRICS FOR GLOSS DISCRIMINATION impacted the ability to make asymmetric comparisons of gloss across different lighting conditions and object shapes. Although substantial failures of gloss constancy were found in these experiments, participants were highly consistent in their deviations from physical ground truth. This finding agrees with the high inter-participant correlation obtained in Experiment 1 of the current study, which was also conducted online in uncontrolled viewing conditions.</p><p>Apparently, whether the comparisons are symmetric or asymmetric, participants have little trouble consistently judging differences in gloss, or in making consistent adjustments to match gloss levels under different viewing conditions. The current study contributes to a growing body of literature on gloss perception, demonstrating the remarkable consistency of judgments across various viewing conditions. This means that there are good grounds for thinking that a quantitative, image-based approach can be used to predict discriminability of gloss and other surface reflectance characteristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>Our study demonstrates the potential of using image metrics to predict gloss discrimination across a range of viewing conditions, challenging prior assumptions about the complexity of this task. While our findings show that judgments of gloss can vary under different viewing conditions, they also reveal a surprising degree of precision in how these judgments are made. These insights not only advance our understanding of material appearance but also point to potential practical applications in industrial quality control and computer graphics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Identical differences in surface reflectance can be visible (A) or invisible (B) depending on lighting direction.</figDesc><graphic coords="4,73.50,187.34,468.00,233.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Running</head><label></label><figDesc>pavlovia.org/Wake/screenscale).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Scene variables that were used to create the stimulus images, including shapes (A), viewpoints (B), and illumination conditions (C). An example scene is shown in (D).</figDesc><graphic coords="8,73.50,130.42,468.00,263.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(A) and (B), along with (C) RMSE difference across RGB channels between the two images and (D) predictions of one image metric (High Dynamic Range Visual Difference Predictor; Mantiuk et al., 2023) versus human judgements for all</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>human visual system, HDR-VDP-3 does explicitly model the optical and retinal transformations that occur in the first stages of human visual processing, as well as subsequent parsing of spatial frequency and orientation information in primary visual cortex. These features are used to model contrast masking and (neural) contrast sensitivity, and collectively influence the Running Head: IMAGE METRICS FOR GLOSS DISCRIMINATION metric's assessment of image quality and visibility of image differences. Despite the complexity of HDR-VDP-3, its overall predictive power in our study appears to depend on simpler, more fundamental image attributes such as contrast.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. The scene with the largest visible difference (A-B), together with the luminance difference between the high and low gloss images (C). The scatterplot (D) shows a correlation of .81 between the VDP predictions (arbitrary units) and human judgments (proportion of trials each scene was chosen). Each datapoint in the scatter plot represents one of the hundred scenes; the datapoint highlighted in red corresponds to the scene with the largest visible difference (A-B).</figDesc><graphic coords="12,131.25,149.39,385.50,385.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Correlations between image metrics and human data. Error bars represent SEM. The inter-participant correlations span the range highlighted in orange, with an average of .83.</figDesc><graphic coords="13,157.88,73.50,296.25,193.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>to validate the performance of the VDP across finer roughness levels. The standard roughness value was 0.1, and test images were rendered with the following values of roughness: 0.0702, 0.0824, 0.0901, 0.0950, 0.0981, 0.1019, 0.1050, 0.1099, 0.1176, and 0.1298. All other scene parameters remained identical to those used in Experiment 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. A histogram of the VDP predictions for the full set of images (A). Images used in Experiment 1 are highlighted in red. Images used for Experiment 2 (panels B and C) were selected from the 10th and 90th percentile bins of the histogram, shown highlighted in dark blue.</figDesc><graphic coords="14,109.50,301.18,393.00,303.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Roughness (α) parameter values of the Ward model used to create stimulus images for Experiment 2. The values of roughness used to create the low and high gloss images in Experiment 1 correspond to the minimum and maximum values of the range shown here.</figDesc><graphic coords="15,121.88,73.50,368.25,162.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Figure 7. The minimum and maximum roughness images for the 10th and 90th percentile scenes (A), along with their corresponding psychometric functions calculated from pooled observer data (B). The significantly steeper slope of the psychometric function corresponding to the 90th percentile scene validates the prediction of HDR-VDP-3. Error bars signify the standard deviation of psychometric function fits across participants.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Figure 8. (A) Rank-ordered combinations of lighting, shape, and viewpoint that should yield the highest sensitivity to differences in reflectance, according to an analysis of our full image set using HDR-VDP-3. (B) The scene with the combination of these scene variables that yielded the highest predicted discriminability. (C) A random forest model was used to determine the overall importance score of each variable, revealing that for our image set, object shape has the largest contribution to variance in discriminability. (D) According to this analysis, the sphere contributed the highest variation in discriminability across different lighting environments and viewpoints.</figDesc><graphic coords="18,73.50,73.50,468.00,413.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="19,73.50,73.50,468.00,318.00" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was funded by the <rs type="funder">H2020 Marie Skłodowska-Curie Actions</rs> (<rs type="grantNumber">H2020-MSCA-ITN-2017</rs>) '<rs type="projectName">DyViTo: Dynamics in Vision and Touch</rs>'-project number <rs type="grantNumber">765121</rs>, the <rs type="funder">Deutsche Forschungsgemeinschaft (DFG, German Research Foundation</rs>)-project number <rs type="grantNumber">222641018-SFB/TRR 135 TP C1</rs>, the <rs type="funder">European Research Council (ERC)</rs> <rs type="grantName">Advanced Grant "STUFF"</rs> (project number <rs type="grantNumber">ERC-2022-AdG-101098225</rs>), and by the <rs type="funder">Excellence Program of the Hessian Ministry of Higher Education, Science, Research and Art (HMWK)</rs>-project '<rs type="projectName">The Adaptive Mind</rs>'. TM is supported by a <rs type="grantName">Sir Henry Wellcome Postdoctoral Fellowship</rs> (<rs type="grantNumber">218657/Z/19/Z</rs>) and a <rs type="grantName">Junior Research Fellowship</rs> from <rs type="affiliation">Pembroke College, University of Oxford</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_vu8JZdJ">
					<idno type="grant-number">H2020-MSCA-ITN-2017</idno>
					<orgName type="project" subtype="full">DyViTo: Dynamics in Vision and Touch</orgName>
				</org>
				<org type="funding" xml:id="_SYhTBpw">
					<idno type="grant-number">765121</idno>
				</org>
				<org type="funding" xml:id="_cQd8Cwy">
					<idno type="grant-number">222641018-SFB/TRR 135 TP C1</idno>
					<orgName type="grant-name">Advanced Grant &quot;STUFF&quot;</orgName>
				</org>
				<org type="funded-project" xml:id="_RR4FVvD">
					<idno type="grant-number">ERC-2022-AdG-101098225</idno>
					<orgName type="grant-name">Sir Henry Wellcome Postdoctoral Fellowship</orgName>
					<orgName type="project" subtype="full">The Adaptive Mind</orgName>
				</org>
				<org type="funding" xml:id="_C5DTmjs">
					<idno type="grant-number">218657/Z/19/Z</idno>
					<orgName type="grant-name">Junior Research Fellowship</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Naturally glossy: Gloss perception, illumination statistics and tone mapping</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kucukoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Landy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Mantiuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Image statistics do not explain the perception of gloss and lightness</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.1167/9.11.10</idno>
		<ptr target="https://doi.org/10.1167/9.11.10" />
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Perception and discrimination as a function of stimulus orientation: The &quot;oblique effect&quot; in man and animals</title>
		<author>
			<persName><forename type="first">S</forename><surname>Appelle</surname></persName>
		</author>
		<idno type="DOI">10.1037/h0033117</idno>
		<ptr target="https://doi.org/10.1037/h0033117" />
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="266" to="278" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Remote effects of highlights on gloss perception</title>
		<author>
			<persName><forename type="first">J</forename><surname>Berzhanskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mingolla</surname></persName>
		</author>
		<idno type="DOI">10.1068/p5401</idno>
		<ptr target="https://doi.org/10.1068/p5401" />
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="565" to="575" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Optimizing Environment Maps for Material Depiction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bousseau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chapoulie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1467-8659.2011.01975.x</idno>
		<ptr target="https://doi.org/10.1111/j.1467-8659.2011.01975.x" />
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1171" to="1180" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Band-sifting decomposition for image-based material editing</title>
		<author>
			<persName><forename type="first">I</forename><surname>Boyadzhiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adelson</surname></persName>
		</author>
		<idno type="DOI">10.1145/2809796</idno>
		<ptr target="https://doi.org/10.1145/2809796" />
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Random Forests</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1010933404324</idno>
		<ptr target="https://doi.org/10.1023/A:1010933404324" />
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spatial-frequency discrimination in human vision</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">W</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nachmias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jukes</surname></persName>
		</author>
		<idno type="DOI">10.1364/JOSA.60.000555</idno>
		<ptr target="https://doi.org/10.1364/JOSA.60.000555" />
	</analytic>
	<monogr>
		<title level="j">Journal of the Optical Society of America</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="555" to="559" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Application of Fourier analysis to the visibility of gratings</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">W</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Robson</surname></persName>
		</author>
		<idno type="DOI">10.1113/jphysiol.1968.sp008574</idno>
		<ptr target="https://doi.org/10.1113/jphysiol.1968.sp008574" />
	</analytic>
	<monogr>
		<title level="j">The Journal of Physiology</title>
		<imprint>
			<biblScope unit="volume">197</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="551" to="566" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The perception of gloss: A review</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Chadwick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Kentridge</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.visres.2014.10.026</idno>
		<ptr target="https://doi.org/10.1016/j.visres.2014.10.026" />
	</analytic>
	<monogr>
		<title level="m">Running Head: IMAGE METRICS FOR GLOSS DISCRIMINATION</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="221" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Cheeseman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Ferwerda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Maile</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Fleming</surname></persName>
		</author>
		<idno type="DOI">10.17605/OSF.IO/9H75A</idno>
		<ptr target="https://doi.org/10.17605/OSF.IO/9H75A" />
		<title level="m">Supplemental materials: Scaling and discriminability of perceived gloss</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Scaling and discriminability of perceived gloss</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Cheeseman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Ferwerda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Maile</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Fleming</surname></persName>
		</author>
		<idno type="DOI">10.1364/JOSAA.409454</idno>
		<ptr target="https://doi.org/10.1364/JOSAA.409454" />
	</analytic>
	<monogr>
		<title level="j">Journal of the Optical Society of America A</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="210" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Visible differences predictor: An algorithm for the assessment of image fidelity</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Daly</surname></persName>
		</author>
		<idno type="DOI">10.1117/12.135952</idno>
		<ptr target="https://doi.org/10.1117/12.135952" />
	</analytic>
	<monogr>
		<title level="m">SPIE 1666, Human Vision, Visual Processing, and Digital Display III</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="1614" to="1666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Estimating the glossiness transfer function induced by illumination change and testing its transitivity</title>
		<author>
			<persName><forename type="first">K</forename><surname>Doerschner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Boyaci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Maloney</surname></persName>
		</author>
		<idno type="DOI">10.1167/10.4.8</idno>
		<ptr target="https://doi.org/10.1167/10.4.8" />
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Visual motion and the perception of surface material</title>
		<author>
			<persName><forename type="first">K</forename><surname>Doerschner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Fleming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Schrater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hartung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kersten</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cub.2011.10.036</idno>
		<ptr target="https://doi.org/10.1016/j.cub.2011.10.036" />
	</analytic>
	<monogr>
		<title level="j">Current Biology</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="2010" to="2016" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Perceived glossiness in high dynamic range scenes</title>
		<author>
			<persName><forename type="first">K</forename><surname>Doerschner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Maloney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Boyaci</surname></persName>
		</author>
		<idno type="DOI">10.1167/10.9.11</idno>
		<ptr target="https://doi.org/10.1167/10.9.11" />
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">European coatings dossier on testing and measuring</title>
		<ptr target="http://european-coatings-promotions.com/downloads/ec-dossier-testing-measuring/ec_dossier_2019_testing_and_measuring" />
	</analytic>
	<monogr>
		<title level="m">European Coatings Journal. Vincentz</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The influence of Fresnel effects on gloss perception</title>
		<author>
			<persName><forename type="first">F</forename><surname>Faul</surname></persName>
		</author>
		<idno type="DOI">10.1167/19.13.1</idno>
		<ptr target="https://doi.org/10.1167/19.13.1" />
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Perceived roughness of glossy objects: The influence of Fresnel effects and correlated image statistics</title>
		<author>
			<persName><forename type="first">F</forename><surname>Faul</surname></persName>
		</author>
		<idno type="DOI">10.1167/jov.21.8.1</idno>
		<ptr target="https://doi.org/10.1167/jov.21.8.1" />
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Visual perception of surface properties through manipulation</title>
		<author>
			<persName><forename type="first">Running</forename><surname>Head</surname></persName>
		</author>
		<author>
			<persName><forename type="first">: Image Metrics For Gloss Discrimination</forename><surname>Ferwerda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Padhye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename></persName>
		</author>
		<idno type="DOI">10.2352/issn.2169-2629.2021.29.66</idno>
		<ptr target="https://doi.org/10.2352/issn.2169-2629.2021.29.66" />
	</analytic>
	<monogr>
		<title level="m">Color and Imaging Conference</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Psychophysically based model of surface gloss perception. Human Vision and Electronic Imaging VI</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Ferwerda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pellacini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Greenberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">4299</biblScope>
			<biblScope unit="page" from="4211" to="4299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Perceptual attributes analysis of real-world materials</title>
		<author>
			<persName><forename type="first">J</forename><surname>Filip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kolafová</surname></persName>
		</author>
		<idno type="DOI">10.1145/3301412</idno>
		<ptr target="https://doi.org/10.1145/3301412" />
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Applied Perception</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visual perception of materials and their properties</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Fleming</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.visres.2013.11.004</idno>
		<ptr target="https://doi.org/10.1016/j.visres.2013.11.004" />
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="62" to="75" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Real-world illumination and the perception of surface reflectance properties</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Fleming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">O</forename><surname>Dror</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
		<idno type="DOI">10.1167/3.5.3</idno>
		<ptr target="https://doi.org/10.1167/3.5.3" />
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to see stuff</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Fleming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Storrs</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cobeha.2019.07.004</idno>
		<ptr target="https://doi.org/10.1016/j.cobeha.2019.07.004" />
	</analytic>
	<monogr>
		<title level="j">Current Opinion in Behavioral Sciences</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="100" to="108" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Perceptual gloss space BRDF projection, uniformity validation, and lightness distance metric</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fores</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Fairchild</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Tastl</surname></persName>
		</author>
		<idno type="DOI">10.1145/2628257.2628355</idno>
		<ptr target="https://doi.org/10.1145/2628257.2628355" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Symposium on Applied Perception</title>
		<meeting>the ACM Symposium on Applied Perception</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">136</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The generic viewpoint assumption in a framework for visual perception</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<idno type="DOI">10.1038/368542a0</idno>
		<ptr target="https://doi.org/10.1038/368542a0" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">368</biblScope>
			<biblScope unit="issue">6471</biblScope>
			<biblScope unit="page" from="542" to="545" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Assessing gloss under diffuse and specular lighting</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ged</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rabal-Almazor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Himbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Obein</surname></persName>
		</author>
		<idno type="DOI">10.1002/col.22510</idno>
		<ptr target="https://doi.org/10.1002/col.22510" />
	</analytic>
	<monogr>
		<title level="j">Color Research &amp; Application</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A new Ward BRDF model with bounded albedo</title>
		<author>
			<persName><forename type="first">D</forename><surname>Geisler-Moroder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dür</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1467-8659.2010.01735.x</idno>
		<ptr target="https://doi.org/10.1111/j.1467-8659.2010.01735.x" />
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1391" to="1398" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Swets</surname></persName>
		</author>
		<title level="m">Signal detection theory and psychophysics</title>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1966">1966</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Running Head: IMAGE METRICS FOR GLOSS DISCRIMINATION Trumbore</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Torrance</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shirley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Arvo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lafortune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Ferwerda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pattanaik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Foo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename></persName>
		</author>
		<idno type="DOI">10.1145/258734.258914</idno>
		<ptr target="https://doi.org/10.1145/258734.258914" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Annual Conference on Computer Graphics and Interactive Techniques</title>
		<meeting>the 24th Annual Conference on Computer Graphics and Interactive Techniques</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="477" to="494" />
		</imprint>
	</monogr>
	<note>A framework for realistic image synthesis</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Comparison-based framework for psychophysics: Lab versus crowdsourcing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Haghiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rubisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wichmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Luxburg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Definition and measurement of gloss: A survey of the published literature</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">G W</forename><surname>Harrison</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1945">1945</date>
			<publisher>W. Heffer &amp; Sons Ltd</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-0-387-84858-7</idno>
		<ptr target="https://doi.org/10.1007/978-0-387-84858-7" />
		<title level="m">The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">How direction of illumination affects visually perceived surface roughness</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">X</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Landy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Maloney</surname></persName>
		</author>
		<idno type="DOI">10.1167/6.5.8</idno>
		<ptr target="https://doi.org/10.1167/6.5.8" />
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The effect of viewpoint on perceived visual roughness</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">X</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Maloney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Landy</surname></persName>
		</author>
		<idno type="DOI">10.1167/7.1.1</idno>
		<ptr target="https://doi.org/10.1167/7.1.1" />
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">The measurement of appearance</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Harold</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Perceptual dimensions of high gloss materials</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kildau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
		<respStmt>
			<orgName>Justus-Liebig-Universität Gießen</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Image statistics and the perception of surface gloss and lightness</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Anderson</surname></persName>
		</author>
		<idno type="DOI">10.1167/10.9.3</idno>
		<ptr target="https://doi.org/10.1167/10.9.3" />
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Visual sensitivities to color differences in daylight</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Macadam</surname></persName>
		</author>
		<idno type="DOI">10.1364/JOSA.32.000247</idno>
		<ptr target="https://doi.org/10.1364/JOSA.32.000247" />
	</analytic>
	<monogr>
		<title level="j">Journal of the Optical Society of America</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="247" to="274" />
			<date type="published" when="1942">1942</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Maximum likelihood difference scaling</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Maloney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1167/3.8.5</idno>
		<ptr target="https://doi.org/10.1167/3.8.5" />
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">HDR-VDP-3: A multi-metric for predicting image differences, quality and contrast distortions in high dynamic range and regular content</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Mantiuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hammou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hanji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Generative constraints on image cues for perceived Running Head: IMAGE METRICS FOR GLOSS DISCRIMINATION gloss</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Marlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Anderson</surname></persName>
		</author>
		<idno type="DOI">10.1167/13.14.2</idno>
		<ptr target="https://doi.org/10.1167/13.14.2" />
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">14</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The perception and misperception of specular surface reflectance</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Marlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Anderson</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cub.2012.08.009</idno>
		<ptr target="https://doi.org/10.1016/j.cub.2012.08.009" />
	</analytic>
	<monogr>
		<title level="j">Current Biology</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="1909" to="1913" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Limits on the estimation of shape from specular surfaces</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mazzarella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cholewiak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fleming</surname></persName>
		</author>
		<idno type="DOI">10.1167/14.10.721</idno>
		<ptr target="https://doi.org/10.1167/14.10.721" />
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="721" to="721" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Color and gloss constancy under diverse lighting environments</title>
		<author>
			<persName><forename type="first">T</forename><surname>Morimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Akbarinia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Storrs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Cheeseman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Smithson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Gegenfurtner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Fleming</surname></persName>
		</author>
		<idno type="DOI">10.1167/jov.23.7.8</idno>
		<ptr target="https://doi.org/10.1167/jov.23.7.8" />
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="8" to="8" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Variability in constancy of the perceived surface reflectance across different illumination statistics</title>
		<author>
			<persName><forename type="first">I</forename><surname>Motoyoshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Matoba</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.visres.2011.11.010</idno>
		<ptr target="https://doi.org/10.1016/j.visres.2011.11.010" />
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="39" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Image statistics and the perception of surface qualities</title>
		<author>
			<persName><forename type="first">I</forename><surname>Motoyoshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nishida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature05724</idno>
		<ptr target="https://doi.org/10.1038/nature05724" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">447</biblScope>
			<biblScope unit="page">206</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Structure of light fields in natural scenes</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Mury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Pont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Koenderink</surname></persName>
		</author>
		<idno type="DOI">10.1364/AO.48.005386</idno>
		<ptr target="https://doi.org/10.1364/AO.48.005386" />
	</analytic>
	<monogr>
		<title level="j">Appl. Opt</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">28</biblScope>
			<biblScope unit="page" from="5386" to="5395" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Visual detection and discrimination of luminance increments</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nachmias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Kocher</surname></persName>
		</author>
		<idno type="DOI">10.1364/JOSA.60.000382</idno>
		<ptr target="https://doi.org/10.1364/JOSA.60.000382" />
	</analytic>
	<monogr>
		<title level="j">Journal of the Optical Society of America</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="382" to="389" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Geometrical considerations and nomenclature for reflectance</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">E</forename><surname>Nicodemus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Richmond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Hsia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Ginsberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Limperis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">~f</forename><surname>Galloway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Roitman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Final Report National Bureau of Standards</title>
		<meeting><address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
	<note>Inst. For Basic Standards</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Use of image-based information in judgments of surface-reflectance properties</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nishida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shinya</surname></persName>
		</author>
		<idno type="DOI">10.1364/JOSAA.15.002951</idno>
		<ptr target="https://doi.org/10.1364/JOSAA.15.002951" />
	</analytic>
	<monogr>
		<title level="j">Journal of the Optical Society of America A</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2951" to="2965" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">IMAGE METRICS FOR GLOSS DISCRIMINATION</title>
		<author>
			<persName><forename type="first">Running</forename><surname>Head</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Effects of illumination on the categorization of shiny materials</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Todd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Phillips</surname></persName>
		</author>
		<idno type="DOI">10.1167/jov.20.5.2</idno>
		<ptr target="https://doi.org/10.1167/jov.20.5.2" />
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Difference scaling of gloss: Nonlinearity, binocularity, and constancy</title>
		<author>
			<persName><forename type="first">G</forename><surname>Obein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Knoblauch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Viéot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Joint effects of illumination geometry and object shape in the perception of surface reflectance</title>
		<author>
			<persName><forename type="first">M</forename><surname>Olkkonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Brainard</surname></persName>
		</author>
		<idno type="DOI">10.1068/i0480</idno>
		<ptr target="https://doi.org/10.1068/i0480" />
	</analytic>
	<monogr>
		<title level="j">I-Perception</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1014" to="1034" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Building experiments in PsychoPy</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peirce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Macaskill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>SAGE Publications Ltd</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Toward a psychophysically-based light reflection model for image synthesis</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pellacini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Ferwerda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Greenberg</surname></persName>
		</author>
		<idno type="DOI">10.1145/344779.344812</idno>
		<ptr target="https://doi.org/10.1145/344779.344812" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th Annual Conference on Computer Graphics and Interactive Techniques</title>
		<meeting>the 27th Annual Conference on Computer Graphics and Interactive Techniques</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Wavelength Discrimination in the Presence of Added Chromatic Fields</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pokorny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">C</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.1364/JOSA.60.000562</idno>
		<ptr target="https://doi.org/10.1364/JOSA.60.000562" />
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="562" to="569" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Material-Illumination ambiguities and the perception of solid objects</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Pont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Pas</surname></persName>
		</author>
		<idno type="DOI">10.1068/p5440</idno>
		<ptr target="https://doi.org/10.1068/p5440" />
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1331" to="1350" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Perception of high gloss materials</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Prokott</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
		<respStmt>
			<orgName>Justus-Liebig-Universität Gießen</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">An efficient representation for irradiance environment maps</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<idno type="DOI">10.1145/383259.383317</idno>
		<ptr target="https://doi.org/10.1145/383259.383317" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual Conference on Computer Graphics and Interactive Techniques</title>
		<meeting>the 28th Annual Conference on Computer Graphics and Interactive Techniques</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="497" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Visual equivalence: Towards a new standard for image fidelity</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ramanarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ferwerda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
		<idno type="DOI">10.1145/1276377.1276472</idno>
		<ptr target="https://doi.org/10.1145/1276377.1276472" />
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Photographic tone reproduction for digital images</title>
		<author>
			<persName><forename type="first">E</forename><surname>Reinhard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shirley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ferwerda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="267" to="276" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<author>
			<persName><forename type="first">Running</forename><surname>Head</surname></persName>
		</author>
		<idno type="DOI">10.1145/566654.566575</idno>
		<ptr target="https://doi.org/10.1145/566654.566575" />
		<title level="m">IMAGE METRICS FOR GLOSS DISCRIMINATION</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Material category of visual objects computed from specular image structure</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Doerschner</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41562-023-01601-0</idno>
		<ptr target="https://doi.org/10.1038/s41562-023-01601-0" />
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1152" to="1169" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Painfree and accurate Bayesian estimation of psychometric functions for (potentially) overdispersed data</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Macke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.visres.2016.02.002</idno>
		<ptr target="https://doi.org/10.1016/j.visres.2016.02.002" />
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="page" from="105" to="123" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Task-dependent extraction of information from videos of iridescent and glossy samples</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shiwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Morimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Smithson</surname></persName>
		</author>
		<idno type="DOI">10.1364/JOSAA.479795</idno>
		<ptr target="https://doi.org/10.1364/JOSAA.479795" />
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Am. A</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="160" to="A168" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">A comparison of material and illumination discrimination performance for real rough, real smooth and computer generated smooth spheres</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Te Pas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Pont</surname></persName>
		</author>
		<idno type="DOI">10.1145/1080402.1080415</idno>
		<ptr target="https://doi.org/10.1145/1080402.1080415" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2Nd Symposium on Applied Perception in Graphics and Visualization</title>
		<meeting>the 2Nd Symposium on Applied Perception in Graphics and Visualization</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="75" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">The influence of shape complexity on gloss constancy</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tiedemann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
		<respStmt>
			<orgName>Christian-Albrechts-Universität Kiel</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Three perceptual dimensions for specular and diffuse reflection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Toscani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Guarnera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Guarnera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Hardeberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Gegenfurtner</surname></persName>
		</author>
		<idno type="DOI">10.1145/3380741</idno>
		<ptr target="https://doi.org/10.1145/3380741" />
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Applied Perception</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">The influence of shape on the perception of material reflectance</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vangorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Laurijssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dutré</surname></persName>
		</author>
		<idno type="DOI">10.1145/1276377.1276473</idno>
		<ptr target="https://doi.org/10.1145/1276377.1276473" />
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">26</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Gamut relativity: A new computational approach to brightness and lightness perception</title>
		<author>
			<persName><forename type="first">T</forename><surname>Vladusich</surname></persName>
		</author>
		<idno type="DOI">10.1167/13.1.14</idno>
		<ptr target="https://doi.org/10.1167/13.1.14" />
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">\bf S_3: A Spectral and Spatial Measure of Local Perceived Sharpness in Natural Images</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chandler</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2011.2169974</idno>
		<ptr target="https://doi.org/10.1109/TIP.2011.2169974" />
	</analytic>
	<monogr>
		<title level="j">IMAGE METRICS FOR GLOSS DISCRIMINATION</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="934" to="945" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>IEEE Transactions on Image Processing</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Increasing the complexity of the illumination may reduce gloss constancy. I-Perception</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wendt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Faul</surname></persName>
		</author>
		<idno type="DOI">10.1177/2041669517740369</idno>
		<ptr target="https://doi.org/10.1177/2041669517740369" />
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">2041669517740369</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Can color and motion information be used to disentangle the influence of multiple light sources on gloss perception? I-Perception</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wendt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Faul</surname></persName>
		</author>
		<idno type="DOI">10.1177/2041669518803964</idno>
		<ptr target="https://doi.org/10.1177/2041669518803964" />
		<imprint>
			<date type="published" when="2018">2018. 2041669518803964</date>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Disparity, motion, and color information improve gloss constancy performance</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wendt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Faul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ekroll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mausfeld</surname></persName>
		</author>
		<idno type="DOI">10.1167/10.9.7</idno>
		<ptr target="https://doi.org/10.1167/10.9.7" />
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
