<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Language-Specific or Universal? The Nature and Roles of Consistency and Gradiency in Speech Perception</title>
				<funder ref="#_7fEDhvb">
					<orgName type="full">Spanish Ministry of Science and Innovation</orgName>
				</funder>
				<funder ref="#_U4EsaFx #_YVMPgDm #_MsnPemT #_byhy3UC #_MXkcAVE #_Ka7mcu7 #_dwQxw6K #_S6AMJjB">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_ShywdkS">
					<orgName type="full">Harris</orgName>
				</funder>
				<funder ref="#_bmUbNuv">
					<orgName type="full">Spanish State Research Agency</orgName>
				</funder>
				<funder ref="#_XdKSmGu">
					<orgName type="full">Basque Government</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><roleName>BCBL</roleName><forename type="first">Brian</forename><forename type="middle">W L</forename><surname>Wong</surname></persName>
							<email>bwong@bcbl.eu</email>
							<affiliation key="aff0">
								<orgName type="department">Basque Center on Brain, Language and Cognition</orgName>
								<orgName type="institution">BCBL</orgName>
								<address>
									<addrLine>Donostia-San Sebastian</addrLine>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of the Basque Country (UPV-EHU)</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Arthur</forename><forename type="middle">G</forename><surname>Samuel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Basque Center on Brain, Language and Cognition</orgName>
								<orgName type="institution">BCBL</orgName>
								<address>
									<addrLine>Donostia-San Sebastian</addrLine>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">Stony Brook University</orgName>
								<address>
									<settlement>New York</settlement>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Basque Foundation for Science</orgName>
								<address>
									<settlement>Ikerbasque, Bilbao</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Efthymia</forename><forename type="middle">C</forename><surname>Kapnoula</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Basque Center on Brain, Language and Cognition</orgName>
								<orgName type="institution">BCBL</orgName>
								<address>
									<addrLine>Donostia-San Sebastian</addrLine>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Basque Foundation for Science</orgName>
								<address>
									<settlement>Ikerbasque, Bilbao</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mikeletegi</forename><surname>Pasealekua</surname></persName>
						</author>
						<author>
							<persName><forename type="first">San</forename><surname>Sebastián</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Spain</forename><surname>Email</surname></persName>
						</author>
						<title level="a" type="main">Language-Specific or Universal? The Nature and Roles of Consistency and Gradiency in Speech Perception</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">73FC88A5AECD2DBA3D765C20F4F7C15B</idno>
					<idno type="DOI">10.13039/501100011033</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-10-22T06:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>B. W. L. W., A. G. S., E. C. K.</term>
					<term>Investigation: B. W. L. W.</term>
					<term>Software: B. W. L. W., E. C. K</term>
					<term>Formal analysis: B. W. L. W., E. C. K</term>
					<term>Writing -original draft preparation: B. W. L. W.</term>
					<term>Writing -review and editing: B. W. L. W., A speech perception, gradiency, perceptual consistency, categorical perception, individual differences GRADIENCY, CONSISTENCY, AND GARDEN-PATHS 5</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The present study is part of the first author's PhD dissertation at BCBL. We would like to thank Brendan Costello and James Magnuson for their constructive feedback on the manuscript. We would also like to acknowledge Amets Esnal and Itziar Basterra for organizing the data collection, Ainhoa Eguiguren for the translations of the task instructions, and Daphne Weiss and Elena Alguirrebengoa for recording the stimuli for the present study.</p><p>Additionally, we thank Candice Frances and Tanja Roembke for developing the Corsi blocktapping task and spatial Stroop task used in the present study.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>When processing spoken language, listeners have to map acoustic cues onto speech categories. An example of an acoustic cue is Voice Onset Time (VOT), which is the delay between the release of a stop consonant and the beginning of vocal cord vibration for the subsequent vowel. VOT serves as the primary cue enabling listeners to differentiate between voiced and voiceless stop consonants in English (e.g., /b/ and /p/; <ref type="bibr" target="#b37">Lisker, 1986)</ref>. In most cases, VOTs close to 0 ms map to a /b/ sound and those close to 30 ms or above would reflect a /p/ sound. However, because cue values depend on talker identity <ref type="bibr" target="#b0">(Allen &amp; Miller, 2004)</ref>, speaking rate <ref type="bibr" target="#b52">(Miller et al., 1986)</ref>, etc., the same cue value does not always map onto the same category -this is known as the lack of invariance problem.</p><p>Early views of speech perception suggested that listeners are able to overcome this problem by largely ignoring within-category information. This idea is based on the empirical phenomenon known as "categorical perception", i.e., the finding that two speech sounds from the same phoneme category are often less distinguishable compared to an equidistant pair of sounds that map onto two different phonemes <ref type="bibr" target="#b35">(Liberman et al., 1961;</ref><ref type="bibr" target="#b60">Repp, 1984;</ref><ref type="bibr"></ref> M. E. H. <ref type="bibr" target="#b68">Schouten &amp; van Hessen, 1992)</ref>. This well-replicated finding lent support to the idea that during speech perception within-category differences are largely ignored, and speech sounds are perceived categorically. For example, an English /b/ sound with a VOT of 0 ms should be essentially indistinguishable from one with a VOT of 20 ms, whereas a /b/ with VOT of 10 ms and a /p/ with VOT of 30 ms should be more easily distinguished.</p><p>Despite the early popularity of the categorical perception account, advances in psycholinguistics have since challenged this perspective by providing evidence that listeners are sensitive to within-category information, thus suggesting that speech categorization is fundamentally gradient (see <ref type="bibr" target="#b43">McMurray, 2022</ref>, for a review). According to the gradient view of speech perception, listeners are capable of detecting subtle, continuous variations in phonetic cues, rather than perceiving speech sounds in terms of distinct phonemic categories.</p><p>Supporting evidence for gradiency emerges from a variety of methodologies, including various behavioral tasks (e.g., <ref type="bibr" target="#b10">Carney et al., 1977;</ref><ref type="bibr" target="#b27">Kapnoula et al., 2017;</ref><ref type="bibr" target="#b40">Massaro &amp; Cohen, 1983;</ref><ref type="bibr" target="#b51">Miller, 1994;</ref><ref type="bibr" target="#b57">Pisoni &amp; Lazarus, 1974;</ref><ref type="bibr" target="#b58">Pisoni &amp; Tash, 1974;</ref><ref type="bibr" target="#b65">Samuel, 1977</ref><ref type="bibr" target="#b66">Samuel, , 1982))</ref>, eyetracking (e.g., <ref type="bibr" target="#b49">McMurray et al., 2002</ref><ref type="bibr" target="#b45">McMurray et al., , 2008))</ref>, and EEG (e.g., <ref type="bibr">Kapnoula &amp; McMurray, 2021;</ref><ref type="bibr" target="#b56">Ou &amp; Yu, 2022;</ref><ref type="bibr" target="#b72">Toscano et al., 2010)</ref>.</p><p>Recent work on this topic has focused on individual differences as a means to examine the underlying mechanisms and functional roles of gradiency in spoken language comprehension. Building on that work, the present study further examines the mechanistic nature of speech perception gradiency by asking whether its function depends on language status (L1 vs. L2) and/or language-specific properties (i.e., VOT).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Measuring Individual Differences in Gradiency: The Case for the VAS Task</head><p>While perception of speech sounds is generally gradient, some individuals show higher gradiency than others <ref type="bibr" target="#b27">(Kapnoula et al., 2017;</ref><ref type="bibr">Kapnoula &amp; McMurray, 2021)</ref>. These individual differences are thought to be due to variations in the degree of category-driven perceptual warping around the boundary <ref type="bibr">(Kapnoula et al., 2021)</ref>. These individual differences can be captured using the Visual Analogue Scale (VAS) task, initially introduced by <ref type="bibr" target="#b40">Massaro and Cohen (1983)</ref>. In this task, participants are presented with a continuum labelled, for example, with "ba" at one end and "pa" at the opposite end. Upon hearing a sound, participants are asked to indicate its position along this continuum. Gradiency measures extracted from VAS responses have illuminated individual differences in speech perception: some individuals are highly attuned to differences within phoneme categories, yielding more gradient responses characterized by a shallow slope (hereafter referred to as more gradient listeners). Conversely, others exhibit diminished sensitivity to such finegrained distinctions, as evidenced by responses with a steeper slope (hereafter less gradient listeners; <ref type="bibr">Kapnoula et al., 2021;</ref><ref type="bibr">Kapnoula &amp; McMurray, 2021;</ref><ref type="bibr" target="#b27">Kapnoula et al., 2017;</ref><ref type="bibr" target="#b30">Kong &amp; Edwards, 2016)</ref>.</p><p>Importantly, <ref type="bibr">Kapnoula and McMurray (2021)</ref> aimed at identifying the exact aspect of speech perception that is reflected in VAS responses. To do so, they looked at the N1 (an event-related potential approximately 100 ms after stimulus onset that reflects encoding of continuous acoustic information). The results revealed a linear relationship between VOT and N1 amplitude; however, this relationship was disrupted close to the category boundary, but only in the case of listeners who exhibited a more categorical (or less gradient) pattern of VAS responses. This finding suggests that gradiency measured by VAS likely reflects the initial phases of speech cue encoding <ref type="bibr">(Kapnoula &amp; McMurray, 2021)</ref>. Thus, relative to other tasks, like the two-alternative forced choice (2AFC) task, the VAS task likely offers a more nuanced perspective on auditory perception, capturing the subtleties of how individuals discern sound variations. Previous research has critically examined the use of 2AFC tasks <ref type="bibr" target="#b4">(Apfelbaum et al., 2022;</ref><ref type="bibr" target="#b21">Hary &amp; Massaro, 1982;</ref><ref type="bibr" target="#b45">McMurray et al., 2008;</ref><ref type="bibr" target="#b53">Munson et al., 2017;</ref><ref type="bibr" target="#b58">Pisoni &amp; Tash, 1974)</ref>. Such tasks require a binary response (e.g., "ba" or "pa"), potentially oversimplifying the listeners' perceptual experience and overlooking their capacity to perceive within-category nuances. In fact, a recent study has shown that while VAS slopes are indicative of gradiency, 2AFC slopes primarily capture perceptual consistency, or the stability of acoustic cue encoding <ref type="bibr" target="#b22">(Honda et al., 2024;</ref><ref type="bibr">details</ref> of perceptual consistency will be described below). This distinction underscores the advantage of VAS in accurately assessing gradiency 1 .</p><p>Recent work has provided evidence supporting the reliability and validity of the VAS task. Gradiency, as measured by the VAS task, exhibits stability across repeated sessions 1 Apart from 2AFC, the discrimination task is another method used to demonstrate categorical perception <ref type="bibr">(Studdert-Kennedy et al., 1970)</ref>. Nonetheless, it also has issues, such as the influence of working memory on results and the possibility that the task tests category rather than encoding level (see <ref type="bibr" target="#b4">Apfelbaum et al., 2022;</ref><ref type="bibr" target="#b18">Gerrits &amp; Schouten, 2004;</ref><ref type="bibr" target="#b43">McMurray, 2022;</ref><ref type="bibr">B. Schouten et al., 2003, for relevant discussion)</ref>.</p><p>when the same stimuli are used <ref type="bibr" target="#b30">(Kong &amp; Edwards, 2016)</ref>, which speaks to its test-retest reliability (see also <ref type="bibr" target="#b22">Honda et al., 2024</ref> for a relevant discussion). Furthermore, it has been shown that VAS responses are not indicative of general scale usage biases. This is partly evidenced by the minimal correlation in responses between auditory and visual VAS tasks <ref type="bibr">(Kapnoula et al., 2021;</ref><ref type="bibr">Kapnoula &amp; McMurray, 2021;</ref><ref type="bibr" target="#b26">Kapnoula &amp; Samuel, 2024)</ref>. VAS gradiency measures corresponding to different speech contrasts are correlated with each other <ref type="bibr" target="#b7">(Bidelman et al., 2024;</ref><ref type="bibr" target="#b16">Fuhrmeister &amp; Myers, 2021;</ref><ref type="bibr" target="#b17">Fuhrmeister et al., 2023;</ref><ref type="bibr" target="#b31">Kong &amp; Kang, 2023)</ref>, but crucially the correlation strength between slopes seems to depend on the acoustic similarity between the speech continua (e.g., higher correlation between labial and alveolar stops compared to labial stops and fricatives; <ref type="bibr" target="#b27">Kapnoula et al., 2017</ref><ref type="bibr">Kapnoula et al., , 2021;;</ref><ref type="bibr">Kapnoula and McMurray, 2021)</ref>. Taken together, these results are in line with the idea that VAS responses tap something fundamental about speech categorization. Finally, and perhaps most importantly, VAS gradiency aligns with corresponding measures extracted from the same participants using different methodologies. Specifically, it has been found that VAS slope is related to lexical gradiency as measured via an eye-tracking Visual World Paradigm (VWP) (discussed below) and to speech category gradiency as measured by the P3 ERP component, a pattern that further underscores the validity of VAS as a measure of speech perception <ref type="bibr">(Kapnoula &amp; McMurray, 2021)</ref>. Collectively, these findings speak to the reliability and validity of the VAS task as a way of assessing listeners' speech perception gradiency in specific contrasts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Functional Role(s) of Gradiency in L1 and L2 Speech Perception</head><p>Higher gradiency is linked to a superior ability to process and integrate a range of acoustic cues <ref type="bibr" target="#b27">(Kapnoula et al., 2017;</ref><ref type="bibr">Kapnoula &amp; McMurray, 2021;</ref><ref type="bibr" target="#b28">D. Kim et al., 2020;</ref><ref type="bibr" target="#b30">Kong &amp; Edwards, 2016;</ref><ref type="bibr" target="#b31">Kong &amp; Kang, 2023;</ref><ref type="bibr" target="#b55">Ou et al., 2021)</ref>, indicating that individuals with higher gradiency are more attuned to the fine-grained details within the acoustic signal. Furthermore, there is some preliminary evidence that L1 gradiency is linked to L2 proficiency, as measured by a vocabulary task <ref type="bibr" target="#b26">(Kapnoula &amp; Samuel, 2024)</ref>. This finding points to a potentially significant role of gradiency in L2 acquisition, likely by aiding the establishment of new categories or preserving the ability to discern within-category differences for non-native contrasts.</p><p>At its core, gradiency is associated with the ability to discern small cue differences, fostering enhanced flexibility amid uncertain auditory scenarios <ref type="bibr" target="#b9">(Brown-Schmidt &amp; Toscano, 2017;</ref><ref type="bibr" target="#b11">Clayards et al., 2008)</ref>. One method to assess this kind of flexibility is by examining how participants with different degrees of gradiency cope when they encounter misleading or ambiguous auditory cues. For example, <ref type="bibr" target="#b50">McMurray et al. (2009)</ref> presented participants with lexical garden-paths (items that sound like one word initially that ultimately turn out to be another word) and used the Visual World Paradigm <ref type="bibr">(VWP;</ref><ref type="bibr">Tanenhaus et al., 1995)</ref> to track the activation of the two competing words. This allowed them to investigate how individuals dealt with the induced misunderstandings in real time. In their study, participants were exposed to auditory stimuli such as "ϸeachball", with the initial phoneme transitioning between /b/ and /p/ along a continuum (/ϸ/ represents a sound between /b/ and /p/). Notably, with a VOT of 40 ms (e.g., "peachball"), listeners were prone to first look at the picture of "peachpit" (competitor). The rate (i.e., likelihood) of making this garden-path reflects erroneous initial lexical activation. In this situation, since the subsequent input is consistent with "beachball" rather than "peachpit", listeners may recover from the garden-path and recognize "beachball" (target). The rate (i.e., likelihood) and latency (i.e., speed) of this recovery reflect speech perception flexibility. Crucially, both the garden-path rate and the recovery speed were linearly related to the acoustic distance between target and stimulus (i.e., more competitor-like initial sounds induced more garden-paths and slower recoveries). Thus, the findings by <ref type="bibr" target="#b50">McMurray et al. (2009)</ref> support the notion that listeners generally exhibit gradiency, which helps them to recover from initial errors generated by ambiguous/misleading linguistic inputs.</p><p>Combining this paradigm with VAS, <ref type="bibr">Kapnoula et al. (2021)</ref> investigated the relationship between gradiency and flexibility in dealing with garden-path situations. The results showed that, when faced with an auditory stimulus like "ϸeachball", more gradient listeners were more likely to recover from erroneous initial lexical activations ("peachpit"), and reach the correct interpretation ("beachball"), especially when the acoustic distance from the target was high. This finding is in line with the idea that more gradient listeners are adept at considering multiple interpretations simultaneously, thereby avoiding early commitment to an incorrect lexical choice, which in turn facilitates recovery from misunderstandings. In contrast, less gradient listeners may show warping in the acoustic cue space around the category boundary, which prevents lexical-level processes from fully recovering. Essentially, the sensitivity of more gradient listeners to the nuances of speech sounds equips them with the flexibility to adjust their interpretations in the face of ambiguous or misleading information. Such flexibility is advantageous for managing the variability inherent in different linguistic environments, since successful comprehension requires continuous adjustment to variations in speech caused by differences in characteristics such as coarticulation and accent.</p><p>While <ref type="bibr">Kapnoula et al. (2021)</ref> provided evidence for a positive relationship between gradiency and speech perception flexibility in monolingual English speakers, it remains unknown whether this relationship represents a universal pattern or one tied to languagespecific acoustic characteristics (VOT; discussed below) or language status (i.e., a person's L1 vs. their L2). The present study will directly address this question of whether gradiency is a generic trait affecting how listeners process speech. individuals may exhibit high or low perceptual consistency regardless of their gradiency level, highlighting the importance of distinguishing between these two constructs. For example, a shallow slope on the VAS could reflect continuous and consistent responses across stimuli with different VOT steps (e.g., from /b/ to /p/), but it might also result from inconsistent responses in each step. In the latter case, the shallow slope would not accurately represent true gradiency, but rather noisy encoding <ref type="bibr" target="#b22">(Honda et al., 2024;</ref><ref type="bibr" target="#b33">Kutlu et al., 2022;</ref><ref type="bibr">Sorensen et al., 2024;</ref><ref type="bibr"></ref> for a discussion on this, see <ref type="bibr" target="#b4">Apfelbaum et al., 2022)</ref>. A method to differentiate the two constructs will be discussed in the Method section.</p><p>Unlike VAS gradiency, perceptual consistency appears to tap something similar to more traditional measures of speech perception such the slope extracted from the 2AFC tasks <ref type="bibr" target="#b22">(Honda et al., 2024)</ref>. In the 2AFC tasks, steeper categorization slope is usually taken to indicate better/sharper categorization, whereas shallow slopes indicate atypical/impaired speech perception <ref type="bibr" target="#b19">(Godfrey et al., 1981;</ref><ref type="bibr" target="#b23">Joanisse et al., 2000;</ref><ref type="bibr" target="#b39">López-Zamora et al., 2012;</ref><ref type="bibr" target="#b69">Serniclaes et al., 2001</ref><ref type="bibr" target="#b70">Serniclaes et al., , 2005;;</ref><ref type="bibr" target="#b73">Werker &amp; Tees, 1987)</ref>. Therefore, it is not surprising that higher perceptual consistency has been linked to better language and reading abilities in children (H. <ref type="bibr" target="#b29">Kim et al., 2024)</ref> and improved learning of non-native contrasts in adults <ref type="bibr" target="#b17">(Fuhrmeister et al., 2023;</ref><ref type="bibr" target="#b22">Honda et al., 2024)</ref>. These findings suggest that cue encoding stability may play an important role in individual differences in both native and non-native speech perception and language learning in general. Given the emerging nature of research in the area of perceptual consistency, its impact on different aspects of speech perception, particularly the ability to recover from misleading and ambiguous auditory information, remains largely unexplored. This gap in knowledge is addressed in the present study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Present Study</head><p>Our first research objective is to examine whether the functional roles of gradiency in speech perception flexibility are modulated by language characteristics and/or language status. The main manipulation in the present experiments involves initial voicing contrasts in Spanish and English, leveraging on the VOT difference between /b/ and /p/ in these two languages. In Spanish, the typical VOTs for these two consonants are /b/ ≈ -80 ms and /p/ ≈ 16 ms <ref type="bibr">(Souganidis et al., 2022)</ref>, in sharp contrast to their English counterparts (/b/ ≈ 0 ms; /p/ ≈ 60 ms; <ref type="bibr" target="#b36">Lisker &amp; Abramson, 1964)</ref>. This discrepancy indicates that, in Spanish, the /b/ sound is prevoiced, resulting in a negative VOT, while the Spanish /p/ aligns more closely with the English /b/. The different characteristics between Spanish and English provide an opportunity for us to examine whether the functional roles of gradiency are tied to languagespecific acoustic properties of a speech contrast. In addition to language characteristics, it is important to understand whether the function of gradiency in spoken perception flexibility depends on language status. Hence, the present study examines how gradiency influences speech perception flexibility not only in L1 (Spanish) but also in L2 (English). L2 gradiency has not been extensively examined, with only a few studies making inroads into this domain <ref type="bibr" target="#b31">(Kong &amp; Kang, 2023;</ref><ref type="bibr" target="#b33">Kutlu et al., 2022)</ref>. L2 speech perception, especially in noisy settings, necessitates the correction of errors induced by misleading or ambiguous cues, suggesting that gradient listening would be particularly valuable in these situations. More importantly, understanding the possible roles of gradiency can shed light on whether the function of gradiency is generic or if it depends on language status (i.e., L1 vs. L2). In sum, our first research question is whether the relationship between gradiency and speech perception flexibility is modulated by language characteristics and/or language status. To test this, we will examine the relationship between gradiency and speech perception flexibility in L1 Spanish and L2 English.</p><p>Regarding the first research question, we hypothesized that gradiency would facilitate recovery from lexical garden-paths in L1 Spanish, as seen in L1 English <ref type="bibr">(Kapnoula et al., 2021)</ref>. This hypothesis rests on the premise that greater within-category sensitivity would allow listeners to achieve more effective lexical recovery, despite the different VOTs between Spanish and English. However, it is possible that the relationship may be less obvious in Spanish because native Spanish speakers (our current participants) may show more categorical responses in the Spanish VAS. In Spanish, the contrast between /b/ and /p/ is based on the presence or absence of pre-voicing (i.e., it is more qualitative), whereas in English, it is more quantitative. Therefore, Spanish speakers may focus less on withincategory differences (cf. <ref type="bibr" target="#b26">Kapnoula &amp; Samuel, 2024)</ref>. Regarding language status, we hypothesized that, assuming participants have accurate categorical representations of English stop consonants, we will find a positive relationship between gradiency and speech perception flexibility.</p><p>Our second research objective is to examine the relationships between perceptual consistency and spoken word recognition, including initial lexical activation and speech perception flexibility, and whether these relationships are modulated by language. We predicted that listeners who exhibit higher stability in encoding speech sounds would have higher speech perception flexibility. This prediction is based on previous findings regarding the advantages of perceptual consistency in speech perception <ref type="bibr" target="#b17">(Fuhrmeister et al., 2023;</ref><ref type="bibr" target="#b22">Honda et al., 2024)</ref>. We consider this research question to be exploratory due to current gaps in our understanding of the role of perceptual consistency in speech perception. It is important to examine perceptual consistency to understand what this underexamined construct reflects and its advantages in speech perception.</p><p>Finally, our third research objective is to test whether gradiency and perceptual consistency are language-specific or language-general by directly comparing the measures collected in the two languages. In the present study, we extracted both measures from the same contrast (/b/-/p/), but in two different languages. This allows us to directly ask the question of whether gradiency and perceptual consistency are language-specific or not. To our knowledge, this is the first attempt to compare both gradiency and consistency with the same phonemes across languages. This comparison could shed light on the mechanisms underlying these two constructs.</p><p>The correlation between gradiency measures extracted from different contrasts has been found to depend on the acoustic similarity between these contrasts <ref type="bibr" target="#b27">(Kapnoula et al., 2017</ref><ref type="bibr">(Kapnoula et al., , 2021;;</ref><ref type="bibr">Kapnoula &amp; McMurray, 2021)</ref>. This suggests that gradiency is, to some degree, contrast-specific. However, previous work, albeit limited, has consistently found positive correlations between different measures of perceptual consistency extracted from the 2AFC and VAS <ref type="bibr" target="#b22">(Honda et al., 2024)</ref>, as well as between different phonetic contrasts <ref type="bibr" target="#b17">(Fuhrmeister et al., 2023)</ref>. Based on these findings, we predicted that gradiency is tied to language-specific acoustic characteristics, while perceptual consistency is a general speech perception trait.</p><p>To summarize, this study adopts an individual differences approach to examine the nature and roles of speech perception gradiency and perceptual consistency. Our three research questions include: (1) Is the relationship between gradiency and spoken word recognition modulated by language (language characteristics and/or language status)? (2) Is the relationship between perceptual consistency and spoken word recognition modulated by language? (3) Are gradiency and perceptual consistency stable across languages?</p><p>To address these three research questions, we tested native Spanish speakers in both their native language (Spanish) and their L2 (English<ref type="foot" target="#foot_1">3</ref> ). Additionally, we included English proficiency, English exposure, working memory, inhibitory control, and musical training as covariates due to their potential links to language processing or gradiency found in previous studies (e.g., <ref type="bibr" target="#b27">Kapnoula et al., 2017;</ref><ref type="bibr">Kapnoula &amp; McMurray, 2021;</ref><ref type="bibr" target="#b26">Kapnoula &amp; Samuel, 2024;</ref><ref type="bibr" target="#b28">D. Kim et al., 2020;</ref><ref type="bibr" target="#b46">McMurray et al., 2018;</ref><ref type="bibr" target="#b71">Smayda et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Participants</head><p>We recruited 70 native Spanish speakers (53 females) 4 residing in San Sebastian, Spain for this study. This sample size was informed by a previous study conducted by <ref type="bibr">Kapnoula et al. (2021)</ref>, in which they tested 67 participants. A power analysis (Supplementary Material I) indicates that our sample size is sufficiently large to detect the anticipated effect, if it exists.</p><p>Participants were between 18 and 40 years old (M = 27.8) and had normal or corrected-to-normal vision and no known hearing or neurological impairments. Apart from English, most participants were also familiar with Basque, which was taken into account in the preparation of the materials and data analyses. Detailed demographic information is presented in Table <ref type="table" target="#tab_0">S1</ref>. Given the diverse characteristics of our participants (see Table <ref type="table" target="#tab_0">S1</ref>), we believe that our results are relatively generalizable. The experiment was approved by the BCBL Ethics Review Board, adhering to the guidelines of the Helsinki Declaration. All participants provided written informed consent and were paid for their participation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overview of Design</head><p>In each language, VWP stimuli consisted word pairs with similar onsets, differing only in the voicing of the first consonant, and different offsets (e.g., "beachball" [bitʃbɔl], "peachpit" [pitʃpɪt]). Words were manipulated to create /b/-to-/p/ voicing continua (e.g., "beachball"-to-"peachball") with the goal of progressively increasing the probability that participants would first activate the competitor word (reflecting initial lexical activation) and evaluate if and how quickly they would recover after hearing the disambiguating offset 4 All participants except two had a Spanish Age of Acquisition (AoA) of 0. Given that these two participants reported Spanish as their dominant language and had higher Spanish proficiency than Basque or English proficiency, they were included in the analyses.</p><p>(reflecting speech perception flexibility). These measures were examined using a VWP task with eye-tracking (as in <ref type="bibr">Kapnoula et al., 2021 and</ref><ref type="bibr" target="#b50">McMurray et al., 2009)</ref>. Apart from the VOT step of the stimulus onset, the two independent variables (IVs) of interest were the participant's (1) VAS slope (reflecting speech perception gradiency) and their (2) VAS response consistency (reflecting perceptual consistency). The VAS and VWP tasks were administered in English (L2) with the same stimuli and design as in <ref type="bibr">Kapnoula et al. (2021)</ref>; both tasks were also conducted in Spanish (L1) with Spanish stimuli in a separate session (order of sessions was counterbalanced, see "Order of Tasks" for details).</p><p>A set of additional measures were included as covariates. Specifically, participants completed an English picture naming task (assessing English proficiency), a questionnaire on language exposure and musical training, a spatial Stroop task (assessing inhibitory control), and a Corsi block-tapping task (assessing working memory). Our results showed that including these variables did not improve the model fit related to our research questions (see "Primary Analyses I: Effects of Gradiency on Initial Lexical Activation and Speech Perception Flexibility in L1 Spanish and L2 English" section for details). Therefore, the details of these tasks are not further discussed in the manuscript but are provided in Supplementary Material III. In addition, a training session was scheduled one to seven days before the English VWP to ensure participants were familiar with the English words (see Supplementary Material III for details). The tasks are summarized in Table <ref type="table" target="#tab_0">1</ref>. Note. VAS = Visual Analogue Scale; VWP = Visual World Paradigm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Measuring Gradiency and Perceptual Consistency: VAS Tasks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stimuli</head><p>The English stimuli were the same as those used in <ref type="bibr" target="#b27">Kapnoula et al. (2017)</ref>.</p><p>Specifically, we used a "buh"-"puh" continuum consisting of natural speech items that varied factorially along (a) seven VOT steps, ranging from 1 to 45 ms, and (b) five fundamental frequency (F0) steps, ranging from 90 to 125 Hz. Similarly, the Spanish stimuli formed a natural speech "ba"-"pa" continuum that had seven VOT steps from -35 to +10 ms and five F0 steps from 179 to 193 Hz. In both cases, continua were generated employing the progressive cutback and replacement approach. This involved progressively deleting the onset of a word with the /b/ sound and replacing it with a roughly equivalent amount of the /p/ onset of its counterpart <ref type="bibr" target="#b2">(Andruski et al., 1994)</ref>. For the Spanish items, we used the Praat script developed by Winn (2020; Version 33), which was modified to account for the presence of negative VOT in Spanish voiced sounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure</head><p>Experiment Builder <ref type="bibr">(version 2022.2.5;</ref><ref type="bibr">SR Research Ltd., 2022)</ref> was used to program and run the experiment. Participants completed the VAS task in two languages (English and Spanish) and were informed about the language of the task before it started. Each trial presented participants with a line labelled at both ends according to the two speech sound categories. For English, "buh" was consistently positioned on the left, and "puh" on the right.</p><p>For Spanish, "ba" and "pa" were used in the same respective positions. Participants listened to each stimulus and clicked on the line to indicate where they perceived it to fall on the continuum. After their first click, a rectangular bar appeared at the point where they had clicked, and participants had the option to modify their response or press the space bar to confirm it. Within each language, stimuli were presented in random order. Each stimulus was presented three times, resulting in 105 trials for each language. The VAS task lasted approximately five to eight minutes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Measuring Gradiency and Perceptual Consistency</head><p>The analysis procedure for the VAS task closely adhered to previous methods (e.g., <ref type="bibr" target="#b27">Kapnoula et al., 2017</ref><ref type="bibr">Kapnoula et al., , 2021))</ref>. Click locations on the x-axis were converted from pixels to a VAS rating ranging from 0 to 100. In addition, we utilized the VAS ratings to extract a measure of perceptual consistency, employing the same procedure outlined in <ref type="bibr" target="#b27">Kapnoula et al. (2017)</ref>. First, we computed the residuals, or the difference between the VAS rating on each trial and the predicted value for that stimulus based on that participant's fitted curve. Subsequently, we calculated the standard deviation of these residuals for each participant. A larger standard deviation indicates lower consistency in participants' responses and, thus, lower perceptual consistency. To simplify interpretation, we reversed the standard deviation by multiplying by -1, so that higher values represent higher perceptual consistency.</p><p>In this study, our goal was to investigate gradiency and perceptual consistency as separate factors. As explained in the introduction, a shallow VAS slope may reflect either true gradiency or noisy encoding. Using Pearson correlations, we found nonsignificant correlations between perceptual consistency and slope (log) for both the Spanish VAS, r(66) = -.12, p = .329, and for the English VAS, r(65) = -.08, p = .536. However, we cannot exclude the possibility that some participants might respond inconsistently despite exhibiting a gradient pattern. Therefore, we adjusted for the effect of perceptual consistency on the slope to derive a pure gradiency measure. To accomplish this, we conducted a linear regression with VAS slope (log) as the dependent variable and perceptual consistency as the predictor and extracted the standardized residual. This residual was used as the gradiency measure (hereafter referred to as slope) in the subsequent analyses. A higher slope corresponds to lower gradiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Measuring Initial Lexical Activation and Speech Perception Flexibility: VWP Tasks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Design and Materials</head><p>To assess participants' degree of initial lexical activation and speech perception flexibility, we employed a VWP task <ref type="bibr">(Kapnoula et al., 2021;</ref><ref type="bibr" target="#b50">McMurray et al., 2009)</ref> designed to induce lexical garden-paths. This task presented participants with auditory stimuli based on word pairs such as "beachball"-"peachpit". Each pair was characterized by differing initial voicing (/b/ vs. /p/), but shared two to five middle phonemes. Instances of initial VOT ambiguity were disambiguated by phonetic information at the word offset (e.g., the "-ball" or "-pit"). The English words and pictures utilized, including five pairs of critical stimuli (starting with /b/ and /p/) and five pairs of fillers (starting with /l/ and /r/), were the same as those developed by <ref type="bibr">Kapnoula et al. (2021)</ref> (see Table <ref type="table" target="#tab_2">2</ref> for details on all critical English stimuli and Table <ref type="table" target="#tab_2">S2</ref> for all filler stimuli). Each pair underwent VOT manipulation along a seven-step continuum, resulting in a word-to-nonword spectrum (e.g., "beachball"</p><p>[bitʃbɔl] to "peachball" [pitʃbɔl]). The following descriptions will focus on the Spanish VWP (the design and materials of the English task can be found in <ref type="bibr">Kapnoula et al. (2021)</ref>). Note. Underlined portions mark phonemic overlap between the words in a pair; bolded portions mark offsets.</p><p>The Spanish stimuli were generated in accordance with the procedures outlined by <ref type="bibr">Kapnoula et al. (2021)</ref>. We developed a set of twenty Spanish words, including critical stimuli consisting of five /b/-onset and five /p/-onset words. Additionally, filler words were incorporated, featuring five /l/-onset and five /r/-onset words. The characteristics of these stimuli, along with corresponding pictures, mirrored those of the English stimuli (see Table <ref type="table" target="#tab_3">3</ref> for details on all critical Spanish stimuli and Table <ref type="table" target="#tab_3">S3</ref> for all filler stimuli). Each word encompassed five to eight phonemes and two or three syllables, with three phonemes overlapping for each /b/-/p/ word pair. Note. Underlined portions mark phonemic overlap between the words in a pair; bolded portions mark offsets.</p><p>The methods for processing the Spanish stimuli closely followed the procedures outlined by <ref type="bibr">Kapnoula et al. (2021)</ref>. The stimuli were created through splicing natural recordings. Initially, a native Spanish speaker recorded complete exemplars of both items in each pair, including both voiced and voiceless onsets (e.g., "balanza", "palanza", "balacio", and "palacio"), within a sound-attenuated room. The recordings underwent background noise reduction using the default settings in Audacity. Then, each recording was divided into two segments: the onset (e.g., "bala-" from "balanza") and the offset (e.g., "-nza"). The stimuli were cut at the zero-crossing point nearest to the point of disambiguation (POD), with an average POD occurring at approximately 433 ms. The recordings were adjusted to a 70 dB intensity level using Praat software (version 6.3.03; <ref type="bibr" target="#b8">Boersma &amp; Weenink, 2023)</ref>. A 100-ms silence was added to the beginning and end of each word.</p><p>Given that the onset portions might include co-articulatory cues predicting the offset (e.g., the "bala" from "balanza" may predict "-nza" more than "-cio"), each of the two voiced onsets in a pair (e.g., "balanza" and "balacio") was spliced onto each of the two offsets to counterbalance the co-articulatory cues in the onsets (e.g., "balanza" and "balacio").</p><p>Consequently, half of the stimuli were generated by combining parts from the same item (e.g., "balanza" and "balanza"; matching splice), while the other half were formed from different items (e.g., "balanza" and "palanza"; mismatching splice). More details can be found in Figure <ref type="figure" target="#fig_1">S4</ref> in the Supplemental Materials of <ref type="bibr">Kapnoula et al. (2021)</ref>.</p><p>Finally, the VOT continua were constructed by pairing items that differed solely in the voicing of the onset consonant (e.g., "balanza" and "palanza"). These pairs were then utilized as the endpoints for generating seven-step VOT continua (Spanish: -35 -+15 ms;</p><p>English: 0 -+48 ms). The construction of these continua followed the aforementioned method employed in creating the VAS stimuli. This process yielded 140 auditory items (5 pairs × 2 splice conditions × 2 offsets × 7 VOT steps). The F0 was standardized at 181 Hz for the onsets of all Spanish stimuli. Each item was presented three times, resulting in 420 experimental trials.</p><p>For filler stimuli, only the correct (e.g., "lavabo" [sink] and "regalo" [gift]) and misarticulated (e.g., "ravabo" and "legalo") versions were recorded and used. An equivalent number of filler trials (i.e., 420 trials) were interspersed to introduce variety and obscure the task purpose, yielding a total of 840 trials. These trials were presented in a randomized order.</p><p>Each experimental pair was paired with a filler pair to create a four-item set (e.g., "balanza", "palacio", "lavabo", and "regalo" formed one set in the Spanish VWP), with the constraint that all items within a VWP set were semantically unrelated and shared the same stress pattern. Items in a VWP set always appeared together. Five such VWP sets were created in each language.</p><p>A picture was selected for each of the four words within each of the five VWP sets, following the procedure reported by <ref type="bibr" target="#b48">McMurray et al. (2010)</ref>. Several pictures for each word were initially sourced from a clipart database, mainly utilizing the MultiPic database <ref type="bibr" target="#b13">(Duñabeitia et al., 2018)</ref> for the Spanish task. A single picture was chosen as the most characteristic exemplar for each word. The selected images underwent refinement, including the removal of extraneous elements and optimization for clarity to ensure a faithful depiction of the intended word. The final images were approved by a lab member with extensive experience with VWP methodologies.</p><p>Experiment Builder (version 2022.2.5; SR Research Ltd., 2022) was used to program and run the experiment with the same settings across the two languages. Visual stimuli were presented on a 24" monitor with a resolution of 1024 × 768 pixels. Each display consisted of five visual stimuli, including four pictures and an "X". The display was arranged in a pentagonal configuration, as illustrated in Figure <ref type="figure">1</ref>. The pentagonal configuration was designed to maintain an equal distance between the center of each picture and screen center (320 pixels), as well as from one picture to another (376 pixels apart). Each picture was 240 × 240 pixels, while the "X" was 66 × 80 pixels. The position of the four pictures was randomized across trials, except for the "X", which was always at the bottom.</p><p>Figure <ref type="figure">1</ref> VWP Display Note. Lines and pixel values are for illustration and were not shown during the experiment.</p><p>Images shown here were from the Spanish VWP; the same configuration was used in the English VWP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure</head><p>Participants were first fitted with the eye tracker and were given instructions. Then, a familiarization phase was administered, in which each picture and its corresponding word were shown one by one.</p><p>At the beginning of each trial of the main experiment, a red circle appeared at the center of the screen along with four pictures and an "X" presented in a pentagonal configuration. After 500 ms, the circle turned blue and participants clicked on it to hear a word. This duration between the appearance of red circle and the mouse click allowed participants a momentary visual preview of the pictures before the onset of the auditory stimulus to minimize potential eye movements driven by visual search <ref type="bibr" target="#b3">(Apfelbaum et al., 2021)</ref>. Participants then clicked on the corresponding picture or the "X" if they thought none of the four pictures matched what they had heard. While there was no explicit time constraint, participants typically responded in less than two seconds (Spanish: M = 1452 ms, SD = 138 ms; English: M = 1645 ms, SD = 195 ms). The trial ended once participants clicked on a picture (or the "X"), and the next trial began. The VWP task for each language took about 52 minutes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Eye-tracking Recording and Analysis</head><p>Eye movements were recorded at a sampling rate of 1,000 Hz utilizing the SR Research EyeLink 1000 Plus system, configured in a head-stabilized mode with a chin-rest and a 35mm lens, within a sound-attenuated booth. The distance between the participant's eyes and the screen was maintained at 70 cm. The overhead lighting and audio volume were set at a consistent level across participants. Auditory stimuli were presented through BeyerDynamic DT-770 Pro 250 Ohm headphones. Calibration and validation of the eye-tracker (conducted between the familiarization phase and the main experiment) used the standard nine-point procedure. A drift check was performed every 20 trials, which was also when participants could take a break. If a drift check failed, the eye-tracker was recalibrated. The eye-tracking data, recorded from the onset of the trial (the appearance of the blue circle) to the participant's response (mouse click), was automatically parsed into saccades and fixations using default psychophysical parameters with EyelinkAnalysis (version 3.5.1). Adjacent saccades and fixations were amalgamated into a single look, commencing at the onset of the saccade and concluding at the fixation offset, consistent with established methodologies <ref type="bibr" target="#b49">(McMurray et al., 2002</ref><ref type="bibr" target="#b48">(McMurray et al., , 2010))</ref>.</p><p>For analyses, the eye-tracking data were downsampled to 250 Hz, and a fixed trial duration of 2,000 ms relative to stimulus onset was established. In instances where trials terminated before this point, the last eye movement was extended. Conversely, trials surpassing the 2,000-ms threshold were truncated. This approach has been used in many previous studies (e.g., <ref type="bibr" target="#b1">Allopenna et al., 1998;</ref><ref type="bibr">Kapnoula et al., 2021;</ref><ref type="bibr" target="#b49">McMurray et al., 2002;</ref><ref type="bibr"></ref> noise or head drift in the eye-track record. This extension did not result in any overlap, with a 118-pixel vertical and 136-pixel horizontal space maintained between pictures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Order of Tasks</head><p>Participants were randomly assigned to either Group A (N = 35) or Group B (N = 35).</p><p>The order of tasks for each group is shown in Figure <ref type="figure">2</ref>. All participants attended two lab sessions between seven and fourteen days apart. The first session lasted approximately 1.5 hours, while the second session lasted about one hour. In addition, all participants completed an online language exposure questionnaire and a training task, designed to familiarize them with the English stimuli, one to seven days prior to their English session.</p><p>Figure <ref type="figure">2</ref> Order of Tasks for Each Group</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transparency and Openness</head><p>Data collection took place during the summer of 2023. We report how we determined our sample size, all data exclusions, all manipulations, and all measures in the study. All data, analysis code, and stimuli for both the VAS and VWP tasks are available on the OSF page [https://osf.io/9g63y/]. This study's design and its analysis were not pre-registered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>All 70 participants finished all the tasks. However, data from five participants were excluded from the Spanish VWP, and data from four participants were excluded from the English VWP due to problematic eye-tracking data. As mentioned above, the Spanish VAS data of two participants and the English VAS data of three participants were excluded from the analyses. To maximize statistical power, all remaining data from the participants mentioned above were included in the analyses. Therefore, the number of participants included in subsequent data analyses varied from 63 to 68, with specific numbers provided in each corresponding table and figure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preliminary VAS Analyses: Individual Differences in Gradiency and Perceptual</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consistency</head><p>The expected patterns were observed in the VAS tasks, with substantial individual differences in both gradiency and perceptual consistency. That is, some individuals clicked more frequently on intermediate points on the line, while others displayed a strong preference for the endpoints. Similarly, participants differed in how consistently they rated the same stimuli. To illustrate the observed patterns, Figure <ref type="figure" target="#fig_0">3</ref> provides some examples of individualsubject Spanish VAS data with different gradiency and perceptual consistency patterns.</p><p>Figure <ref type="figure" target="#fig_1">4</ref> shows the average VAS data for Spanish and English. The relatively high /p/ response when the VOT is low in the English VAS indicates that the recognition of the English /b/ sound is challenging for our participants; recall that the VOT range for /b/ in English is similar to that for /p/ in Spanish. In contrast, there is no bilabial consonant in Spanish with a VOT similar to that of the English /p/.</p><p>Correlation analyses were conducted between gradiency, perceptual consistency, our covariates (i.e., working memory, inhibitory control, L2 proficiency, L2 exposure, and musical training), and demographic variables to examine the data structure. Detailed results can be found in Supplementary Material VI.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preliminary VWP Analyses</head><p>Before the analyses, the VOT step of initial phoneme of each auditory stimulus was recoded to reflect its acoustic distance from the target (henceforth: tDist), ranging from 0 to 6. To illustrate, consider a stimulus with VOT Step 1 (VOT = -35 ms in Spanish and 0 ms in English). In this case, tDist would be coded as 0 for the extreme /b/ targets (e.g., "balanza"</p><p>and "beachball") and 6 for the extreme /p/ targets (e.g., "balacio" and "beachpit"). Likewise, for a stimulus with VOT Step 7 (VOT = +15 ms in Spanish and +48 ms in English), tDist would be 0 for the extreme /p/ targets (e.g., "palacio" and "peachpit") and 6 for the extreme /b/ targets (e.g., "palanza" and "peachball"). This recoding allowed us to assess the effect of acoustic distance from the target across both voiced and voiceless stimuli.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analyses of Click Responses</head><p>For the Spanish VWP, in the case of completely unambiguous target stimuli (i.e., tDist = 0), average accuracy was 96.3% (SD = 7.1%). Participants clicked on the competitor on 0.1% (SD = 0.4%) of the trials, on the filler item 0.1% (SD = 0.3%), and on the "X" 3.5%</p><p>(SD = 7.1%). RTs averaged 1,452 ms (SD = 138 ms). For the English VWP, average accuracy for completely unambiguous target stimuli was good but noticeably lower (M = 81.4%, SD = 11.4%). In these instances, participants clicked on the competitor on 2.3% (SD = 3.3%) of the trials, on the filler item on 0.1% (SD = 0.3%), and on the "X" on 16.2% (SD = 12.0%). Participants exhibited slightly slower RTs in English (M = 1,645 ms, SD = 195 ms).</p><p>The clicking response patterns for L1 vs. L2 differed more when the tDist was larger.</p><p>For both languages, an increase in tDist reasonably led participants to be less inclined to click on the target and more inclined to click on the "X" to indicate that none of the pictures matched the heard word; however, this pattern was less pronounced for English. For Spanish, when the VOT was highly misleading (tDist = 6), participants selected the target on 21.6% of the trials and selected the "X" on 77.6% (Figure <ref type="figure" target="#fig_2">5a</ref>). contrast, for highly misleading items in English, participants selected the target on 54.7% of the trials and selected the "X" only on 42.5% (Figure <ref type="figure" target="#fig_2">5b</ref>). Hence, participants' clicking responses in English seem to be more affected by the disambiguating (lexical) information. While the above results showed that participants performed the VWP reasonably, preliminary VWP analyses on accuracy, RT, and fixations were conducted to further ensure that our manipulation was successful. Importantly, as expected, when the acoustic distance from the target (tDist) was higher in both English and Spanish, participants were less likely to click on the target, clicked on the target more slowly, and fixated the target less often. For more details, interested readers can refer to Supplementary Material VII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Primary Analyses I: Effects of Gradiency on Initial Lexical Activation and Speech</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Perception Flexibility in L1 Spanish and L2 English</head><p>Our first research question is whether gradiency in speech categorization influences initial lexical activation and/or speech perception flexibility in L1 Spanish and L2 English.</p><p>To address this question, we first look at the initial lexical activation as indexed by the garden-path rate -the proportion of trials when a participant fixated on the competitor picture before the was reached (corrected for a 200 ms oculomotor delay; <ref type="bibr" target="#b20">Hallett, 1986;</ref><ref type="bibr" target="#b64">Salverda et al., 2014)</ref> -as a measure of initial lexical activation. Then, we use two measures to capture speech perception flexibility: (1) recovery rate, the proportion of trials in which a participant ultimately recovered by looking back at the target picture after making a gardenpath (after the POD); (2) recovery latency, the time it took a participant to recover.</p><p>Building upon the methodology established by <ref type="bibr">Kapnoula et al. (2021)</ref>, linear mixedeffects models (LMEMs) were employed to examine the relationship between slope (the inverse of gradiency; extracted from the VAS) and how participants coped with garden-path situations (i.e., garden-path rate, recovery rate, and recovery latency). Apart from slope, each analysis involved two additional factors: the tDist and target voicing. As described above, tDist represents the acoustic distance between the target and the auditory stimulus in terms of the VOT step of the initial phoneme. Target voicing depends on whether the target started with a /b/ (e.g., "balanza" in Spanish or "beachball" in English) or /p/ (e.g., "palacio" in Spanish or "peachpit" in English).</p><p>For all the following analyses, LMEMs were conducted using the lmerTest package (Version 3.1-3; <ref type="bibr" target="#b34">Kuznetsova et al., 2017)</ref>, an extension of the lme4 package (Version 1.1-35.1; <ref type="bibr" target="#b6">Bates et al., 2015)</ref>, in R (Version 4.3.2; R Core <ref type="bibr" target="#b59">Team, 2023)</ref> via RStudio (Version 2023.12.0+369; RStudio <ref type="bibr" target="#b59">Team, 2023)</ref>. Graphs were created with ggplot2 package <ref type="bibr" target="#b74">(Wickham, 2016)</ref>. For each analysis, we compared models with increasing complexity to determine the random effect structures justified by our data using likelihood ratio tests (LRTs; <ref type="bibr" target="#b41">Matuschek et al., 2017)</ref>. We also decided whether the covariates should be included in the model as fixed effects using the same method. Based on the results, we only kept tDist, slope (or consistency), and target voicing in our main analyses. The model used in each analysis and the complete statistics are specified in Supplementary Material VIII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Garden-path Rate and Gradiency</head><p>The first analysis examined the likelihood of participants looking at the competitor image (e.g., "palacio" when hearing "balanza") before reaching the POD. Each trial was given a value of 1 if the participant looked at the competitor at any time before the POD of the stimulus on that trial, and a 0 otherwise. This measure was averaged within-cell, empirical-logit-transformed, and analyzed as a function of tDist (centered), target voicing (effect-coded; /b/ target = 1; /p/ target = -1), and slope (centered). The maximal random effects structure justified by our data included random intercepts and random slopes of tDist for subjects and items (see Supplementary Material VIII for the complete model).</p><p>Greater tDist predicted a higher proportion of garden-path trials both in L1 Spanish, B = 0.24, t(11) = 8.45, p &lt; .001 (Figure <ref type="figure" target="#fig_3">6a</ref>) and in L2 English, B = 0.06, t(11) = 3.32, p = .007 (Figure <ref type="figure" target="#fig_3">6b</ref>). This pattern is in line with the L1 English findings from <ref type="bibr" target="#b50">McMurray et al. (2009)</ref> and <ref type="bibr">Kapnoula et al. (2021)</ref> in showing that the strength of early lexical activation depends on fine-grained differences in VOT. Therefore, these results extend the previous finding to L1 Spanish and L2 English spoken word recognition. Crucially, VAS slope (gradiency) did not predict garden-path rate (Spanish: B = 0.05, t(61) = 1.03, p = .307; English: B = 0.04, t(61) = 0.58, p = .563), and its interaction between slope and tDist was also nonsignificant (Spanish: B = 0.00, t(61) = 0.31, p = .754; English: B = 0.00, t(61) = 0.35, p = .726), corroborating the findings from <ref type="bibr">Kapnoula et al. (2021)</ref>. Full results for Spanish and English are reported in Tables <ref type="table">S9a</ref> and<ref type="table">S9b</ref>, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recovery Rate and Gradiency</head><p>Next, we analyzed the likelihood of recovery, measured as the proportion of recovered trials out of all garden-path trials. Recovered trials were defined as instances where participants initially fixated on the competitor picture before the POD was reached, and subsequently directed their gaze to the target after the POD. This analysis includes trials where participants clicked on any picture (including "X") at the end. The proportion of recovered trials was empirical-logit-transformed and served as the DV in the LMEM. Due to singularity or convergence issues even after changing the optimizer to bobyqa and increasing the maximum iterations <ref type="bibr">(Brauer &amp; Curtin, 2018;</ref><ref type="bibr" target="#b41">Matuschek et al., 2017)</ref>, by-item random slopes were excluded for the Spanish model, and by-subject and by-item random slopes were excluded for the English model (see Supplementary Material VIII for the complete models and statistics).</p><p>Again, in line with <ref type="bibr">Kapnoula et al. (2021)</ref>, greater tDist predicted lower recovery rates, both for L1 Spanish, B = -0.19, t(60) = -10.77, p &lt; .001 (Figure <ref type="figure" target="#fig_3">6c</ref>) and for English, B = -0.03, t(2994) = -3.72, p &lt; .001 (Figure <ref type="figure" target="#fig_3">6d</ref>). The slope (gradiency) effect was marginally significant, B = 0.09, t(60) = 1.83, p = .073 for Spanish, and nonsignificant for English, B = -0.00, t(56) = -0.03, p = .974. Unlike previous results, the interaction between tDist and slope (gradiency) was not significant (Spanish: B = 0.03, t(59) = 1.56, p = .123; English: B = 0.00, t(2996) = 0.36, p = .719). Full results are reported in Table <ref type="table" target="#tab_0">S10a</ref> and<ref type="table" target="#tab_0">S10b</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recovery Latency and Gradiency</head><p>Next, we looked at the relationship between gradiency and the time it took participants to recover from garden-paths. This was calculated as the log-transformed time from the POD until the first fixation to the target. Only recovered trials were included. A mixed-effects model was employed, incorporating the same fixed effects as in the preceding analyses. By-subject and by-item random slopes for tDist were incorporated in the English model, while the by-item random slope was omitted in the Spanish model due to issues related to singularity.</p><p>For L1 Spanish (Figure <ref type="figure" target="#fig_3">6e</ref>), tDist significantly predicted recovery latency, B = 0.01, t(58) = 3.40, p = .001, with slower recovery at greater tDist. The main effect of slope (gradiency) was nonsignificant, B = 0.02, t(59) = 1.63, p = .108. Crucially, the interaction between tDist and slope (gradiency) was significant, B = 0.01, t(54) = 2.33, p = .024. To examine this interaction, we divided the dataset into high and low tDist (median point excluded) and ran the model with slope (gradiency) as the predictor in each dataset. In the high tDist model, slope (gradiency) was significant, B = 0.04, t(58) = 2.86, p = .006. In contrast, in the low tDist model, it was not significant, B = 0.00, t(58) = 0.01, p = .995. These results suggest that more gradient listeners recovered more quickly than less gradient ones, particularly when the tDist was high. For English (Figure <ref type="figure" target="#fig_3">6f</ref>), tDist once again significantly predicted recovery latency, B = 0.02, t(11) = 4.87, p = .001, with slower recovery at greater tDist. Neither the main effect of slope (gradiency), B = 0.00, t(56) = 0.39, p = .698, nor its interaction with tDist were significant, B = 0.00, t(53) = -0.06, p = .949. Full results are reported in Tables <ref type="table" target="#tab_0">S11a</ref> and<ref type="table" target="#tab_0">S11b</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interim Summary</head><p>These analyses revealed that gradiency has an impact on recovery latency in L1 Spanish: More gradient listeners recovered more quickly when the tDist was high. This finding suggests that gradiency is helpful in recovering from garden-paths, despite the VOT differences between Spanish and English (cf. <ref type="bibr">Kapnoula et al., 2021)</ref>. However, this effect was not observed in L2 English in the present study, suggesting that the functional role of gradiency is qualified by language status.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Primary Analyses II: Effects of Perceptual Consistency on Initial Lexical Activation and</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Speech Perception Flexibility in L1 Spanish and L2 English</head><p>Next, we examined our second research question: whether perceptual consistency -as opposed to gradiency -affects how listeners deal with lexical garden-path situations. All the predictors were the same as in the gradiency analyses above, but instead of slope, we used our perceptual consistency measure (centered). For brevity, we focus here on the main effect and interactions of perceptual consistency because the main effects of tDist were essentially unchanged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Garden-path Rate and Perceptual Consistency</head><p>Besides the significant effect of tDist that we have already reported, we found a marginal interaction between tDist and Spanish perceptual consistency, B = 0.01, t(61) = 1.91, p = .061 (Figure <ref type="figure" target="#fig_4">7a</ref>). Therefore, listeners who demonstrated greater perceptual consistency in the Spanish VAS were numerically but not significantly more inclined to make more garden-paths when the tDist was higher (Table <ref type="table" target="#tab_2">S12a</ref>). Similarly, for English (Figure <ref type="figure" target="#fig_4">7b</ref>), more consistent listeners made slightly more garden-paths when the tDist was higher, B = 0.002, t(61) = 1.71, p = .092 (Table <ref type="table" target="#tab_2">S12b</ref>). While the interactions between perceptual consistency and tDist were marginally significant, the same pattern emerged in both languages. This suggests that it is possible that we failed to detect a weak effect due to insufficient power. Therefore, we combined both the Spanish and English datasets and reran the same analyses, but with language (1: English; -1: Spanish) as an additional predictor. In this case, the interaction between tDist and perceptual consistency was significant, B = 0.004, t(269) = 3.03, p = .003. The three-way interaction of tDist, perceptual consistency, and language was not significant, B = -0.001, t(1243) = -1.20, p = .231, indicating that the interaction between tDist and perceptual consistency was not specific to one language.</p><p>Therefore, in general, listeners with higher perceptual consistency made more garden-paths when the tDist was high, compared with listeners with lower perceptual consistency (Table <ref type="table" target="#tab_2">S12c</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recovery Rate and Perceptual Consistency</head><p>No significant main effects of perceptual consistency (Spanish: B = -0.00, t(61) = -0.06, p = .954; English: B = 0.00, t(55) = -0.06, p = .954) or interactions between perceptual consistency and tDist (Spanish: B = -0.00, t(59) = -0.54, p = .592; English: B = 0.001, t(2989) = 0.45, p = .651) were observed in both languages (Figures <ref type="figure" target="#fig_4">7c</ref> and<ref type="figure">d</ref>; Tables <ref type="table" target="#tab_3">S13a</ref> and<ref type="table">b</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recovery Latency and Perceptual Consistency</head><p>No significant main effects of perceptual consistency (Spanish: B = 0.00, t(64) = 0.10, p = .923; English: B = -0.00, t(55) = -0.36, p = .717) or interactions between perceptual consistency and tDist (Spanish: B = 0.00, t(60) = -0.20, p = .841; English: B = 0.00, t(54) = -0.64, p = .523) were observed (Figures <ref type="figure" target="#fig_4">7e</ref> and<ref type="figure">f</ref>; Tables <ref type="table" target="#tab_0">S14a</ref> and<ref type="table">b</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interim Summary</head><p>In summary, participants who responded more consistently in the VAS tended to make more garden-paths when the tDist was higher, compared with listeners with lower perceptual consistency, in both L1 Spanish and L2 English. This suggests that listeners with higher perceptual consistency tend to make higher use of acoustic information to activate lexical candidates during early stages of spoken word recognition. In addition, the functional role of perceptual consistency appears to be language-general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Primary Analyses III: Are Gradiency and Perceptual Consistency Stable Across</head><p>Languages?</p><p>To test our third research question, we looked at the relationship between L1 and L2 gradiency by conducting Spearman's correlations between the Spanish and English slopes.</p><p>Similar analyses were conducted for perceptual consistency. The positive correlation between Spanish and English perceptual consistency was significant, r(63) = .44, p &lt; .001, indicating that perceptual consistency is not language-specific (Figure <ref type="figure" target="#fig_5">8a</ref>). In contrast, the correlation between Spanish and English slopes was nonsignificant, r(63) = .13, p = .314, indicating that gradiency is language-specific (Figure <ref type="figure" target="#fig_5">8b</ref>). Hence, perceptual consistency seems to reflect an individual trait, while gradiency seems to be language-specific. This is in line with previous work showing that gradiency and perceptual consistency are independent (e.g., <ref type="bibr" target="#b27">Kapnoula et al., 2017)</ref>. In addition, the stability of speech sound perception is likely a more general mechanism that spans across languages. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bayesian Analyses</head><p>Given the observation of some null effects in our findings, we conducted Bayesian analyses to evaluate the evidence for these effects. Following the recommendations of Dienes (2024), we re-ran all analyses (not only those corresponding to null effects) using a Bayesian approach to ensure consistency. The results were comparable to our main findings.</p><p>Importantly, there was decisive evidence supporting the finding that more gradient listeners recovered more quickly than less gradient ones when the tDist was high for Spanish. Weak evidence against the interaction was found for English, which means that we cannot strongly conclude in favor of the absence of the interaction. Additionally, there was very strong evidence in Spanish that listeners with higher perceptual consistency made more gardenpaths when the tDist was high, compared to listeners with lower perceptual consistency.</p><p>Strong evidence for this pattern was also found for English. Furthermore, the evidence for the positive correlation between Spanish and English perceptual consistency was decisive. The absence of correlation between Spanish and English gradiency was weakly supported, which means that we cannot strongly conclude that the correlation was absent or present. Interested readers can refer to Supplementary Material IX for detailed information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>In the present study, we investigated the impacts of speech perception gradiency and perceptual consistency on initial lexical activation and speech perception flexibility. Testing both L1 and L2 spoken word recognition allowed us to better understand the nature of these constructs and the degree to which they are language-specific vs. generic traits. Specifically, our research extends prior work in this domain in three substantial ways: (1) We examined whether the functional role of gradiency in speech perception flexibility is modulated by language-specific properties and language status, (2) we asked whether the functional role of perceptual consistency in spoken word recognition is modulated by language, and (3) we directly compared gradiency and perceptual consistency across languages. Similarly to <ref type="bibr">Kapnoula et al. (2021)</ref>, we found that in L1 Spanish, more gradient listeners showed enhanced speech perception flexibility in dealing with misleading auditory inputs. In contrast, the same effects of gradiency were minimal in listeners' non-native language processing.</p><p>Therefore, the functional role of gradiency appears to be qualified by language status but not by acoustic (in this case, VOT) information. Furthermore, listeners with greater perceptual consistency were more likely to activate lexical candidates at an early stage in both languages, suggesting that, in contrast to gradiency, consistency is a generic speech perception trait. This conclusion was further supported by the finding that perceptual consistency is more stable across languages, while gradiency is more language-specific.</p><p>These findings offer significant contributions to our understanding of speech perception, as discussed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gradient Listeners Recovered More Quickly in L1</head><p>A critical finding of our study is that in their L1, listeners with higher gradiency recovered more quickly than less gradient ones, after they were initially misled by gardenpath stimuli. In other words, they corrected their interpretation more rapidly after being presented with disambiguating information later in the word. This effect was supported by decisive evidence according to the Bayesian analysis. Notably, this rapid adjustment was especially evident when the acoustic deviation from the target was high. This observation is in line with the idea that more gradient listeners are better able at considering multiple hypotheses concurrently. This ability helps them avoid early commitment to a single lexical interpretation, enabling more efficient resolution of misunderstandings <ref type="bibr">(Kapnoula et al., 2021)</ref>. In contrast, less gradient listeners experience more category-driven warping, which may lead them to strongly commit to one word and suppress all others, slowing down their recovery from misleading information. Our results demonstrate that the ability of more gradient listeners to recover from misinterpretations is not language-specific, as we successfully extended <ref type="bibr">Kapnoula et al.'s (2021)</ref> findings in English to Spanish, despite the different VOTs of stop consonants between the two languages. This suggests that a fundamental benefit of gradiency is its enhancement of speech perception flexibility.</p><p>Although the same conclusion was reached in the present study and <ref type="bibr">Kapnoula et al. (2021)</ref>, it is important to note that this conclusion is based on different measures. <ref type="bibr">Kapnoula et al. (2021)</ref> observed that as the acoustic distance from the target increased, monolingual English speakers with higher gradiency showed a higher recovery rate compared with those with lower gradiency. On the other hand, our findings indicated that more gradient L1 Spanish bilinguals had shorter recovery latency than less gradient bilinguals. This difference may be attributed to differences in VOT distributions between Spanish and English. In Spanish, the /b/-/p/ contrast is defined by the presence (/b/) vs. absence (/p/) of pre-voicing -a qualitative difference; the distinction in English is more quantitative, with shorter positive VOTs for /b/ than for /p/. As a result, native Spanish speakers might exhibit more categorical responses on the Spanish VAS, making them potentially less sensitive to within-category differences (for preliminary evidence for this, see <ref type="bibr" target="#b26">Kapnoula &amp; Samuel, 2024)</ref>. Thus, the relationship between gradiency and recovery rate might be obscured by a narrower range of gradiency among Spanish speakers. Consequently, the effect was only captured by recovery latency, a measure more sensitive than the recovery rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Minimal Gradiency Effects on Recovery in L2</head><p>Another main question in the current study is whether the gradiency effects found in L1 operate in the same way in L2. This question clarifies whether the functional roles of gradiency are affected by language status. Contrary to our expectations, the present study revealed minimal gradiency effect on the recovery latency in L2 English, even though these same listeners showed such effect in their L1 Spanish, suggesting that the functional role of gradiency is affected by language status. The Bayesian analyses revealed weak evidence against the interaction between gradiency and recovery latency. Although we cannot strongly conclude in favor of the absence of the interaction, the weak evidence can potentially be explained by at least two factors. Firstly, Spanish learners of English may predominantly utilize the secondary cue of F0 when determining voicing distinctions within the positive VOT range <ref type="bibr" target="#b38">(Llanos et al., 2013;</ref><ref type="bibr"></ref> see Figure <ref type="figure" target="#fig_0">S3</ref> for present findings supporting this account).</p><p>Consequently, our VOT manipulations might exert less influence on their early encoding and categorization processes compared to native English speakers.</p><p>Secondly, in general, our participants confined their ratings to a narrow range in the English VAS, for example, only responding from 40 to 60 on a scale from 0 to 100 to signify sounds from a clear /b/ to a clear /p/, suggesting that they might not have a clear representation of the category (see Figure <ref type="figure" target="#fig_1">4b</ref>). A paired-sample t-test showed that the rating range (calculated by the difference between the maximum and minimum values based on the rotated logistic function) of the English VAS rating was significantly narrower than that of Spanish VAS, t(64) = -15.9, p &lt; .001. Importantly, the correlation between the rating range and English proficiency score was not significant, r(65) = .12, p = .341, indicating that this difference was not driven by differences in proficiency. Rather, this aligns with the view that non-native learners often face challenges in forming categorical representations, despite</p><p>extensive training (see Baese-Berk et al., 2022, for a review). Indeed, our findings highlighted a particular challenge with the English /b/ sound, as evidenced by lower accuracy rates for /b/ compared to /p/, even when the acoustic distance from the target was minimal in the English VWP (see Figure <ref type="figure">S1b</ref>). As we have noted, this difficulty can be attributed to the similarity in VOT between the English /b/ and Spanish /p/. According to the revised speech learning model (SLM-r) by <ref type="bibr" target="#b15">Flege and Bohn (2021)</ref>, L2 speech sounds that fall close to a learner's L1 categories are particularly challenging to acquire. Related to this, note that only those participants who demonstrated an increase in their responses from a complete /b/ to a /p/ sound were included in the analysis. It would be informative to test balanced bilinguals in languages with similar VOTs when examining the relationship between gradiency and speech perception flexibility, in order to better understand the relationship between language status and the functional roles of gradiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Perceptual Consistency Facilitates Initial Lexical Activation</head><p>Variability is a fundamental aspect of speech perception, encompassing not just external environmental factors, such as the varying VOTs articulated by different speakers, but also variability inherent to speech processing, which is what our perceptual consistency measure aimed at capturing. Previous work suggests that higher perceptual consistency is linked to more effective learning of non-native phonetic contrasts <ref type="bibr" target="#b17">(Fuhrmeister et al., 2023;</ref><ref type="bibr" target="#b22">Honda et al., 2024)</ref> and better reading and language abilities <ref type="bibr" target="#b29">(Kim et al., 2024)</ref>. Notably, our study is the first to highlight another possible advantage of perceptual consistency: its role in facilitating initial lexical activation. Specifically, in our study, more consistent listeners looked at competitors more before the POD was reached when the word onset was highly misleading -an effect observed in both L1 Spanish with very strong evidence and L2 English with strong evidence in the Bayesian analyses. In this context, the absence of a garden-path can be taken to index delayed initial lexical activation due to the short period of time before the POD (around 400 ms). Therefore, this finding directly speaks to the role of perceptual consistency in early spoken word recognition. Our interpretation is the following: The speech perception system of listeners with lower perceptual consistency is likely characterized by higher noise and/or lower stability of cue-to-category mapping, leading to greater uncertainty.</p><p>This uncertainty may lead listeners to use a more wait-and-see strategy in early stages of spoken word recognition. In other words, the initial lexical activation is slowed down to avoid committing to a lexical candidate until more information arrives.</p><p>This explanation aligns with the ideal observer model, which posits that high uncertainty prompts listeners to adjust their mapping of cue values onto phoneme categories, allowing them to keep multiple options open until further clarifying information is received <ref type="bibr" target="#b11">(Clayards et al., 2008;</ref><ref type="bibr" target="#b54">Nixon et al., 2016)</ref>. Additionally, this idea is consistent with previous findings showing that listeners delay lexical activation when the input is noisy (e.g., cochlear implant users and normal-hearing listeners presented with noise-vocoded speech; <ref type="bibr" target="#b14">Farris-Trimble et al., 2014;</ref><ref type="bibr" target="#b47">McMurray et al., 2017;</ref><ref type="bibr">Smith &amp; McMurray, 2022)</ref>. This wait-and-see strategy may be a natural consequence of lower stability in perceptual encoding or a coping mechanism adopted by listeners in high uncertainty situations, allowing them to keep alternatives available and enhancing flexibility in recovering from a misperception (see <ref type="bibr">McMurray et al., 2022, for relevant discussion)</ref>. Regardless of the specifics of the underlying mechanism, this finding, along with prior research, collectively suggests that phonetic encoding stability confers multiple benefits in speech perception. Furthermore, the present study demonstrated that the functional roles of perceptual consistency in spoken word recognition is not language-specific, as we found evidence for this in both languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gradiency and Perceptual Consistency between L1 and L2</head><p>We observed some intriguing findings regarding gradiency and perceptual consistency in L1 vs. L2. L1 and L2 gradiency were not correlated, supported by weak evidence against the alternative hypothesis in the Bayesian analyses. Although we cannot strongly conclude that the correlation was absent or present, the weak evidence may suggest that gradiency is not a generic trait of a listener but rather depends on the specific properties of different contrasts and cues. This interpretation should be approached with caution due to the narrow range of ratings by some participants on the English VAS in this study. However, previous studies also support this interpretation, showing that the correlation between gradiency measures is stronger when contrasts rely on similar acoustic cues (e.g., English /b/-/p/ and English /d/-/t/; r(64) = .41, p = .001; <ref type="bibr">Kapnoula &amp; McMurray, 2021)</ref> compared to different cues (e.g., English /b/-/p/ and English /s/-/sh/; r(57) = .19, p = .16; <ref type="bibr">Kapnoula et al., 2021)</ref>.</p><p>This pattern suggests that gradiency depends on how listeners process specific acoustic cues, aligning with the idea that differences in gradiency stem from differences in early encoding of speech cues <ref type="bibr">(Kapnoula &amp; McMurray, 2021)</ref>. Other studies have found higher correlations between gradiency measures extracted from different languages (English /d/-/t/ and Korean /t/-/th/; R 2 = .309; <ref type="bibr" target="#b31">Kong &amp; Kang, 2023)</ref> and different contrasts (English /d/-/t/ and English /s/-/sh/; r(55) = .32, p = .02; <ref type="bibr" target="#b16">Fuhrmeister &amp; Myers, 2021)</ref>. However, direct comparisons are challenging due to substantial methodological differences.</p><p>On the other hand, for perceptual consistency, we found a significant positive correlation between L1 and L2 supported by decisive evidence in the Bayesian analyses. This finding suggests that perceptual consistency is a generic trait, aligning with the study by <ref type="bibr" target="#b17">Fuhrmeister et al. (2023)</ref>, which found a positive correlation in perceptual consistency across different phonetic categories, specifically between stops ("ba"-"pa") and fricatives ("s"-"sh").</p><p>Additionally, <ref type="bibr" target="#b22">Honda et al. (2024)</ref> identified a positive correlation in perceptual consistency between 2AFC and VAS tasks, suggesting that cue encoding stability is a stable and taskindependent trait. The current study extends these findings by showing the stability of perceptual consistency when processing the same acoustic cue but in different languages.</p><p>This suggests that cue encoding stability is likely a more general mechanism compared to within-category sensitivity.</p><p>More broadly, the different correlation patterns for gradiency vs. perceptual consistency suggest that speech perception cue encoding stability and within-category sensitivity are distinct mechanisms, corroborating previous findings <ref type="bibr" target="#b16">(Fuhrmeister &amp; Myers, 2021;</ref><ref type="bibr" target="#b22">Honda et al., 2024)</ref>. Indeed, prior work has highlighted the important theoretical distinction between gradiency and perceptual consistency (e.g., <ref type="bibr" target="#b4">Apfelbaum et al., 2022;</ref><ref type="bibr" target="#b16">Fuhrmeister &amp; Myers, 2021;</ref><ref type="bibr" target="#b22">Honda et al., 2024)</ref>. Our study echoes prior work and significantly expands it by examining the link between gradiency and perceptual consistency in the context of L1 vs. L2</p><p>processing. Additionally, our study extends previous work by examining the role of perceptual consistency in speech perception and, more specifically, its relationship with early lexical activation and speech perception flexibility.</p><p>The underlying causes and functions of gradiency and perceptual consistency require more in-depth examination. Recent research has started to address these issues. For example, <ref type="bibr" target="#b26">Kapnoula and Samuel (2024)</ref> found that gradiency is predicted by (a) temporal auditory acuity (i.e., the domain-general ability to discern subtle acoustic differences across various acoustic dimensions such as pitch, duration, and intensity; Saito, 2023) and (b) cumulative exposure to spoken language (as indicated by age). Additionally, social network diversity has been shown to promote gradient perception <ref type="bibr" target="#b32">(Kutlu et al., 2024)</ref>. Despite the increased interest in recent years, many open questions remain regarding the origins of gradiency and perceptual consistency, as well as their functional roles in spoken language processing.</p><p>Addressing these questions will have high theoretical value, leading to a more comprehensive understanding of basic speech perception mechanisms, and could also be instrumental in identifying strategies to enhance L2 acquisition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>The current study provides evidence that both gradiency and perceptual consistency play important roles in spoken word recognition and, most importantly, it sheds new light onto the different nature of these two properties. On the one hand, perceptual consistency is crucial at the early stages of spoken word recognition and appears to be a generic trait of how listeners process speech; listeners with lower consistency are less likely to start activating words early on, possibly to avoid committing to the wrong word, in both L1 and L2. On the other hand, gradiency enhances speech perception flexibility by speeding up recovery from misleading auditory stimuli. Importantly, this effect was (once again) observed in L1, but only weak evidence was found in L2, suggesting that the functional role of gradiency in spoken word recognition is language-specific.</p><p>Apart from their theoretical value, these findings have substantial practical implications. In everyday life, individuals often encounter ambiguous and misleading auditory inputs due to factors such as background noise, speaker accents, or coarticulation.</p><p>Our findings indicate that individuals with higher perceptual consistency and gradiency are better equipped to identify and deal with such complexity.</p><p>There is a clear need for further research to examine not only the functions of gradiency and perceptual consistency but also their developmental trajectories in both monolingual and multilingual settings. Addressing these issues will deepen our understanding of speech perception and may also lead to practical applications in language learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3</head><label>3</label><figDesc>Figure 3 Examples of Spanish VAS Data Showing Different Gradiency and Perceptual Consistency of Four Participants</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4</head><label>4</label><figDesc>Figure 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5</head><label>5</label><figDesc>Figure 5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 (</head><label>6</label><figDesc>Figure 6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 (</head><label>7</label><figDesc>Figure 7</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8</head><label>8</label><figDesc>Figure 8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Summary of Tasks and Corresponding Independent and Dependent Variables</figDesc><table><row><cell>Task</cell><cell>Measuring</cell><cell>Duration (min)</cell></row><row><cell>Independent variables</cell><cell></cell><cell></cell></row><row><cell>VAS</cell><cell>Speech perception gradiency</cell><cell>5-8 for each language</cell></row><row><cell></cell><cell>and perceptual consistency</cell><cell></cell></row><row><cell>Spatial Stroop task</cell><cell>Inhibitory control</cell><cell>5</cell></row><row><cell>Corsi Block-tapping task</cell><cell>Working memory</cell><cell>5</cell></row><row><cell cols="2">English picture naming task English proficiency</cell><cell>5</cell></row><row><cell>Questionnaire</cell><cell>Language exposure and</cell><cell>3</cell></row><row><cell></cell><cell>musical training</cell><cell></cell></row><row><cell>Dependent variable</cell><cell></cell><cell></cell></row><row><cell>VWP</cell><cell>Initial lexical activation and</cell><cell>52 for each language</cell></row><row><cell></cell><cell>speech perception flexibility</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>gradiency of responses, with shallower slopes reflecting more gradient responses. This approach, unlike standard logistic regression, provides orthogonal measures of gradiency and secondary cue use (i.e., use of F0).</figDesc><table><row><cell>The equation was fitted to each participant's VAS responses implemented in</cell></row><row><cell>MATLAB (version R2022a, the MathWorks Inc., R2022a) that minimized the least squared</cell></row><row><cell>error (free software available at McMurray, 2017). For the Spanish VAS, two participants</cell></row><row><cell>were excluded due to problematic fits. For the English VAS, one participant was excluded for</cell></row><row><cell>exhibiting a flat response curve (i.e., identical responses, regardless of stimulus variation</cell></row></table><note><p>The degree of gradiency was quantified by fitting a rotated logistic function (see Supplementary Material IV for more details), the slope of which indicated the from /b/ to /p/), one because of a reversed response curve (i.e., a decrease in the number of /p/ responses as stimuli changed from /b/ to /p/), and one due to problematic fit. The remaining fittings were good, with an average R 2 of .941 and .839 for the Spanish and English tasks, respectively.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>Critical Stimuli used in the English VWP with International Phonetic Alphabet (IPA), as in</figDesc><table><row><cell cols="2">Kapnoula et al. (2021)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Set</cell><cell cols="2">Voiced word</cell><cell cols="2">Voiceless word</cell><cell>Overlapping</cell></row><row><cell></cell><cell>Spelling</cell><cell>IPA</cell><cell>Spelling</cell><cell>IPA</cell><cell>phonemes</cell></row><row><cell>1</cell><cell>bumpercar</cell><cell>bʌmpərkɑr</cell><cell cols="2">pumpernickel pʌmpərnɪkəl</cell><cell>5</cell></row><row><cell>2</cell><cell>barricade</cell><cell>baerəkeɪd</cell><cell>parakeet</cell><cell>paerəkit</cell><cell>4</cell></row><row><cell>3</cell><cell>blanket</cell><cell>blaeŋkɪt</cell><cell>plankton</cell><cell>plaeŋktən</cell><cell>4</cell></row><row><cell>4</cell><cell>beachball</cell><cell>bitʃbɔl</cell><cell>peach-pit</cell><cell>pitʃpɪt</cell><cell>2</cell></row><row><cell>5</cell><cell>billboard</cell><cell>bɪlbɔrd</cell><cell>pillbox</cell><cell>pɪlbɒks</cell><cell>3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>Critical Stimuli used in the Spanish VWP with IPA and Meanings</figDesc><table><row><cell>Set</cell><cell></cell><cell>Voiced word</cell><cell></cell><cell cols="3">Voiceless word</cell><cell>Overlapping</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>phonemes</cell></row><row><cell></cell><cell>Spelling</cell><cell>IPA</cell><cell>Meaning</cell><cell>Spelling</cell><cell>IPA</cell><cell>Meaning</cell><cell></cell></row><row><cell>1</cell><cell cols="2">balanza balanθa</cell><cell>scale</cell><cell>palacio</cell><cell>palaθjo</cell><cell>palace</cell><cell>3</cell></row><row><cell>2</cell><cell>bañar</cell><cell>baɲaɾ</cell><cell>to bathe</cell><cell>pañal</cell><cell>paɲal</cell><cell>diaper</cell><cell>3</cell></row><row><cell>3</cell><cell>baraja</cell><cell cols="5">baɾaxa deck of cards paraguas paɾaɣwas umbrella</cell><cell>3</cell></row><row><cell>4</cell><cell cols="2">vaquero bakeɾo</cell><cell>cowboy</cell><cell>paquete</cell><cell>pakete</cell><cell>parcel</cell><cell>3</cell></row><row><cell>5</cell><cell>vestido</cell><cell>bestiðo</cell><cell>dress</cell><cell cols="3">pestañas pestaɲas eyelash</cell><cell>3</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>The rationale behind this decision is that we wish to avoid using a term like "response consistency" that refers to the extracted measure and rather focus on the underlying construct of interest. The specific term "perceptual consistency" is based on the working hypothesis that VAS responses reflect listeners' perception, as supported by previous results (e.g.,Kapnoula &amp; McMurray, 2021).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>In this study, English is referred to as L2, while acknowledging that for many participants, English may serve as their second, third, or even subsequent language.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div> Smith, F. X., &amp; McMurray, B. (2022)<p>. Lexical access changes based on listener needs: Realtime word recognition in continuous speech in cochlear implant users. Ear and Hearing, 43(5), <rs type="grantNumber">1487-1501</rs>. https://doi.org/10.1097/aud.0000000000001203 Sorensen, E., Oleson, J., Kutlu, E., &amp; McMurray, B. (2024). A Bayesian hierarchical model for the analysis of visual analogue scaling tasks. Statistical Methods in Medical Research. https://doi.org/10.1177/09622802241242319 Souganidis, C., Molinaro, N., <rs type="person">&amp; Stoehr</rs>, A. (2022). Bilinguals produce language-specific voice onset time in two true-voicing languages: The case of <rs type="person">Basque-Spanish</rs> early bilinguals. Linguistic Approaches to Bilingualism. https://doi.org/10.1075/lab.21081.sou SR Research Ltd. (2022). <rs type="person">Experiment Builder</rs> (<rs type="grantNumber">Version 2022.2.5</rs>) [Computer software]. https://www.sr-research.com/experiment-builder/ Studdert-Kennedy, M., Liberman, A. M., <rs type="funder">Harris</rs>, K. S., &amp; Cooper, F. S. (1970). Motor theory of speech perception: A reply to Lane's critical review. Psychological Review, 77(3), <rs type="grantNumber">234-249</rs>. https://doi.org/10.1037/h0029078 Tanenhaus, M. K., <rs type="person">Spivey-Knowlton</rs>, M. J., Eberhard, K. M., <rs type="person">&amp; Sedivy</rs>, J. C. (1995). Integration of visual and linguistic information in spoken language comprehension. Science, 268(5217), <rs type="grantNumber">1632-1634</rs>. https://doi.org/ 10.1126/science.7777863 Theodore, R. M., <rs type="person">&amp; Monto</rs>, N. R. (2019). Distributional learning for speech reflects cumulative exposure to a talker's phonetic distributions. <rs type="person">Psychonomic Bulletin &amp; Review</rs>, <rs type="grantNumber">26</rs>(3), <rs type="grantNumber">985-992</rs>. https://doi.org/10.<rs type="grantNumber">3758/s13423-018-1551-5</rs></p></div>
			</div>
			<div type="funding">
<div><p>This research is supported by the <rs type="funder">Basque Government</rs> through the <rs type="programName">BERC 2022-2025 program</rs> and funded by the <rs type="funder">Spanish State Research Agency</rs> through BCBL Severo Ochoa excellence accreditation <rs type="grantNumber">CEX2020-001010/AEI/10.13039/501100011033</rs>. The research is also supported by grant ref. <rs type="grantNumber">PRE2021-097223</rs>; project <rs type="grantNumber">CEX2020-001010-S-21-2</rs> funded by <rs type="grantNumber">MCIN/AEI/10.13039/501100011033</rs> and FSE+" awarded to <rs type="person">Brian W. L. Wong. Support</rs> for this project was provided by the <rs type="funder">Spanish Ministry of Science and Innovation</rs> through Grant #</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_U4EsaFx">
					<idno type="grant-number">1487-1501</idno>
				</org>
				<org type="funding" xml:id="_ShywdkS">
					<idno type="grant-number">Version 2022.2.5</idno>
				</org>
				<org type="funding" xml:id="_YVMPgDm">
					<idno type="grant-number">234-249</idno>
				</org>
				<org type="funding" xml:id="_MsnPemT">
					<idno type="grant-number">1632-1634</idno>
				</org>
				<org type="funding" xml:id="_byhy3UC">
					<idno type="grant-number">26</idno>
				</org>
				<org type="funding" xml:id="_MXkcAVE">
					<idno type="grant-number">985-992</idno>
				</org>
				<org type="funding" xml:id="_Ka7mcu7">
					<idno type="grant-number">3758/s13423-018-1551-5</idno>
				</org>
				<org type="funding" xml:id="_XdKSmGu">
					<orgName type="program" subtype="full">BERC 2022-2025 program</orgName>
				</org>
				<org type="funding" xml:id="_bmUbNuv">
					<idno type="grant-number">CEX2020-001010/AEI/10.13039/501100011033</idno>
				</org>
				<org type="funding" xml:id="_dwQxw6K">
					<idno type="grant-number">PRE2021-097223</idno>
				</org>
				<org type="funding" xml:id="_S6AMJjB">
					<idno type="grant-number">CEX2020-001010-S-21-2</idno>
				</org>
				<org type="funding" xml:id="_7fEDhvb">
					<idno type="grant-number">MCIN/AEI/10.13039/501100011033</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Listener sensitivity to individual talker differences in voice-onset-time</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.1121/1.1701898</idno>
		<ptr target="https://doi.org/10.1121/1.1701898" />
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3171" to="3183" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tracking the time course of spoken word recognition using eye movements: Evidence for continuous mapping models</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Allopenna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Magnuson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Tanenhaus</surname></persName>
		</author>
		<idno type="DOI">10.1006/jmla.1997.2558</idno>
		<ptr target="https://doi.org/10.1006/jmla.1997.2558" />
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="419" to="439" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The effect of subphonetic differences on lexical access</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Andruski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Blumstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burton</surname></persName>
		</author>
		<idno type="DOI">10.1016/0010-0277(94)90042-6</idno>
		<ptr target="https://doi.org/10.1016/0010-0277" />
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="90042" to="90046" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The pictures who shall not be named: Empirical support for benefits of preview in the Visual World Paradigm</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Apfelbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Klein-Packard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcmurray</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jml.2021.104279</idno>
		<ptr target="https://doi.org/10.1016/j.jml.2021.104279" />
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="page" from="121" to="104279" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Don&apos;t force it! Gradient speech categorization calls for continuous categorization tasks</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Apfelbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcmurray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Kapnoula</surname></persName>
		</author>
		<idno type="DOI">10.1121/10.0015201</idno>
		<ptr target="https://doi.org/10.1121/10.0015201" />
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3728" to="3745" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The nature of non-native speech sound representations</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Baese-Berk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Roark</surname></persName>
		</author>
		<idno type="DOI">10.1121/10.0015230</idno>
		<ptr target="https://doi.org/10.1121/10.0015230" />
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="3025" to="3034" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fitting linear mixed-effects models using lme4</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maechler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bolker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<idno type="DOI">10.18637/jss.v067.i01</idno>
		<ptr target="https://doi.org/10.18637/jss.v067.i01" />
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="48" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hearing in categories aids speech streaming at the &quot;cocktail party</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Bidelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Skubic</surname></persName>
		</author>
		<idno type="DOI">10.1101/2024.04.03.587795</idno>
		<ptr target="https://doi.org/10.1101/2024.04.03.587795" />
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Linear mixed-effects models and the analysis of nonindependent data: A unified framework to analyze categorical and continuous independent variables that vary within-subjects and/or within-items</title>
		<author>
			<persName><forename type="first">P</forename><surname>Boersma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weenink</surname></persName>
		</author>
		<idno type="DOI">10.1037/met0000159</idno>
		<ptr target="https://doi.org/10.1037/met0000159" />
	</analytic>
	<monogr>
		<title level="j">Psychological Methods</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Brauer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Curtin</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="389" to="411" />
			<date type="published" when="2018">2023. January 2023. 2018</date>
		</imprint>
	</monogr>
	<note>Praat: doing phonetics by computer [Computer program]. Version 6.3.03, retrieved 13</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Gradient acoustic information induces longlasting referential uncertainty in short discourses</title>
		<author>
			<persName><forename type="first">S</forename><surname>Brown-Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Toscano</surname></persName>
		</author>
		<idno type="DOI">10.1080/23273798.2017.1325508</idno>
		<ptr target="https://doi.org/10.1080/23273798.2017.1325508" />
	</analytic>
	<monogr>
		<title level="j">Language, Cognition and Neuroscience</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1211" to="1228" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Noncategorical perception of stop consonants differing in VOT</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Carney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Widin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">F</forename><surname>Viemeister</surname></persName>
		</author>
		<idno type="DOI">10.1121/1.381590</idno>
		<ptr target="https://doi.org/10.1121/1.381590" />
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="961" to="970" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Perception of speech reflects optimal use of probabilistic speech cues</title>
		<author>
			<persName><forename type="first">M</forename><surname>Clayards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Tanenhaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Aslin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Jacobs</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cognition.2008.04.004</idno>
		<ptr target="https://doi.org/10.1016/j.cognition.2008.04.004" />
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="804" to="809" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Use one system for all results to avoid contradiction: Advice for using significance tests, equivalence tests, and Bayes factors</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Dienes</surname></persName>
		</author>
		<idno type="DOI">10.1037/xhp0001202</idno>
		<ptr target="https://doi.org/10.1037/xhp0001202" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="531" to="534" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">MultiPic: A standardized set of 750 drawings with norms for six European languages</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Duñabeitia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Crepaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>New</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pliatsikas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Smolka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brysbaert</surname></persName>
		</author>
		<idno type="DOI">10.1080/17470218.2017.1310261</idno>
		<ptr target="https://doi.org/10.1080/17470218.2017.1310261" />
	</analytic>
	<monogr>
		<title level="j">Quarterly Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="808" to="816" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The process of spoken word recognition in the face of signal degradation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Farris-Trimble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcmurray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cigrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tomblin</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0034353</idno>
		<ptr target="https://doi.org/10.1037/a0034353" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="308" to="327" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The revised speech learning model (SLM-r)</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Flege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">S</forename><surname>Bohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second Language Speech Learning: Theoretical and Empirical Progress</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Wayland</surname></persName>
		</editor>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Structural neural correlates of individual differences in categorical perception</title>
		<author>
			<persName><forename type="first">P</forename><surname>Fuhrmeister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Myers</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.bandl.2021.104919</idno>
		<ptr target="https://doi.org/10.1016/j.bandl.2021.104919" />
	</analytic>
	<monogr>
		<title level="j">Brain and Language</title>
		<imprint>
			<biblScope unit="page" from="215" to="104919" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Relationships between native and non-native speech perception</title>
		<author>
			<persName><forename type="first">P</forename><surname>Fuhrmeister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Mccoach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Myers</surname></persName>
		</author>
		<idno type="DOI">10.1037/xlm0001213</idno>
		<ptr target="https://doi.org/10.1037/xlm0001213" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1161" to="1175" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Categorical perception depends on the discrimination task</title>
		<author>
			<persName><forename type="first">E</forename><surname>Gerrits</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E H</forename><surname>Schouten</surname></persName>
		</author>
		<idno type="DOI">10.3758/bf03194885</idno>
		<ptr target="https://doi.org/10.3758/bf03194885" />
	</analytic>
	<monogr>
		<title level="j">Perception &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="363" to="376" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Performance of dyslexic children on speech perception tests</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Godfrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Syrdal-Lasky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Millay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Knox</surname></persName>
		</author>
		<idno type="DOI">10.1016/0022-0965(81)90105-3</idno>
		<ptr target="http://dx.doi.org/10.1016/0022-0965(81)90105-3" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Child Psychology</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="401" to="424" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Eye movements</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Hallett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of perception and human performance</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Boff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Kaufman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">&amp;</forename><forename type="middle">J</forename><surname>Thomas</surname></persName>
		</editor>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1986">1986</date>
			<biblScope unit="page" from="10" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Categorical results do not imply categorical perception</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Massaro</surname></persName>
		</author>
		<idno type="DOI">10.3758/bf03202770</idno>
		<ptr target="https://doi.org/10.3758/bf03202770" />
	</analytic>
	<monogr>
		<title level="j">Perception &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="409" to="418" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Exploring individual differences in native phonetic perception and their link to nonnative phonetic perception</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Honda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Clayards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Baum</surname></persName>
		</author>
		<idno type="DOI">10.1037/xhp0001191</idno>
		<ptr target="https://doi.org/10.1037/xhp0001191" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Language deficits in dyslexic children: Speech perception, phonology, and morphology</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Joanisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Manis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Keating</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Seidenberg</surname></persName>
		</author>
		<idno type="DOI">10.1006/jecp.1999.2553</idno>
		<ptr target="http://dx.doi.org/10.1006/jecp.1999.2553" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Child Psychology</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="60" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Gradient activation of speech categories facilitates listeners&apos; recovery from lexical garden-paths, but not perception of speech-in-noise</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Kapnoula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcmurray</surname></persName>
		</author>
		<idno type="DOI">10.1037/xhp0000900</idno>
		<ptr target="https://doi.org/10.1037/xhp0000900" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="578" to="595" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Idiosyncratic use of bottom-up and top-down information leads to differences in speech perception flexibility: Converging evidence from ERPs and eye-tracking</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Kapnoula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcmurray</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.bandl.2021.105031</idno>
		<ptr target="https://doi.org/10.1016/j.bandl.2021.105031" />
	</analytic>
	<monogr>
		<title level="j">Brain and Language</title>
		<imprint>
			<biblScope unit="page" from="223" to="105031" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sensitivity to subphonemic differences in first language predicts vocabulary size in a foreign language</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Kapnoula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Samuel</surname></persName>
		</author>
		<idno type="DOI">10.1111/lang.12650</idno>
		<ptr target="https://doi.org/10.1111/lang.12650" />
	</analytic>
	<monogr>
		<title level="j">Language Learning. Advance online publication</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Evaluating the sources and functions of gradiency in phoneme categorization: An individual differences approach</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Kapnoula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcmurray</surname></persName>
		</author>
		<idno type="DOI">10.1037/xhp0000410</idno>
		<ptr target="https://doi.org/10.1037/xhp0000410" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1594" to="1611" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Individual differences in perceptual adaptation to unfamiliar phonetic categories</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Clayards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Kong</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.wocn.2020.100984</idno>
		<ptr target="https://doi.org/10.1016/j.wocn.2020.100984" />
	</analytic>
	<monogr>
		<title level="j">Journal of Phonetics</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page">100984</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Inconsistent speech categorization in school-age children with language and reading disabilities</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Klein-Packard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sorensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Oleson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tomblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcmurray</surname></persName>
		</author>
		<idno type="DOI">10.31234/osf.io/un6bx</idno>
		<ptr target="https://doi.org/10.31234/osf.io/un6bx" />
	</analytic>
	<monogr>
		<title level="j">PsyArXiv</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Individual differences in categorical perception of speech: Cue weighting and executive function</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Edwards</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.wocn.2016.08.006</idno>
		<ptr target="https://doi.org/10.1016/j.wocn.2016.08.006" />
	</analytic>
	<monogr>
		<title level="j">Journal of Phonetics</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="40" to="57" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Individual differences in categorical judgment of L2 stops: A link to proficiency and acoustic cue-weighting</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kang</surname></persName>
		</author>
		<idno type="DOI">10.1177/00238309221108647</idno>
		<ptr target="https://doi.org/10.1177/00238309221108647" />
	</analytic>
	<monogr>
		<title level="j">Language and Speech</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="354" to="380" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Social network diversity leads to more flexible speech perception in school-aged children</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Apfelbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sorensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Oleson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcmurray</surname></persName>
		</author>
		<idno type="DOI">10.31234/osf.io/c9u4y</idno>
		<ptr target="https://doi.org/10.31234/osf.io/c9u4y" />
	</analytic>
	<monogr>
		<title level="j">PsyArXiv</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Moving away from deficiency models: Gradiency in bilingual speech categorization</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcmurray</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2022.1033825</idno>
		<ptr target="https://doi.org/10.3389/fpsyg.2022.1033825" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">1033825</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">lmerTest package: Tests in linear mixed effects models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Brockhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H B</forename><surname>Christensen</surname></persName>
		</author>
		<idno type="DOI">10.18637/jss.v082.i13</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The discrimination of relative onset-time of the components of certain speech and nonspeech patterns</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Liberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lane</surname></persName>
		</author>
		<idno type="DOI">10.1037/h0049038</idno>
		<ptr target="https://doi.org/10.1037/h0049038" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="379" to="388" />
			<date type="published" when="1961">1961</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A cross-language study of voicing in initial stops: Acoustical measurements</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lisker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Abramson</surname></persName>
		</author>
		<idno type="DOI">10.1080/00437956.1964.11659830</idno>
		<ptr target="https://doi.org/10.1080/00437956.1964.11659830" />
	</analytic>
	<monogr>
		<title level="j">WORD</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="384" to="422" />
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Voicing&quot; in English: A catalogue of acoustic features signaling /b/ versus /p/in trochees</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lisker</surname></persName>
		</author>
		<idno type="DOI">10.1177/002383098602900102</idno>
		<ptr target="https://doi.org/10.1177/002383098602900102" />
	</analytic>
	<monogr>
		<title level="j">Language and Speech</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="34" to="45" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Auditory enhancement and second language experience in Spanish and English weighting of secondary voicing cues</title>
		<author>
			<persName><forename type="first">F</forename><surname>Llanos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Dmitrieva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Francis</surname></persName>
		</author>
		<idno type="DOI">10.1121/1.4817845</idno>
		<ptr target="https://doi.org/10.1121/1.4817845" />
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2213" to="2224" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Individual differences in categorical perception are related to sublexical/phonological processing in reading</title>
		<author>
			<persName><forename type="first">M</forename><surname>López-Zamora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Luque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Álvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Cobos</surname></persName>
		</author>
		<idno type="DOI">10.1080/10888438.2011.588763</idno>
		<ptr target="http://dx.doi.org/10.1080/10888438.2011.588763" />
	</analytic>
	<monogr>
		<title level="j">Scientific Studies of Reading</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="443" to="456" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Categorical or continuous speech perception: A new test</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Massaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="DOI">10.1016/0167-6393(83)90061-4</idno>
		<ptr target="https://doi.org/10.1016/0167-6393" />
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="90061" to="90064" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Balancing Type I error and power in linear mixed models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Matuschek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kliegl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vasishth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Baayen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bates</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jml.2017.01.001</idno>
		<ptr target="https://doi.org/10.1016/j.jml.2017.01.001" />
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="305" to="315" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Mcmurray</surname></persName>
		</author>
		<title level="m">Nonlinear Curvefitting for Psycholinguistic (and other) Data</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The myth of categorical perception</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mcmurray</surname></persName>
		</author>
		<idno type="DOI">10.1121/10.0016614</idno>
		<ptr target="https://doi.org/10.1121/10.0016614" />
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3819" to="3842" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The slow development of realtime processing: Spoken-word recognition as a crucible for new thinking about language acquisition and language disorders</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mcmurray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Apfelbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tomblin</surname></persName>
		</author>
		<idno type="DOI">10.1177/09637214221078325</idno>
		<ptr target="https://doi.org/10.1177/09637214221078325" />
	</analytic>
	<monogr>
		<title level="j">Current Directions in Psychological Science</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="305" to="315" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Gradient sensitivity to within-category variation in words and syllables</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mcmurray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Aslin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Tanenhaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Spivey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Subik</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0011747</idno>
		<ptr target="https://doi.org/10.1037/a0011747" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1609" to="1631" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Speech categorization develops slowly through adolescence</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mcmurray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Danelz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seedorff</surname></persName>
		</author>
		<idno type="DOI">10.1037/dev0000542</idno>
		<ptr target="https://doi.org/10.1037/dev0000542" />
	</analytic>
	<monogr>
		<title level="j">Developmental Psychology</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1472" to="1491" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Waiting for lexical access: Cochlear implants or severely degraded input lead listeners to process speech less incrementally</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mcmurray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farris-Trimble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rigler</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cognition.2017.08.013</idno>
		<ptr target="https://doi.org/10.1016/j.cognition.2017.08.013" />
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">169</biblScope>
			<biblScope unit="page" from="147" to="164" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Individual differences in online spoken word recognition: Implications for SLI</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mcmurray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Samelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tomblin</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cogpsych.2009.06.003</idno>
		<ptr target="https://doi.org/10.1016/j.cogpsych.2009.06.003" />
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="39" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Gradient effects of within-category phonetic variation on lexical access</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mcmurray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Tanenhaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Aslin</surname></persName>
		</author>
		<idno type="DOI">10.1016/s0010-0277(02)00157-9</idno>
		<ptr target="https://doi.org/10.1016/s0010-0277(02)00157-9" />
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="33" to="B42" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Within-category VOT affects recovery from &quot;lexical&quot; garden-paths: Evidence against phoneme-level inhibition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mcmurray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Tanenhaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Aslin</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jml.2008.07.002</idno>
		<ptr target="https://doi.org/10.1016/j.jml.2008.07.002" />
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="91" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">On the internal structure of phonetic categories: A progress report</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.1016/00100277(94)90031-0</idno>
		<ptr target="https://doi.org/10.1016/00100277(94)90031-0" />
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="271" to="285" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Speaking rate and segments: A look at the relation between speech production and speech perception for the voicing contrast</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Reeves</surname></persName>
		</author>
		<idno type="DOI">10.1159/000261764</idno>
		<ptr target="https://doi.org/10.1159/000261764" />
	</analytic>
	<monogr>
		<title level="j">Phonetica</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="106" to="115" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Bias in the perception of phonetic detail in children&apos;s speech: A comparison of categorical and continuous rating scales</title>
		<author>
			<persName><forename type="first">B</forename><surname>Munson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Schellinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Edwards</surname></persName>
		</author>
		<idno type="DOI">10.1080/02699206.2016.1233292</idno>
		<ptr target="https://doi.org/10.1080/02699206.2016.1233292" />
	</analytic>
	<monogr>
		<title level="j">Clinical Linguistics &amp; Phonetics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="56" to="79" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">The temporal dynamics of perceptual uncertainty: eye movement evidence from Cantonese segment and tone perception</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Nixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Rij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Baayen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jml.2016.03.005</idno>
		<ptr target="https://doi.org/10.1016/j.jml.2016.03.005" />
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="103" to="125" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Individual differences in categorization gradience as predicted by online processing of phonetic cues during spoken word recognition: Evidence from eye movements</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="DOI">10.1111/cogs.12948</idno>
		<ptr target="https://doi.org/10.1111/cogs.12948" />
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">12948</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Neural correlates of individual differences in speech categorisation: evidence from subcortical, cortical, and behavioural measures. Language</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C L</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1080/23273798.2021.1980594</idno>
		<ptr target="https://doi.org/10.1080/23273798.2021.1980594" />
	</analytic>
	<monogr>
		<title level="j">Cognition and Neuroscience</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="269" to="284" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Categorical and noncategorical modes of speech perception along the voicing continuum</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Pisoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Lazarus</surname></persName>
		</author>
		<idno type="DOI">10.1121/1.1914506</idno>
		<ptr target="https://doi.org/10.1121/1.1914506" />
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="328" to="333" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Reaction times to comparisons within and across phonetic categories</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Pisoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tash</surname></persName>
		</author>
		<idno type="DOI">10.3758/bf03213946</idno>
		<ptr target="https://doi.org/10.3758/bf03213946" />
	</analytic>
	<monogr>
		<title level="j">Perception &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="285" to="290" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">R: A Language and Environment for Statistical Computing</title>
		<author>
			<persName><forename type="first">Team</forename><surname>Core</surname></persName>
		</author>
		<ptr target="https://www.R-project.org/" />
	</analytic>
	<monogr>
		<title level="m">R Foundation for Statistical Computing</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Repp</surname></persName>
		</author>
		<idno type="DOI">10.1016/b978-0-12-608610-2.50012-1</idno>
		<ptr target="https://doi.org/10.1016/b978-0-12-608610-2.50012-1" />
		<title level="m">Categorical perception: Issues, methods, findings. Speech and Language</title>
		<imprint>
			<date type="published" when="1984">1984</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="243" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Functional benefits of continuous vs. categorical listening strategies on the neural encoding and perception of noise-degraded speech</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rizzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Bidelman</surname></persName>
		</author>
		<idno type="DOI">10.1101/2024.05.15.594387</idno>
		<ptr target="https://doi.org/10.1101/2024.05.15.594387" />
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">RStudio: Integrated Development Environment for</title>
		<author>
			<persName><forename type="first">Rstudio</forename><surname>Team</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">R</forename><surname>Boston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename></persName>
		</author>
		<ptr target="http://www.rstudio.com/" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">How does having a good ear promote successful second language speech acquisition in adulthood? Introducing auditory acuity hypothesis-L2. Language Teaching</title>
		<author>
			<persName><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<idno type="DOI">10.1017/s0261444822000453</idno>
		<ptr target="https://doi.org/10.1017/s0261444822000453" />
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Immediate effects of anticipatory coarticulation in spoken-word recognition</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Salverda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kleinschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Tanenhaus</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jml.2013.11.002</idno>
		<ptr target="https://doi.org/10.1016/j.jml.2013.11.002" />
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="145" to="163" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">The effect of discrimination training on speech perception: Noncategorical perception</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Samuel</surname></persName>
		</author>
		<idno type="DOI">10.3758/BF03199697</idno>
		<ptr target="https://doi.org/10.3758/BF03199697" />
	</analytic>
	<monogr>
		<title level="j">Perception &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="321" to="330" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Phonetic prototypes</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Samuel</surname></persName>
		</author>
		<idno type="DOI">10.3758/bf03202653</idno>
		<ptr target="https://doi.org/10.3758/bf03202653" />
	</analytic>
	<monogr>
		<title level="j">Perception &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="307" to="314" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">The end of categorical perception as we know it</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schouten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gerrits</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Hessen</surname></persName>
		</author>
		<idno type="DOI">10.1016/s0167-6393</idno>
		<ptr target="https://doi.org/10.1016/s0167-6393" />
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="94" to="98" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Modeling phoneme perception. I: Categorical perception</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E H</forename><surname>Schouten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Van Hessen</surname></persName>
		</author>
		<idno type="DOI">10.1121/1.403841</idno>
		<ptr target="https://doi.org/10.1121/1.403841" />
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1841" to="1855" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Perceptual discrimination of speech sounds in developmental dyslexia</title>
		<author>
			<persName><forename type="first">W</forename><surname>Serniclaes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sprenger-Charolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Carré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Demonet</surname></persName>
		</author>
		<idno type="DOI">10.1044/1092-4388(2001/032</idno>
		<ptr target="http://dx.doi.org/10.1044/1092-4388(2001/032" />
	</analytic>
	<monogr>
		<title level="j">Journal of Speech, Language, and Hearing Research</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="384" to="399" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Categorical perception of speech sounds in illiterate adults</title>
		<author>
			<persName><forename type="first">W</forename><surname>Serniclaes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ventura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Morais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kolinsky</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cognition.2005.03.002</idno>
		<ptr target="http://dx.doi.org/10.1016/j.cognition.2005.03.002" />
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="35" to="B44" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Enhanced cognitive and perceptual processing: a computational basis for the musician advantage in speech learning</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Smayda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Maddox</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2015.00682</idno>
		<ptr target="https://doi.org/10.3389/fpsyg.2015.00682" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Continuous perception and graded categorization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Toscano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcmurray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dennhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Luck</surname></persName>
		</author>
		<idno type="DOI">10.1177/0956797610384142</idno>
		<ptr target="https://doi.org/10.1177/0956797610384142" />
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1532" to="1540" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Speech perception in severely disabled and average reading children</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Werker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Tees</surname></persName>
		</author>
		<idno type="DOI">10.1037/h0084150</idno>
		<ptr target="http://dx.doi.org/10.1037/h0084150" />
	</analytic>
	<monogr>
		<title level="j">Canadian Journal of Psychology</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="48" to="61" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">ggplot2: Elegant Graphics for Data Analysis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wickham</surname></persName>
		</author>
		<ptr target="http://ggplot2.org" />
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Manipulation of voice onset time in speech stimuli: A tutorial and flexible Praat script</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Winn</surname></persName>
		</author>
		<idno type="DOI">10.1121/10.0000692</idno>
		<ptr target="https://doi.org/10.1121/10.0000692" />
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">147</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="852" to="866" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
