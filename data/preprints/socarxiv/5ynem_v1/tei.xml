<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The European Union&apos;s Artificial Intelligence Act and Trust: Towards an AI Bill of Rights in Healthcare?</title>
				<funder>
					<orgName type="full">INNOVATION AND TECHNOLOGY</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Barry</forename><surname>Solaiman</surname></persName>
						</author>
						<title level="a" type="main">The European Union&apos;s Artificial Intelligence Act and Trust: Towards an AI Bill of Rights in Healthcare?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">350A638BCFC7BA0E31606B939C513D07</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-10-22T06:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The AI Act adopts a risk-based regulatory framework, categorizing AI systems according to the level of risk they present and imposing corresponding regulatory requirements. While the regulation aims to cultivate trust by focusing on risk mitigation, it has faced criticism for its insufficient attention to the ethical and relational factors that are crucial for establishing genuine trust, especially within healthcare settings. In healthcare, trust encompasses a complex network of relationships involving patients, providers, and regulatory bodies, and is influenced by personal interactions, transparency, ethical practices, and the perceived intentions behind AI technologies and their implementers. This paper argues that trust in AI could be better fostered in healthcare through a Bill of Rights for AI that explicitly incorporates trust-building measures into the regulatory framework. Such an initiative could clarify the role and expectations of AI in healthcare, making it more trustworthy for both patients and healthcare providers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Keywords</head><p>Artificial Intelligence Act (AI Act), Trust, Trustworthiness, Bill of Rights, Healthcare mechanisms. Instead, the focus here is solely on the AI-Act, its inadequate approach to building trust in healthcare, and what should be done to remedy that gap.</p><p>Much attention has already been given to the intersection of AI and trust. Policymakers are concerned that society will either trust AI too little (and thus be unable to reap the benefits) or trust AI too much (and with it, the implications of AI errors causing harm). <ref type="bibr" target="#b2">3</ref> It has been said that AI's successful adoption in society 'hinges on trust'. <ref type="bibr" target="#b3">4</ref> Properly regulating AI may help deal with these concerns, but regulation alone will not directly establish trust. <ref type="bibr" target="#b2">3</ref> The following sections examine what 'trust' means in healthcare, how the AI-Act incorporates trust as a concept, and whether its provisions enhance trust. For policymakers, it proposes that a Bill of Rights for AI in health tied to the Act could help to remedy the existing gaps in the regulation of strengthening trust in healthcare.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Would European Union (EU) citizens trust they are being safeguarded from the risks of artificial intelligence (AI) where the law regulates the cumulative amount of compute used for AI training that is measured in floating point operations (FLOPs) greater than 10^25'? <ref type="bibr" target="#b0">1</ref> The disconnection between the language used in the Artificial Intelligence Act (AI-Act) and the patient on the ground is perhaps symbolized no better than in Article 52a(2) of the regulation above. That article is the culmination of a protracted process beginning formally in 2021 that evolved considerably during a period in which each revision to the law exponentially added to the complexity of the regulation. With the dust finally settling, what has emerged is a behemoth-like bureaucratic regulation that links the palatability of risk to trust. While the EU ought to be applauded for developing the most comprehensive law on AI in the world to date, this article argues that much more work is needed to enhance trust in the use of AI in healthcare, building on the foundations established by the regulation.</p><p>A lack of regulation and inadequate regulations might lead to widespread harm and, consequently, loss of trust in AI in health. <ref type="bibr" target="#b1">2</ref> The lack of regulation is being addressed with the AI-Act. That Act forms only one part of an emerging regulatory framework under development, which includes the Artificial Intelligence Liability Directive (AILD), <ref type="bibr" target="#b40">41</ref> a revised Product Liability Directive (PLD), <ref type="bibr" target="#b41">42</ref> and the European Health Data Space (EHDS). <ref type="bibr" target="#b42">43</ref> It is beyond the scope of this paper to appraise all these</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The Concept of Trust in Healthcare</head><p>The subject of trust in healthcare has long been examined. <ref type="bibr" target="#b4">5</ref> Trust has been conceptualized differently, using terminology often interchangeably without a clear definition. <ref type="bibr" target="#b5">6</ref> Consequently, the lens of analysis can become conceptually complex. In generic terms, trust can be defined as an expectation or attitude of 'the trusting agent that the trusted agent will act in a predetermined manner'. <ref type="bibr" target="#b2">3</ref> In healthcare, there must be trust between healthcare professionals and patients, doctors and their employers, between those parties and regulators, suppliers, developers, and so much more. If trust involves interpersonal relationships, such as between healthcare professionals and patients, then different rules governing those relationships will affect the level of trust between people. For example, fiduciary law uses restorative remedies for any unexpected consequences of relationships based on trust. <ref type="bibr" target="#b6">7</ref> The relationship between patients and healthcare professionals requires honesty and openness to foster trust in those professionals and the health system more broadly. <ref type="bibr" target="#b7">8</ref> Another example is the trust between clinicians and their employer healthcare organizations. <ref type="bibr" target="#b8">9</ref> Events can impact trust in that relationship, which occurred during COVID-19 when the increased demands on professionals led to moral injury and feelings of betrayal by leaders. <ref type="bibr" target="#b8">9</ref> The positive and negative effects on trust depend on what different people value. At a micro-relational level, studies examining trust between doctors and patients find that trust is positively affected by the doctor's behaviour, the perceived comfort levels of the patient with the doctor, the personal involvement with the patient, and, to a lesser extent, the doctor's cultural competence and physical appearance. <ref type="bibr" target="#b9">10</ref> Some patients value comfort, so trust is enhanced when doctors exhibit more comforting and caring behavior. <ref type="bibr" target="#b8">9</ref> For others, trust is influenced more by having a personally involved clinician who offers personalized care. <ref type="bibr" target="#b9">10</ref> Some patients care that the doctor shares common traits like language, religion, cultural beliefs, and values for emotional connectedness. <ref type="bibr" target="#b9">10</ref> The doctor's behaviour is also critical. This involves their 'communication skills, smiling face, kindness and nondiscrimination'. <ref type="bibr" target="#b9">10</ref> Communicating the truth is essential, but that communication should be patientcentred and patient-friendly. <ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11</ref> While pinpointing individual factors is context-dependent, some fundamental principles thread through healthcare -principles that, if undermined, would be detrimental to trust in the medical field. These are examined in more detail later in this paper, but one example is informed consent, which is 'required because of trust and without regard to the effects on trust'. <ref type="bibr" target="#b11">12</ref> Patients place a special kind of trust in doctors who have special responsibilities to the patient, such as sharing honest and clear information with them so they can make informed decisions about their care. <ref type="bibr" target="#b11">12</ref> This is called a 'predicated' stance toward trust, whereby ethics and law impose particular obligations because trust is a factual premise, so 'trust is the source of the obligation, not its object'. <ref type="bibr" target="#b11">12</ref> Confidentiality is another example. Patients must trust that the information they share with their doctor is kept private and secure, and laws require this. <ref type="bibr" target="#b11">12</ref> This has been called a 'supportive' stance towards trust, whereby rules are created to maintain and promote trust. <ref type="bibr" target="#b11">12</ref> Where it is perceived that trust does not exist or cannot be sustained, this may be used to justify a regime to institutionalize trust. <ref type="bibr" target="#b11">12</ref> Trust can also operate at a macro level. People value health systems not only for the individual care they receive but also for their broader contribution to the well-being of society. <ref type="bibr" target="#b12">13</ref> It has been argued that it is important to develop the legitimacy of state action within health systems to garner trust. <ref type="bibr" target="#b12">13</ref> The state has a central role, and its actions must be seen as legitimate in persuading patients to cooperate and accept health interventions. <ref type="bibr" target="#b12">13</ref> Trust must be built in the state and its agencies to establish legitimacy. Even in this context, building trust involves examining personal behaviours between healthcare providers, healthcare professionals and patients, between healthcare professionals and their employers, and between the public and private sectors. <ref type="bibr" target="#b12">13</ref> Managerial and organisational practices can help to foster trust. For example, by creating spaces for caring engagement, dialogue, and interpersonal interactions. <ref type="bibr" target="#b12">13</ref> These considerations about trust should be borne in mind when examining the EU's AI-Act, which will ultimately impact healthcare. However, the EU is not a state, and does not have competence to regulate healthcare systems. Instead, the EU regulates the internal market, which may lead to some interface with healthcare systems at a state level. This paradigm underscores a limitation of the supposed trust-building aims of the AI-Act and how the principles of trust manifest in healthcare and the state level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Trust and the AI-Act</head><p>The AI-Act is not explicitly designed for healthcare or other sectors. Instead, it aims to 'promote the uptake of human centric and trustworthy artificial intelligence while ensuring a high level of protection of health, safety, fundamental rights'. <ref type="bibr" target="#b0">1</ref> This is to be achieved by mitigating risks inherent in AI through the Act's risk-based regulatory approach that imposes restrictions and controls on AI deemed risky. While the emphasis on trust seems clear, the term 'trust' seldom appears in the 245-page regulation (appearing only once in the articles and a dozen times in the recitals with a simple keyword search of the regulation).</p><p>According to the recitals, trust will be achieved through a 'consistent and high level of protection' throughout the EU. <ref type="bibr" target="#b0">1</ref> The concern is that AI will significantly impact society, so there is a need to build trust (aligning with the macro-level considerations about trust noted above). The regulation offers a method for 'building' trust -through a regulatory framework developed 'according to Union values enshrined in Article 2 TEU, the fundamental rights and freedoms enshrined in the Treaties, the Charter'. <ref type="bibr" target="#b0">1</ref> AI should be 'human-centric' and serve as a 'tool for people, with the ultimate aim of increasing human well-being'. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Basis of Trust</head><p>Given the work that preceded it, the articulation of trust in the AI-Act is unsurprising. In 2019, the European Commission appointed the High-Level Expert Group on AI (AI HLEG) to develop Ethics Guidelines for Trustworthy AI. <ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b13">14</ref> The Guidelines establish a framework for trustworthy AI, how to realise such trustworthy AI systems, and how they can be assessed for their trustworthiness. <ref type="bibr" target="#b13">14</ref> Such ethical guidelines have been criticised for being 'useless'. <ref type="bibr" target="#b47">48</ref> Indeed, analysing the basis for the Guidelines reveals several shortcomings. 'Trustworthy' AI should have three components according to the Guidelines. First, it should comply with all applicable laws and regulations (primary EU law, like the treaties and the Charter of Fundamental Rights, and secondary law, such as the GDPR, the ECHR, and medical device regulations). <ref type="bibr" target="#b13">14</ref> Second, AI systems should adhere to ethical principles and values, such as respect for human autonomy, prevention of harm, fairness, and explicability. <ref type="bibr" target="#b13">14</ref> Third, AI should be robust from a technical and social perspective. <ref type="bibr" target="#b13">14</ref> This includes standardization and certification (such as ISO standards), which can play an important role in garnering trust. <ref type="bibr" target="#b13">14</ref> All components should work together and overlap in their operation, and act as a check on one another. <ref type="bibr" target="#b13">14</ref> In 2020, the European Commission stated that 'trustworthiness is a prerequisite' for the uptake of AI, tying the concept of trust to values and fundamental rights, such as ensuring human dignity and protecting privacy. <ref type="bibr" target="#b14">15</ref> It recommended the development of an 'ecosystem of trust'. <ref type="bibr" target="#b14">15</ref> The same year, the Parliamentary Assembly of the Council of Europe stated that there was a 'big question of trust regarding some AI applications in health'. <ref type="bibr" target="#b15">16</ref> Regarding the COVID-19 pandemic, it noted that 'had there been a trusted and well-defined regulatory framework, maybe AI could have had a much larger positive impact on the managing of this pandemic'. <ref type="bibr" target="#b15">16</ref> The Guidelines form part of a theoretical and governance framework concerning trust in AI. The AI-Act cites seven non-binding principles from those guidelines that supposedly help ensure AI is trustworthy and ethically sound. Namely, human agency and oversight; technical robustness and safety; privacy and data governance; transparency; diversity, non-discrimination and fairness; societal and environmental well-being, and accountability. <ref type="bibr" target="#b0">1</ref> Together, these principles are the basis of the conception of trust in the regulation. However, that basis is not captured in the resulting law.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Equating Risk and Trust</head><p>The AI-Act does not meaningfully entrench the concepts designed to enhance trust. Instead, it creates a risk-based approach towards regulating AI vaguely linked to trust-building.</p><p>The recitals state that the regulation applies mandatory requirements to high-risk AI systems to mitigate 'risks' from placing them on the market and to 'ensure a high level of trustworthiness'. <ref type="bibr" target="#b0">1</ref> To ensure a 'high level of trustworthiness,' a conformity assessment will be required. <ref type="bibr" target="#b0">1</ref> This is the final recital that mentions trust explicitly. After that, the only actual article in the regulation that mentions 'trust' is Article 1(1) to reiterate the articulation in the recitals that the regulation exists to promote the 'uptake of human centric and trustworthy artificial intelligence'. <ref type="bibr" target="#b0">1</ref> Readers are left to ponder how the other provisions may enhance trust in congruence with the stated ethical basis. That approach does not inspire confidence that trust-building has been conceptualized coherently nor implemented systematically as an underlying aim of the regulation.</p><p>Instead, the provisions distinguish between AI systems that pose unacceptable risks, high risks, and low risks. <ref type="bibr" target="#b0">1</ref> Devices posing an unacceptable risk, such as systems that use social scoring or manipulative AI, are prohibited. Low risk systems are not captured by the law, encompassing apps one might find on an app store, such as wellness apps. For healthcare, the regulation is most likely to capture systems subject to the EU's Medical Device Regulation (MDR) and the regulation on in-vitro diagnostic medical devices (IVD). <ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18</ref> Those regulations are noted in the AI-Act's Annex, meaning that medical devices that incorporate AI will be subject to additional rules. For example, a medical device provider must follow the MDR's conformity assessment, and the AI-Act's conformity assessment will be an additional element within that assessment process.</p><p>It is doubtful that additional conformity assessments on 'risk' would do much to enhance trust in AI. Devices for the MDR should bear 'CE marking' (Conformité Européenne) to demonstrate conformity with the EU's safety requirements. <ref type="bibr" target="#b16">17</ref> However, information submitted for CE marking is confidential, and does not indicate device effectiveness, only that it is legally compliant with minimum EU-level safety standards. <ref type="bibr" target="#b18">19</ref> CE marking is based on 'unsubstantiated trust' that evidence of safety and effectiveness will be provided later once the product is on the market. <ref type="bibr" target="#b18">19</ref> Consequently, it has been argued that the MDR is not aligned with international ethical standards for clinical research. <ref type="bibr" target="#b18">19</ref> It must be queried whether stakeholders will trust AI devices brought to market on already flawed regulatory mechanisms.</p><p>Further, the AI-Act enshrines factors for determining high-risk classifications, but it is unclear what the factors mean as a practical matter for compliance. <ref type="bibr" target="#b39">40</ref> Under art 7(2), the intended purpose of the system is considered; whether special categories of data are processed (like health data); whether humans can override autonomous systems; the extent to which a system has already caused harm to health and human safety; the extent of imbalances of power, such as users being in vulnerable positions; and how easy it is to reverse the decision of the AI system. <ref type="bibr" target="#b0">1</ref> These are all important considerations, but the regulation does not clearly tie these requirements to its trust-building beyond creating additional requirements on top of existing conformity assessment requirements from other regulations. Indeed, it has been argued that the AI-Act's understanding of trust in terms of acceptability of risk is a 'simplistic conceptualization of trust', and the EU oversells its regulatory ambition. <ref type="bibr" target="#b19">20</ref> While there is a relationship between trust, trustworthiness and acceptability of risks, the AI-Act 'conflates' the distinction between what experts deem acceptable risks, and whether society trusts the AI systems eventually implemented. <ref type="bibr" target="#b19">20</ref> Trust is a longitudinal process of 'controls, communication, and accountability', rather than a binary analysis of AI risk. <ref type="bibr" target="#b19">20</ref> What fosters public trust is a domain-specific question that 'casts doubt on the effectiveness of a horizontal regulatory law such as the AI Act'. <ref type="bibr" target="#b19">20</ref> The AI-Act also creates AI auditors (notified bodies) to verify the conformity of high-risk systems, but there are concerns that those bodies may be influenced by private industry. <ref type="bibr" target="#b19">20</ref> While notified bodies must be independent of the provider of the high-risk AI system, they charge a fee for their services, creating the obvious incentive to receive repeat commissions from AI developers. <ref type="bibr" target="#b19">20</ref> Lay people who cannot assess the trustworthiness of an AI system will need to trust these accountability mechanisms. <ref type="bibr" target="#b19">20</ref> It is doubtful that misaligned incentives embedded within the regulatory framework will do much to garner trust. These criticisms are underscored when examining the AI-Act's approach to generative AI (GenAI), like ChatGPT. The use of ChatGPT raises significant concerns about trust because there is a very worrying trend of some healthcare professionals utilizing it in clinical practice despite it not being designed or approved for medical use, potentially putting patients at substantial risk. <ref type="bibr" target="#b0">1</ref> Provisions cover GenAI posing a 'systemic risk,' meaning where the floating point operations (FLOPs) are greater than 10^25. <ref type="bibr" target="#b0">1</ref> In other words, GenAI with capabilities of GPT4 or higher. Systems captured by the regulation must comply with additional transparency requirements. The practical consequence is that patients must be informed they are interacting with a GenAI system if it is not 'obvious' to them. <ref type="bibr" target="#b0">1</ref> The technicality of the 'flops' terminology used in the regulation, with its attendant focus on 'system risk', belies the resulting protections for users on the ground. Even if healthcare professionals notify their patients when using ChatGPT (satisfying the AI-Act), they should not use it in the first instance because its use may fall below the legal standard of care. It is also worth noting that (at the time of writing) GPT4 is an additional paid service, whereas the free version runs on GPT 3.5 (not captured by the AI-Act at all). There is a significant chasm of standards that are pertinent to the healthcare domain and a disconnect between the basis of trust underlying the Act and the safety standards needed for stakeholders in healthcare.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion: Refocusing on Trust Through an AI Bill of Rights for Healthcare</head><p>The AI-Act falls short of connecting its premise on trust with its risk-based approach. However, it does create space for further developments that stakeholders should build on to enhance trust in the health domain. Unfortunately, the space created by the AI-Act encourages the 'development of voluntary best practices and standards' that take into account ethical principles. <ref type="bibr" target="#b0">1</ref> Voluntary codes are hardly inspiring because they could inevitably lead to incongruent interpretation and application, and the scope of the codes proposed in the AI-Act is minimal. However, that is the space created by the AI-Act which will likely be built upon. This may be an area where some 'public benefit' can be established as called for elsewhere. <ref type="bibr" target="#b48">49</ref> Here, it is recommended that a voluntary code can take the form of an AI Bill of Rights for Health to accompany the Act. The articulation of a 'bill of rights' is proposed to connect trust-building principles directly to the aims of the AI-Act on trust. Also, it will be easier to educate patients about the protections they can expect for using AI in health when framed as rights they can expect when compared to technical requirements about conformity. The proposal here is intended to encourage debate and is not exhaustive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Systemic Considerations</head><p>It is first helpful to note the distinction between the 'trust' and 'trustworthiness' concepts because they indicate the scope of a potential Bill of Rights. Trust and trustworthiness are separate concepts; the former does not necessarily lead to the latter. <ref type="bibr" target="#b5">6</ref> Trust encapsulates the idea of placing trust in someone or something. Trust is a prerequisite to trustworthiness -one must trust that something is trustworthy to have trust in it. <ref type="bibr" target="#b20">21</ref> Unever al., note that this idea of trust portrays the approach taken in the EU's Guidelines for Trustworthy AI that examine trust from the perspective of both the inherent properties of the AI system as well as the qualities of the broader socio-technical system in which AI exists. <ref type="bibr" target="#b20">21</ref> The authors emphasize that trust must derive from the operation of the entire socio-technical ecosystem within which AI exists as one component. Trust in AI as a standalone device is a concept that can be disproved because liability systems are premised on the responsibility of the clinician or their employer. <ref type="bibr" target="#b20">21</ref> They argue that AI itself cannot have the capacity to be trusted because it does not have an emotive state and cannot be held responsible for its actions. <ref type="bibr" target="#b20">21</ref> Nevertheless, while AI lacks morality, those developing AI have moral agency. <ref type="bibr" target="#b21">22</ref> Attention should, therefore, be given to factors that affect trust within the overall healthcare system, such as ethical principles of transparency, responsibility, accountability, fairness, and privacy. <ref type="bibr" target="#b20">21</ref> AI requires trust in socio-technical systems, which involves the interconnection of different relationships and actors, such as healthcare institutions, healthcare professionals, patients and their families, AI developers, and more. <ref type="bibr" target="#b6">7</ref> Regulation requires a multi-disciplinary and multi-stakeholder strategy that involving AI creators, the users of those systems, and politicians. <ref type="bibr" target="#b22">23</ref> This holistic approach is essential because concerns may spill over from one domain to the other. If people do not trust the government to protect their data in one sector, they may not trust the same government to protect their data in other sectors like healthcare. <ref type="bibr" target="#b5">6</ref> Trust, therefore, operates on a multidimensional level, requiring different values to be responsive to patient safety to ensure physical, social, and emotional well-being. <ref type="bibr" target="#b20">21</ref> The responsibilities of those interacting with AI and the conditions attached to their use of AI within that system should be clear. <ref type="bibr" target="#b20">21</ref> Similar calls made by others for trust in AI to encompass the 'entire relationship-building process and not on the trait of trustworthiness alone'. <ref type="bibr" target="#b5">6</ref> However, this is particularly complex because AI is used in different healthcare contexts and environments and integrated in different ways. Ensuring trust in different clinical contexts requires institutions to enable trust from the beginning. For example, from training AI to the eventual day-to-day usage in clinical practice, with ethical safeguards ensuring accountability for possible harms. <ref type="bibr" target="#b6">7</ref> This requires designating responsibilities for different staff, such as doctors and clinicians, and outlining how the safeguards govern workflows concerning diagnosis and treatment using AI tools. <ref type="bibr" target="#b6">7</ref> The use of AI in these contexts entails a holistic concept whereby AI, as a subsystem operating in a more extensive healthcare system, should serve the overall trust in that system. <ref type="bibr" target="#b6">7</ref> The problem is that these trust requirements move beyond the AI-Act's scope, indicating the need for a broader regulatory framework. The AI-Act is premised on allowing high-risk systems to market and does not cover the responsibilities of hospitals, clinicians, and their workflows. Another problem is that people are less likely to trust AI than a human doctor, even where AI provides the same level of care as a doctor. <ref type="bibr" target="#b23">24</ref> People perceive that AI cares less than a human doctor and cannot share similar values. Patients do not trust diagnoses provided by AI as much as those provided by human healthcare professionals. <ref type="bibr" target="#b24">25</ref> There is a 'substantial resistance to the use of AI' in diverse populations of patients. <ref type="bibr" target="#b24">25</ref> Patients' concerns about perceived care and value are 'significant determinants of trust in risk perception research.' <ref type="bibr" target="#b23">24</ref> This leads to decreased trust in AI.</p><p>It is unclear whether the AI-Act may interact with the fundamental question of the doctor-patient relationship. Nevertheless, the Act must promote the values necessary for trust in AI in healthcare so that any product coming to market is checked for compliance with such values at the front end. These values have been examined for several years and are now encapsulated within health, AI, the law, and bioethics. <ref type="bibr" target="#b25">26</ref> Indeed, there is a positive relationship between AI trust and the ethical governance of AI. <ref type="bibr" target="#b22">23</ref> The values are typically premised on consent, medical liability, data accuracy, privacy, bias, security, efficacy, safety, and transparency. <ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Values for Trust</head><p>These values cover longstanding recommendations in academic literature on ensuring provider competence, protecting the patient's interests, and ensuring information integrity. <ref type="bibr" target="#b28">29</ref> Below are some non-exhaustive examples of what could be included in a Bill of Rights under the AI-Act to promote the values.</p><p>One primary concern for doctors and patients is informed consent and explainability. <ref type="bibr" target="#b29">30</ref> Patients should be fully informed about their care to make informed decisions. AI is problematic because the reasons for an AI's decision or recommendation are often unknown or unknowable owing to the complex neural network underpinning its operation. The opaqueness of AI systems can make attaining transparency, accountability, and responsibility difficult. <ref type="bibr" target="#b5">6</ref> This is called the black box problem, and one aim is to enhance the 'explainability' of AI decisions so that people can understand the underlying logic. However, explainability in AI is easier said than done. One can disclose the learning algorithms, but they can lack data features and detailed explanations of the weights that influence the eventual output, which may do little to explain the underlying logic of a decision. <ref type="bibr" target="#b30">31</ref> A Bill of Rights should require meaningful disclosure of information that experts can access to alleviate matters of broad public concern in health. <ref type="bibr" target="#b30">31</ref> Disclosure could enable experts to determine risks for informed consent that a particular algorithm poses in terms of explainability. That is not to say that explainability is a panacea. Some methods offer post hoc rationalisations of a black box, which are unlikely to assist in understanding the inner workings of the AI system. <ref type="bibr" target="#b31">32</ref> Nevertheless, the AI-Act's framework should work towards enshrining the right of patients to obtain sufficient information for their care.</p><p>Data considerations present other critical facets of trust. AI requires large representative datasets, and patients may benefit from AI using such data. However, patients do not trust that governance systems will keep their data confidential. <ref type="bibr" target="#b1">2</ref> The problems of deidentification, reidentification, and inferences have been well documented. <ref type="bibr" target="#b32">33</ref> A Bill of Rights might require compliance with specific provisions in the General Data Protection Regulation (GDPR) concerning the use of sensitive data or other stricter standards of confidentiality found in health. Another problem is that the data used to train systems may be biased, producing incorrect recommendations for the demographic it is used on. <ref type="bibr" target="#b43">44</ref> A Bill of Rights can require additional checks and verification by creators of AI systems, anyone operating the system, anyone interpreting its outputs to check for biases. <ref type="bibr" target="#b22">23</ref> Those checks could help ensure that diverse and representative data was used in AI development. <ref type="bibr" target="#b33">34</ref> The European Health Data Space (EHDS) may also address this challenge. It will allow researchers and others 'non-discriminatory access to health data and the training of artificial intelligence algorithms'. <ref type="bibr" target="#b0">1</ref> Accountability for AI decisions is also essential. Patients must trust both the healthcare professional's decision (made with the assistance of AI) and the AI system itself. <ref type="bibr" target="#b21">22</ref> If a doctor relies on an AI's recommendation, harm may result to patients, raising questions of whether the doctor, hospital or AI developer is liable, particularly under tort law. <ref type="bibr" target="#b35">36</ref> The AI-Act does not cover liability concerns, but the proposed Artificial Intelligence Liability Directive (AILD) and the revised Product Liability Directive (revised PLD) may. A Bill of Rights associated with the AI-Act should enshrine the right to redress for harms through existing standards of care in tort law and include forthcoming directives. However, it should move beyond the right to legal redress by requiring a human point of reference to answer questions and resolve problems where harms occur. <ref type="bibr" target="#b22">23</ref> This could mean a responsible person or standing committee within the hospital setting convened to examine AI incidents. <ref type="bibr" target="#b44">45</ref> A final example is safety and effectiveness. <ref type="bibr" target="#b34">35</ref> Patients should expect that AI outputs are accurate and safe for the context in which they are applied. Datasets should be reliable and valid, but they often are not because of a lack of training data. <ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47</ref> For example, developing accurate AI systems for psychiatric wards is challenging because of the small sample sizes of data available. <ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38</ref> Practical limitations can also affect accuracy. Monitoring systems used in clinical contexts may require multiple sensors with overlapping fields of view to track patients in real-time and make accurate predictions. <ref type="bibr" target="#b38">39</ref> A Bill of Rights should require that systems are designed to be accurate to help ensure they are safe and effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion: Moving Forward</head><p>These are some areas where a Bill of Rights could stipulate standards for using AI in healthcare that instil trust. Rather than focusing purely on risk classification and equating that classification to enhancing trustworthiness, measures should be set out that tie trust specifically to health. In the first instance, the Bill of Rights could operate as a voluntary code within the regulation's remit. Provisions focussing on data, consent, and safety will align with the aims of the Act and be directed towards those most affected by AI -the end users and the patients subject to their decisions.</p></div>		</body>
		<back>

			<div type="funding">
<div><p>This article has been accepted for publication in 2025 in LAW, <rs type="funder">INNOVATION AND TECHNOLOGY</rs>, published by <rs type="person">Taylor &amp; Francis</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Proposal for a Regulation of the European Parliament and of the Council Laying Down Harmonised Rules on Artificial Intelligence (Artificial Intelligence Act) and Amending Certain Union Legislative Acts&apos; (AI-Act) COM (2021) 0106 (COM)</title>
		<author>
			<persName><forename type="first">European</forename><surname>Ai-Act</surname></persName>
		</author>
		<author>
			<persName><surname>Parliament</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024-02">January-2 February 2024</date>
		</imprint>
	</monogr>
	<note>Provisional Agreement Resulting from Interinstitutional Negotiations. Final Draft version as of 21</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Legal concerns in artificial intelligence: a scoping review protocol</title>
		<author>
			<persName><forename type="first">Da</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Horsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
		<idno type="DOI">10.1186/s13643-022-01939-y</idno>
	</analytic>
	<monogr>
		<title level="j">Syst Rev</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">123</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Regulating for trust: Can law establish trust in artificial intelligence?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tamò-Larrieux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guitton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lutz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Regulation and Governance</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Regulating artificial intelligence: Proposal for a global solution</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">J</forename><surname>Erdélyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Goldsmith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Government Information Quarterly</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><surname>Law</surname></persName>
		</author>
		<author>
			<persName><surname>Medicine</surname></persName>
		</author>
		<author>
			<persName><surname>Trust</surname></persName>
		</author>
		<idno type="DOI">10.2307/1229596</idno>
		<ptr target="https://doi.org/10.2307/1229596" />
	</analytic>
	<monogr>
		<title level="j">Stanford Law Rev</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="463" to="527" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Why public trust in health care systems matters and deserves greater research attention</title>
		<author>
			<persName><forename type="first">F</forename><surname>Gille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mays</surname></persName>
		</author>
		<idno type="DOI">10.1177/1355819614543161</idno>
	</analytic>
	<monogr>
		<title level="j">J Health Serv Res Policy</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="64" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Governing fiduciary relationships or building up a governance model for trust in AI? Review of healthcare as a socio-technical system</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Unver</surname></persName>
		</author>
		<idno type="DOI">10.1080/13600869.2023.2192569</idno>
	</analytic>
	<monogr>
		<title level="j">Int Rev Law Comput Technol</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="198" to="226" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Duties of Candour in Healthcare: The Truth, the Whole Truth, and Nothing but the Truth?</title>
		<author>
			<persName><forename type="first">O</forename><surname>Quick</surname></persName>
		</author>
		<idno type="DOI">10.1093/medlaw/fwac004</idno>
		<ptr target="https://doi.org/10.1093/medlaw/fwac004" />
	</analytic>
	<monogr>
		<title level="j">Med Law Rev. 2022 Spring</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="324" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Understanding Clinician Trust in Health Care Organizations Should Be a Research Priority</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Simpson</surname></persName>
		</author>
		<idno type="DOI">10.1001/jamahealthforum.2023.1015</idno>
	</analytic>
	<monogr>
		<title level="j">JAMA Health Forum</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">231015</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Factors influencing trust in doctors: a community segmentation strategy for quality improvement in healthcare</title>
		<author>
			<persName><forename type="first">V</forename><surname>Gopichandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Chetlapalli</surname></persName>
		</author>
		<idno type="DOI">10.1136/bmjopen-2013-004115</idno>
	</analytic>
	<monogr>
		<title level="j">BMJ Open</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4115</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Physician behaviors that predict patient trust</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Thom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Fam Pract</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="323" to="328" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The Importance of Trust for Ethics, Law, and Public Policy</title>
		<author>
			<persName><forename type="first">Hall</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1017/S096318010505019X</idno>
	</analytic>
	<monogr>
		<title level="j">Camb Q Healthc Ethics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="156" to="167" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Trust and the development of health care as a social institution</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soc Sci Med</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1453" to="1468" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Independent High-Level Expert Group on Artificial Intelligence. Ethics Guidelines for Trustworthy AI</title>
		<author>
			<orgName type="collaboration">European Commission</orgName>
		</author>
		<ptr target="https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai" />
		<imprint>
			<date type="published" when="2019-04-08">8 April 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">White Paper on Artificial Intelligence: a European approach to excellence and trust</title>
		<author>
			<orgName type="collaboration">European Commission</orgName>
		</author>
	</analytic>
	<monogr>
		<title level="j">COM</title>
		<imprint>
			<biblScope unit="page">65</biblScope>
			<date type="published" when="2020-02-19">19 February 2020. 2020</date>
			<pubPlace>Brussels</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Artificial intelligence in health care: medical, legal and ethical challenges ahead</title>
	</analytic>
	<monogr>
		<title level="m">Rapporteur: Ms Selin Sayek Böke, Turkey, SOC. Parliamentary Assembly, Council of Europe</title>
		<imprint>
			<publisher>Committee on Social Affairs, Health and Sustainable Development</publisher>
			<date type="published" when="2022-09-22">22 Sept 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Regulation (EU) 2017/745 of the European Parliament and of the Council of 5 April 2017 on Medical Devices, Amending Directive</title>
		<author>
			<persName><surname>Mdr)</surname></persName>
		</author>
		<idno>1223/2009 and Repealing Council Directives 90/385/EEC and 93/42/EEC [2017] OJ L117/1</idno>
	</analytic>
	<monogr>
		<title level="j">Regulation</title>
		<imprint>
			<biblScope unit="issue">83</biblScope>
			<date type="published" when="2001">2001. 2002</date>
		</imprint>
	</monogr>
	<note>Regulation</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Regulation (EU) 2017/746 of the European Parliament and of the Council of 5 April</title>
		<author>
			<persName><surname>Ivd)</surname></persName>
		</author>
		<idno>98/79/EC and Commission Decision 2010/227/EU [2017] OJ L117</idno>
	</analytic>
	<monogr>
		<title level="m">Vitro Diagnostic Medical Devices and Repealing Directive</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gaps in the evidence underpinning high-risk medical devices in Europe at market entry, and potential solutions</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hulstaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pouppez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Primus-De</forename><surname>Jong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
		<idno type="DOI">10.1186/s13023-023-02801-7</idno>
		<ptr target="https://doi.org/10.1186/s13023-023-02801-7" />
	</analytic>
	<monogr>
		<title level="j">Orphanet J Rare Dis</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">212</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Trustworthy artificial intelligence and the European Union AI act: On the conflation of trustworthiness and acceptability of risk</title>
		<author>
			<persName><forename type="first">J</forename><surname>Laux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wachter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mittelstadt</surname></persName>
		</author>
		<idno type="DOI">10.1111/rego.12512</idno>
		<ptr target="https://doi.org/10.1111/rego.12512" />
	</analytic>
	<monogr>
		<title level="j">Regulation Governance</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="32" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Role of Trust in AI-Driven Healthcare Systems: Discussion from the Perspective of Patient Safety</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Unver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Asan</surname></persName>
		</author>
		<idno type="DOI">10.1177/2327857922111026</idno>
	</analytic>
	<monogr>
		<title level="j">Proc Int Symp Hum Factors Ergon Health Care</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="129" to="134" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Influences on User Trust in Healthcare Artificial Intelligence: A Systematic Review [version 1; peer review: 4 approved with reservations]</title>
		<author>
			<persName><forename type="first">E</forename><surname>Jermutus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kneale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Michie</surname></persName>
		</author>
		<idno type="DOI">10.12688/wellcomeopenres.17550.1</idno>
		<ptr target="https://doi.org/10.12688/wellcomeopenres.17550.1" />
	</analytic>
	<monogr>
		<title level="j">Wellcome Open Res</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">65</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">To trust or not to trust? An assessment of trust in AI-based systems: Concerns, ethics and contexts</title>
		<author>
			<persName><forename type="first">N</forename><surname>Omrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rivieccio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Fiore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schiavone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Agreda</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.techfore.2022.121763</idno>
		<ptr target="https://doi.org/10.1016/j.techfore.2022.121763" />
	</analytic>
	<monogr>
		<title level="j">Technological Forecasting &amp; Social Change</title>
		<imprint>
			<biblScope unit="volume">181</biblScope>
			<biblScope unit="page">121763</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Artificial Intelligence Is Trusted Less than a Doctor in Medical Treatment Decisions: Influence of Perceived Care and Value Similarity</title>
		<author>
			<persName><forename type="first">R</forename><surname>Yokoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Eguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fujita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nakayachi</surname></persName>
		</author>
		<idno type="DOI">10.1080/10447318.2020.1</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="981" to="990" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Diverse patients&apos; attitudes towards Artificial Intelligence (AI) in diagnosis. PLoS Digital Health</title>
		<author>
			<persName><forename type="first">C</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Woods</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bergstrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Findley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Balser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Slepian</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pdig.0000237</idno>
		<ptr target="https://doi.org/10.1371/journal.pdig.0000237" />
		<imprint>
			<date type="published" when="2023-05-19">May 19, 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A Framework for Health, AI and the Law</title>
		<author>
			<persName><forename type="first">B</forename><surname>Solaiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">G</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Research Handbook on Health, AI and the Law</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Solaiman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><forename type="middle">G</forename><surname>Cohen</surname></persName>
		</editor>
		<imprint>
			<publisher>Edward Elgar</publisher>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Guidance: A guide to good practice for digital and data-driven health technologies</title>
		<ptr target="https://www.gov.uk/government/publications/code-of-conduct-for-data-driven-health-and-care-technology/initial-code-of-conduct-for-data-driven-health-and-care-technology" />
		<imprint>
			<date type="published" when="2021-01-19">January 19. 2021</date>
			<publisher>UK Department of Health &amp; Social Care</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The impact of artificial intelligence on the doctor-patient relationship</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mittelstadt</surname></persName>
		</author>
		<ptr target="https://www.coe.int/en/web/bioethics/report-impact-of-ai-on-the-doctor-patient-relationship" />
	</analytic>
	<monogr>
		<title level="m">relationship#:~:text=The%20potential%20human%20rights%20impact,account%20of%20well-being%3B%20</title>
		<imprint>
			<date type="published" when="2021-12">December 2021</date>
		</imprint>
	</monogr>
	<note>Council of Europe. text=The%20potential%20human%20rights%20impact,account%20of%20well-being%3B%20</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Artificial Intelligence in the Health care Space: How We Can Trust What We Cannot Know</title>
		<author>
			<persName><forename type="first">R</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Aldana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Stein</surname></persName>
		</author>
		<ptr target="https://repository.uchastings.edu/faculty_scholarship/1753" />
	</analytic>
	<monogr>
		<title level="j">Stanford Law &amp; Policy Review</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="413" to="416" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Solaiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bloom</surname></persName>
		</author>
		<author>
			<persName><surname>Ai</surname></persName>
		</author>
		<title level="m">Explainability and Safeguarding Patient Safety in Europe: Toward a Science-Focused Regulatory Model</title>
		<editor>
			<persName><forename type="first">I</forename><forename type="middle">G</forename><surname>Cohen</surname></persName>
		</editor>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>The Future of Medical Device Regulation: Innovation and Protection</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Distrust of Artificial Intelligence: Sources &amp; Responses from Computer Science &amp; Law</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Minow</surname></persName>
		</author>
		<idno type="DOI">10.1162/daed_a_01918</idno>
		<ptr target="https://doi.org/10.1162/daed_a_01918" />
	</analytic>
	<monogr>
		<title level="j">Daedalus</title>
		<imprint>
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="309" to="321" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Beware explanations from AI in health care</title>
		<author>
			<persName><forename type="first">B</forename><surname>Babic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gerke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">G</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.abg1834</idno>
		<idno type="PMID">34437144</idno>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">373</biblScope>
			<biblScope unit="issue">6552</biblScope>
			<biblScope unit="page" from="284" to="286" />
			<date type="published" when="2021-07-16">2021 Jul 16</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Problematic Interactions Between AI and Health Privacy</title>
		<author>
			<persName><forename type="first">N</forename><surname>Price</surname></persName>
		</author>
		<idno>10.26054/0</idno>
	</analytic>
	<monogr>
		<title level="j">ULR</title>
		<imprint>
			<biblScope unit="volume">925</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note>dth4e-sgvq</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fairness of artificial intelligence in healthcare: review and recommendations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ueda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kakinuma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fujita</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11604-023-01474-3</idno>
		<ptr target="https://doi.org/10.1007/s11604-023-01474-3" />
	</analytic>
	<monogr>
		<title level="j">Jpn J Radiol</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="3" to="15" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Ethical and legal challenges of artificial intelligence-driven healthcare</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gerke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Minssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="DOI">10.1016/B978-0-12-818438-7.00012-5</idno>
		<idno type="PMCID">PMC7332220</idno>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence in Healthcare</title>
		<imprint>
			<biblScope unit="page" from="295" to="336" />
			<date type="published" when="2020-06-26">2020. 2020 Jun 26</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Liability for Use of Artificial Intelligence in Medicine</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">N</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><surname>Ii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gerke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">G</forename><surname>Cohen</surname></persName>
		</author>
		<editor>Solaiman B, Cohen IG</editor>
		<imprint>
			<date type="published" when="2024">2024</date>
			<publisher>Edward Elgar Publishing</publisher>
		</imprint>
	</monogr>
	<note>Research Handbook on Health, AI and the Law</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Monitoring Mental Health: Legal and Ethical Considerations of Using Artificial Intelligence in Psychiatric Wards</title>
		<author>
			<persName><forename type="first">B</forename><surname>Solaiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghuloum</surname></persName>
		</author>
		<idno type="DOI">10.1017/amj.2023.30</idno>
		<idno type="PMID">38344795</idno>
	</analytic>
	<monogr>
		<title level="j">Am J Law Med</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="250" to="266" />
			<date type="published" when="2023-07">2023 Jul. 2024 Feb 12</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Artificial Intelligence and Machine Learning in Mental Health Services: An Environmental Scan. CADTH Health Tech Rev</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wells</surname></persName>
		</author>
		<ptr target="https://www.cadth.ca/sites/default/files/attachments/2021-06/artificial_intelligence_and_machine_learning_in_mental_health_services_environmental_scan.pdf" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Towards Vision-Based Smart Hospitals: A System for Tracking and Monitoring Hand Hygiene Compliance</title>
		<author>
			<persName><forename type="first">A</forename><surname>Haque</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="75" to="77" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Governing AI in the European Union: Emerging Infrastructures and Regulatory Ecosystems in Health</title>
		<author>
			<persName><forename type="first">T</forename><surname>Minssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Solaiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Köttering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wested</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Malik</surname></persName>
		</author>
		<editor>Solaiman B, Cohen IG</editor>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Proposal for a Directive of the European Parliament and of the Council on Adapting Non-contractual Civil Liability Rules to Artificial Intelligence (AI Liability Directive)</title>
		<author>
			<persName><surname>Aild) Commission</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">COM</title>
		<imprint>
			<biblScope unit="page">496</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Directive 85/374/EEC of 25 July 1985 on the Approximation of the Laws, Regulations and Administrative Provisions of the Member States Concerning Liability for Defective Products</title>
		<author>
			<persName><surname>Pld) Council</surname></persName>
		</author>
		<idno>OJ L210</idno>
		<imprint>
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Proposal for a Regulation of the European Parliament and of the Council on the European Health Data Space</title>
		<author>
			<persName><surname>Ehds) Commission</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">COM</title>
		<imprint>
			<biblScope unit="page" from="197" to="198" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Foundational Principles of Ophthalmic Imaging and Algorithmic Interpretation Working Group of the Collaborative Community for Ophthalmic Imaging Foundation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Abràmoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Tarver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Loyo-Berrios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Trujillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Char</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Obermeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Eydelman</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41746-023-00913-9</idno>
		<idno type="PMID">37700029</idno>
		<idno type="PMCID">PMC10497548</idno>
	</analytic>
	<monogr>
		<title level="j">NPJ Digit Med</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">170</biblScope>
			<date type="published" when="2023-09-12">2023 Sep 12</date>
			<pubPlace>Washington, D.C.</pubPlace>
		</imprint>
	</monogr>
	<note>Maisel WH. Considerations for addressing bias in artificial intelligence for health equity</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A governance model for the application of AI in health care</title>
		<author>
			<persName><forename type="first">S</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Coghlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cooper</surname></persName>
		</author>
		<idno type="DOI">10.1093/jamia/ocz192</idno>
		<idno type="PMID">31682262</idno>
		<idno type="PMCID">PMC7647243</idno>
	</analytic>
	<monogr>
		<title level="j">J Am Med Inform Assoc</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="491" to="497" />
			<date type="published" when="2020-03-01">2020 Mar 1</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Reshaping healthcare with wearable biosensors</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zth</forename><surname>Tse</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41598-022-26951-z</idno>
	</analytic>
	<monogr>
		<title level="j">Sci Rep</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">4998</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A review on intelligent wearables: Uses and risks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
		<idno type="DOI">10.1002/hbe2.173</idno>
	</analytic>
	<monogr>
		<title level="j">Hum Behav &amp; Emerg Tech</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="287" to="294" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The uselessness of AI ethics</title>
		<author>
			<persName><forename type="first">L</forename><surname>Munn</surname></persName>
		</author>
		<idno type="DOI">10.1007/s43681-022-00209-w</idno>
	</analytic>
	<monogr>
		<title level="j">AI Ethics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="869" to="877" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">New AI regulation in the EU seeks to reduce risk without assessing public benefit</title>
		<author>
			<persName><forename type="first">B</forename><surname>Prainsack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Forgó</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41591-024-02874-2</idno>
	</analytic>
	<monogr>
		<title level="j">Nat Med</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
