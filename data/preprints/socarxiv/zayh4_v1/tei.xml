<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LEVERAGING MULTIMODAL LLMS FOR PLANT SPECIES IDENTIFICATION AND EDUCATIONAL INSIGHTS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuze</forename><surname>Du</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Northeast Yucai Foreign Language School</orgName>
								<address>
									<settlement>Benxi Liaoning</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yingjia</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Applied Math</orgName>
								<orgName type="institution">Northeast Yucai Foreign Language School</orgName>
								<address>
									<settlement>Shenyang Liaoning</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eric</forename><surname>Zhao</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Northwestern University Evanston</orgName>
								<address>
									<region>Illinois</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LEVERAGING MULTIMODAL LLMS FOR PLANT SPECIES IDENTIFICATION AND EDUCATIONAL INSIGHTS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9A33E30DBC198D414728C844A87B1D75</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-22T20:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this study, we investigate the potential of multimodal large language models (LLMs) for plant species identification and educational enrichment. Using an annotated dataset focused on fungi, particularly those classified as edible or non-edible, we implement a practical application that allows users to upload plant images. The LLM then identifies the species, determines its edibility, and provides detailed information on its characteristics. For edible species, the model offers culinary insights and preparation methods, while also delivering comprehensive educational content on plant ecology and cultural significance. Our approach showcases the ability of LLMs to bridge image recognition with rich, text-based knowledge, facilitating an interactive learning experience that promotes plant literacy and practical understanding. This study highlights the effectiveness of LLMs in educational tools and their potential to enhance public awareness of plant species, including fungi, through visual and contextual data fusion.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction and Background</head><p>The rapid advancement of large language models (LLMs), such as GPT-3, BERT, and their successors, has revolutionized natural language processing (NLP) by enabling machines to understand and generate human language with impressive accuracy. <ref type="bibr" target="#b0">[1]</ref> More recently, the development of multimodal LLMs-models capable of processing both text and images-has expanded the possibilities of AI applications. These models, such as CLIP and GPT-4's multimodal capabilities, bridge the gap between textual and visual information, making them well-suited for tasks involving image recognition and text generation. Leveraging multimodal LLMs can significantly enhance a variety of applications, particularly those requiring contextual understanding from visual data, such as species identification from plant images. <ref type="bibr" target="#b1">[2]</ref> Identifying plants through photographs has long been a necessity for both professional botanists and amateur naturalists. <ref type="bibr" target="#b2">[3]</ref> The accurate identification of plant species can help in research, conservation efforts, and even casual foraging. However, existing tools often rely on simple image classification algorithms that lack depth in providing educational content. For general users, these tools may not offer enough context, such as how the identified plant fits into its ecological environment, its role in biodiversity, or whether it poses any dangers-such as toxicity or allergens. This lack of information is a significant gap, especially considering the increasing interest in sustainable living, foraging, and understanding our natural surroundings.</p><p>There is a growing need for educational tools that not only identify plants but also provide comprehensive insights, including potential ecological benefits, protection statuses, and safety precautions. Many plants and fungi are misidentified, which can lead to harmful consequences. For example, mistaking toxic mushrooms for edible varieties can have dire health outcomes, as mushroom poisoning is not uncommon in some regions. Educating the general public about</p><p>Edible Mushrooms Poisonous Mushrooms Training Set: N = 158 Testing Set: N = 42 Training Set: N = 202 Testing Set: N = 48 the environment, as well as the risks and benefits of various species, is essential to promote responsible interaction with nature. <ref type="bibr" target="#b3">[4]</ref> Moreover, understanding plant ecology and recognizing rare or endangered species can aid in conservation efforts and protection of biodiversity.</p><p>LLMs, particularly multimodal ones, are well-suited to address these challenges. Unlike traditional image classifiers that are limited to visual recognition, LLMs can combine their contextual understanding of text with image analysis to generate rich educational content. These models can provide species identification from a single photo, followed by detailed information about the plant's edibility, toxicity, ecological role, and even practical uses such as medicinal benefits or culinary preparation. This approach not only improves the accuracy of identification but also adds educational value by explaining the significance of the species, thereby bridging the gap between image-based recognition and text-based knowledge. Moreover, LLMs require no additional training once implemented, simplifying their use in various applications.</p><p>In this study, we focus on an important use case: the identification of edible fungi and their corresponding educational insights. In many regions, such as China, mushroom poisoning is a significant public health issue, primarily caused by the misidentification of poisonous mushrooms as edible species. According to recent research <ref type="bibr" target="#b4">[5]</ref>, mushroom poisonings often result in gastrointestinal distress, but more severe cases can lead to liver failure, kidney damage, and even death. From 2010 to 2022, over 10,000 mushroom poisoning outbreaks were reported in China, resulting in nearly 800 deaths. <ref type="bibr" target="#b4">[5]</ref> These outbreaks peak between May and October, a time when foraging activity is highest. The need for a reliable identification tool that can prevent mushroom poisoning is evident.</p><p>We present an application that leverages multimodal LLMs to identify fungi species and determine their edibility based on user-uploaded images. This tool goes beyond mere identification by offering educational content on the risks associated with consuming certain fungi, cooking methods for edible varieties, and ecological information to help users better understand their environment. Given the high stakes associated with fungi misidentification, especially in regions with a tradition of foraging, such a tool has the potential to save lives by reducing accidental poisonings. This study showcases the utility of LLMs in practical applications while demonstrating their ability to educate the public on important ecological and safety considerations surrounding fungi and plants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Development of the Application</head><p>We developed a web-based application that allows users to upload an image and receive educational insights about the plant or fungus species in the image. The app was built using the OpenAI GPT-4 API, integrated with the Streamlit framework for ease of use. When a user uploads an image, it is resized to a resolution of 512x512 pixels. This resizing helps reduce the token usage for the GPT-4o API, improves consistency, and minimizes computational load while retaining the visual details necessary for accurate species identification. The application is hosted on a local workstation, accessible through port access. This enables real-time processing and feedback, ensuring that the image is processed swiftly and insights are delivered promptly to the user. The backend processes include image handling, resizing, and sending data to the OpenAI GPT-4o API for prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Prompt Engineering</head><p>The accuracy of species identification and educational insight generation depends heavily on prompt engineering. First, the model identifies the species shown in the uploaded image, followed by determining whether the plant or fungus is edible. If the species is determined to be poisonous or inedible, the application provides safety information and educational guidance.</p><p>If the species is edible, the model suggests appropriate culinary recipes. To support the process, we incorporated a crowdsourced list of common edible fungi. For cases where GPT-4 is unable to identify the species with confidence, the model requests additional images from different angles. The prompt is structured to ensure the model provides species-specific protection and conservation insights, educating users on how the plant or fungus fits into the larger ecological context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Test Cases</head><p>To evaluate the performance of our application, we curated a dataset consisting of fungi images found in China. We selected 200 images of edible mushrooms and 250 images of poisonous mushrooms. The edible mushroom images were based on the Chinese Edible Fungi Checklist <ref type="bibr" target="#b5">[6]</ref>, and the poisonous mushroom images were drawn from the checklist revised by Bau et al. <ref type="bibr" target="#b6">[7]</ref>.</p><p>The dataset is balanced to cover a diverse set of mushroom species found in China, ensuring the model encounters a variety of real-world test cases. Each species is represented only once in the dataset, with no duplicate images. This dataset is open-sourced for public access to benefit researchers interested in mushroom classification. Access to the dataset is available at: <ref type="url" target="https://drive.google.com/drive/folders/13NFDI5UhcLHPSL2WMcOrFs6QhhmrDjxQ?usp=sharing">https://drive.google.com/drive/folders/13NFDI5UhcLHPSL2WMcOrFs6QhhmrDjxQ?  usp=sharing</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Evaluation</head><p>For the evaluation in your study, the performance of the multimodal LLM is compared with four well-known deep convolutional neural networks: AlexNet, VGG15, DenseNet121, and ResNet50. <ref type="bibr" target="#b7">[8]</ref> The key distinction here is that the multimodal LLM is training-free, meaning it does not require specific training on the dataset, whereas the deep CNNs were trained on the training split of the evaluation dataset. This dataset contains approximately 100 images for each class-edible and poisonous mushrooms.</p><p>For the deep CNNs, the models were fine-tuned to optimize classification performance based on the mushroom dataset, where both classes (edible and poisonous) were used. The evaluation was performed by comparing accuracy, F1 score, and other metrics, providing insights into the benefits and trade-offs between using training-free approaches like multimodal LLMs and conventional CNNs that require extensive training.</p><p>This setup highlights the efficiency and ease of deployment of multimodal LLMs for certain tasks, compared to CNNs, which require significant computational resources and time for training, especially with smaller datasets like the one used in this study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Implementation</head><p>The entire pipeline was implemented using Python 3.8. Image preprocessing, including resizing, was performed using the Pillow library. Data handling and preprocessing were managed using Pandas and NumPy. The OpenAI GPT-4o API was used for plant and fungus identification, integrated into the Streamlit application for a user-friendly interface.</p><p>For model evaluation, the Scikit-learn library was employed to calculate accuracy, AUC, precision, recall, and F1-score. All processes were run on a high-performance local workstation, and visualizations such as confusion matrices and ROC curves were created using Matplotlib and Seaborn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>The results of the developed Streamlit interface demonstrate a user-friendly and educational platform that allows users to upload an image of a plant or fungus. After uploading, the image is processed using GPT-4 to identify the species depicted. If the species is recognized, the system provides relevant information such as edibility, protection status, and ecological insights. For edible species, the platform also suggests recipes, as shown in the oyster mushroom example, which includes step-by-step cooking instructions. If the species remains unidentifiable, the user is prompted to upload more images for better analysis. This combination of species identification and educational content makes the application a valuable tool for enhancing plant and fungi awareness, especially in terms of safe consumption and conservation knowledge. The platform balances simplicity of use with the delivery of rich, informative content, benefiting both casual users and plant enthusiasts.</p><p>The performance comparison between the multimodal LLM (GPT-4o) and four deep convolutional neural networks (CNNs) -AlexNet, VGG16, DenseNet121, and ResNet50 -in classifying poisonous mushrooms is summarized in Table <ref type="table" target="#tab_0">1</ref>. The multimodal LLM outperformed the traditional CNN models across all evaluation metrics.</p><p>ResNet50, which was the best-performing CNN model, achieved an accuracy of 0.756, with an AUC of 0.809 and an F1 score of 0.766. However, GPT-4o surpassed ResNet50, with an accuracy of 0.773 and an AUC of 0.821, showing a marked improvement. The GPT-4o model also achieved the highest F1 score of 0.777, indicating its robustness in handling this binary classification task.</p><p>While the CNN models demonstrated competitive performance, particularly ResNet50 and DenseNet121, the multimodal LLM provided an advantage in classification, despite being training-free. This highlights the potential of LLMs in efficiently classifying visual data without the need for intensive model training, offering significant time savings and computational efficiency. Furthermore, the use of GPT-4o also allows for integration with text-based outputs, enabling it to provide educational information in addition to classification results, which is particularly valuable in real-world applications such as mushroom identification and education.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Limitation and Conclusion</head><p>Despite the promising results of leveraging multimodal large language models (LLMs) for plant and fungi species identification, several limitations exist in this study. One major limitation is the reliance on predefined datasets and curated crowdsourced images, which may not comprehensively capture the diversity of species, especially in uncontrolled environments. Moreover, the model's reliance on user-uploaded images poses a risk of incorrect predictions if the image quality is poor or ambiguous, as LLMs have inherent limitations when faced with noisy data. Another key limitation is the inability of LLMs to generalize beyond species or datasets that were well-represented in the training phase. Thus, novel species or plants with atypical features may yield incorrect classifications. Furthermore, the performance of LLMs in identifying nuanced differences between closely related species remains to be rigorously tested. The system also depends on the GPT-4 API, which raises concerns about long-term accessibility and scalability, given the reliance on proprietary technologies.</p><p>In this study, we explored the use of multimodal LLMs to identify plant and fungi species from user-uploaded images, while providing educational insights, such as ecological information and edibility status. Through practical application in mushroom classification, we demonstrated that LLMs offer comparable performance to deep learning models without requiring intensive training, making them highly suitable for lightweight, user-friendly applications. The proposed approach not only simplifies species identification but also enhances user engagement through contextual knowledge delivery, making it a promising tool for plant education and safety awareness. Future research should focus on enhancing the system's robustness, expanding the training dataset, and addressing the limitations associated with rare or atypical species identification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Summary statistics and examples of the fungi dataset</figDesc><graphic coords="2,187.20,169.86,72.67,72.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Interface for the plant identifier</figDesc><graphic coords="3,165.60,72.00,280.82,455.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance of four deep convolutional neural networks and one multimodal LLM on the classification of poisonous mushrooms.</figDesc><table><row><cell>Model</cell><cell cols="4">Accuracy AUC Precision Recall F1</cell></row><row><cell>AlexNet</cell><cell>0.700</cell><cell>0.748 0.705</cell><cell>0.750</cell><cell>0.727</cell></row><row><cell>VGG16</cell><cell>0.689</cell><cell>0.735 0.689</cell><cell>0.833</cell><cell>0.755</cell></row><row><cell cols="2">DenseNet121 0.744</cell><cell>0.764 0.756</cell><cell>0.708</cell><cell>0.731</cell></row><row><cell>ResNet50</cell><cell>0.756</cell><cell>0.809 0.782</cell><cell>0.750</cell><cell>0.766</cell></row><row><cell>GPT-4o</cell><cell>0.773</cell><cell>0.821 0.797</cell><cell>0.762</cell><cell>0.777</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5">Author Contribution Statement</head><p>All authors discuss and define the research problem. Y.W. extracts and pre-process all the research data. Y.D. and <rs type="person">E.Z.</rs> implement all deep neural networks, analyze and visualize the results. Y.D. and Y.W. write the manuscript advised by E.Z., who also organizes the manuscript to a better academic standard.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A review on large language models: Architectures, applications, taxonomies, open issues and challenges</title>
		<author>
			<persName><forename type="first">Mohaimenul</forename><surname>Azam Khan Raiaan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><surname>Saddam Hossain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaniz</forename><surname>Mukta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nur</forename><surname>Fatema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadman</forename><surname>Mohammad Fahad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Most</forename><surname>Sakib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jannat</forename><surname>Marufatul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jubaer</forename><surname>Mim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><forename type="middle">Eunus</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sami</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><surname>Azam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<publisher>IEEE Access</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A comprehensive review of multimodal large language models: Performance and challenges across different tasks</title>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanqi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peiran</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sichen</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjun</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.01319</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Plant image identification application demonstrates high accuracy in northern europe</title>
		<author>
			<persName><forename type="first">Jaak</forename><surname>Pärtel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meelis</forename><surname>Pärtel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jana</forename><surname>Wäldchen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AoB Plants</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">50</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Conserving plants within and beyond protected areas-still problematic and future uncertain</title>
		<author>
			<persName><forename type="first">Heywood</forename><surname>Vernon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Plant Diversity</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="36" to="49" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mushroom poisoning outbreaks-china, 2010-2020</title>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><forename type="middle">M</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yafang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jikai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haihong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">China CDC weekly</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page">518</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A revised checklist of edible fungi in china</title>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuliang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaan</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taihui</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mycosystema</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A revised checklist of poisonous mushrooms in china</title>
		<author>
			<persName><forename type="first">Tolgor</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bao</forename><surname>Haiying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">I</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mycosystema</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="517" to="548" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Review of image classification algorithms based on convolutional neural networks</title>
		<author>
			<persName><forename type="first">Leiyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaobo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanlong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanming</forename><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page">4712</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
