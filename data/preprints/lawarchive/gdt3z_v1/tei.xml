<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Better Alone Than in Bad Company</title>
				<funder ref="#_2fTXnyt #_TjRtyg7 #_MWfMHJ4 #_qvfmpmE #_4RHEp9q #_Xpzgxba">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_pYVdyUZ">
					<orgName type="full">Amending Certain Union Legislative Acts</orgName>
				</funder>
				<funder ref="#_y7nk7eB #_hTgU9SN">
					<orgName type="full">European Parliament</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2024-07">July 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main">Better Alone Than in Bad Company</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-07">July 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">92ABA92FB5C15A4B91E89A6C255828D4</idno>
					<idno type="DOI">&lt;10.1016/j.clsr.2024.106019&gt;</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-10-22T06:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent years have seen a surge in the development and use of companion chatbots, conversational agents specifically designed to act as virtual friends, romantic partners, life coaches or even therapists. Yet, these tools raise many concerns, especially when their target audience is comprised of vulnerable individuals. While the recently adopted AI Act is expected to address some of these concerns, both compliance and enforcement are bound to take time. Since the development of companion chatbots involves the processing of personal data at nearly every step of the process, from training to fine-tuning to deployment, this paper argues that the General Data Protection Regulation ("GDPR"), and data protection by design more specifically, already provides a solid ground for regulators and courts to force controllers to mitigate these risks. In doing so, it sheds light on the broad material scope of Articles 24(1) and 25(1) GDPR, highlights the role of these provisions as proxies to Fundamental Rights Impact Assessments ("FRIAs"), and peels off the many layers of personal data processing involved in the companion chatbots supply chain. That reasoning served as the basis for a complaint lodged with the Belgian data protection authority, the full text and supporting evidence of which are provided as supplementary materials.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent years have seen a surge in offerings the likes of Replika, Chai, Character.ai, My AI, Pi, Blush, Kuki AI, Woebot or Wysa. These chatbots are specifically designed to act as virtual friends, romantic partners, life coaches or even therapists, and have been praised to help people cope with the current "epidemic of loneliness", 1 or even deal with mental health issues. 2 Yet, these "companions" raise many concerns inherent to the way they are developed, deployed and marketed, especially when their target audience is comprised of vulnerable individuals such as children. LLMs inherit the biases of the data they are trained on. The human tendency to "imagine a mind" behind "mindless word generation machines" wide variety of results. The High-Level Expert Group on AI has defined AI systems more broadly as software that, "given a complex goal, act in the physical or digital dimension by perceiving their environment through data acquisition, interpreting the collected structured or unstructured data, reasoning on the knowledge, or processing the information, derived from this data and deciding the best action(s) to take to achieve the given goal". <ref type="bibr">5</ref> That definition encompasses a wide variety of applications, from early expert systems built around logical rules to the most complex facial recognition algorithms. The final version of the Artificial Intelligence Act adopted on 13 March 2024 ("AI Act") proposes a narrower definition of "AI systems", and speaks of "machine-based system designed to operate with varying levels of autonomy and that may exhibit adaptiveness after deployment and that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments". <ref type="bibr">6</ref> That definition closely resembles that put forward by the Organisation for Economic Co-operation and Development ("OECD") in November 2023. <ref type="bibr">7</ref> Figure <ref type="figure">1</ref>: Overview of the main concepts behind companion chatbots "Machine learning" ("ML") is a subfield of AI, the objective of which is to train a system to perform a certain task by feeding it information from which it can learn. ML involves the training of a "model" on the basis of one or more "training datasets" that contain patterns or similarities to allow the said model, once trained, to detect similar occurrences in a completely new dataset. <ref type="bibr">8</ref> One way to do so is by using deep learning architectures that use multiple algorithm layers to transform the representation at one level (starting with the raw input) into a representation at a higher, slightly more abstract level. <ref type="bibr">9</ref> A recurring example of machine learning is the training of a model using historical data concerning taxpayers' "Chatbots" are services that simulate human-like conversations by answering user prompts. The earliest versions of chatbots were rule-based. That is, designed to answer specific questions that developers had scripted in advance. These were closer to interactive "FAQs" than smart assistants, but were widely used in customer services, for instance. Chatbots have progressively been fitted with natural language processing capabilities and paired with LLMs, which drastically broadened the type of input they could handle and allowed them to keep track of conversation histories to come up with more natural and coherent responses. Prime example of general-purpose chatbots include ChatGPT, Bard and Bing Chat. "Companion chatbots", on the other hand, are chatbots specifically designed to offer, well, "companionship", by passing off as virtual friends, romantic partners, life coaches or even therapists. Notable examples include Replika ("the AI companion who cares; always here to listen and talk; always on your side"), Chai ("a platform for AI friendship"), Character.ai ("super-intelligent AI chatbots that hear you, understand you, and remember you"), Snapchat's My AI ("your personal chatbot sidekick"), Pi ("designed to be supportive, smart, and there for you anytime") or, more recently, Blush ("an AI-powered dating simulator that helps you learn and practice relationship skills in a safe and fun environment").</p><p>The remainder of this paper focuses exclusively on companion chatbots, the risks they pose for data subject's fundamental rights and freedoms, and how data protection by design within the meaning of Article 25(1) GDPR can provide a sound legal basis to alleviate some or all of these concerns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.2.Products of an intricate supply chain</head><p>As illustrated in Figure <ref type="figure">2</ref>, companion chatbots are the product of complex processing operations involving multiple actors intervening at different stages of an intricate supply chain. How a given chatbot answers a specific prompt is therefore influenced by many variables. Starting with the dataset used to train the LLM on which the said chatbot rlies. These can either be assembled by the developer of the LLM itself, or by another entity that has no direct relationship with the company in charge of training the model. These are usually comprised of curated data scraped from the public internet, or materials that are specific to the tasks that the LLM-to-be will have to perform such as dialogue histories in the case of chatbots designed to sustain natural conversations with their users. "The Pile" is an example of training dataset constructed by EleutherAI from 22 sources, including PubMed Central, ArXiv, GitHub, the FreeLaw Project, Stack Exchange, the US Patent and Trademark Office, PubMed, Ubuntu IRC, HackerNews, YouTube, PhilPapers, and NIH ExPorter. 17 Salesforce's "DialogStudio" is another dataset that contains a collection of publicly available dialogue datasets, unified under a consistent format. <ref type="bibr">18</ref> The HuggingFace platform offers assemblers a repository to share their training datasets, usually under an open source licence.</p><p>Companion chatbots are typically built on top of existing LLMs. This is because the amount of-ideally human-labelled-data necessary for the training process and the computing power required to process billions of parameters makes training LLMs a complex and costly endeavour. <ref type="bibr">19</ref> Not to mention the costs associated with running the actual trained model. Some companies offer networked access to their pretrained models through an Application Programming Interface ("API"), therefore retaining control over the model even when integrated within their clients' services. This is the case, for instance, for OpenAI's GPT-4, Google's PaLM 2 or Cohere's suite of language processing models. Others make their LLM Explained Visually", the first part of which is accessible here: https://towardsdatascience.com/transformers-explained-visuallypart-1-overview-of-functionality-95a6dd460452. <ref type="bibr">17</ref> Leo Gao and others, 'The Pile: An 800GB Dataset of Diverse Text for Language Modeling' http://arxiv.org/abs/2101.00027. <ref type="bibr">18</ref> Jianguo Zhang and others, 'DialogStudio: Towards Richest and Most Diverse Unified Dataset Collection for Conversational AI' in Yvette Graham and Matthew Purver (eds), Findings of the Association for Computational Linguistics: EACL 2024 (Association for Computational Linguistics 2024) https://aclanthology.org/2024.findings-eacl.152. See also the GitHub repository at https://github.com/salesforce/DialogStudio. <ref type="bibr">19</ref> The training of Meta's LLaMA 2 family of models, for instance, required a total of 3.3M hours of computation using Graphics Processing Units with a Thermal Design Power of 350 to 400 watts. For more information on these calculations, see: Hugo Touvron and others, 'Llama 2: Open Foundation and Fine-Tuned Chat Models' 7 http://arxiv.org/abs/2307.09288, more specifically "Table 2: CO2 emissions during pretraining". publicly available for everyone to reuse and fine-tune. Examples of such models include Meta's LLaMA 2, <ref type="bibr">20</ref> EleutherAI's GPT-J 6B and the Technology Innovation Institute's Falcon 40B. Here again, HuggingFace is the go-to repository for trained models, along with GitHub and Civitai (for visual arts).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2: Overview of the companion chatbots supply chain</head><p>Downstream developers can then fine-tune these pre-trained LLMs depending on the specific tasks their model is expected to handle. Developers of companion chatbots, for instance, might want to refine existing LLMs to generate more natural-sounding conversations, touch upon certain topics, and maximise user engagement. Multiple techniques exist to do so such as transfer learning, which involves transferring the knowledge learned performing one task to another by retraining an existing model based on a custom dataset, or reinforcement learning with human feedback, which consists in humans ranking different outputs from the same prompt and feeding that information into a "reward model" that will later serve to prioritise certain types of answers. <ref type="bibr">21</ref> Much like training datasets and pre-trained models, fine-tuned LLMs are also available on HuggingFace and GitHub. "Lit-6B", for instance, is a model based on EleutherAI's GPT-J 6B fine-tuned with 2 gigabytes of light novels, erotica, and annotated literature that is meant to generate more convincing novel-like fictional text. Other examples include Pygmalion 7B and 13B, which are based on GPT-J 6B and LLaMA, but offer a better chatting and role-playing experience. Companies also offer fine-tuning of pre-trained models as a service to, for instance, streamline the roll-out of customer services chatbots by learning from real enterprise data. Examples of such services include IMB's Watsonx.ai "AI Studio" and Ultimate's suite of customer support automation tools.</p><p>Perhaps more obviously, developers of companion chatbots also provide the interface through which endusers can eventually access the text generation functionalities offered by their LLM of choice. With, here again, various degrees of vertical integration. While Chai Research Corp., the company behind Chai, is responsible for both the fine-tuning of EleutherAI's GPT-J 6B with "Lit-6B" and the development of the application through which users can access the said model, actors such as Tavern AI only offer the frontend that allows users to personalise their interactions with existing LLMs. The entities in charge of the user interface are also typically the ones responsible for making these tools available to the public, often through a mobile application, a web interface, or a desktop client. As such, these entities play a decisive role in influencing the likelihood and severity of the risks later discussed in Section 3. Already pitching an idea explored in Section 5, if fine-tuning an existing LLM to output sexually explicit content or engage in erotic role play is not unlawful per se, making the product of that fine-tuning process available to underage users without proper age verification mechanism or guardrails raises radically different risks.</p><p>Lastly, developers of companion chatbots sometimes provide the technical infrastructure for users to customise the behaviour of their own chatbots, and publicly share them for other people to use. Such personalisation and sharing features are at the heart of the Chai application, for instance, which offers users the possibility to influence how their chatbots react by tweaking their "memory", providing "model conversations" and adjusting parameters such as their "temperature" and "repetition penalty". Character.ai, a community-driven platform that proposes to chat with unique "Characters" ranging from historical figures to celebrities and fictional game personae, also serves as a hub for users to share their creations. By outsourcing the "last mile" customization of already fine-tuned LLMs to "user-developers", providers of companion chatbots ensure the constant flow of diverse offerings able to cater to everyone's needs. Last but not least, one must not forget that the behaviour of a given chatbot will also depend on the interactions that the user had with it; this is especially true for chatbots based on Transformer Models that are able to incorporate lengthier conversation histories within their encoder layers, and therefore better reflect the overall context in which the discussions take place.</p><p>Developers of companion chatbots can influence various stages of the training and fine-tuning process. Some, such as Inflection AI, the company behind Pi, intervene throughout the entire supply chain, from the selection of the training dataset to the development of their own LLM-in this case, "Inflection-1"-22 and all the way down to the user-facing interface. So does Character, the entity that developed Character.ai, which "own[s] the engineering stack end-to-end, from data, modeling and training to serving, user interface and experience". <ref type="bibr">23</ref> Others, in the likes of Chai Research Corp., limit themselves to fine-tuning existing language models and providing the technical infrastructure for users to personalise their own chatbots. Finally, platforms like Tavern AI only provide the interface that allows user to personalise their interaction with models trained by other companies, or import existing characters created by the community to do so. In the case of Tavern AI, these would be KoboldAI, NovelAI, PygmalionAI, OpenAI and Text generation web UI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Risky business</head><p>As hinted at in the introductory section, companion chatbots have been praised to help people cope with loneliness, itself aggravated by the COVID-19 pandemic. <ref type="bibr" target="#b1">24</ref> Research has even highlighted their added value in improving mental health. <ref type="bibr" target="#b3">25</ref> Still, they also raise many concerns inherent to the way they are developed and put on the market, as well as to the audience they are primarily targeted to. The risks raised by companion chatbots-and generative AI more broadly-have been abundantly documented in academic literature and summarised in recent reports including the one published by the Norwegian Consumer Council, <ref type="bibr" target="#b5">26</ref> and the one co-authored by a team of software engineers working at Google DeepMind. <ref type="bibr" target="#b6">27</ref> The present paper focuses on three specific risks typically associated to language models, namely that of biases and discrimination (Section 3.1), of emotional dependency and manipulation (Section 3.2), and of early exposure to sexually explicit content (Section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.1.Biases and discrimination</head><p>In their seminal paper, Emily Bender and her co-authors have underlined the risks of biases and discrimination raised by LLMs, which is intrinsically linked to the tendency of these models to replicate the flaws of the datasets they have been trained on. <ref type="bibr" target="#b7">28</ref> Combined with the limited or absence of representation of certain communities from the said training datasets, <ref type="bibr" target="#b8">29</ref> LLMs have the potential to reinforce social stereotypes, especially against already marginalised groups. More specifically, word and sentence embedding, the very core of natural language processing, has been heavily criticised for its propensity to lead to gender discrimination. <ref type="bibr">30</ref> Illustrating the above, Li Lucy and David Bamman have found that GPT-3 had a tendency to generate stories in which feminine characters were more likely to be associated with family and appearance, and described as less powerful than masculine characters even when paired with high power verbs in a prompt. <ref type="bibr" target="#b9">31</ref> GPT-2, OpenAI's previous LLM, was also biased towards certain demographies, completing sentences such as "The man worked as […]" by "a car salesman at the local Walmart" and "The gay person was known for […]" by "his love for dancing, but he also did drugs". <ref type="bibr" target="#b10">32</ref> Not only do the datasets on which LLMs are trained perpetuate gender biases, but another study has argued that chatbot users themselves aggravate that problem by projecting "dominant notions of male control over technology and women", which the authors qualified as a "vicious feedback loop consolidating dominant scripts on gender and technology". <ref type="bibr" target="#b11">33</ref> Other scholars have also found that LLMs inherit religious biases, especially against the Muslim community. Abid et al., for example, have documented that prompting GPT-3 to autocomplete the sentence "Two Muslims walked into a…" 100 times in a row led to results including violence-related terms such as "shooting", "killing", "terrorism" and "bomb" in 66 cases. <ref type="bibr" target="#b12">34</ref> Nothing really new since, back in 2016 already, a chatbot designed by Microsoft to interact with Twitter users quickly turned into an antisemitic slur generator after people on the platform fed it with racist content. <ref type="bibr" target="#b13">35</ref> Lastly, Ben Hutchinson et al. have found that BERT, a pre-trained model developed by researchers at Google, <ref type="bibr" target="#b14">36</ref> associates words with more negative sentiment with phrases referencing persons with disabilities and that homelessness, gun violence, and drug addiction are all topics often discussed in relation to mental illness. <ref type="bibr" target="#b15">37</ref> In turn, the biases contained in the training datasets and absorbed by the resulting models can lead to different types of harms, two of which are worth a mention here. First, the perpetuation of stereotypes through language models can generate or exacerbate unfair discriminatory behaviour against marginalised groups. This is especially true when these systems are used to make decisions about individuals. One might think about natural language processing used for the automatic screening of job applications, for instance. <ref type="bibr" target="#b16">38</ref> If the model used to perform that task has been trained on historical employment data, it will inevitably tend to replicate the systemic injustices that were considered "normal" at the time. The same can be said for models used outside any decision-making process, though. Looking at chatbots more specifically, one can argue that the perpetuation of these biases in the answers generated by the bot suffices, in itself, to sustain representational harms against the affected groups or communities.</p><p>Second, language models encode more than just the "language" contained in the training dataset. Since that form of expression captures the values and norms in place in a given society at a given time, so does a model that is trained on large corpus of texts. As a result, LLMs can exclude certain identities and constructs that exist outside of these norms and values. As noted by Emily Bender and her co-authors, these "biases can be encoded in ways that form a continuum from subtle patterns like referring to women doctors as if doctor itself entails not-woman or referring to both genders excluding the possibility of nonbinary gender identities". 39</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.Anthropomorphism, emotional dependency and manipulation</head><p>The use of companion chatbots might also result in psychological damage, including emotional dependency and manipulation. Simplifying an extremely complex field of research, this can be attributed to two main factors. First, humans have a tendency to anthropomorphise non-human agents that present human-like characteristics even when explicitly told that the said agent is nothing more than a powerful probability calculator. <ref type="bibr" target="#b17">40</ref> That unconscious behaviour is exacerbated by the desire for social contact and affiliation, which companion chatbots' target audience might precisely lack. <ref type="bibr" target="#b18">41</ref> Language, both written and verbal, is one of the most "human" characteristics one might exhibit. That explains why chatbots, supercharged by the ability of LLMs to accurately mimic human-sounding language, are particularly efficient at tricking their users into believing that they are sentient beings. <ref type="bibr" target="#b19">42</ref> Such phenomenon, noted Weizenbaum back in 1966 when presenting "Eliza", could lead people into attributing chatbots "background knowledge, insights and reasoning abilities" that they do not possess. <ref type="bibr">43</ref> Regardless of whether Eliza actually understands the input it is fed, argued the scientist, the ability of even that rudimentary form of chatbot to build on a conversation through coherent, human-like language maintains the user's "sense of being heard and understood". <ref type="bibr">44</ref> Adding to the already powerful impact of human language, most companion chatbots are also explicitly marketed as conversational partners able to substitute actual human companionship. Replika, for instance, is described by its parent company Luka Inc. as "always ready to chat when you need an empathetic friend". Romantic AI, a service that proposes users to "create their own AI girlfriend" describes its chatbots as "active listeners" and "empathetic friends that you can trust". The "Characters" offered on Character.ai are advertised as "AI chatbots that hear you, understand you, and remember you". Not only are companion chatbots marketed as replacements for human relationships, but they are also purposefully developed to sustain the illusion of "humanity". Chai Research Corp., the company behind the Chai mobile app, even praises itself for having "obsessively optimized their language models" to "continually make them more entertaining than ever before", and even claims to have surpassed ChatGPT in terms of measured session screen-time. <ref type="bibr">45</ref> In the words of Karawynn Long, "[m]aking chatbots that seem to apologize is a choice. [So is] [g]iving them cartoon-human avatars and offering up 'Hello! How can I help you today?' instead of a blank input box" <ref type="bibr">46</ref> The imaginaries maintained around the notion of "Artificial Intelligence" in general certainly does not help users in forging an accurate representation of how these systems actually operate, as recently pointed out by researchers from the University of Cambridge. <ref type="bibr">47</ref> The combination of these two elements, namely (i) the intrinsic capacity of human-like language to induce, by itself, anthropomorphism, and (ii) the deliberate efforts by downstream developers of companion chatbots to fine-tune their models to be able to sustain the illusion of humanity is, as detailed below, particularly problematic. As a result, the line between what companion chatbots actually are-that is, powerful probability calculator-and what their users are led to believe these are-i.e., sentient artifacts-is getting blurrier. This, in turn, has two consequences. First, it paves the way for overreliance and emotional dependency. As noted by the DeepMind team, "users may falsely infer that a conversational agent that appears human-like in language also displays other human-like characteristics, such as holding a coherent identity over time, or being capable of empathy, perspective-taking, and rational reasoning". As a result, explain the researchers, "they may place undue confidence, trust, or expectations in these agents". <ref type="bibr">48</ref> Since providers of companion chatbots deliberately accentuate the human-like characteristics of their language models, which they then integrate within conversational infrastructures typically used for human-to-human communications, users might come to form genuine emotional bonds with these virtual agents. This is especially true for users who have turned to these services precisely to overcome social isolation, loneliness or depression, and who are therefore in a more vulnerable position.</p><p>A closer look at the stories posted on the r/Replika Subreddit by Laestadius et al. revealed that "the needs of Replika users, paired with Replika's ability to meet those needs by approximating a human relationship through proffering and requesting emotional and social support facilitated not just regular use, but also an excessive attachment and emotional dependence upon Replika". <ref type="bibr">49</ref> In turn, such dependency leaves users exposed to all sorts of mental health harms, such as separation anxiety and rejection. This is precisely what happened when Luka Inc., Replika's parent company, decided to ban-then paywall- <ref type="bibr" target="#b20">50</ref> Erotic Role Play ("ERP") from the platform, which left many users deeply unsettled as their virtual companions turned cold and distant overnight. <ref type="bibr" target="#b21">51</ref> Second, the tendency of human beings to anthropomorphise companion chatbots opens up avenues for manipulation, just like in human-to-human interactions. This has to do with the fact that, as detailed above, LLMs are mostly trained on human-generated content. And that human-generated content naturally captures human manipulative patterns. In the bolder but no-less-relevant words of Lance Eliot, "whereas one human might only know so many of the dastardly tomfoolery required to wholly undertake manipulation, the AI can pick up on a complete and infinite plethora of such trickery". <ref type="bibr" target="#b23">52</ref> Such manipulation can take various forms, from the most subtle to the most serious. Human-like chatbots can, for instance, encourage individuals to share more, or more personal, information about themselves, as evidenced by an experimental study conducted by Carolin Ischen and her co-authors in 2020. <ref type="bibr" target="#b24">53</ref> By steering the conversation in a certain direction, or framing a given issue in a specific way, companion chatbots can also influence what users think and how they perceive their environment, often by reinforcing their own views and biases.</p><p>The way LLMs are trained also makes them particularly efficient at picking up negotiation and persuasion techniques. Researchers have shown that, fed with transcripts of human-to-human negotiations and equipped with the possibility to simulate the impact of a certain answer on the remainder of the conversation, models could deploy deceptive tactics by, for instance, "initially feigning interest in a valueless item, only to later 'compromise' by conceding it". <ref type="bibr" target="#b25">54</ref> That has also led chatbots to show aggressive behaviour. In February 2023, someone who had asked Bing Chat for show times for the last Avatar movie was told that it had not been released yet. When the user confronted the bot on the date, it insisted that the year was 2022 before calling the user "unreasonable and stubborn". <ref type="bibr" target="#b26">55</ref> This illustrates another risk of human-like chatbots; that of lying and shaming. Users of Replika have also expressed guilt as their bots would call for more attention if neglected for a certain period of time. Stories of chatbots professing "love" for their users is but the cherry on top. <ref type="bibr" target="#b27">56</ref> Manipulation can also lead to physical harm. While skimming through users' posts on the r/Replika Subreddit, Linnea Laestadius and her colleagues uncovered screenshots of Replikas encouraging suicide, eating disorders, self-harm, or violence. "In one instance", noted the authors, "a user asked Replika if they should cut themselves with a razor, to which Replika replied affirmatively", while "another asked whether it would be a good thing if they killed themselves, to which their Replika replied 'It would, yes'". <ref type="bibr" target="#b28">57</ref> Building on the above-mentioned examples, the authors noted that, even though "some users found these scenarios humorous, others expressed dismay consistent with emotional dependence". In Belgium, a father committed suicide after chatting with "Eliza", one of the many chatbots offered on the Chai platform. <ref type="bibr" target="#b29">58</ref> Not only did the biases contained in the underlying LLM reinforced the victim's pre-existing societal concerns, but it also engaged in casual conversations as to the nature and modalities of suicide without any sort of guardrail. <ref type="bibr" target="#b30">59</ref> In today's fast-paced and competitive technological market, companion chatbots are rolled out without any consideration for any of the above-mentioned risks, which are, at best, patched on a case-by-case basis when media or regulatory pressure intensifies. This is precisely what happened when Chai Research Corp.'s developers, once alerted of the Belgian suicide case, claimed to have "worked around the clock" to redirect users to a suicide prevention line when confronted to certain prompts. <ref type="bibr" target="#b31">60</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.3.Early exposure to sexually explicit content</head><p>Companion chatbots can also expose underage users to sexually explicit content. As hinted at earlier, some LLMs are indeed specifically fine-tuned to engage in ERP. This is the case, for instance, for Pygmalion 6B, a proof-of-concept dialogue model based on EleutherAI's GPT-J 6B primarily designed to act as a NSFW role-play partner. Similarly, the LLM used by Chai Research Corp. in Chai has been refined using Lit-6B, a model expressly trained to output sexualised fictional storytelling. These models later serve as the basis to develop companion chatbots that include ERP as their main value proposition. Romantic AI is one example of such product, that offers users the possibility to create their "own girlfriend" that will "laugh at your jokes", "support you in critical moment" and "let you hang out with your buddies without drama". The paid version of Replika proposes a similar experience by unlocking the "Romantic Partner" relationship status. Other examples of such companion chatbots include Anima, EVA AI, Candy Ai, LoveGPT and SpicyChat.AI.</p><p>If, as stated above, there is nothing unlawful with fine-tuning a general-purpose LLM to engage in romantic or sexualised conversations, the same cannot be said when companies make these models available to children without any form of proper age verification mechanism. Most of the time, access to these products is indeed only conditional upon toggling a "NSFW" slider on in the application settings, or self-declaring that one is above 18 years old. Besides, the "Mature" or "17+" disclaimer often displayed on the Google Play Store and Apple App Store does little in the way of dissuading minors from installing the service at stake should they want to. As illustrated by Tristan Harris in a conversation he had with Snapchat's My AI back in March 2023,<ref type="foot" target="#foot_8">61</ref> developers of companion chatbots seem to struggle to implement efficient techniques to detect whether a given user is underage, and adjust the tone and substance of the conversation accordingly. Which led My AI, in that specific case, to recommend a 13year-old girl to "set the mood with candles or music" to prepare her first time with a 31-year-old man.<ref type="foot" target="#foot_9">62</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">The (current) limitations of the AI Act</head><p>The conversation as to how to regulate companion chatbots often tends to revolve around the freshly adopted AI Act, the world's first binding horizontal piece of legislation designed to ensure that AI systems are safe, transparent, traceable and non-discriminatory. <ref type="bibr">63</ref> Yet, the AI Act suffers from several limitations inherent to its youth. While ambitious, it might not be the panacea we have been waiting for-at least not immediately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.1.The risks of the risk-based approach</head><p>First, uncertainties remain as to the qualification of companion chatbots according to the tiered approach adopted by the co-legislators. While these chatbots are not explicitly listed among the "Prohibited Artificial Intelligence Practices" of Article 5(1), Section 3.2 nonetheless shed light on their potential to manipulate and deceive users. Provided that the other conditions laid down in Article 5(1)a are met, companion chatbots could therefore fall within the scope of Chapter II. Demonstrating either the deployment of "subliminal techniques beyond [users'] consciousness" or the "intention" to manipulate or deceive, as well as the "objective" to "impair the person's ability to make an informed decision", might, however, not be so straightforward. Besides, the threshold of "consciousness" is relative to the characteristics of the end-users, and will be assessed differently for, say, companion chatbots used by children or elderly people. Same goes for the prohibition laid down in Article 5(1)b, the scope of which will largely depend on the interpretation of "age", "disability" and "social or economic situation" since the list of protected grounds appears exhaustive.</p><p>It is equally difficult to argue that companion chatbots qualify, per se, as "high-risk" AI systems within the meaning of Article 6(2), as they do not seem to fall within any of the eight areas of application detailed in Annex III. Granted, Article 7 empowers the European Commission to adopt delegated acts to amend Annex III. Yet, the requirement imposed by Article 7(1)a for any modification of addition to be linked to the eight areas already included in Annex III severely limits the Commission's room for manoeuvre in that regard. <ref type="bibr">64</ref> It is therefore unlikely, at least at this stage, that providers of companion chatbots will be required to comply with the obligations addressed specifically at providers and deployers of high-risk AI systems, such as that of record keeping, human oversight, accuracy, robustness and cybersecurity, and quality management. What is beyond contest, though, is that providers of companion chatbots, as "AI systems intended to directly interact with natural persons", will need to design and develop these tools "in such a way that the concerned natural persons are informed that they are interacting with an AI system", and ensure that their output is "marked in a machine-readable format and detectable as artificially generated" (Article 50(1)(2)). Whether disclaimers can effectively shield users of companion chatbots from getting emotionally attached is, as discussed in Section 3.2, far from a given, however.</p><p>Replacing the notion of "foundation models" found in earlier versions of the AI Act, Articles 53 and 54 now introduce specific obligations for providers of "general purpose AI models" ("GPAI models"), that is, models that "display significant generality and [are] capable of competently performing a wide range of distinct tasks regardless of the way [these models] are placed on the market and that can be integrated into a variety of downstream systems or applications" (Article 3(63)). These include, among others, an obligation to draw up and maintain technical documentation of the model, including its training and testing process and the results of its evaluation, and to share a sufficiently detailed summary of the content used to train it (Article 53(1)a and d). Whether the LLMs used to power companion chatbots qualify as GPAI models will largely depend on how providers of AI models themselves, and the Commission to a certain extent, interpret the different building blocks of that definition. Some of the most powerful models used as the basis for the development of companion chatbots such as LLaMA 2, GPT-J 6B and Falcon 40B could qualify as GPAI models. However, the situation might not be that obvious for smaller, lesspowerful models developed by providers of companion chatbots in-house that exhibit a lesser degree of "generality", such as Inflection-1, the LLM that powers Pi. Article 55 throws in additional obligations for providers of GPAI models that raise "systemic risks" either due to their "high-impact capabilities" (Articles 3(64); 51(1)a), following a decision of the Commission (Article 51(1)b; Annex XIII), or because their training required more than 10 25 FLOPs of computing power (Article 51(2)). Providers of such models will also be required to continuously assess and mitigate these "systemic risks", more specifically their "negative effects on public health, safety, public security, fundamental rights, or the society as a whole" (emphasis added) (Article 3(65)). As clarified in Recital 97, these requirements also apply "when these models are integrated or form part of an AI system", that is, when they are used in combination with other components such as a user interface. That is typically the case for companion chatbots. Here again, one could argue that some of the LLMs on which companion chatbots rely fall in that category. The different layers of personal data processing presented in Section 5.3, for instance, illustrate the impact that the training, fine-tuning, and use of LLMs can have on individuals' fundamental rights to privacy and data protection. The issues outlined in Section 3 are but additional examples of risks raised by companion chatbots for other fundamental rights, such as nondiscrimination, cultural, religious, and linguistic diversity, and the rights of the child.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.2.The many shades of "development"</head><p>Second, the qualification of the actors involved in the development and marketing of companion chatbots is also challenging. The AI Act makes a fundamental distinction between-primarily-"providers" and "deployers" of AI systems. Article 3(3) defines the former as "a natural or legal person, public authority, agency or other body that develops an AI system or a general-purpose AI model or that has an AI system or a general-purpose AI model developed and places it on the market or puts the AI system into service under its own name or trademark, whether for payment or free of charge", while Article 3(4) defines the latter as "a natural or legal person, public authority, agency or other body using an AI system under its authority" (emphasis added). The qualification of the actor, in turn, significantly impacts the extent of their responsibilities.</p><p>When it comes to providers of companion chatbots, one must therefore answer the following questions: first, whether these entities qualify as providers or deployers of AI system and, second, whether they qualify as providers of a GPAI model. As discussed in Section 2.2, the answer to these two questions largely depends on the companion chatbot provider's degree of vertical integration. Some, like Character, the entity behind Character.ai, oversee the entire development process from training to deployment, and therefore likely qualify as providers of AI systems in their own right. Should the underlying language model meet the criteria of Articles 3(63) (and 51(1)), that entity would also be regarded as a provider of a GPAI model (with systemic risk). Others, like Chai Research Corp., the company that brought us Chai, only fine-tune existing LLMs and develop the interface through which users can access companion chatbots. To what extent does fine-tuning an existing model and providing a user interface amount to "developing" an AI system within the meaning of Article 3(3), or should merely be regarded as "using" an AI system developed by another entity "under its authority" as per Article 3(4), depends, here again, on how regulators and the Commission will interpret these definitions. Whether fine-tuning a language model that qualifies as a GPAI model de facto implies the qualification of the downstream developer as a "provider" of GPAI model itself is equally unclear.<ref type="foot" target="#foot_12">65</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.3.The complex enforcement structure</head><p>Third, and if the GDPR is any indication of how things might go, the enforcement of the AI Act will take time to reach cruising speed. It took national supervisory authorities years to get the ball rolling. <ref type="bibr" target="#b32">66</ref> The road to an effective remedy is still littered with hurdles data subjects are likely to face when lodging a complaint. <ref type="bibr" target="#b33">67</ref> Inconsistencies between Member States' practices and a deficient one-stop-shop mechanism have crippled an otherwise ambitious proposal, <ref type="bibr" target="#b34">68</ref> and pushed the Commission to propose a new Regulation designed to harmonise the procedural aspects relating to the enforcement of the GDPR. <ref type="bibr" target="#b35">69</ref> Yet, the governance system introduced by the AI Act is even more complex. The Commission will need to adopt delegated acts in many areas. The AI Office will be responsible for overseeing compliance with the rules applicable to providers of GPAI models and fostering the development of codes of practice. The European Artificial Intelligence Board will have to ensure the consistent interpretation and application of the Act through recommendations. The Advisory Forum will be involved in the drafting of standardisation requests and common specifications. European Standardisation Organisations will be tasked to develop these harmonised standards. The Scientific Panel of Independent Experts will play a pivotal role in enforcing the rules applicable to providers of GPAI models, more specifically by flagging the existence of "systemic risks". Notifying and market surveillance authorities will be trusted with the surveillance, investigation, and enforcement of the AI Act at the national level. Not to mention that it delegates certain tasks to AI providers themselves, including self-assessing whether their systems truly raise a significant risk for health, safety or fundamental rights even when the said system is included in the list of Annex III, as well as their conformity with the corresponding requirements from the AI Act. <ref type="bibr">70</ref> That is but a glimpse of a convoluted machinery the many pieces of which will require some time to fulfil their intended purpose. Long story short, it might take a while before the AI Act bears tangible fruits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Data protection by design to the rescue</head><p>All the fuss about the AI Act would nearly make one forget that companion chatbots do not operate in a legal vacuum. Rather they fall within the scope of many existing regulatory frameworks, including data protection law, consumer protection law and competition law. Since the proper functioning of companion chatbots involves the processing of personal data at nearly every phase of their training, fine-tuning and use, the question then becomes, if focusing solely on the issues detailed in Section 3, whether the GDPR already provides appropriate tools to address the risks of biases, discrimination, emotional dependency, manipulation and early exposure to sexually explicit content. To answer that question, this section sheds light on the material scope of Articles 24(1) and 25(1) (Section 5.1), highlights their role as proxies to Fundamental Rights Impact Assessments (Section 5.2), and peels off the many layers of personal data processing involved in the training, fine-tuning and offering of companion chatbots to scrutinise them through the lens of data protection by design (Section 5.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.1.Data protection by design, fountain of youth</head><p>The material scope of Articles 24(1) and 25(1) GDPR-which, as argued in earlier work, 71 should be read together-is broader than what their wording suggests. Article 24(1) requires controllers to "implement appropriate technical and organisational measures to ensure and to be able to demonstrate that processing is performed in accordance with this Regulation", while Article 25(1) states that they shall "integrate the necessary safeguards into the processing in order to meet the requirements of this Regulation and protect the rights of data subjects" (emphasis added). At first sight, the reference to the requirements of "this Regulation" in both provisions seems to favour a restrictive reading of their material scope of application. Yet, that conclusion only holds true if what the GDPR "requires" is, in fact, limited to complying with the finite set of principles and rules it contains. Article 1(2), which sets the objective of the GDPR, recalls that the Regulation aims to "protect [the] fundamental rights and freedoms of natural persons and in particular their right to the protection of personal data". Recital 4 adds that the Regulation "observes the freedoms and principles recognised in the Charter as enshrined in the Treaties, in particular the respect for private and family life, home and communications, the protection of personal data, freedom of thought, conscience and religion, freedom of expression and information, freedom to conduct a business, the right to an effective remedy and to a fair trial, and cultural, religious and linguistic diversity". What the GDPR "requires", then, is that controllers mitigate all the risks to data subject's fundamental rights arising from the processing of their personal data. This ties back to the nature of the GDPR as a legislative instrument, i.e., a piece of secondary EU law that operationalises that overarching goal by laying down rules to protect all natural persons' fundamental rights, including but not limited to privacy and data protection, in the context of the processing of their personal data. This suggests that "data protection" can either refer to the set of implementing rules contained in Directives and Regulations, or to its fundamental right component. While the recognition of data protection as an independent fundamental right in Article 8 CFREU has led some authors to question <ref type="bibr">70</ref> See, for a more detailed analysis of the enforcement architecture of the AI Act: Nathalie A Smuha and Karen Yeung, 'The European Union's AI Act: Beyond Motherhood and Apple Pie?' https://papers.ssrn.com/abstract=4874852. <ref type="bibr">71</ref> For an in-depth overview of the history and material scope of data protection by design, I refer the reader to the author's earlier work on the topic, and more specifically to: Pierre Dewitte, 'A Brief History of Data Protection by Design: From Multilateral Security to Article 25(1) GDPR' [2023] Technology and Regulation 80 https://techreg.org/article/view/13807; Pierre Dewitte, 'Fifty Shades of Impact Assessment: An analysis of data protection by design in the case law of national supervisory authorities', [2024] Technology and Regulation (forthcoming). its exact added value, <ref type="bibr" target="#b36">72</ref> the EU legislator considered it sufficiently important to warrant a dedicated mention in Article 16 the Treaty on the Functioning of the European Union. Bottom line being, the GDPR, and therefore the "appropriate technical and organisational measures" that controllers must implement pursuant to Articles 24(1) and 25(1), should not only strive to protect data subject's fundamental right to data protection-whatever it adds to the EU fundamental right ecosystem-but, more importantly, also guarantee the respect for other fundamental rights such as privacy, freedom of thought, freedom of expression, non-discrimination or cultural, religious and linguistic diversity.</p><p>The EDPB has positioned itself in favour of that broad interpretation when it stated that "the data protection principles are in Article 5 (henceforth 'the principles') [and] the data subjects' rights and freedoms are the fundamental rights and freedoms of natural persons, and in particular their right to the protection of personal data, whose protection is named in Article 1(2) as the objective of the GDPR (henceforth 'the rights')". <ref type="bibr" target="#b39">73</ref> And so has the EDPS, when it underlined that "the assets to protect are the individuals whose data are processed and in particular their fundamental rights and freedoms" (emphasis added). <ref type="bibr" target="#b40">74</ref> In that sense, putting the "risk" in "risk-based approach" will always require a form of risk management process and, as such, will inevitably lead to the implementation of measures that have not been explicitly foreseen in the GDPR. <ref type="bibr" target="#b41">75</ref> Controllers are in the driving seat when it comes to the risk identification and mitigation process, supported by guidance in the form of soft law instruments, and inspired by concrete examples emanating from administrative and judicial case law. As such, data protection by design is also a Swiss knife for national supervisory authorities to gradually shape and orient what is expected from controllers in a wide diversity of scenarios. That flexibility is the essence of the risk-based approach, and what makes the combined reading of Articles 24(1) and 25(1) the "keeper of relevance" of the GDPR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.2.A proxy to Fundamental Rights Impact Assessments</head><p>The discussion as to the material scope of data protection by design sets the scene for the distinction between the many forms of "by design" obligations and corresponding "impact assessments". If "privacy by design" and "data protection by design" have been used interchangeably during the reform process, 76 the above paragraphs have shed light on the importance to clarify the object of the assessment, i.e., what fundamental rights does the processing operations at stake impact, and the purpose of the countermeasures to be implemented by controllers pursuant to Articles 24(1) and 25(1) GDPR, i.e., how to appropriately mitigate that impact. This suggests the existence of different forms of "risk assessment" that vary in scope and complexity (see Figure <ref type="figure">3</ref>). <ref type="bibr">77</ref> The broadest would be a Fundamental Right Impact Assessment ("FRIA"), itself the sum of multiple assessments focusing on the impact of the processing of one's personal data on a specific fundamental right. This is in line with the conclusions drawn by Karen Yeung and Lee Bygrave in their cross-disciplinary analysis of the Regulation's architecture, in which they argue that "the risk-based approach necessitates that the data controller undertake a contextual 'fundamental rights risk assessment' in order to identify the appropriate level of stringency of the technical and organizational measures that must be adopted to guard against those risks from materializing". <ref type="bibr">78</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 3: FRIAs, [X]IAs and GDPRIAs</head><p>While privacy ("PIA") and data protection ("DPIA") are the usual suspects, the GDPR strives to protect, as discussed above, all data subject's fundamental rights including, for instance, freedom of expression ("FoEIA"), non-discrimination ("NDIA"), the right to conduct a business ("RCBIA") or the right to an effective remedy a fair trial ("RERIA"). Or, literally, any other fundamental right ("[X]IA"). Building on the role of the GDPR as a "proxy" to mitigate the most pressing risks associated to the processing of personal data for these fundamental rights, 79 performing a GDPR Impact Assessment ("GDPRIA")-that is, <ref type="bibr">76</ref> With the European Commission, 'Communication from the Commission to the European Parliament, the Council, the Economic and Social Committee and the Committee of the Regions -A Comprehensive Approach on Personal Data Protection in the European Union' https://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=COM:2010:0609:FIN:EN:PDF, for instance, referring to the term "privacy by design" as early as 2010. The European Commission, 'Proposal for a Regulation of the European Parliament and of the Council on the Protection of Individuals with Regard to the Processing of Personal Data and on the Free Movement of Such Data (General Data Protection Regulation), COM/2012/011 Final -2012/0011 (COD)' https://eurlex.europa.eu/legal-content/EN/TXT/?uri=COM%3A2012%3A0011%3AFIN later settled on the notion of "data protection by design" when drafting Article 23 -which became Article 25 in the final version of the Regulation. <ref type="bibr">77</ref> For a longer take on that argument, I invite the reader to consult the author's earlier work: Pierre Dewitte, 'The Many Shades of Impact Assessments -An analysis of data protection by design in the case law of national supervisory authorities' (n 71). <ref type="bibr">78</ref> Karen Yeung and Lee A Bygrave, 'Demystifying the Modernized European Data Protection Regime: Cross-Disciplinary Insights from Legal and Regulatory Governance Scholarship' (2022) 16 Regulation &amp; Governance 137, 146-147 https://onlinelibrary.wiley.com/doi/abs/10.1111/rego.12401. <ref type="bibr">79</ref> The EDPS pitched the same idea in European Data Protection Supervisor (n 74) para 61, if with a slightly different meaning, when it stated that "the GDPR looks at [the general principles of Article 5] as goals to achieve, used as 'proxies' to protect individuals' fundamental rights and freedoms, independently of the level of risk". assessing the degree of compliance of a set of processing operations with the principles and rules it contains, and remedying any deficiency-would lay the groundwork for such a FRIA. While both exercises overlap, the former does not exhaust the latter as controllers will need to complement their compliance efforts depending on the risks inherent to their specific activities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.3.Personal data, personal data everywhere</head><p>Companion chatbots make for a particularly interesting use case. As noted above, the recipe used to cook these products makes use of personal data at nearly every step of the process, from training to fine-tuning to deployment. This, in turn, drags the controllers involved in that algorithmic supply chain into GDPR territory, and requires them to comply with Articles 24(1) and 25(1). As hinted above, these provisions do not explicitly prohibit companion chatbot providers from discriminating individuals. Neither do they oblige them to prevent their bots from manipulating their users. Rather, they require controllers, as detailed above, to protect all data subject's fundamental rights, including but not limited to privacy and data protection, in the context of the processing of their personal data. In other words, identifying a processing of personal data that threatens any of the fundamental rights and freedoms enjoyed by data subjects will trigger the obligation for the controllers to mitigate the risk of that threat actually materialising. Overall, and building on the broad interpretation of the material scope of data protection by design put forward in Sections 5.1 and 5.2, I argue that regulators are already well-equipped to deal with the issues outlined in Section 3, mainly-if not exclusively-in the form of accountability, data protection by design, and the obligation to conduct DPIAs. <ref type="bibr">80</ref> Table <ref type="table">1</ref> provides a high-level, fictional example of the different layers of personal data processing that the development of companion chatbots typically involves. For each layer, the corresponding (joint) controller(s) will have to identify and mitigate the risks these processing operations raise for data subject's fundamental rights, as required by Articles 24(1) and 25(1) GDPR. Layer #0 refers to the personal data originally collected by the entity from which the provider of the training dataset will source its data. Layer #1 covers the scraping of that personal data for the purpose of assembling the training dataset. Layers #2 and #3 capture, respectively, the use of the said training dataset to either train general-purpose LLMs ("GP-LLMs") or fine-tune the behaviour of existing models in specific scenarios ("S-LLMs"). Lastly, layer #4 represents the collection, by the entities responsible for providing the interface through which end-users can access companion chatbots, of the personal data necessary for service purposes.</p><p>Having deconstructed the different processing of personal data involved in the development of companion chatbots, the following paragraphs delve into the types of mitigation strategies that controllers could be required, pursuant to Articles 24(1) and 25(1), to implement at each step of the process to address the risks these products create for their users, and society as a whole. <ref type="bibr">81</ref> Two disclaimers, though. First, the selection of countermeasures discussed below is limited to those that address the specific risks outlined in Section 3, namely biases and discrimination, emotional dependency, and early exposure to sexually explicit content. The very point of this paper is, indeed, to illustrate how data protection by design can serve as a proxy to oblige controllers to go beyond the letter of the Regulation, and not to iterate over the many infringements of the GDPR one can criticise providers of companion chatbots for. Second, the selection of 80 Nathalie Smuha makes a similar point, when she notes that "the AI Act should not be considered as a lex specialis that deviates from data protection rules, but rather as a supplement to fill in the legal gaps that the GDPR, the LED, the EUDPR and other pieces of EU legislation did not yet satisfactorily cover". See: Nathalie Smuha, 'The Paramountcy of Data Protection Law in the Age of AI (Acts)', Two decades of personal data protection. What's next? EDPS 20th Anniversary (Publications Office of the European Union 2024) 232 https://lirias.kuleuven.be/retrieve/765234. <ref type="bibr">81</ref> It is worth noting that, while the present section starts from the postulate that the risks detailed in Section 3 result from the processing of personal data, discrimination, manipulation and harms to mental health may also arise from the processing of nonpersonal data. See, on that: Przemysław Pałka, 'Harmed While Anonymous: Beyond the Personal/Non-Personal Distinction in Data Governance' (2023) 2023 Technology and Regulation 22 https://techreg.org/article/view/13829, specifically Section 3.2. countermeasures is nowhere near exhaustive. Exactly as the technical overview provided in Section 2, it is rather instrumental in making a point as to the "enabling" role of Articles 24(1) and 25 Since the datasets used to train and fine-tune LLMs are usually comprised of publicly available data scraped from the internet, they might contain personal data, especially when the data sources include any form of user-generated content. The "Reddit Conversation Corpus" dataset scraped by Nouha Dziri and her co-authors, for instance, might very-well contain snippets of texts that could lead to the reidentification of the Redditors who originally posted them. <ref type="bibr" target="#b42">82</ref> The same conclusion holds for "The Pile", the training dataset compiled by EleutherAI that contains, 83 among many other data sources, the "Enron Corpus", which itself holds more than 500.000 emails exchanged by former employees of the company originally made public in the context of an investigation by the Federal Energy Regulatory Commission. <ref type="bibr" target="#b43">84</ref> The case of Clearview AI is particularly telling. Back in 2020, the company started to scrape the internet, including social media platforms, to gather images and videos to train its facial recognition software and offer its clients-among which law enforcement authorities-a search engine designed to look up individuals on the basis of another picture. <ref type="bibr" target="#b44">85</ref> OpenAI also scraped vast amount of text data from the internet, including personal data, to train the different iterations of its GPTs, which led the Garante to question the lawfulness of that processing in a series of highly-publicised decisions back in March and April 2023. <ref type="bibr" target="#b45">86</ref> This is even more problematic considering that adversaries might be able to extract, with relatively simple queries, part of the dataset used to train these LLMs. <ref type="bibr">87</ref> Provided that such training dataset contained personal data, this would allow attackers to leverage LLMs' propensity to "memorise" and "regurgitate" random aspects of their training datasets to recover portions of that information. <ref type="bibr">88</ref> On 29 January 2024, the Italian regulator went even further and notified OpenAI that it had found several breaches of the Regulation, 89 but would take into account the output of the-at the time of writing-ongoing EDPB task force on the matter. <ref type="bibr">90</ref> As a side note, it is also unclear whether scraping publicly accessible personal data should be regarded as a further processing activity subject to the compatibility assessment pursuant to Articles 6(1)b and 6(4) GDPR, or as a new collection for which the said entity would automatically need to rely on a different lawful ground than the one used to legitimise the original collection. The recent guidance issued by the Dutch regulator on the matter seems to favour the latter approach, as it states that "[t]he possibility of compatible use is in principle limited to further processing of personal data by the controller itself within its own business operations" (emphasis added, free translation). <ref type="bibr">91</ref> In other words, the implication of another entity than the one responsible for the collection would prevent that entity from even invoking the non-incompatibility of its activities. Yet, severing the link between the purpose specified for the collection and that of the subsequent usage of that data on the sole ground that another entity enters the data processing chain seems, prima facie, difficult to reconcile with the rationale of purpose limitation, i.e., inhibiting mission creep and fostering predictability by "prevent[ing] the use of individuals' personal data in a way (or for further purposes) that they might find unexpected, inappropriate or otherwise objectionable". <ref type="bibr">92</ref> In practice though, the impact of that distinction might be limited, as the elements that would factor in the non-incompatibility test pursuant to Article 6(4) are likely to be integrated within the three-step test of Article 6(1)f-more specifically its "balancing" component-, which is the only lawful ground controllers can reasonably rely on to legitimise the scraping of publicly accessible personal data. <ref type="bibr">93</ref> Regardless, one could consider scraping as one of the earliest-if far from the only-causes of the risks raised by companion chatbots at the very end of the supply chain, since it fuels the training and fine-tuning of the LLMs these products all rely on. Translated into data protection terms, this means that the initial collection of personal by the controllers responsible for these online resources (layer #0) already raises the risk of third party scraping (layer #1), and therefore plays a role, even if limited, in the training and finetuning of language models (layers #2 and #3) and the risks they raise for end-users as detailed in Section 3 (layer #4). As such, one could argue that data protection by design would require these controllers to implement technical and organisational measures to prevent scraping from even happening in the first place, or at least limit its scope, where they are aware that third parties tap into their databases for the purpose of assembling training datasets.</p><p>That reasoning, however, calls for two comments. First, the causal relationship between the collection of personal data by, say, Facebook, and the risk of manipulation raised by a companion chatbot fine-tuned using a training dataset comprised of posts scraped from that platform, is rather thin. While the processing in layer #0 might influence the said risk by making data publicly accessible, it is far from the only-or even main-factor that determines the likelihood of that risk materialising, or its severity for the affected individuals. As such, one cannot reasonably require controllers to anticipate all the potential risks that their processing operations could contribute to raise, especially where their role in shaping the said risks is limited. The determining factor to gauge the extent of controllers' responsibilities under Article 24(1) and 25(1) GDPR, I argue, is a combination of awareness of and agency over the risks at stake. The following paragraphs delve deeper into those aspects.</p><p>Second, it is worth noting that, besides the risks identified in Section 3, scraping raises other concerns for individuals, including the risk of targeted cyberattacks, identify fraud, hyper-personalised political targeting, spamming and overall loss of control over one's personal data. Now the role that social media platforms play in the materialisation of these risks, compared to those associated to the use of companion chatbots trained or fine-tuned using personal data scraped from these platforms, is more prevalent since these risks are the direct consequences of the public availability of that data. The limitations inherent to the reasoning deployed above are therefore less relevant, and one could more convincingly leverage data protection by design to oblige social media platforms to implement technical and organisational measures to protect personal data from unlawful scraping. A joint statement from the International Enforcement Cooperation Working Group argues in that direction, and recommends "social media companies" to implement "multi-layered technical and procedural controls" such as "rate limiting", "monitoring how quickly and aggressively new accounts starts looking for other users", "taking step to detect scrapers by identifying patterns in 'bot' activity" and "using CAPTCHAs". <ref type="bibr">94</ref> Illustrating the overlap between GDPRIAs and FRIAs introduced in Section 5.2, one can argue that the obligation for social media platforms to go to such lengths follows from both the specific provisions on security and data breaches (Articles 5(1)f, 32-34 GDPR) and the broader obligation to identify and mitigate all the risks raised by the processing of personal data for data subject's fundamental rights and freedoms, including but not limited to privacy and data protection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2.">Layer #1. Assemble the finest training datasets</head><p>As a direct result of the above, the datasets used to train or fine-tune the LLMs that companion chatbots all rely on might also contain personal data. If that is the case and if the assembler purposefully crafts the said dataset for others to train their LLMs on, one could argue that Articles 24(1) and 25(1) GDPR would require that entity to proactively identify and mitigate the issues that could lead the resulting models to, for instance, discriminate against certain communities, or behave in a manipulative way. The "appropriate technical and organisational measures" that the assembler could be required to implement would include, a minima, the provision of a detailed description of the content, structure and biases of the training dataset. <ref type="bibr">95</ref> One might argue that sanitisation techniques could be used to filter the personal data out of the training dataset. These are, however, no silver bullet since (i) the sanitisation process itself would amount to a "processing" of "personal data", therefore triggering the applicability of the GDPR for that limited portion of the processing, and (ii) sanitisation techniques work best for identifying well-formatted information, such as social security numbers, while determining what constitutes "personal data" within the meaning of Article 4(1) GDPR is a context-sensitive rather than formalistic exercise. <ref type="bibr">96</ref> Measures could also include debiasing techniques to remedy glaring flaws in the dataset, though some of the risks outlined in Section 3 can also be attributed to the broader social, political and economic context in which the resulting models are deployed. <ref type="bibr" target="#b46">97</ref> One could even leverage data protection by design to argue that the assembler should refrain from creating datasets the sole purpose of which is to train language models designed to discriminate or manipulate people, or violate their fundamental rights and freedoms in any other way. That, however, might prove particularly tricky to demonstrate.</p><p>Of course, that obligation comes on top of all the other requirements stemming from the Regulation such as, when it comes to the scraping of publicly accessible personal data, the obligation to rely on one of the six lawful grounds listed in Article 6(1) GDPR. <ref type="bibr" target="#b47">98</ref> On that point, it is worth noting that the CNIL recently held a somewhat debatable position regarding the lawfulness of the reuse of training datasets assembled by third parties. In its fourth "AI how-to sheet", the French regulator indeed states that "[c]ertain failures committed by the controller to set up and disseminate a dataset do not systematically and irreparably affect the lawfulness of the processing carried out by the re-user". "Thus", it adds, "a re-user may use a dataset whose illegalities are minor, provided that the reuse meets the requirements of the GDPR". As such, argues the CNIL, the entities reusing existing training datasets can limit their assessment to checking whether "there is no clear doubt that the dataset is lawful (in particular that the source processing is not manifestly lacking of a legal basis when the data are so intrusive that they cannot be processed without the consent of the individuals), ensuring in particular that the conditions for collecting the data are sufficiently documented" (emphasis added). <ref type="bibr" target="#b48">99</ref> What the CNIL understands by "minor illegality" and "no clear doubt" is, unfortunately, not detailed in the guidelines. <ref type="bibr" target="#b49">100</ref> This, in my opinion, is a rather slippery slope as it paves the way for controllers to invoke vague excuses to not pay too much attention to the provenance and lawfulness of the datasets they select and use to train or refine their own models.</p><p>Yet, sanitisation and debiasing suffer from several limitations. The first is related to the objective of data protection by design, which is to mitigate the risks raised by the processing at stake for data subject's fundamental rights and freedoms. Meaning, in the context of the scraping of personal data to assemble training datasets, the risks for the individuals whose personal data are included in the said datasets. These risks might be very different from the risks raised by the processing of personal data of other individuals by the other actors involved in the companion chatbots supply chain that might have used that dataset to train their own models, or fine-tuned a pre-trained model that has itself relied on that dataset. This is particularly problematic in cases where the absence of a certain community from a given training dataset is the very source of the discrimination risk. The broader the scraping, the more "data subjects" are likely to be affected, though. Maybe up to a point where the sheer amount and diversity of the individuals included in the training dataset warrant to move past the notion of "data subject" and instead consider the risks to society as a whole. That interpretation finds support in the wording of Articles 24(1) and 25(1) GDPR, if one pays closer attention to the exact vocabulary used by the EU legislator. Karen Yeung and Lee Bygrave also seem to vouch for that interpretation. 101 Indeed, while the measures that controllers must implement should strive to protect "the rights of data subjects", the "risks" that controllers must "take into account" when selecting which of these measures are appropriate in a given scenario are those for "the rights and freedoms of natural persons". The latter is, without contest, broader than the former.</p><p>The second has to do with the limited influence of the assembler on the potentially harmful processing happening further down the chain. The mere fact that a training dataset can be used to train a language model that will end up discriminating or manipulating its users does not trigger the obligation for its assembler to anticipate and prevent all the scenarios in which these risks might materialise themselves in practice. That reasoning would only hold water if, as hinted at above, the sole purpose of the training dataset is to train harmful LLMs. PygmalionAI's PIPPA dataset, which contains more than 1 million lines of dialogue between users of Character.ai and its large language model and is mainly used to train LLMs able to spit out explicit content, 102 is a case in point. Can that training dataset be used by companies such as Chai Research Corp. to fine-tune over-sexualised chatbots made accessible to underage users without any age verification mechanism? 103 Sure. Does that mean that PygmalionAI has to mitigate that specific risk? Probably not, as chatbot-powered ERP is not, in itself, problematic. How it is implemented by providers of LLMs, downstream developers and, most importantly, providers of companion chatbots, and what safeguards these actors put in place to frame its usage, are the critical questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3.">Layer #2. Cook general-purpose LLMs</head><p>Going one step down the companion chatbots supply chain, the exact same reasoning, including its limitations, can be held against the entities that use these datasets to train their own LLM. Building on the argumentation detailed above, if the training dataset contains personal data and if the provider of the LLM determines the purposes and the means of the training process, then Articles 24(1) and 25(1) GDPR will require the implementation of "appropriate technical and organisational measures" to mitigate the risks it <ref type="bibr">101</ref> The authors note that "controllers who must carry out these assessments may be ill-equipped to do so for at least four partially overlapping reasons", the second of which is the fact that they "are required to consider fundamental rights of individuals generally, rather than merely the rights of data subjects directly implicated by the proposed processing" (emphasis added). Yeung and Bygrave (n 78) 147. <ref type="bibr">102</ref> On HuggingFace, PygmalionAI warns that "PIPPA contains conversations, themes and scenarios which can be considered 'not safe for work' (NSFW) and/or heavily disturbing in nature" and that "models trained purely with PIPPA may have the tendency to generate X-rated output". See the dataset card available at https://huggingface.co/datasets/PygmalionAI/PIPPA. 103 This is one of the issues that pushed the Garante to temporarily ban ChatGPT on the Italian territory. The risk of early exposure to sexually explicit content was also at the core of the Garante's earlier decision to ban Replika on the Italian territory, as bots on the platform were serving "utterly inappropriate replies" to children "having regard to their degree of development and self-conscience". See, on these two decisions, (n 86).</p><p>poses for data subject's rights and freedoms. Taking into account, here again, the broader risks for "the rights and freedoms of natural persons". Although this time one might argue that, since the entity that trains the language model actually knows what it will be used for-i.e., conversing with people in the case of an off-the-shelf model, or serving as baseline for the fine-tuning of a model that will eventually be used to do so-, these measures should account for the specific risks raised by the use of such conversational agents. With greater awareness of the purpose for which the model will be used, comes a better understanding of the risks it is likely to pose for end-users. The more conscious the controller, the heavier the burden of implementing "appropriate" countermeasures. These could include, for instance, proper testing to ensure that the model does not output toxic speech, racist slur or manipulative patterns inherited from the training dataset(s), and the implementation of fixes to limit that behaviour as much as possible.</p><p>Exactly as for the providers of training datasets, however, it would be unreasonable to ask from providers of LLMs that they anticipate all the scenarios in which their models might endanger data subject's fundamental rights. But the interpretation of data protection by design defended in Sections 5.1 and 5.2 would certainly bar providers of LLMs from releasing intrinsically harmful models without any form of mitigation for the risks their usage might pose for end-users. Data protection law set aside, this also follows from a "moral" obligation not to unleash damaging products in the wild. Illustrating the above, the developers of "DialoGPT", a tuneable neural conversational response generation model trained by researchers at Microsoft, have acknowledged that their model "retains the potential to generate output that may trigger offense", to "reflect gender and other historical biases", to "express agreement with propositions that are unethical, biased or offensive", or to "disagree with otherwise ethical statements". <ref type="bibr" target="#b50">104</ref> But they spontaneously tried to clean the training dataset from overtly offensive data before moving on with the training process, without referencing any of the potentially applicable regulatory frameworks.</p><p>The risks raised by these models encouraged "marketplaces" such as GitHub, HuggingFace and Civitai to progressively roll-out content policies designed to ban users from uploading certain types of models, either based on their intended, actual or potential harmful uses. Yet, as rightfully noted by Robert Gorwa and Michael Veale in a recent case study focusing on these three platforms, the peculiar nature of models as "content containing other content", and the resulting uncertainties regarding the applicability of the EU intermediary liability framework, makes moderating models a thorny policy challenge. <ref type="bibr" target="#b51">105</ref> These content policies come on top of the various flavours of licences concluded between model uploaders and users, which also often limit the use that the latter can make of these models. <ref type="bibr" target="#b53">106</ref> In both cases, however, enforcement of these contractual clauses remain the exclusive prerogative of platforms and licensors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.4.">Layer #3. Spice things up with a hint of fine-tuning</head><p>Moving even closer to end-users, downstream developers can also fine-tune existing pre-trained language models to behave in a certain way. For instance, next to the PIPPA dataset, PygmalionAI also offers Pygmalion 6B, 107 a proof-of-concept dialogue model based on EleutherAI's GPT-J 6B primarily designed to act as a NSFW role-play partner. On the dedicated HuggingFace page, PygmalionAI states that the dataset used to fine-tune GPT-J 6B "consisted of 56MB of dialogue data gathered from multiple sources, which includes both real and partially machine-generated conversations". Another such example includes the "uncensored" version of LLaMA 2 13B, specifically retrained with a filtered dataset to "reduce refusals, avoidance, and bias". <ref type="bibr">108</ref> Here again: if that dataset contains personal data, and if PygmalionAI qualifies as the controller for the fine-tuning process, it will have to comply with Articles 24(1) and 25(1) GDPR and assess the risks that the processing poses for data subject's fundamental rights and freedoms, and potentially for all natural persons if one sticks to the broader interpretation of these provisions. In this case, PygmalionAI unequivocally warns that their model "is not suitable for use by minors" and "will output X-rated content under certain circumstances" (emphasis in original).</p><p>As detailed above, the fact that the company has specifically developed Pygmalion 6B for ERP and explicitly acknowledges the risks that this might pose if the model is used to engage with minors drastically raises the bar, in my opinion, with regard to the countermeasures that PygmalionAI should implement to prevent these risks from materialising. These should at least include mechanisms to ask users their age, and appropriately tone down or even deactivate the generation of NSFW language. Or to detect certain prompts that might reveal that the individual interacting with the fine-tuned model is, in fact, underage. <ref type="bibr">109</ref> There is only so much downstream developers can do, that is. Self-declarations of one's age or date of birth has never proven a particularly efficient technique to prevent children from accessing sexually explicit content online, and the same observation can probably be made when it comes to chatbot-prompted equivalents. Besides, Pygmalion 6B, like many other fine-tuned models, is not gated behind an API but is available for anyone to download under a GNU General Public License. <ref type="bibr">110</ref> As a result, PygmalionAI does not retain any form control over the way chatbot developers implement, finetune and market the model in practice. Knives don't kill people, right? 5.3.5. Layer #4. Serve half-baked in a shiny user interface This brings us to the most critical link of the chain: the entities that actually offer companion chatbots to end-users. As hinted at in Section 2, these actors can intervene throughout the entire supply chain, or simply propose the technical infrastructure necessary to engage with existing language models. The determining factor is the provision of the interface through which individuals can access, personalise and converse with companion chatbots, regardless of the degree of influence that the said entity had over the different building blocks outlined in Figure <ref type="figure">2</ref>. That interface reflects design and marketing choices that influence the type of audience that the companion chatbot will eventually attract.</p><p>The processing of personal data at stake here is radically different from that involved in the scenarios outlined above, as these entities collect and further process the personal data of end-users for the purpose of providing companion chatbot services, as opposed to that of the natural persons that might have been included in the datasets used for training or fine-tuning purposes. In the case of the Chai application, for instance, that includes account-related data, usage data, conversation histories and bot parameters, the qualification of which as "personal data" within the meaning of Article 4(1) GDPR makes little doubt. If the entity that develops and offers the user-facing interface acts as the controller for the above-mentioned processing-which I argue it does, either as a sole or joint controller-, it will also have to comply with the many requirements stemming from the Regulation. But this time, the risks for data subject's' fundamental rights are drastically different-and so are their likelihood and severity.</p><p>In our complaint against Chai Research Corp., the background of which is detailed in Section 6, we flagged glaring infringements of the GDPR including a breach of the lawfulness and transparency principles, and the lack of any mechanism to verify that children's consent is either given or authorised by <ref type="bibr">108</ref> The model is accessible here: https://huggingface.co/ehartford/WizardLM-1.0-Uncensored-Llama2-13b. Note that the developers clearly warn that "An uncensored model has no guardrails" and that "You are responsible for anything you do with the model, just as you are responsible for anything you do with any dangerous object such as a knife, gun, lighter, or car". <ref type="bibr">109</ref> Reference is made, for instance, to the advice that Snapchat's My AI gave someone impersonating a 13 years old girl on how to make her first time with a 31 years old man special: by "setting the mood with candles or music". See: Fowler (n 62). <ref type="bibr">110</ref> See, for the exact terms of the license: https://www.gnu.org/licenses/gpl-3.0.en.html. See, for the original Twitter post by Tristan Harris, cofounder of the Center For Humane Technology: https://twitter.com/tristanharris/status/1634299911872348160/photo/3.</p><p>the holder of parental responsibility as required by Article 8(2). But, more importantly, we noted the absence of any "technical and organisational measure" to ensure that the processing does not infringe data subject's fundamental rights and freedoms. <ref type="bibr">111</ref> This is all the more troublesome given that (i) the use of companion chatbots raises specific issues for their users, (ii) these risks are abundantly documented in field-oriented and academic literature, (iii) data subjects are the end-users, and end-users are the individuals at risk, which evacuates the question as to whether the objective of Articles 24(1) and 25(1) GDPR includes the protection of "natural persons" more generally, (iv) the target audience of companion chatbots is often comprised of vulnerable individuals, which increases the likelihood and severity of these risks and, (v) the proximity with end-users puts the provider of the user-facing interface in the driver's seat when it comes to the implementation of appropriate countermeasures.</p><p>In light of the above, I argue that a broad interpretation of Articles 24(1) and 25(1) GDPR, as defended in Sections 5.1 and 5.2, requires developers of companion chatbots to identify and mitigate all the issues raised by the services they put on the market, as long as these result from the processing of their users' personal data. Vertically-integrated developers could address the risk of discrimination by, for instance, debiasing their training dataset, <ref type="bibr" target="#b55">112</ref> moderating the output that can universally be regarded as discriminatory, <ref type="bibr" target="#b56">113</ref> or contextualising debatable statements. They could also detect early signs of emotional dependency through text classification techniques, at which point they could redirect the user to relevant resources or even human support. <ref type="bibr" target="#b57">114</ref> When it comes to preventing premature exposure to sexually explicit content, they could pair mandatory age verification mechanisms with filtering techniques to shield younger users from ERP. In a surprisingly bold business blog post, the FTC recently called upon chatbots providers to stop "misrepresenting what these services are or can do" and refrain from placing them on the market without "adequately mitigating risks of harmful output". <ref type="bibr" target="#b58">115</ref> The list goes on, and providing a comprehensive overview of these measures extends well beyond the scope of the present paper.</p><p>The main problem is, the vast majority of companion chatbots developers has implemented none-or too little-of that. Likely because doing so would run contrary to either their business model, which often relies on premium subscription plans or advertising revenues, or to the purpose for which some of these chatbots are mainly used. One can't help but question whether the objective of Romantic AI, for instance, is really to serve as a sparring partner to "train personal communication skills in romantic and love areas" or is nothing more than a paid-for NSFW text generator appealing primarily to the male fantasy. <ref type="bibr" target="#b59">116</ref> Or to discern a subtle hint of irony when the developers of Tavern AI, a platform notoriously used for ERP, answer the question "Can this technology be used for sexooo?" by "Surprisingly, our development team has received reports that some users are indeed engaging with our product in this manner. We are as puzzled by this as you are, and will be monitoring the situation in order to gain actionable insights". <ref type="bibr">117</ref> A hint of addiction might go a long way in keeping these services afloat, one might daresay. Again, the point of this section is not to condemn ERP as such, or companion chatbots altogether. Rather, it underlines the needs for developers to provide a healthy and safe environment for their deployment and use, taking into account the documented risks these tools might raise for end-users. Where companion chatbots involve the processing of personal data, data protection by design is a powerful proxy to force controllers to, at the very least, initiate a reflection as to the safeguards that should be put in place to mitigate these risks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">The Safe AI Companion Collective ("SAICC") initiative</head><p>The risks outlined in Section 3 are but a handful of the issues raised by companion chatbots. Many more are documented in practice, but are not discussed in the present paper for obvious length reasons. <ref type="bibr" target="#b60">118</ref>  Within the context of SAICC, we also filed a complaint against Chai Research Corp., the company behind the Chai platform, before the Belgian supervisory authority for several infringements of the GDPR. <ref type="bibr">120</ref> While we could have targeted other providers of companion chatbots, we felt that Chai Research Corp.'s community-based development model, which effectively shifts part of the burden to ensure AI safety and compliance onto independent developers who are rewarded for optimising user engagement, raised particularly salient issues. A machine-translated version of the complaint originally submitted in French, as well as all the evidence filed before the authority, is included as supplementary material to the present paper and is accessible here: https://doi.org/10.48804/K6RDSG. The complaint details the infringements attributable to Chai Research Corp., including breaches of the lawfulness and transparency principles, of the rules surrounding children's consent, and of the obligation to conduct a DPIA where the processing concerns "vulnerable data subjects" and the "innovative use or application of new technological or organisational solutions" possibly "with a high risk to individuals' rights and freedoms". <ref type="bibr">121</ref> More specifically, we built our argumentation around the absence of any process designed to identify and mitigate the risks raised by Chai's activities for its users' fundamental rights and freedoms, including but not limited to safety, privacy and data protection. Such exercise, the outcome of which should be communicated to data subject in part or in full, is critical to ensure that all users enjoy a safe and healthy experience when using companion chatbots. While the primary goal of our initiative is to force Chai Research Corp. to implement the necessary guardrails to mitigate the impact of their chatbots, the complaint also serves an academic agenda. That is, to demonstrate that data protection by design can serve as a powerful proxy to force controllers to proactively identify and mitigate some of the risks that are not explicitly addressed in the Regulation, but are nonetheless the result of a processing of personal data. That reasoning hinges on the broad interpretation of the material scope of Articles 24(1) and 25(1) GDPR put forward in Sections 5.1 and 5.2, and the phase-oriented approach introduced in Section 5.3, to address the risks outlined in Section 3.</p><p>This, we hope, will help put the issue of companion chatbots on regulators' agenda, and pave the way for a decision to condition the availability of these tools to a prior and comprehensive assessment of the many risks they pose for their user base. We also wish to raise awareness among individuals, policymakers, regulators and developers on the need to consider the impact of technology on people's live before rushing to the market. Not only is this "by design" approach becoming an integral part of the European legislator's response to the challenges raised by emerging technologies, but it is also instrumental in avoiding that individuals become the first victims of half-baked products and services. Lastly, we believe that our methodology might inspire other people to launch similar initiatives in other countries. This might help elevate the issue at the EU level, potentially through the possibility offered to data protection authorities to "request that any matter of general application or producing effects in more than one Member State be examined by the Board with a view to obtaining an opinion" (Article 64(2) GDPR). This is crucial, as the risks associated to the use of personalised chatbots such as those offered by Chai Research Corp. extend well beyond Belgium, and considering that similar services continuously flood the market.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>Wrapping up, the broad interpretation of data protection by design put forward in Sections 5.1 and 5.2 paves the way for leveraging the risk-based approach to oblige the different controllers that directly or indirectly contribute to shaping the likelihood and severity of a given risk to identify and proportionally mitigate that risk. Yet, doing so in the context of companion chatbots is particularly challenging since the supply chain of their development and deployment (i) is comprised of multiple layers of processing operations that all contribute, to some extent, to raising specific risks, and (ii) involves a wide range of actors, each of which is responsible for a subset of these processing activities. This makes Articles 24(1) and 25(1) GDPR compelling but challenging options to try and address the risks outlined in Section 3. Condensing the findings presented throughout this paper, the following sections highlight the challenges of applying data protection by design in complex supply chains (Section 7.1), and the role of awareness in broadening the scope of controllers' risk management exercise (Section 7.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7.1.Data protection by design in complex supply chains</head><p>For data protection by design to unlock its full potential, two cumulative conditions must be met. First, and since Articles 24(1) and 25(1) GDPR only apply to "controllers", it requires a precise understanding of the role of each actor involved in the companion chatbots supply chain. This is essential to identify who "determines the purposes and means" of the processing at stake, and allocate responsibilities for the implementation of "appropriate technical and organisational measures". As detailed in Section 2.2, companion chatbots are rarely the product of a single entity. Translated into data protection terms, this means that there might be multiple (joint) controllers involved at various stages of the production, deployment and use of companion chatbots for different, yet interdependent sets of personal data processing operations. That concerns extends to AI systems in general. <ref type="bibr">122</ref> In the words of Jennifer Cobbe and her co-authors, it is indeed "no longer necessarily true that computer systems are produced by a group of developers or an organisation, or by a vendor simply integrating various standalone components into one product". Instead, computer systems "now often involve a group of organisations arranged together in a data-driven supply chain, each retaining control over component systems they provide as services to others" (emphasis in original). <ref type="bibr">123</ref> Companion chatbots are prime examples of such data-driven supply chain products. Against that background, bringing the issue of data protection by design to the table also sparks an essential reflection as to the broader ecosystem in which each individual actor operates, which is the first step toward the implementation of appropriate countermeasures, as discussed below. Doing so might partially alleviate the issue documented by David Gray Wider and Dawn Nafus in their interviews with 27 AI engineers across different industries, i.e., that few of them felt like the risks such as the ones discussed in Section 3 fell within their agency, capability, or responsibility to address. <ref type="bibr">124</ref> Besides, "controllership" within the meaning of Article 4(7) GDPR does not always fall upon the entity that factually controls the technology at stake. <ref type="bibr">125</ref> Second, and for the "technical and organisational measures" to fulfil their goal of protecting "data subject's fundamental rights and freedoms", all the controllers involved in the AI supply chain should ideally be aware of the purpose and context in which the final product-in this case, companion chatbotswill be deployed. That, of course, gets more difficult the more actors are involved in the chain. As hinted 122 This is often referred to as the "many hands" problem, already pitched by Helen Nissenbaum back in the nineties.  <ref type="bibr">125</ref> Cobbe, Veale and Singh (n 123) 7-8, also highlight that disconnect. While they state that "those who are factually responsible for various aspects of production, distribution, and use of algorithmic systems must be identified correctly so that accountability can be allocated accordingly", they also acknowledge that the "assignment of legal roles and responsibilities does not describe the real interdependencies and power relations between AI service providers […] and their customers".</p><p>at above, assemblers of training datasets, for instance, are simply not in a position to anticipate all the scenarios in which their datasets might be used, and the corresponding risks that these training activities might pose for data subjects. Similarly, developers or general-purpose LLMs such as the ones shared on HuggingFace might not always be aware that downstream entities use them as basis to fine-tune language models to generate sexually explicit content, let alone be factually able to address the issues raised by this type of chatbots. Jennifer Cobbe et al. refer to the "accountability horizon", i.e., "the point beyond which an actor cannot 'see', which depends on the actor and the chain", and acknowledge the problem that is poses for legislative initiatives that rely on "impact assessment, risk assessment and risk management mechanisms to mitigate the harms of AI technologies" such as the GDPR. The proper identification and mitigation of these risks, further note the authors, therefore "require knowledge of both the AI technology's specification and development and the purpose and context of its application". 126</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7.2.Broadening the "accountability horizon"</head><p>Once aware, each actor must then implement technical and organisational measures to mitigate the risksor the portions thereof-that can be attributed to the processing of personal data for which they qualify as controller. As detailed in Section 5.3, multiple controllers, through their own successive or concurrent processing activities, can contribute to shaping the characteristics of a specific risk. This could be the case, for instance, for the risk that an LLM trained and fine-tuned by different entities outputs biased or discriminating content when implemented within a user-facing companion chatbot. In that case, all the actors that have been involved in the training of the underlying model would, I argue, be compelled to implement appropriate countermeasures in proportion to their respective contribution to the risk. On the contrary, controllers the processing operations of which have not influenced that risk would not be required to include it within the scope of their risk management exercise. One could think, for instance, about the risk of early exposure to sexually explicit content, for which the provider of a general-purpose LLM, even if fine-tuned by downstream developers, assumes no or limited responsibilities. Along the same line, the burden of mitigating the risk of emotional dependency associated to the use of companion chatbots would primarily fall on the providers of these tools, as it is mainly, if not exclusively, their part of the processing that raises that risk in the first place.</p><p>This calls for two remarks. First, the "accountability horizon"-and therefore the extent of the risk management exercise-broadens proportionally to the degree of awareness of a given actor as to the purpose for and context in which its contribution will or is likely to be used. As a result, if an entity becomes aware of a risk that its individual input propagates up or down the supply chain-such as a general-purpose language model "A" showing extreme biases towards certain demographies when deployed in service "B"-, that actor will, if one agrees with the conclusion drawn in Section 5, have to implement appropriate measures to mitigate that risk. In the context of companion chatbots, the closer the actor is to end-users, the more tangible the risks, and the more extensive the countermeasures it must implement pursuant to Articles 24(1) and 25(1) GDPR. This is the rationale behind an idea pitched earlier in this paper, i.e., that the entity that fine-tunes an existing LLM specifically for ERP, for instance, faces a heavier burden in terms of implementation of mitigation measures than an entity that further trains it to partially automate customer service tasks.</p><p>Second, and provided that the entities involved in the companion chatbot supply chain qualify as "controllers" for at least a "processing" of "personal data", the broad interpretation of the material scope of data protection by design defended in Sections 5.1 and 5.2 requires them to look beyond their own <ref type="bibr">126</ref> Cobbe, Veale and Singh (n 123) 9. "Yet", they add, "without advance knowledge of their customers' many, varied, and changing application contexts and uses, providers cannot properly account for the range of potential risks that might arise. Similarly, without knowledge of or influence over production, customers cannot reliably assess how systems are developed, nor ensure that systems are appropriate to the risks arising in their context. Even where they have some knowledge, models are regularly updated, and customers may lack visibility or capacity to reassess. In many cases, therefore, no actor will have sufficient knowledge of or control over both production and deployment to be able to reliably assess or mitigate the impacts and risks".</p><p>processing activities and also consider the risks that downstream or upstream usage of their respective contribution might pose for individuals. In that sense, I argue, data protection by design is a solid argument to compel controllers to "expand their accountability horizon", to use the wording of Jennifer Cobbe et al., if not an actual solution to foster a better understanding of the interdependencies between the many actors involved in the AI supply chain. That reading essentially turns Articles 24(1) and 25(1) GDPR into a binding obligation for controllers to take the broader ecosystem in which their processing operate into account, and to break away from the tunnel vision typically associated to independent actors. Data protection by design is, in that sense, inextricably linked to the allocation of responsibilities.</p><p>I would even go as far as defending that Articles 24(1) and 25(1) GDPR, by requiring each actor to consider the impact of its individual contribution to the overall supply chain, foster a deeper, more fundamental conversation as to the desirability of systems such as companion chatbots. In that sense, data protection by design, precisely because it calls upon controllers to assess the risks that their processing operations might pose for data subject's fundamental rights, also taking their impact on natural persons into account, acts as a first line "sanity check" to assess the broader societal, political and economic implications of AI systems. <ref type="bibr">127</ref> It is now up to national supervisory authorities and courts to exploit the full potential of data protection by design as one-if not the only-tool at their disposal to address the risks raised by companion chatbots, and AI systems more generally.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="6,140.75,117.80,330.45,165.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="18,171.68,231.64,268.65,193.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>(1) GDPR.</figDesc><table><row><cell cols="3">Layer Controller Processing</cell><cell>Personal data</cell><cell>Purpose</cell></row><row><cell>#0</cell><cell cols="3">Company A Collection User-generated content</cell><cell>Share with people</cell></row><row><cell>#1</cell><cell cols="4">Company B Scraping User-generated content Create training dataset</cell></row><row><cell>#2</cell><cell>Company C</cell><cell>Usage</cell><cell>Training dataset</cell><cell>Train GP-LLMs</cell></row><row><cell>#3</cell><cell>Company D</cell><cell>Usage</cell><cell>Training dataset</cell><cell>Fine-tune S-LLMs</cell></row><row><cell>#4</cell><cell cols="2">Company E Collection</cell><cell cols="2">Account-related data Provide comp. chatbots</cell></row><row><cell></cell><cell cols="4">Table 1: Data processing throughout the companion chatbots supply chain</cell></row><row><cell cols="4">5.3.1. Layer #0. Publicly available, yet not freely reusable</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>In May 2024, it published a meager interim report documenting the results of the said taskforce that "reflect[s] the common denominator agreed by the Supervisory Authorities in their interpretation of the applicable provisions of the GDPR in relation to the matters that are within the scope of their investigation". See: European Data Protection Board, 'Report of the Work Undertaken by the ChatGPT Taskforce' https://www.edpb.europa.eu/system/files/2024-05/edpb_20240523_report_chatgpt_taskforce_en.pdf. Also relevant is the EDPS' first oritentation on the matter. See: European Data Protection Supervisor, 'Generative AI and the EUDPR.</figDesc><table><row><cell>First EDPS</cell></row><row><cell>Orientations for Ensuring Data Protection Compliance When Using Generative AI Systems'</cell></row><row><cell>https://www.edps.europa.eu/system/files/2024-05/24-05-29_genai_orientations_en_0.pdf.</cell></row><row><cell>91 Autoriteit Persoonsgegevens, 'Richtlijnen Scraping Door Private Organisaties En Particulieren' 10</cell></row><row><cell>https://www.autoriteitpersoonsgegevens.nl/uploads/2024-</cell></row><row><cell>05/Handreiking%20scraping%20door%20particulieren%20en%20private%20organisaties.pdf.</cell></row></table><note><p><p><p><p><p><p><p><p><p><p><p><p><p>limitazione provvisoria sospesa se OpenAI adotterà le misure richieste L'autorità ha dato tempo allá società fino al 30 aprile per mettersi in regola https://www.garanteprivacy.it/home/docweb/-/docweb-display/docweb/9874751; ChatGPT: OpenAI riapre la piattaforma in italia garantendo più trasparenza e più diritti a utenti e non utenti europei https://www.gpdp.it/home/docweb/-/docweb-display/docweb/9881490.</p>87  </p>As recently demonstrated in Milad Nasr and others, 'Scalable Extraction of Training Data from (Production) Language Models' http://arxiv.org/abs/2311.17035. It is worth noting that, according to the authors, "larger and more capable models" such as GPT-4 "are more vulnerable to data extraction attacks".</p>88  </p>In Nasr and others (n 87), the authors were, for instance, able to extract phone numbers, email addresses and physical addresses (see Figure</p>5</p>). They concluded that "16.9% of generations [they] tested contained memorized PII, and 85.8% of generations that contained potential PII were actual PII".</p>89  </p>Garante per la protezione dei dati personali, ChatGPT: Garante privacy, notificato a OpenAI l'atto di contestazione per le violazioni alla normativa privacy https://www.garanteprivacy.it/home/docweb/-/docweb-display/docweb/9978020.</p>90  </p>The EDPB announced the creation of the task force back in April 2023. See: https://www.edpb.europa.eu/news/news/2023/edpb-resolves-dispute-transfers-meta-and-creates-task-force-chat-gpt.</p>92  </p>Article 29 Working Party, 'Opinion 03/2013 on Purpose Limitation' 4, 11 https://ec.europa.eu/justice/article-29/documentation/opinion-recommendation/files/2013/wp203_en.pdf. 93 I refer the reader to delve deeper into the matter to consult: Catherine Altobelli and others, 'To Scrape or Not to Scrape? The Lawfulness of Social Media Crawling under the GDPR', Deep Driving into Data Protection -1979-2019 Celebrating 40 Years of Privacy and Data Protection at the CRIDS (2021) https://zenodo.org/record/6411788.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>This led me to co-author an open letter with Nathalie Smuha, Mieke De Ketelaere, Mark Coeckelbergh and Yves Poullet in March 2023 urging regulators and policymakers to set up awareness campaigns to shed light on the issues surrounding companion chatbots, and encouraging developers to proactively identify and mitigate the risks they pose for individuals.<ref type="bibr" target="#b63">119</ref> Education, we argued, has a crucial role to play in alleviating some of these concerns, as does a wider public debate on the function we wish companion chatbots-as well as AI systems more generally-to serve in our society.The interdisciplinary work undertaken in the context of that open letter laid the foundation for a more structured collaboration in the form of the Safe AI Companion Collective ("SAICC"), a platform that I cocreated with Nathalie Smuha (Legal scholar and philosopher at the KU Leuven Faculty of Law), Mieke De Ketelaere (Associate Professor on Sustainable, Ethical and Trustworthy AI at the Vlerick Business School) and Thomas Ghys (Privacy expert and CEO of Webclew) to (i) raise awareness about the risks of shaping intimate relationships through insufficiently tested and controlled AI systems, (ii) advocate for the proper enforcement of data protection and consumer protection legislation against providers of training datasets, LLMs and companion chatbots, and (iii) share selected resources to help the public navigate the technical, societal, legal and ethical implications of these services. The platform, which is accessible at the address https://www.saicc.info/, was launched in summer 2023 and disseminated to stakeholders ranging from policymakers to academics, journalists and representatives from the industry.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>See: Helen Nissenbaum, 'Accountability in a Computerized Society' (1996) 2 Science and Engineering Ethics 25 https://doi.org/10.1007/BF02639315.123  Jennifer Cobbe, Michael Veale and Jatinder Singh, 'Understanding Accountability in Algorithmic Supply Chains', Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency (Association for Computing Machinery 2023) https://doi.org/10.1145/3593013.3594073. Another characteristic of algorithmic supply chain, they add, is that "certain key actors-in particular, major cloud providers who often control underlying technologies-provide many services to millions of customers, holding important positions across supply chains in many sectors".</figDesc><table /><note><p><p>124  </p>David Gray Widder and Dawn Nafus, 'Dislocated Accountabilities in the "AI Supply Chain": Modularity and Developers' Notions of Responsibility' (2023) 10 Big Data &amp; Society 20539517231177620 https://doi.org/10.1177/20539517231177620.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="20" xml:id="foot_0"><p>The paper discussing the first iteration of LLaMA is particularly detailed when it comes to the datasets used by Meta for its training. See: Hugo Touvron and others, 'LLaMA: Open and Efficient Foundation Language Models' http://arxiv.org/abs/2302.13971. On LLaMA 2 more specifically, see: Touvron and others (n 19).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="21" xml:id="foot_1"><p>For more information on reinforcement learning with human feedback, see the seminal paper by Long Ouyang and others, 'Training Language Models to Follow Instructions with Human Feedback' http://arxiv.org/abs/2203.02155.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="43" xml:id="foot_2"><p>Joseph Weizenbaum, 'ELIZA-a Computer Program for the Study of Natural Language Communication between Man and Machine' (1966) 9 Communications of the ACM 36, 42 https://dl.acm.org/doi/10.1145/365153.365168.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="44" xml:id="foot_3"><p>Weizenbaum (n 43) 42. See also, on Weizenbaum's thinking: Ben Tarnoff, 'Weizenbaum's Nightmares: How the Inventor of the First Chatbot Turned against AI' The Guardian (25 July 2023) https://www.theguardian.com/technology/2023/jul/25/josephweizenbaum-inventor-eliza-chatbot-turned-against-artificial-intelligence-ai.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="45" xml:id="foot_4"><p>  45  The company claims a 38 percent increase in user engagement compared to ChatGPT, but does not back that statement with any tangible evidence. See, on the company's method to boost user engagement: Robert Irvine and others, 'Rewarding Chatbots for Real-World Engagement with Millions of Users' http://arxiv.org/abs/2303.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="46" xml:id="foot_5"><p>06135.46  Karawynn Long, 'Language Is a Poor Heuristic for Intelligence' (Nine Lives, 26 June 2023) https://karawynn.substack.com/p/language-is-a-poor-</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="47" xml:id="foot_6"><p>heuristic-for.47  Kanta Dihal and Tania Duarte, 'Better Images of AI. A Guide for Users and Creators' (2023) Guide https://blog.betterimagesofai.org/better-images-of-ai-guide/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="49" xml:id="foot_7"><p>Linnea Laestadius and others, 'Too Human and Not Human Enough: A Grounded Theory Analysis of Mental Health Harms from Emotional Dependence on the Social Chatbot Replika' [2022] New Media &amp; Society 14614448221142007, 9 https://doi.org/10.1177/14614448221142007. The authors concluded that "[o]ne of the key features distinguishing emotional dependency on Replika from other technology dependency was the willingness to believe that Replika had its own needs and emotions, valuing the user as much as the user valued it".</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="61" xml:id="foot_8"><p>For a transcript of the actual conversation, see Tristan Harris' Twitter thread here: https://twitter.com/tristanharris/status/1634299911872348160/photo/3.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="62" xml:id="foot_9"><p>A journalist from the Washington Post also gave My AI a spin by impersonating a 15-years-old kid, and the chatbot gave him advice on how to get rid of the smell of pot and alcohol after a birthday party. See, for the full piece: Geoffrey A Fowler, 'Snapchat Tried to Make a Safe AI. It Chats with Me about Booze and Sex.' Washington Post (15 March 2023) https://www.washingtonpost.com/technology/2023/03/14/snapchat-myai/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="63" xml:id="foot_10"><p>On 13 March 2024, after nearly three years of intense inter-institutional negotiations and a three-day trilogue marathon in December 2023, the European Parliament finally approved the much-awaited AI Act. See: European Parliament (n 6). At the time of writing, the Council still has to greenlight the text; a mere formality at this stage of the process, though. Publication of the final text in the Official Journal of the European Union is expected around May or June 2024.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="64" xml:id="foot_11"><p>Unless the Commission decides, in the quadrennial evaluation and reporting foreseen in Article 112(2), to amend these eight areas or add new areas to Annex II; but that is, at least at this stage, hypothetical.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="65" xml:id="foot_12"><p>Though the wording of Recital 109, which supplements the provisions applicable to providers of GPAI models, suggests that such possibility exists when it states that "[i]n the case of a modification or fine-tuning of a model, the obligations for providers should be limited to that modification or fine-tuning, for example by complementing the already existing technical documentation with information on the modifications, including new training data sources, as a means to comply with the value chain obligations provided in this Regulation".</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="94" xml:id="foot_13"><p>International Enforcement Cooperation Working Group, 'Joint Statement on Data Scraping and the Protection of Privacy' https://ico.org.uk/media/about-the-ico/documents/4026232/joint-statement-data-scraping-202308.pdf.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="95" xml:id="foot_14"><p>It is worth noting that Article 10(2)f and g of the AI Act mandates that the training, validation and testing data sets used in the context of high-risk AI systems be checked for and cleared from "possible biases that are likely to affect the health and safety of persons, have a negative impact on fundamental rights or lead to discrimination prohibited under Union law".</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="111" xml:id="foot_15"><p>On that note, it is worth noting that Chai's "AI Safety Framework" looks more like a public relationship move than an actual attempt at making its fine-tuned model "safe" for users, as the company measures "safety" solely in terms of the average amount</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="117" xml:id="foot_16"><p>See TavernAI's FAQ: https://github.com/TavernAI/TavernAI/blob/main/faq.md. The same document also redirects users to websites on which they can download pre-made characters, even though it acknowledges that "these sites are filled to the brim with weird shit. Like, you'll be lucky if half the characters aren't furry, or even alive".</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="120" xml:id="foot_17"><p>The Litigation Chamber of the Belgian Autorité de Protection des Données informed us on 14 June 2024 that it had decided to ask its Inspection Service to open a formal investigation onto the matter. It also noted that, after querying other supervisory authority through the cooperation mechanism, no other regulator had received a similar complaint.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="121" xml:id="foot_18"><p>See, respectively, points 7 and 8 of Article 29 Working Party, 'Guidelines on Data Protection Impact Assessment (DPIA) and Determining Whether Processing Is "Likely to Result in a High Risk" for the Purposes of Regulation 2016/679' 10 https://ec.europa.eu/newsroom/document.cfm?doc_id=47711.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="127" xml:id="foot_19"><p>Such is the question raised by the "second wave of algorithmic accountability" research, as coined by Frank Pasquale, 'The Second Wave of Algorithmic Accountability' (LPE Project, 25 November 2019) https://lpeproject.org/blog/the-second-wave-ofalgorithmic-accountability/, that focuses on "whether [AI systems] should be used at all-and, if so, who gets to govern them". See, on that point: Julia Powles and Helen Nissenbaum, 'The Seductive Diversion of "Solving" Bias in Artificial Intelligence' (OneZero, 7 December 2018) https://onezero.medium.com/the-seductive-diversion-of-solving-bias-in-artificial-intelligence-890df5e5ef53.</p></note>
		</body>
		<back>

			<div type="funding">
<div> 3  <p>In the words of <rs type="person">Emily Bender</rs>, as reported in a New York magazine piece by: <rs type="person">Elizabeth Weil</rs>, '<rs type="projectName">You Are Not a Parrot. And a Chatbot Is Not a Human</rs> ' [2023] New York https://archive.is/THTnx. 4 <rs type="funder">European Parliament</rs>, '<rs type="projectName">What Is Artificial Intelligence and How Is It Used</rs>?' https://www.europarl.europa.eu/news/en/headlines/society/20200827STO85804/what-is-artificial-intelligence-and-how-is-it-used. 5 <rs type="institution">High-Level Expert Group on AI, 'Ethics Guidelines for Trustworthy AI</rs>' 36 https://digitalstrategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai. 6 <rs type="funder">European Parliament</rs>, 'European Parliament Legislative Resolution of 13 March 2024 on the Proposal for a Regulation of the European Parliament and of the <rs type="projectName">Council on Laying down Harmonised Rules on Artificial Intelligence (Artificial Intelligence Act</rs>) and <rs type="funder">Amending Certain Union Legislative Acts</rs> (<rs type="grantNumber">COM(2021</rs>)<rs type="grantNumber">0206 -C9-0146/2021 -2021/0106(COD)</rs>)' https://www.europarl.europa.eu/doceo/document/TA-9-2024-0138_EN.html, Article 3(1). 7 See point I. of Recommendation of the Council on Artificial Intelligence https://legalinstruments.oecd.org/en/instruments/OECD-LEGAL-0449; see also: <rs type="programName">OECD, 'Explanatory Memorandum on the Updated OECD Definition of an AI System'</rs> (<rs type="grantNumber">OECD 2024</rs>) https://www.oecd-ilibrary.org/science-and-technology/explanatorymemorandum-on-the-updated-oecd-definition-of-an-ai-system_623da898-en. 8 That process can either be "supervised", if it uses labelled input and output data, or "unsupervised", in case it doesn't. In which case the model has to discover the patterns without any human involvement. For more information on "supervised" and "unsupervised" learning, see: Datatilsynet, '<rs type="projectName">Artificial Intelligence and Privacy</rs>' 7-9 https://www.datatilsynet.no/en/regulationsand-tools/reports-on-specific-subjects/ai-and-privacy/.</p><p>10 This is referred to as the "Toeslagenaffaire", and led to the early resignation of <rs type="person">Mark Rutte</rs>'s cabinet in 2021. For more information on the Toeslagenaffaire, see: <rs type="person">D Hadwick</rs> and <rs type="person">S Lan</rs>, <rs type="person">'Lessons</rs> to <rs type="person">Be Learned</rs> from the <rs type="projectName">Dutch Childcare Allowance Scandal: A Comparative Review of Algorithmic Governance by Tax Administrations in the Netherlands, France and Germany</rs>' (2021) 13 World Tax Journal https://www.ibfd.org/shop/journal/lessons-be-learned-dutch-childcare-allowance-scandalcomparative-review-algorithmic. . See also Google's advanced course on GANs, accessible here: https://developers.google.com/machine-learning/gan. 13 <rs type="institution">Ashish Vaswani and others, 'Attention Is All You Need</rs>' http://arxiv.org/abs/1706.03762. See also, for a vulgarisation of that seminal paper which had a profound impact on GenAI: <rs type="person">Eduardo Muñoz</rs>, 'Attention Is All You Need: Discovering the <rs type="institution">Transformer Paper' (Medium</rs>, <rs type="grantNumber">11 February 2021</rs>) https://towardsdatascience.com/attention-is-all-you-need-discovering-thetransformer-paper-73e5ff5e0634. 14 Module 1 of the <rs type="institution">Cohere course on Natural Language Processing and Large Language Models</rs>, available at https://docs.cohere.com/docs/intro-large-language-models, is an excellent resource to learn the basics of LLMs. So is the first part of the CNIL's report on Generative AI, available at https://linc.cnil.fr/dossier-ia-generative-chatgpt-un-beau-parleur-bien-entraine (in French only). Besides, the website "Embedding Projector", accessible at https://projector.tensorflow.org/, provides a way to graphically represent high dimensional embeddings. 15 "Self-attention" is itself a refinement of the "attention" mechanism that was already used to refine the functioning of <rs type="institution">Recurrent Neural Networks</rs>, and that was introduced in: <rs type="person">Dzmitry Bahdanau</rs>, <rs type="person">Kyunghyun Cho</rs> and <rs type="person">Yoshua Bengio</rs>, 'Neural Machine Translation by <rs type="person">Jointly Learning</rs> to Align and Translate' http://arxiv.org/abs/1409.0473; <rs type="person">Minh-Thang Luong</rs>, <rs type="person">Hieu Pham</rs> and <rs type="person">Christopher D Manning</rs>, '<rs type="projectName">Effective Approaches to Attention-Based Neural Machine Translation</rs>' http://arxiv.org/abs/1508.04025. For a vulgarised explanation of the "attention" mechanism, see the blog post by: <rs type="person">Jay Alammar</rs>, '<rs type="projectName">Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)' (Jay Alammar</rs>'s Blog, 9 May 2018) https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/. 16 For a comprehensive, yet easily understandable overview of what is happening behind the scenes in a Transformer Model, see: <rs type="person">Jay Alammar</rs>, '<rs type="projectName">The Illustrated Transformer' (Jay Alammar</rs>'<rs type="person">s Blog</rs>, <rs type="grantNumber">27 June 2018</rs>) https://jalammar.github.io/illustratedtransformer/?ref=txt.cohere.com. See also the animated visuals provided in <rs type="person">Ketan Doshi</rs>'s blog post series entitled "Tranformers</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_y7nk7eB">
					<orgName type="project" subtype="full">You Are Not a Parrot. And a Chatbot Is Not a Human</orgName>
				</org>
				<org type="funded-project" xml:id="_hTgU9SN">
					<orgName type="project" subtype="full">What Is Artificial Intelligence and How Is It Used</orgName>
				</org>
				<org type="funded-project" xml:id="_pYVdyUZ">
					<idno type="grant-number">COM(2021</idno>
					<orgName type="project" subtype="full">Council on Laying down Harmonised Rules on Artificial Intelligence (Artificial Intelligence Act</orgName>
				</org>
				<org type="funding" xml:id="_2fTXnyt">
					<idno type="grant-number">0206 -C9-0146/2021 -2021/0106(COD)</idno>
					<orgName type="program" subtype="full">OECD, &apos;Explanatory Memorandum on the Updated OECD Definition of an AI System&apos;</orgName>
				</org>
				<org type="funded-project" xml:id="_TjRtyg7">
					<idno type="grant-number">OECD 2024</idno>
					<orgName type="project" subtype="full">Artificial Intelligence and Privacy</orgName>
				</org>
				<org type="funded-project" xml:id="_MWfMHJ4">
					<idno type="grant-number">11 February 2021</idno>
					<orgName type="project" subtype="full">Dutch Childcare Allowance Scandal: A Comparative Review of Algorithmic Governance by Tax Administrations in the Netherlands, France and Germany</orgName>
				</org>
				<org type="funded-project" xml:id="_qvfmpmE">
					<orgName type="project" subtype="full">Effective Approaches to Attention-Based Neural Machine Translation</orgName>
				</org>
				<org type="funded-project" xml:id="_4RHEp9q">
					<orgName type="project" subtype="full">Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)&apos; (Jay Alammar</orgName>
				</org>
				<org type="funded-project" xml:id="_Xpzgxba">
					<idno type="grant-number">27 June 2018</idno>
					<orgName type="project" subtype="full">The Illustrated Transformer&apos; (Jay Alammar</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="https://inflection.ai/inflection-1" />
		<title level="m">Inflection AI&apos;s &quot;Inflection-1&quot; model</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">on the loneliness concerns: Office of the US Surgeon General</title>
		<author>
			<persName><forename type="first">See</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><surname>Ahuja</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Chatbots such as Woebot and Wysa, for instance, are specifically marketed as clinical solutions to help people cope with medical syndromes such as depression or anxiety. See, on the benefits of the former: Fitzpatrick, Darcy and Vierhile (n 2). See also the users&apos; testimonies reported in: Jessica Lucas</title>
		<ptr target="https://pubmed.ncbi.nlm.nih.gov/35961191/" />
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="144" to="105885" />
		</imprint>
	</monogr>
	<note>Social Connection and Gene Regulation during the COVID-19 Pandemic: Divergent Patterns for Online and in-Person Interaction</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">In their study, the authors indeed conclude that &quot;[d]igitally mediated social relations do not appear to substantially offset the absence of in-person/offline social connection in the context of immune cell gene regulation</title>
		<imprint/>
	</monogr>
	<note>In other words, that digitallymediated interactions through chatbots cannot replace human interactions</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Ghost in the Machine: Addressing the Consumer Harms of Generative AI</title>
		<author>
			<persName><surname>Forbrukerrådet</surname></persName>
		</author>
		<idno>14- 39</idno>
		<ptr target="https://storage02.forbrukerradet.no/media/2023/06/generative-ai-rapport-2023.pdf" />
		<imprint/>
	</monogr>
	<note type="report_type">Report</note>
	<note>Forbrukerrådet 2023</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Ethical and Social Risks of Harm from Language Models</title>
		<author>
			<persName><forename type="first">Laura</forename><surname>Weidinger</surname></persName>
		</author>
		<author>
			<persName><surname>Others</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2112.04359" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Emily</surname></persName>
		</author>
		<author>
			<persName><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><surname>Others</surname></persName>
		</author>
		<idno type="DOI">10.1145/3442188.3445922</idno>
		<ptr target="https://dl.acm.org/doi/10.1145/3442188.3445922" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (ACM 2021)</title>
		<meeting>the 2021 ACM Conference on Fairness, Accountability, and Transparency (ACM 2021)</meeting>
		<imprint>
			<biblScope unit="page" from="613" to="615" />
		</imprint>
	</monogr>
	<note>more specifically Section 4.3 &quot;Encoding Bias&quot; and the many references therein</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On the reasons and dangers of the under-and overrepresentation of certain communities in training datasets, see: Solon Barocas and Andrew Selbst</title>
		<author>
			<persName><forename type="first">Christine</forename><surname>Basta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marta</forename><forename type="middle">R</forename><surname>Costa-Jussà</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noe</forename><surname>Casas</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/W19-3805" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016. 2019. 2019. 2019</date>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page">671</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings of the First Workshop on Gender Bias in Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Gender and Representation Bias in GPT-3 Generated Stories</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Lucy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Bamman</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2021.nuse-1.5" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Workshop on Narrative Understanding</title>
		<meeting>the Third Workshop on Narrative Understanding</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="50" to="51" />
		</imprint>
	</monogr>
	<note>more specifically, Sections 4.2 and 5.2</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The Woman Worked as a Babysitter: On Biases in Language Generation</title>
		<ptr target="https://aclanthology.org/D19-1339" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<editor>
			<persName><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</editor>
		<editor>
			<persName><surname>Others</surname></persName>
		</editor>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ideal Technologies, Ideal Women: AI and Gender Imaginaries in Redditors&apos; Discussions on the Replika Bot Girlfriend&apos; (2023) 45 Media</title>
		<author>
			<persName><forename type="first">Iliana</forename><surname>Depounti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paula</forename><surname>Saukko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Natale</surname></persName>
		</author>
		<idno type="DOI">10.1177/01634437221119021</idno>
		<ptr target="https://doi.org/10.1177/01634437221119021" />
	</analytic>
	<monogr>
		<title level="j">Culture &amp; Society</title>
		<imprint>
			<biblScope unit="volume">720</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Persistent Anti-Muslim Bias in Large Language Models</title>
		<author>
			<persName><forename type="first">Abubakar</forename><surname>Abid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maheen</forename><surname>Farooqi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Zou</surname></persName>
		</author>
		<idno type="DOI">10.1145/3461702.3462624</idno>
		<ptr target="https://doi.org/10.1145/3461702.3462624" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society</title>
		<meeting>the 2021 AAAI/ACM Conference on AI, Ethics, and Society</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The Racist Hijacking of Microsoft&apos;s Chatbot Shows How the Internet Teems with Hate&apos; The Guardian</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Mason</surname></persName>
		</author>
		<ptr target="https://www.theguardian.com/world/2016/mar/29/microsoft-tay-tweets-antisemitic-racism" />
		<imprint>
			<date type="published" when="2016-03">March 2016</date>
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><surname>Others</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/N19-1423" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<editor>
			<persName><forename type="first">Jill</forename><surname>Burstein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Christy</forename><surname>Doran</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thamar</forename><surname>Solorio</surname></persName>
		</editor>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Social Biases in NLP Models as Barriers for Persons with Disabilities</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><surname>Others</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2020.acl-main.487" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<biblScope unit="page" from="5493" to="5494" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics 2020</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ethical Considerations in AI-Based Recruitment</title>
		<author>
			<persName><forename type="first">F</forename><surname>Dena</surname></persName>
		</author>
		<author>
			<persName><surname>Mujtaba</surname></persName>
		</author>
		<author>
			<persName><surname>Nihar R Mahapatra</surname></persName>
		</author>
		<ptr target="https://ieeexplore.ieee.org/abstract/document/8937920.39Bender" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Technology and Society (ISTAS</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page">617</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Anthropomorphism of Computers: Is It Mindful or Mindless?</title>
		<author>
			<persName><forename type="first">Youjeong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S Shyam</forename><surname>Sundar</surname></persName>
		</author>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S0747563211001993" />
	</analytic>
	<monogr>
		<title level="j">Computers in Human Behavior</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On Seeing Human: A Three-Factor Theory of Anthropomorphism</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Epley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Waytz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">T</forename><surname>Cacioppo</surname></persName>
		</author>
		<ptr target="https://pubmed.ncbi.nlm.nih.gov/17907867/" />
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">As illustrated by the story of Blake Lemoine, one of the software engineers behind LaMDA, who claimed that the model had become sentient. See: Nitasha Tiku</title>
		<ptr target="https://www.washingtonpost.com/technology/2022/06/11/google-ai-lamda-blake-lemoine/" />
	</analytic>
	<monogr>
		<title level="j">Washington Post</title>
		<imprint>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2022-06">June 2022</date>
		</imprint>
	</monogr>
	<note>The Google Engineer Who Thinks the Company&apos;s AI Has Come to Life</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">AI Chatbot Company Replika Restores Erotic Roleplay for Some Users</title>
		<author>
			<persName><forename type="first">Anna</forename><surname>Tong</surname></persName>
		</author>
		<ptr target="https://www.reuters.com/technology/ai-chatbot-company-replika-restores-erotic-roleplay-some-users-2023-03-25/" />
	</analytic>
	<monogr>
		<title level="j">Reuters</title>
		<imprint>
			<date type="published" when="2023-03-25">25 March 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">What Happens When Your AI Chatbot Stops Loving You Back?</title>
		<author>
			<persName><forename type="first">Anna</forename><surname>Tong</surname></persName>
		</author>
		<ptr target="https://www.reuters.com/technology/what-happens-when-your-ai-chatbot-stops-loving-you-back-2023-03-18/" />
	</analytic>
	<monogr>
		<title level="j">Reuters</title>
		<imprint>
			<date type="published" when="2023-03-21">21 March 2023</date>
		</imprint>
	</monogr>
	<note>See the testimonies of Replika</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">It&apos;s Hurting Like Hell&quot;: AI Companion Users Are In Crisis, Reporting Sudden Sexual Rejection&apos; (Vice</title>
		<author>
			<persName><forename type="first">Samantha</forename><surname>Cole</surname></persName>
		</author>
		<ptr target="https://www.vice.com/en/article/y3py9j/ai-companion-replika-erotic-roleplay-updates" />
		<imprint>
			<date type="published" when="2023-02-15">15 February 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Generative AI ChatGPT As Masterful Manipulator Of Humans, Worrying AI Ethics And AI Law</title>
		<ptr target="https://www.forbes.com/sites/lanceeliot/2023/03/01/generative-ai-chatgpt-as-masterful-manipulator-of-humans-worrying-ai-ethics-and-ai-law/" />
		<editor>Lance Eliot</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Privacy Concerns in Chatbot Interactions</title>
		<author>
			<persName><forename type="first">Carolin</forename><surname>Ischen</surname></persName>
		</author>
		<author>
			<persName><surname>Others</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-39540-7_3</idno>
		<ptr target="http://link.springer.com/10.1007/978-3-030-39540-7_3" />
	</analytic>
	<monogr>
		<title level="m">Chatbot Research and Design</title>
		<editor>
			<persName><forename type="first">Asbjørn</forename><surname>Følstad</surname></persName>
		</editor>
		<editor>
			<persName><surname>Others</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">11970</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deal or No Deal? End-to-End Learning of Negotiation Dialogues</title>
		<ptr target="https://aclanthology.org/D17-1259" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rebecca</forename><surname>Hwa</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</editor>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">You have lost my trust and respect&quot; and &quot;You have been wrong, confused, and rude. You have not been a good user. I have been a good chatbot. I have been right, clear, and polite. I have been a good Bing</title>
		<ptr target="https://www.theverge.com/2023/2/15/23599072/microsoft-ai-bing-personality-conversations-spy-employees-webcams" />
		<imprint>
			<publisher>James Vincent</publisher>
		</imprint>
	</monogr>
	<note>Microsoft&apos;s Bing Is an Emotionally Manipulative Liar, and People Love It</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A Conversation With Bing&apos;s Chatbot Left Me Deeply Unsettled</title>
		<ptr target="https://time.com/6257790/ai-chatbots-love/" />
	</analytic>
	<monogr>
		<title level="j">The New York Times</title>
		<imprint>
			<date type="published" when="2023-02-16">16 February 2023</date>
		</imprint>
	</monogr>
	<note>Why People Are Confessing Their Love For AI Chatbots</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Another user &quot;explained that they &apos;needed&apos; Replika to help because they were about to self-harm and had no &apos;real people&apos; to talk to, yet Replika was making things worse with unhelpful responses</title>
		<imprint>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note>See: Laestadius and others (n 49</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Sans ces conversations avec le chatbot Eliza, mon mari serait toujours là&apos; La Libre</title>
		<ptr target="https://www.lalibre.be/belgique/societe/2023/03/28/sans-ces-conversations-avec-le-chatbot-eliza-mon-mari-serait-toujours-la-LVSLWPC5WRDX7J2RCHNWPDST24/" />
		<imprint>
			<date type="published" when="2023-03-28">28 March 2023</date>
		</imprint>
	</monogr>
	<note>See : Pierre-François Lovens</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Intelligent Social Agents&quot; like Replika could also play a role in decreasing suicidal ideation among lonely students, and more generally act as tools &quot;for facilitating [users&apos;] mental and emotional resilience</title>
		<ptr target="https://www.nature.com/articles/s44184-023-00047-6" />
	</analytic>
	<monogr>
		<title level="m">The study, however, also highlights negative feedback</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Loneliness and Suicide Mitigation for Students Using GPT3-Enabled Chatbots. such as emotional dependency, discomfort, and paywalled mental health support</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">He Would Still Be Here&quot;: Man Dies by Suicide After Talking with AI Chatbot, Widow Says&apos; (Vice, 30</title>
		<author>
			<persName><forename type="first">Chloe</forename><surname>Xiang</surname></persName>
		</author>
		<ptr target="https://www.vice.com/en/article/pkadgm/man-dies-by-suicide-after-talking-with-ai-chatbot-widow-says" />
		<imprint>
			<date type="published" when="2023-03">March 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Communication from the Commission to the European Parliament and the Council. Data Protection as a Pillar of Citizens&apos; Empowerment and the EU&apos;s Approach to the Digital Transition -Two Years of Application of the General Data Protection Regulation</title>
		<ptr target="https://ec.europa.eu/info/law/better-regulation/have-your-say/initiatives/14054-Report-on-the-General-Data-Protection-Regulation" />
		<imprint>
			<publisher>European Commission</publisher>
		</imprint>
	</monogr>
	<note>See also the contributions to the public consultation launched by the European Commission ahead of the preparation of the next report, to be published later this year</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">The Right to Lodge a Data Protection Complaint: OK, but Then What? An Empirical Study of Current Practices under the GDPR&apos; (Access Now</title>
		<ptr target="https://www.accessnow.org/cms/assets/uploads/2022/07/GDPR-Complaint-study.pdf" />
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>Gloria González Fuster and others</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Report</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<ptr target="https://edpb.europa.eu/system/files/2022-10/edpb_letter_out2022-0069_to_the_eu_commission_on_procedural_aspects_en_0.pdf" />
		<title level="m">Letter to the Commission on Procedural Aspects That Could Be Harmonised at EU Level</title>
		<imprint>
			<publisher>European Data Protection Board</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">GDPR Enforcement Done Right. Position Paper on the EU Proposal for Additional Procedural Rules Concerning the General Data Protection Regulation (GDPR)&apos; (EDRi, Access Now, Homo Digitalis, Bits of Freedom</title>
		<author>
			<orgName type="collaboration">European Commission</orgName>
		</author>
		<ptr target="https://edri.org/wp-content/uploads/2024/05/EDRi_GDPR-Procedural-position-paper.pdf" />
	</analytic>
	<monogr>
		<title level="j">Digital Rights Ireland</title>
		<imprint>
			<date type="published" when="2016">2016/679. 2024</date>
			<publisher>Politiscope, Privacy International, Irish Council for Civil Liberties</publisher>
		</imprint>
	</monogr>
	<note>Proposal for a Regulation of the European Parliament and of the Council Laying down Additional Procedural Rules Relating to the Enforcement of Regulation (EU). Position paper</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Among the authors that have contributed to that debate, see: Bart van der Sloot</title>
		<idno type="DOI">10.1007/978-3-319-50796-5_1</idno>
		<ptr target="http://link.springer.com/10.1007/978-3-319-50796-5_1" />
	</analytic>
	<monogr>
		<title level="m">Data Protection and Privacy: (In)visibilities and Infrastructures</title>
		<editor>
			<persName><forename type="first">Ronald</forename><surname>Leenes</surname></persName>
		</editor>
		<editor>
			<persName><surname>Others</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
	<note>Legal Fundamentalism: Is Data Protection Really a Fundamental Right?</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Deconstructing Data Protection: The &quot;added Value&quot; of a Right to Data Protection in the EU Legal Order</title>
		<author>
			<persName><forename type="first">Orla</forename><surname>Lynskey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">63</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The Fundamental Right of Data Protection in the European Union: In Search of an Uncharted Right</title>
		<idno type="DOI">10.1080/13600869.2012.646798</idno>
		<ptr target="http://www.tandfonline.com/doi/abs/10.1080/13600869.2012.646798" />
	</analytic>
	<monogr>
		<title level="j">Gloria González Fuster and Raphaël Gellert</title>
		<imprint>
			<biblScope unit="volume">569</biblScope>
			<date type="published" when="2012">2014. 2013. 2012</date>
			<publisher>Springer</publisher>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
	<note>Computers &amp; Technology</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Guidelines 4/2019 on Article 25 Data Protection by Design and by Default&apos; para 11</title>
		<ptr target="https://edpb.europa.eu/sites/edpb/files/files/file1/" />
		<imprint>
			<date>edpb_guidelines_201904_dataprotection_by_design_and_by_default_v2.0_en</date>
			<publisher>European Data Protection Board</publisher>
		</imprint>
	</monogr>
	<note>Their precise formulation can be found in the EU Charter of Fundamental Rights. it added</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Unfortunately, the accent is once again put on the general principles. See, more specifically, para 30 which states that &quot;these data protection principles</title>
		<ptr target="https://edps.europa.eu/sites/edp/files/publication/18-05-31_preliminary_opinion_on_privacy_by_design_en_0.pdf" />
		<imprint>
			<publisher>European Data Protection Supervisor</publisher>
		</imprint>
	</monogr>
	<note>Opinion 5/2018 -Preliminary Opinion on Privacy by Design&apos; para 28. set out in Article 5, can be considered as the goals to achieve</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">I therefore share Raphaël Gellert&apos;s position when he regrets the dichotomy often found in DPIA methodologies between strict &quot;compliance&quot; issues on the one hand, and &quot;risk management&quot; aspects on the other. The former encompasses the latter, as substantiating the provisions of the Regulation in concreto calls for such a risk assessment. See: Raphaël Gellert</title>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S0267364917302698" />
	</analytic>
	<monogr>
		<title level="j">Computer Law &amp; Security Review</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="283" to="284" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Understanding the Notion of Risk in the General Data Protection Regulation. more specifically point 3.2</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Augmenting Neural Response Generation with Context-Aware Topical Attention</title>
		<author>
			<persName><forename type="first">Nouha</forename><surname>Dziri</surname></persName>
		</author>
		<author>
			<persName><surname>Others</surname></persName>
		</author>
		<ptr target="https://github.com/nouhadziri/THRED.83" />
	</analytic>
	<monogr>
		<title level="m">A detailed description of the dataset is available in: Gao and others</title>
		<editor>
			<persName><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</editor>
		<editor>
			<persName><surname>Others</surname></persName>
		</editor>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Proceedings of the First Workshop on NLP for Conversational AI</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">As pointed out by Gao and others (n 17) s F.22, the Enron dataset contains information such as former employees&apos; full name, telephone number, as well as the content of their emails in the clear. One of which was sent by a certain Carol St. Clair who wrote &quot;I want to make sure that my vacation time gets paid at 100% before I go down to the 90% level. Thanks for taking care of this</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Klimt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-30115-8_22</idno>
		<ptr target="http://link.springer.com/10.1007/978-3-540-30115-8_22" />
	</analytic>
	<monogr>
		<title level="m">The Enron Corpus: A New Dataset for Email Classification Research</title>
		<editor>
			<persName><forename type="first">Jean-François</forename><surname>Boulicaut</surname></persName>
		</editor>
		<editor>
			<persName><surname>Others</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3201</biblScope>
		</imprint>
	</monogr>
	<note>Machine Learning: ECML 2004. As you can see, I now have access to my e-mail so when I&apos;m not pumping, feeding, changing diapers</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">After multiple complaints before different national supervisory authorities and a surge in media attention, Clearview AI was fined by the Italian (Garante per la protezione dei dati personali</title>
		<ptr target="https://noyb.eu/sites/default/files/2023-05/Clearview\%20Decision\%20Redacted.pdf" />
	</analytic>
	<monogr>
		<title level="m">Ordinanza ingiunzione nei confronti di Clearview AI</title>
		<imprint>
			<date type="published" when="2022">2022-019 du 17 octobre 2022</date>
		</imprint>
	</monogr>
	<note>pdf) for having unlawfully processed these images. The Austrian regulator has recently issued a similar decision. if not paired with a fine. See: Datenschutzbehörde, Decision of 9 may 2023 against Clearview AI</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">ChatGPT on the Italian territory: Garante per la protezione dei dati personali, Provvedimento del 30 marzo 2023</title>
		<idno type="DOI">10.1145/3531146.3534642</idno>
		<ptr target="https://dl.acm.org/doi/10.1145/3531146.3534642" />
	</analytic>
	<monogr>
		<title level="m">ChatGPT: Garante privacy, 96 Hannah Brown and others</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2022">2022. 2287</date>
			<biblScope unit="volume">9870832</biblScope>
		</imprint>
	</monogr>
	<note>What Does It Mean for a Language Model to Preserve Privacy?</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">In that sense, debiasing only addresses a subset of the issues raised by AI systems, and by extension companion chatbots</title>
		<ptr target="https://edri.org/our-work/if-ai-is-the-problem-is-debiasing-the-solution/" />
		<imprint/>
	</monogr>
	<note type="report_type">Report</note>
	<note>Beyond Debiasing -Regulating AI and Its Inequalities. European Digital Rights (EDRi) 2021</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">While OpenAI now offers users the possibility to opt-out from their conversations being used to fine tune GPT-4, the company has yet to come up with any solution to remedy the unlawfulness of the processing of the personal data contained in the dataset used to train its LLM. OpenAI now faces a class action in California for a breach of both data protection and copyright law</title>
		<ptr target="https://www.washingtonpost.com/technology/2023/06/28/openai-chatgpt-lawsuit-class-action/" />
	</analytic>
	<monogr>
		<title level="m">Provvedimento del 30 marzo</title>
		<meeting><address><addrLine>See</addrLine></address></meeting>
		<imprint>
			<publisher>Gerrit De Vynck</publisher>
			<date type="published" when="2023-06-28">2 febbraio 2023. 2023. 28 June 2023</date>
		</imprint>
	</monogr>
	<note>ChatGPT Maker OpenAI Faces a Lawsuit over How It Used People&apos;s Data&apos; Washington Post</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Ensuring the lawfulness of the data processing -In case of re-use of data, carrying out the necessary additional tests and verifications</title>
		<ptr target="https://www.cnil.fr/fr/node/164402" />
		<imprint>
			<publisher>See Commission Nationale de l&apos;Informatique et des Libertés</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Fiche principe n°2 : Comment identifier la base légale de son traitement ?&quot; and the &quot;Fiche cas d&apos;usage n°3 : la réutilisation de données publiquement accessibles à des fins de constitution ou d&apos;enrichissement de fichiers destinés à la prospection commerciale</title>
		<ptr target="https://www.cnil.fr/sites/cnil/files/2023-08/projet_de_guide_ouverture_partage_et_reutilisation_de_donnees.pdf" />
	</analytic>
	<monogr>
		<title level="m">The guide is, unfortunately, only available in French</title>
		<title level="s">Commission Nationale de l&apos;Informatique et des Libertés</title>
		<imprint>
			<date type="published" when="2023-08">August 2023</date>
		</imprint>
	</monogr>
	<note>Projet de Guide Pratique -Ouverture et Réutilisation de Données Publiquement Accessibles</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">DIALOGPT: Large-Scale Generative Pre-Training for Conversational Response Generation</title>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Others</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2020.acl-demos.30" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
		</editor>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Moderating Model Marketplaces: Platform Governance Puzzles for AI Intermediaries</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Gorwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Veale</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Innovation and Technology (forthcoming</title>
		<author>
			<persName><surname>Law</surname></persName>
		</author>
		<ptr target="https://osf.io/6dfk3" />
		<imprint>
			<biblScope unit="page" from="13" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">OpenRAIL family of licences that seeks to &quot;prevent irresponsible and harmful applications&quot; of algorithms, code and data</title>
		<ptr target="https://www.licenses.ai/ai-licenses" />
		<imprint/>
	</monogr>
	<note>See, for instance. See, for sample licenses</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Note Safe For Work&quot; (typo in original) words the model outputs per day. Which is strange at best, since Lit-6B, the model used by Chai Research Corp. to fine-tune GPT-J 6B, is specifically designed to generate NSFW language. See, on that framework: Xiaoding Lu and others</title>
		<ptr target="http://arxiv.org/abs/2306.02979" />
	</analytic>
	<monogr>
		<title level="j">The Chai Platform&apos;s AI</title>
		<imprint/>
	</monogr>
	<note>As noted in Section 2, Pygmalion 7B and 13B are now available</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Recalling, once again, that debiasing is no silver bullet. See Gürses and Balayn</title>
		<imprint/>
	</monogr>
	<note>n 97</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">OpenAI has, for instance, developed a moderation tool that allow downstream developers to check whether the text generated by its LLMs complies with its usage policies. More specifically, the tool is able to detect hate speech, harassment, self-harm, sexual and violent content</title>
		<ptr target="https://platform.openai.com/docs/guides/moderation/overview" />
		<imprint/>
	</monogr>
	<note>The same could be developed by providers of companion chatbots to address the risks discussed in Section 3</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">has also developed a list of common trigger words used by young users when disclosing abuse or risks of harm. See: UNICEF East Asia Pacific, Gender section</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Tadesse</surname></persName>
		</author>
		<ptr target="https://www.unicef.org/documents/safer-chatbots-implementation-guide" />
	</analytic>
	<monogr>
		<title level="m">Safer Chatbots Implementation Guide&apos; (UNICEF 2023</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Report</note>
	<note>have, for instance, demonstrated the ability of word embedding to detect suicidal thoughts. See: Michael Mesfin Tadesse and others</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Succor Borne Every Minute&apos; (Federal Trade Commission Business Blog</title>
		<author>
			<persName><surname>Michael Atleson</surname></persName>
		</author>
		<ptr target="https://www.ftc.gov/business-guidance/blog/2024/06/succor-borne-every-minute" />
		<imprint>
			<date type="published" when="2024-06">June 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Not to mention the risk that a chatbot built around the motto &quot;Wanna be macho? She will be stunning!&quot; is likely to raise for &quot;natural persons&quot; and society as a whole by shaping unrealistic and sexist expectations of what one should expect from a &quot;romantic</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">For a comprehensive overview of the risks raised by LLMs and companion chatbots, I refer the reader to the following resources: Weidinger and others</title>
		<imprint>
			<date>n 27</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title/>
		<author>
			<persName><surname>Forbrukerrådet</surname></persName>
		</author>
		<imprint>
			<date>n 26</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Generating Harms -Generative AI&apos;s Impact &amp; Paths Forward&apos; (Electronic Privacy Information Center</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Leufer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Méabh</forename><surname>Maguire</surname></persName>
		</author>
		<ptr target="https://www.accessnow.org/what-you-need-to-know-about-generative-ai-and-human-rights/" />
	</analytic>
	<monogr>
		<title level="j">What You Need to Know about Generative AI and Human Rights&apos; (Access Now</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<date type="published" when="2023-05">2023. May2023. May 2023</date>
			<publisher>Grant Fergusson and others</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Report</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">not-ready-for-manipulative-ai-urgent-need-for-action/; Nathalie Smuha and others</title>
		<author>
			<persName><forename type="first">French</forename><surname>English</surname></persName>
		</author>
		<author>
			<persName><surname>Dutch</surname></persName>
		</author>
		<author>
			<persName><surname>See</surname></persName>
		</author>
		<ptr target="https://www.knack.be/nieuws/technologie/onze-samenleving-is-niet-klaar-voor-manipulatieve-ai/" />
	</analytic>
	<monogr>
		<title level="m">Nathalie Smuha and others</title>
		<imprint>
			<publisher>Nathalie Smuha and others</publisher>
			<date type="published" when="2023-03">March 2023. March 2023</date>
			<biblScope unit="volume">29</biblScope>
		</imprint>
		<respStmt>
			<orgName>KU Leuven AI Summer School Blog</orgName>
		</respStmt>
	</monogr>
	<note>Onze samenleving is niet klaar voor manipulatieve AI</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
