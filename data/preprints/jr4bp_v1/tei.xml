<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Perceptual learning and sensorimotor learning with cochlear-implant simulated speech feedback</title>
				<funder>
					<orgName type="full">Newnham College Amy Whiteley Research Fellowship</orgName>
				</funder>
				<funder ref="#_N969MVt">
					<orgName type="full">UK Medical Research Council funding of the Cognition and Brain Sciences Unit</orgName>
				</funder>
				<funder ref="#_Y6nnQYb">
					<orgName type="full">Experimental Psychology Society</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><roleName>Dr</roleName><forename type="first">Abigail</forename><forename type="middle">R</forename><surname>Bradshaw</surname></persName>
							<email>abbie.bradshaw@mrc-cbu.cam.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">MRC Cognition and Brain Sciences Unit</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<addrLine>15 Chaucer Road</addrLine>
									<postCode>CB2 7EF</postCode>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Susan</forename><surname>Black</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">MRC Cognition and Brain Sciences Unit</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<addrLine>15 Chaucer Road</addrLine>
									<postCode>CB2 7EF</postCode>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Clément</forename><surname>Gaultier</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">MRC Cognition and Brain Sciences Unit</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<addrLine>15 Chaucer Road</addrLine>
									<postCode>CB2 7EF</postCode>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Hearing Institute</orgName>
								<orgName type="institution">Institut Pasteur</orgName>
								<address>
									<addrLine>63 Rue de Charenton</addrLine>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matthew</forename><forename type="middle">H</forename><surname>Davis</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">MRC Cognition and Brain Sciences Unit</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<addrLine>15 Chaucer Road</addrLine>
									<postCode>CB2 7EF</postCode>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">MRC Cognition and Brain Sciences Unit</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<addrLine>15 Chaucer Road</addrLine>
									<postCode>CB2 7EF</postCode>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Perceptual learning and sensorimotor learning with cochlear-implant simulated speech feedback</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">43B75BE444FB162D2BC132801590FAC8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-10-21T13:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Cochlear implants (CIs) provide deaf individuals with access to auditory feedback from their own voice during production. This experiment investigated whether typical hearing participants can use CI simulated speech feedback for perceptual learning and sensorimotor control of speech. CI simulation was achieved via noise vocoding, a technique that degrades the spectral detail in a speech signal in a manner similar to a CI. 32 participants took part in the experiment. First, participants were tested on their recognition of noise vocoded sentences before and after a training task; either perception training, where participants listened to noise vocoded sentences while reading matching text; or production training, where participants read aloud sentences whilst hearing their own voice noise-vocoded in real-time. Both groups of participants then underwent a speech motor adaptation paradigm in which formants were perturbed in real-time noise vocoded speech auditory feedback. Both perception and production training tasks resulted in significant improvements in recognition of noise vocoded sentences, with no effect of training type. Speech motor adaptation however was not significant at the group level in response to the formant perturbations. This suggests that successful perceptual learning for degraded speech is not sufficient for successful sensorimotor learning with degraded auditory feedback.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Successful communication with others requires both intelligible speech production and accurate speech perception. Both of these functions are affected by deafness, limiting the ability for deaf individuals to communicate with others through the oral modality. This lack of access to speech auditory feedback (the sound of one's own voice while speaking) can be associated with atypical development and maintenance of speech articulation. Congenitally deaf infants do not show the typical developmental stage of babbling, thought to facilitate learning of sensorimotor mappings <ref type="bibr" target="#b51">(Oller &amp; Eilers, 1988)</ref>, and such individuals struggle to acquire typical speech articulation, making them less intelligible <ref type="bibr" target="#b64">(Smith, 1975)</ref>. Further, loss of hearing later in life (post-lingually) can result in subtle deterioration of speech <ref type="bibr" target="#b21">(Cowie et al., 1982;</ref><ref type="bibr" target="#b41">Lane &amp; Webster, 1991)</ref>; although there is high variability in outcomes, related to factors such as age of onset of deafness.</p><p>Cochlear implants (CIs) are a sensory prosthesis that can restore hearing to deaf individuals, resulting in substantial improvements in their speech perception abilities <ref type="bibr" target="#b5">(Boisvert et al., 2020)</ref>. This is an impressive feat given the auditory distortions introduced by a CI, resulting in a loss of fine-grained spectral and temporal information which allows for only very coarse frequency analysis compared to that achieved by the biological cochlear. Achieving such improvements in perception however involves an initial period of rehabilitation over the course of weeks and months, as individuals adapt to the distorted auditory input provided by a CI <ref type="bibr" target="#b66">(Svirsky et al., 2001;</ref><ref type="bibr" target="#b68">Tyler &amp; Summerfield, 1996)</ref>. The impact of implantation on speech production abilities has received comparatively less attention; however, studies suggest that speech becomes more intelligible, with changes in vocal pitch and loudness, and increased contrasts between consonants and separation between vowels (for a review, see <ref type="bibr" target="#b28">Gautam et al., 2019)</ref>. A study by <ref type="bibr" target="#b47">(Menard et al., 2007)</ref> found that such improvements continued to increase over the first year following implantation, suggesting gradual learning processes; however, even at one year post-implantation, better performance was found with the implant turned on compared to off, suggesting the contribution of online auditory feedback to improved speech articulation.</p><p>Indeed, online processing of auditory feedback is proposed to play an important role in speech motor control <ref type="bibr" target="#b30">(Guenther, 2016;</ref><ref type="bibr">Parrell et al., 2019;</ref><ref type="bibr" target="#b55">Parrell &amp; Houde, 2019)</ref>. This is demonstrated eloquently in the altered auditory feedback paradigm; here, real-time perturbations of acoustic features such as fundamental frequency (F0) or formants are found to result in implicit compensatory adjustments to speech productions that correct for the apparent sensory error <ref type="bibr" target="#b11">(Burnett et al., 1998;</ref><ref type="bibr" target="#b35">Houde &amp; Jordan, 1998)</ref>. Such perturbations can be implemented randomly across a series of utterances, resulting in rapid within-utterance compensatory changes <ref type="bibr" target="#b42">(Larson et al., 2007)</ref>; or held constant across a period of speaking, resulting in a gradual learning response that builds up across time and persists once the perturbation is removed (i.e. shows after-effects), known as speech motor adaptation <ref type="bibr" target="#b58">(Purcell &amp; Munhall, 2006)</ref>. This body of work demonstrates the central role of auditory feedback to allow for monitoring of the sensory consequences of speech movements online, and for sensorimotor mappings to be continually calibrated through offline updating according to prediction errors.</p><p>To date, there have been a small number of studies that have investigated responses to auditory feedback perturbations in individuals with CIs. <ref type="bibr" target="#b44">Loucks et al., (2015)</ref> investigated compensation to random perturbations of F0 in 6 CI participants and 6 typical hearing participants. Responses across trials were first classified into compensatory or following, according to their direction of change (in the opposing or same direction as the perturbation, respectively). Interestingly, CI participants showed a larger magnitude of change for both compensatory and following responses compared to typical hearing participants, suggesting an atypical pattern of responses to perturbations. The separation of responses by direction however makes it difficult to conclude whether overall CI participants were successful in compensating for the perturbations. A more recent study by <ref type="bibr" target="#b27">Gautam et al., (2020)</ref> reported that CI participants showed significant within-utterance compensatory responses to random F0 perturbations (without excluding any responses on the basis of their direction), but only when these perturbations were sufficiently large (more than 600 cents, as opposed to the more typical perturbation magnitude of 200 cents used in <ref type="bibr">Loucks et al.)</ref>. This study was further unusual in the high proportion of trials including a perturbation (70-80% as opposed to a typical proportion of 40-50%).</p><p>Overall, inter-subject variability in responses was high, and weakly correlated with duration of implant use. <ref type="bibr" target="#b6">Borjigin et al., (2024)</ref> investigated adaptation to sustained formant perturbations in CI participants, using a paradigm in which the magnitude of the perturbation was adaptively ramped up until a participant no longer showed further changes in formants; the perturbation was then held constant at that magnitude for a 'hold' phase. Adaptation in the CI group reached significance during the adaptive ramp phase of the experiment, but ceased to be significant during the hold phase. Compared to a group of typical hearing participants (speaking with perturbed clear speech feedback), CI participants showed reduced adaptation rate and magnitude, as well as smaller after-effects of learning.</p><p>Overall, the inconsistency in results across studies in this small body of work is likely a result of the fact that samples are often small in size (due to difficulties in recruiting from this population), and highly heterogeneous (e.g. in age of onset of deafness, time since CI implantation etc), making it difficult to draw conclusions about the effectiveness of CI speech auditory feedback in supporting speech motor control. In samples of CI users, it is also difficult to disentangle the effects of the degraded CI speech feedback from the effects of the experience of deafness itself; that is, weaker responses to auditory feedback perturbations in CI users could either reflect the insufficiency of degraded CI feedback for supporting compensation/adaptation, or the effects of a period of deprivation of auditory feedback caused by deafness prior to implantation (especially if this occurred early in life in a critical period for speech development).</p><p>An alternative approach is to use simulation of CI speech to test the effects of such an altered signal on speech in typical hearing participants. This allows for the recruitment of larger, more homogeneous samples, and for the isolation of the effects of degraded speech auditory feedback specifically whilst controlling for previous hearing experience. Simulation of the signal processing implemented by a CI can be achieved using a technique known as vocoding <ref type="bibr" target="#b60">(Shannon et al., 1995)</ref>. This implements a degradation of the spectral content of speech similar to that introduced by a CI. This has been used extensively to investigate the effects of such degradation on passive speech perception, by applying such a technique to offline recordings of speech <ref type="bibr" target="#b22">(Davis et al., 2005;</ref><ref type="bibr" target="#b34">Hervais-Adelman et al., 2008;</ref><ref type="bibr" target="#b65">Sohoglu et al., 2014)</ref>. By contrast, only a small number of studies have looked at the effects of a real-time simulation of hearing one's own speech auditory feedback through a CI <ref type="bibr" target="#b14">(Casserly, 2015;</ref><ref type="bibr" target="#b17">Casserly et al., 2018;</ref><ref type="bibr" target="#b15">Casserly &amp; Marino, 2024)</ref>, and none have combined this with a perturbation of formants or F0 to assess compensatory behaviour or speech motor learning. The present study aimed to use real-time vocoding of speech auditory feedback with typical hearing participants, to test whether they show speech motor adaptation to a formant perturbation under CI simulated speech feedback.</p><p>A second aim of the current study was to investigate potential interactions between experience of speaking with CI simulated speech feedback and subsequent perceptual performance when passively listening to recordings of CI simulated speech. There is a large literature documenting interactions between speech production and perception (e.g. <ref type="bibr" target="#b10">Bradshaw et al., 2024;</ref><ref type="bibr" target="#b50">Murphy et al., 2023;</ref><ref type="bibr" target="#b53">Pardo et al., 2022;</ref><ref type="bibr" target="#b63">Skipper et al., 2017)</ref>. Within CI populations, there is evidence that speech production abilities in childhood are significantly correlated with receptive speech and language skills both at the same time point <ref type="bibr" target="#b2">(Blamey et al., 2001)</ref>, and at future time points <ref type="bibr" target="#b16">(Casserly &amp; Pisoni, 2013)</ref>. Gains in speech intelligibility with implantation may therefore result in gains in perceptual abilities. Experience of speaking with CI speech auditory feedback may provide input that can support the process of perceptual learning to facilitate better recognition of speech through the degraded input provided by a CI.</p><p>Previous work with typical hearing participants has shown that recognition accuracy for vocoded speech improves simply with repeated exposure to it, attributed to a process of perceptual learning <ref type="bibr" target="#b22">(Davis et al., 2005;</ref><ref type="bibr" target="#b59">Rosen et al., 1999)</ref>. <ref type="bibr" target="#b22">Davis et al., (2005)</ref> further demonstrated that such an improvement can be accelerated when listeners have prior knowledge of the content of the distorted speech; for example through provision of the written form prior to hearing the distorted speech <ref type="bibr" target="#b22">(Davis et al., 2005)</ref>. Such written cues result in an immediate perceptual pop-out effect, in which previously unintelligible sentences become highly intelligible; additionally, they also lead to improved recognition for subsequent novel vocoded material when written cues are then removed, suggesting top-down facilitation of perceptual learning processes. Experience of speaking with vocoded speech feedback may provide a similar means of facilitating perceptual learning, via provision of prior knowledge and employment of forward modelling processes. Indeed, such experience may be expected to confer a larger facilitation of perceptual learning; for example, due to more optimal temporal integration of prior knowledge of the lexical content of speech with the incoming distorted signal. This experiment therefore additionally aimed to investigate whether experience of speaking with real-time vocoded speech feedback resulted in significant improvements in subsequent recognition of vocoded speech, and whether this was over and above improvements observed with written cues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>The design, hypotheses and analyses for this experiment were pre-registered prior to collection of data on the Open Science Framework (https://osf.io/9rf68).</p><p>Pseudonymised data and analysis code are also available on this platform (https://osf.io/3xj5e/).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>In total 35 typical-hearing participants (30 female, 4 male, 1 prefer not to say), aged between 18-40 years (mean age = 23.7, SD = 5.28) and with no reported history of speech, language, or reading difficulties, took part in this experiment. Data from two participants was excluded as on testing it was found that English was not their first language. A further participant was excluded due to issues with their audio recordings. The remaining 32 participants included in the final analysis all reported their first language as English, and all but one spoke British English (the remaining participant spoke American English).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure and Apparatus</head><p>The experiment was divided into two phases (see Figure <ref type="figure" target="#fig_0">1</ref>). Phase 1 was a vocoded speech recognition task, which tested participants' ability to report words in noise vocoded sentences before and after a training phase. Phase 2 was a speech sensorimotor learning task that tested participants' ability to adapt to a formant perturbation with real-time vocoded speech auditory feedback. Both phases were completed in a single experimental session lasting around one hour. Each participant was seated in front of a computer screen and keyboard in a sound proof booth.</p><p>During both phases, participants wore a headset microphone (Shure WH20) at a distance of approximately 5cm from their mouth, and circumaural headphones (Beyerdynamic DT 770 PRO 80 Ohm). In phase 1, participants undertook a vocoded speech recognition test before and after a training task (pre-and post-tests). In each of these tests, participants listened to a series of 10 vocoded sentences through headphones, and after each were instructed to type out as much of the sentence as they could. Participants were randomly assigned to one of two training conditions; a perceptual training task and a production training task (16 participants per condition). For the perceptual training task, participants again listened to a series of vocoded sentences, but concurrent with the audio each sentence was also presented in written form on the screen.</p><p>Participants were instructed to silently read the sentence while listening to it. For the production training task, participants instead saw each written sentence on the screen, and were instructed to read it aloud whilst hearing their voice vocoded in real-time through headphones. Each training task consisted of 20 trials. To assess participants' recognition of noise vocoded speech in the pre-and post-tests, a metric known as the token sort ratio was calculated using an online assessment tool <ref type="bibr" target="#b7">(Bosker, 2021)</ref>. This measure computes the orthographic similarity between two strings (here, the target sentence and the participant's response), assigning a score from 0 to 100. Briefly, words in the target sentence and the response are first sorted alphabetically, before a ratio is calculated to reflect the extent of shared substrings between the two. This method is more robust against the effects of misspellings than other automated methods, and has been shown to correlate highly with manual human scoring <ref type="bibr" target="#b7">(Bosker, 2021)</ref>.</p><p>In phase 2, both groups of participants from phase 1 underwent a speech motor adaptation paradigm. In this task, participants read sentences aloud whilst hearing their voice played back to them in real-time through headphones, with various manipulations at different stages of the task. Each trial began with visual presentation of the sentence to be read, which remained on screen for 4.5 seconds, with a 1 second inter-trial interval. Participants were instructed to read the sentence as soon as it appeared onscreen. Participants first underwent 10 practice trials with clear speech auditory feedback to familiarise themselves with the task, and practice speaking at a suitably loud level. This was aided by the use of visual feedback from an LED light display, calibrated such that a speaking level of 65-75dB (LAeq 1s) activated green lights, with speech louder or quieter activating red and yellow lights respectively. The level of speech playback through the headphones varied dynamically with changes in the amplitude of the participant's voice, but was calibrated such that the playback was amplified by about 5dB relative to their speaking level. In addition, during this task masking pink noise was played through headphones at 65dB; together with the amplification of speech feedback, this aims to mask perception of the participant's natural (unaltered) voice.</p><p>The main task was made up of 7 blocks of 50 trials, with the same set of 50 sentences presented in a random order in each block. For block 1, participants heard their voice played back through headphones without any manipulation (no vocoding or formant perturbation). From block 2 onwards, participants heard their voice vocoded (see stimuli for details). A perturbation of the first and second formants was gradually introduced in block 3 (ramp phase), ramping up over the first 25 trials of this block before being held constant for a further 3 blocks (blocks 4-6, hold phase).</p><p>The perturbation was then removed for the final block (block 7, after-effect phase).</p><p>The direction of this formant perturbation was manipulated between-subjects, to be either an upward shift of F1 and downward shift of F2, or a downward shift of F1 and upward shift of F2. For both groups, F1 and F2 were perturbed by 49.5 mels each (in their respective directions), resulting in a combined perturbation of 70 mels in F1/F2 space. The assignment of participants to each perturbation condition was counterbalanced with respect to their assignment to training groups in Phase 1 of the experiment.</p><p>The formant perturbation was implemented by an openly available MATLAB-based software application, Audapter <ref type="bibr" target="#b12">(Cai, 2015;</ref><ref type="bibr" target="#b13">Cai et al., 2008)</ref>. Recording, real-time processing and playback of speech was achieved using an audio interface (RME Fireface) and a mixer (Behringer). Speech was recorded at a sampling rate of 48 kHz (down-sampled to 16 kHz) with a buffer size of 96 samples. The total feedback loop latency of the set-up was measured at 20ms, according to the methods outlined in <ref type="bibr" target="#b37">(Kim et al., 2020)</ref>. This latency is well below the delay levels previously reported to disrupt speech adaptation <ref type="bibr" target="#b46">(Max &amp; Maffett, 2015;</ref><ref type="bibr" target="#b62">Shiller et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stimuli</head><p>All sentence stimuli for both phases of the experiment were taken from the Harvard IEEE corpus of sentences (IEEE Subcommittee on Subjective Measurements, 1969). The vocoded stimuli for phase 1 were created using recordings of a female speaker of standard Southern British English reading a set of 40 of these sentences.</p><p>From these 40, two sets of 10 sentences were used for the pre-and post-test tasks, with the order of these sets being counterbalanced across participants. The remaining 20 sentences were used for both the production and perception training tasks (with only the written form used for production training). A further set of 50 sentences were selected from this corpus for use as written stimuli for prompting speech productions in Phase 2. The same 50 sentences were repeated across blocks in a random order.</p><p>Vocoding of speech recordings for use in Phase 1 and of real-time speech in Phase 2 was achieved using a modified version of the software application Audapter <ref type="bibr" target="#b12">(Cai, 2015;</ref><ref type="bibr" target="#b13">Cai et al., 2008)</ref>. This software is designed to perform online modifications to speech in real-time (as in Phase 2 of this experiment), but can also be applied to pre-collected speech recordings 'offline', by feeding in the pre-recorded speech signal frame by frame. Our modified version incorporates custom code to implement spectral degradation of speech via noise vocoding as follows. First, a fast Fourier transform of the frame of speech is taken, and a noise signal of equal frame length to the speech is generated. A moving average is then computed over frequency on the phase and magnitude of the speech signal separately, using a user specified window size. Larger window sizes will result in averaging over wider frequency ranges (akin to averaging values across a larger number of pixels in blurring an image), resulting in a more degraded signal. Varying this parameter can be considered conceptually similar to varying the number of channels in a cochlear implant. The resulting subsampled speech Fourier transform is then multiplied by the noise Fourier transform (by multiplying magnitudes and adding phases). The real and imaginary parts are then recombined, before an inverse Fourier transform is performed to generate a degraded signal for speech feedback.</p><p>The value chosen for the window size parameter for both phases of the experiment (i.e. for vocoding of both pre-recorded stimuli and real-time speech) was roughly equivalent to the spectral degradation introduced by an 8 channel vocoder. This level was chosen based on an online pilot study (n = 10) which ran the perceptual training condition of phase 1; this found an average score (token sort ratio) of 72% for reporting words in vocoded sentences in the pre-test phase, comparable to findings on word report for sentences in quiet with CI participants (74%) as reviewed in <ref type="bibr" target="#b5">(Boisvert et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acoustic analysis</head><p>A custom Praat <ref type="bibr" target="#b4">(Boersma &amp; Weenink, 2021)</ref> script was used to track formants in speech recordings collected from phase 2 of the experiment. This first isolates the vocalised portions of speech using Praat's autocorrelation method <ref type="bibr" target="#b3">(Boersma, 1993)</ref>, before extracting F1 and F2 values in Hertz using a Linear Predictive Coding approach. These were averaged across each sentence by taking the mean, and then converted into mels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantification of adaptation</head><p>Adaptation to the formant perturbation was measured for each participant via two outcome variables: a production change measure and a vector of adaptation measure. Both rely on comparisons of formant frequencies between block 2 (the baseline block with vocoded speech feedback immediately prior to the introduction of the formant perturbation) and the subsequent blocks, with a particular focus on block 6 (the final block with altered feedback).</p><p>Firstly, a production change measure was calculated for F1 and F2 separately in which each of the 50 produced formant frequencies in block 6 (the final adaptation block) was normed to the F1/F2 frequencies for block 1 (baseline) on a sentence-bysentence basis; the average of these values was then taken to give an average production change value for each participant (for each formant).</p><p>Secondly, a vector of adaptation measure was calculated, which quantifies the extent to which these changes in produced F1 and F2 values directly counter the direction of the feedback perturbation in F1-F2 space. To do this, firstly the inverse of the vector representing the feedback shift in F1-F2 space experienced by that participant was found; this vector represents perfect compensation to the feedback perturbation. The angular difference between this inverse shift vector and a vector representing the participant's production change (relative to block 2) was then calculated; the cosine of this difference was then multiplied by the magnitude of production change. This vector of adaptation thus quantifies the degree to which the observed change in produced formants (i.e. the production change measure above) precisely opposed the feedback perturbation. This measure was calculated for each individual trial and then averaged within each block after the introduction of the feedback perturbation (blocks 3-7). Note that since this measure takes into account the direction of the formant perturbation experienced by the participant, positive values for both perturbation conditions represent opposing responses, while negative values indicate a following response (moving in the same direction as the perturbation).</p><p>Our pre-registered exclusion criteria were that whole datasets from a participant would be excluded if they made significant speech errors on more than 20% of trials in either block 2 or block 6 (the main blocks of interest for measuring adaptation). No participants exceeded this criterion, and so all were kept in our analyses. Two participants did not contribute data for block 7 of the adaptation task due to a technical issue with the experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hypotheses</head><p>For phase 1, we predicted that improvement in recognition of noise vocoded speech from pre-to post-training would be significantly greater in the production training group compared to the perception training group. For phase 2, we predicted that both perturbation groups would show significant adaptation, by moving their produced formant frequencies in an opposite direction to the direction of the perturbation they experienced.</p><p>All statistical analyses reported here were pre-registered, unless labelled as exploratory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Phase 1: Perceptual learning Accuracy (token sort ratio scores) for reporting words in noise vocoded sentences before (pre-test) and after (post-test) training are plotted in Figure <ref type="figure" target="#fig_1">2</ref> for each training group. To test whether improvement in recognition accuracy was greater in the production training group than the perception training group, a linear mixed effects model was run on token sort ratio scores using the lmerTest package in R <ref type="bibr" target="#b38">(Kuznetsova et al., 2017)</ref>. Two random intercept models were run with fixed effects of phase (pre-test and post-test) and training group (perception and production); one in which these had additive effects, and one in which they had interactive effects.</p><p>Both models also included random intercepts of participant and target sentence.</p><p>Random slopes were not included, since this resulted in singular fit. A likelihood ratio test found that the interactive model did not provide a better fit to the data than the additive model (χ 2 (1) = 0.049, p = .826). The additive model found a significant effect of phase, with greater accuracy at post-test compared to pre-test (β = -9.65, t(588) = -9.06, p &lt; .001), but no significant effect of group. Follow-up contrasts using estimated marginal means found that both groups showed a significant increase in accuracy from pre-test to post-test (p &lt; .001 in both cases). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Phase 2: Sensorimotor learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline changes: Effect of noise vocoding on speech</head><p>To test if the experience of speaking with real-time noise vocoded feedback (prior to introduction of the formant perturbation) had an effect on participants' produced formants, participants' average F1 and F2 values were compared across block 1 and block 2 by means of paired-samples t-tests. These changes are illustrated in Figure <ref type="figure" target="#fig_2">3</ref>. Across all participants, there was a significant increase in both F1 (t(1599) = 13.59, p &lt; .001) and F2 (t(1599) = 1.9982, p = .046) from block 1 to block 2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adaptation changes: Effect of formant perturbation on speech</head><p>Changes in produced formants from noise-vocoded baseline (block 2) to the final block with formant perturbed noise vocoded feedback (block 6) are illustrated in Figure <ref type="figure" target="#fig_3">4</ref>. To test the significance of adaptation at the group level, two-sided onesample t tests were used on F1 and F2 production changes from Block 2 to Block 6 in each perturbation group. These found no significant changes in either F1 or F2 in either group. In an exploratory analysis, we further tested whether formant changes in block 6 were significantly different between the perturbation direction groups. We ran an LMM analysis on F1 and F2 production changes from block 2 to block 6, with fixed effects of perturbation direction group and formant, and random effects of sentence and participant. Random slopes were not included due to singular fit. A likelihood ratio test found a model with an interaction between group and formant provided a better fit to the data than a model in which these had additive effects (χ 2 (1) = 8.36, p = .004). Follow-up contrasts with this interactive model found a significant group difference for F1, in which F1 changes were significantly lower for the F1+F2-group than the F1-F2+ group (β = -5.95, t(47.5) = -2.19, p = .034). No significant group difference was found for F2 changes. To quantify to what extent changes in produced formants directly opposed the direction of the perturbation, a vector of adaptation measure was calculated. This is plotted for blocks 3 to 7 in Figure <ref type="figure" target="#fig_4">5</ref>. To test whether each participant showed significant adaptation, we ran two-sided one-sample t-tests on each participant's vector of adaptation values from block 6, to classify participants as showing either a significant adaptation response (adaptation significantly greater than zero), a significant following response (adaptation significantly lower than zero), or no significant change. The number of participants in each of these categories is shown for the two groups in Table <ref type="table">1</ref>.  To investigate how adaptation changes across the blocks of altered feedback, a linear mixed effects model analysis was run on the vector of adaptation measure from blocks 3 to 7. We compared two models; one with a fixed effect of block (3-7) and random intercepts of sentence and participant; and an identical one with the addition of a fixed effect of perturbation direction group (F1+F2-or F1-F2+). Random slopes were not included due to failures of model convergence. A likelihood ratio test found the model containing fixed effects of block and group did not provide a better fit to the data than the model containing a fixed effect of block only (χ 2 (1) = 0.058, p = .809), supporting our prediction that this measure of adaptation would not be significantly different between the two groups (since this measure takes into account the direction of the perturbation experienced). Follow-up contrasts with the model containing a fixed effect of block found no significant differences between any of the blocks (p &gt; .2 in all cases, using the Tukey method for adjusting for multiple comparisons). This suggests that there was no build up of adaptation across the blocks of formant perturbed feedback.  frequencies, compared to speaking with clear speech feedback. However, when a formant perturbation was subsequently introduced to this noise vocoded speech feedback, we found no evidence of changes to formants indicative of speech motor adaptation. Overall therefore, the results suggest that successful perceptual learning for degraded speech is not sufficient for successful sensorimotor learning when that same degradation is applied to real-time speech auditory feedback during production.</p><p>In Phase 1, contrary to our predictions, the production training task was not associated with a significantly greater improvement in recognition of noise vocoded sentences compared to the perception training task. One concern could be the potential for ceiling effects at post-test, which may have limited the opportunity for further improvements in recognition. The level of noise vocoding used was chosen based on an online pilot experiment, in which average accuracy in the pre-test was found to be 72%; this is comparable to accuracy for sentence report in quiet in CI participants <ref type="bibr" target="#b5">(Boisvert et al., 2020)</ref>. Average accuracy at pre-test in our in-person study was found to be higher than this (around 86% for both groups), meaning that average accuracy at post-test approached (but did not reach) 100% (around 96% in both groups). Even so, the range of scores in our sample does not suggest that all participants were at ceiling at the post-test (ranging from 90.2% to 100%).</p><p>Nevertheless, this study demonstrates that experience of speaking with noise vocoded feedback is at least as effective at enhancing perceptual learning as the established effect of prior lexical knowledge during passive listening <ref type="bibr" target="#b22">(Davis et al., 2005)</ref>. Experience of speaking with CI simulated speech auditory feedback can therefore lead to gains in passive perception of such degraded input. This suggests that, for those CI individuals who wish to communicate in the oral modality, increased experience of speaking and hearing one's voice through a CI is likely to lead to improvements in passive perception of speech produced by others.</p><p>It is further interesting to consider a potential counter-argument to our original prediction, that instead we might have expected to see reduced perceptual learning during speech production due to speech-induced suppression; the phenomenon in which auditory input that is self-produced is perceptually and neurally suppressed relative to when that same auditory input is passively listened to <ref type="bibr" target="#b33">(Heinks-Maldonado et al., 2005;</ref><ref type="bibr" target="#b48">Merrikhi et al., 2018;</ref><ref type="bibr" target="#b52">Ozker et al., 2024)</ref>. Such suppression could in theory have hindered perceptual learning; instead, the robustness of the effect we observed with production training suggests that speech auditory feedback during production is sufficiently strongly represented and processed to support enhanced perceptual learning. It should be noted however that while auditory input doesn't have to necessarily match the expected sensory consequences of a speech movement to be suppressed (with neural responses to tones presented during a period of speech motor planning showing perceptual and neural suppression <ref type="bibr" target="#b45">(Max &amp; Daliri, 2019;</ref><ref type="bibr" target="#b48">Merrikhi et al., 2018)</ref>), suppression is attenuated when speaking with altered auditory feedback <ref type="bibr" target="#b1">(Behroozmand et al., 2015;</ref><ref type="bibr" target="#b20">Chang et al., 2013;</ref><ref type="bibr" target="#b67">Tourville et al., 2008)</ref>. The unexpected noise vocoded feedback used here is thus likely to be associated with less suppression.</p><p>The observation of increases in produced F1 and F2 with noise vocoded feedback in the baseline blocks of Phase 2 is consistent with previously reported acoustic changes associated with Lombard effects; the tendency for speakers to involuntarily increase their vocal effort when speaking in noise <ref type="bibr" target="#b40">(Lane &amp; Tranel, 1971)</ref>. Masking noise was present throughout both clear and noise vocoded baseline blocks (and for the rest of the adaptation task); however, the switch to noise vocoded speech feedback at baseline block 2 introduces an additional form of 'noise masking', which may have further exacerbated Lombard effects. Previous work has reported increases in F1 and F2 when speaking with noise masking <ref type="bibr" target="#b18">(Castellanos et al., 1996;</ref><ref type="bibr" target="#b69">Van Summers et al., 1988)</ref>. A previous study by <ref type="bibr" target="#b14">Casserly (2015)</ref> looking at the effect of noise vocoded speech auditory feedback on word production in typical hearing participants similarly reported changes in produced F1; however, effects were vowel specific and consistent with a collapsing of vowel height contrast, rather than a global increase. This discrepancy may be due to the use of connected sentence stimuli in the current study, as opposed to isolated words.</p><p>Interestingly, formants did not appear to continue to increase across the remainder of the experiment after introduction of the formant perturbation. However, neither did they show the expected changes in opposition to the direction of the formant perturbations that would be indicative of adaptation. An exploratory analysis did find a significant difference between the perturbation direction groups in F1 changes, in which the group who experienced a downward shift of F1 showed greater increases in F1 than the group who experienced an upward shift of F1; a pattern consistent with adaptation. If the formant perturbation had no effect on production, we would expect no group differences according to the direction of the perturbation. This observed group difference is therefore interesting; however, it is difficult to interpret in the context of no significant changes in formants from baseline in either group.</p><p>Across both groups, only 6 out of 32 participants showed evidence of a significant adaptation response, with nearly as many ( <ref type="formula">5</ref>) showing a following response; that is, moving their formants in the same direction as the perturbation.</p><p>Following responses have generated much discussion in the speech sensory perturbation literature, and their underlying mechanism, as well as whether to exclude them from group-level analyses, remains debated. They have been attributed to an externalisation of the altered auditory feedback as not being selfgenerated (thus acting as an external referent to be matched rather than an error to be corrected) <ref type="bibr" target="#b26">(Franken et al., 2023;</ref><ref type="bibr" target="#b32">Hain et al., 2000;</ref><ref type="bibr" target="#b57">Patel et al., 2014)</ref>; a result of the state of the speech production system at perturbation onset <ref type="bibr">(Franken, Acheson, et al., 2018)</ref>; or simply the tail end of a unimodal distribution <ref type="bibr" target="#b49">(Miller et al., 2023)</ref>. In <ref type="bibr" target="#b49">Miller et al (2023)</ref>, data from across 22 studies of altered auditory feedback during single word production was pooled, and statistical tests performed to establish whether the distribution of compensation responses was unimodal or bimodal. This found clear evidence of a unimodal distribution, suggesting that such following responses do not represent a qualitatively different response type, but simply the tail end of a unimodal distribution. Sentence level adaptation tasks appear to be associated with both fewer following responses, and adaptation responses of greater magnitude than those observed during word production <ref type="bibr" target="#b8">(Bradshaw et al., 2023;</ref><ref type="bibr" target="#b39">Lametti et al., 2018;</ref><ref type="bibr" target="#b61">Shiller et al., 2023)</ref>. This would thus be consistent with the idea of a unimodal distribution shifted in the positive direction (meaning that fewer observations end up on the wrong side of zero). While it is therefore tempting to interpret the increased incidence of following responses in the present study as indicative of engagement in a qualitatively different sensorimotor process during speaking with noise vocoded feedback, it is perhaps more likely that these simply reflect the left-hand tail of a unimodal distribution centred on zero.</p><p>There are multiple possible explanations for why we didn't observe significant adaptation at the group level with noise vocoded feedback in this study. One possibility is that participants did not perceive the noise vocoded speech feedback as self-generated, and so did not engage self-monitoring processes that support the detection and correction of sensory errors <ref type="bibr" target="#b26">(Franken et al., 2023)</ref>. Noise vocoded speech is a highly alien signal, that does not plausibly sound as if it can be generated by a human vocal tract. Sense of agency over a voice does appear to be surprisingly flexible, being maintained even in the face of dramatic alterations of speech auditory feedback, such as shifting its pitch by an octave <ref type="bibr" target="#b25">(Franken et al., 2021)</ref>, replacing it with another voice <ref type="bibr" target="#b71">(Zheng et al., 2011)</ref>, or even replacing it with one's own voice speaking a different word <ref type="bibr" target="#b43">(Lind et al., 2014)</ref>. While the noise vocoded feedback was congruent in its timing and content with the participant's natural voice, the degradation strips the acoustic signal of many cues to vocal identity, making it near impossible to determine speaker identity or even gender. It is possible therefore that this caused source-monitoring processes to reject the speech feedback as self-generated, resulting in reduced compensation to formant perturbations. If this was the case however, we might have expected to see a significant following response at the group level; perception of other voices typically engages phonetic convergence processes, in which we (unconsciously) adjust our voice to become more similar to that of the other speaker <ref type="bibr" target="#b0">(Aubanel &amp; Nguyen, 2020;</ref><ref type="bibr" target="#b9">Bradshaw &amp; McGettigan, 2021;</ref><ref type="bibr" target="#b54">Pardo et al., 2017)</ref>. Such convergence has been observed even when interacting with an artificial agent with a synthetic voice in human-computer interactions <ref type="bibr" target="#b29">(Gessinger et al., 2021)</ref>; however, it is possible that the noise vocoded speech used in the present study was too artificial to be recognised as another speaker.</p><p>An alternative possibility is that the degradation achieved by the noise vocoding process meant that the brain could not perceive or detect the formant perturbation.</p><p>This should not be confused with explicit detection; the magnitude of formant perturbations typically used in altered auditory feedback experiments with clear speech are usually not detected by participants, and studies with pitch perturbations suggest that participants compensate regardless of whether they are explicitly aware of the perturbation or not <ref type="bibr">(Franken, Eisner, et al., 2018;</ref><ref type="bibr" target="#b31">Hafke, 2008)</ref>. There is evidence to suggest however that better auditory discrimination of formant shifts during passive perception is associated with greater adaptation to formant perturbations during production <ref type="bibr" target="#b70">(Villacorta et al., 2007)</ref>. The magnitude of the perturbation of formants used in the present study was relatively small (70 mel change across both F1 and F2); the coarse frequency information available in the degraded auditory feedback may therefore have simply made it too difficult for the brain (implicitly or otherwise) to detect this change. The results from Phase 1 suggests that these stimuli were highly intelligible to participants, with vowels being clearly perceived; however, it is possible that a larger formant perturbation may have been required to elicit adaptation with this level of degradation.</p><p>These results with CI simulated speech in typical hearing participants are somewhat discrepant with previous findings on speech motor adaptation in CI users. <ref type="bibr" target="#b6">Borjigin et al., (2024)</ref> measured CI participant's adaptation to a perturbation of F1, and found evidence of significant adaptation at the group level. However, this adaptation was smaller in magnitude compared to that shown by typical hearing participants (with clear speech feedback), and was only significant during an adaptive ramp phase in which the magnitude of the perturbation was gradually increased in a manner sensitive to participant's adaptation behaviour. This adaptive ramp procedure resulted in a perturbation magnitude of more than 171Hz (218.82 mels), significantly greater than the perturbation magnitude used in the current study (joint 70 mel shift across F1 and F2). Unlike the typical hearing participants however, adaptation then ceased to be significant in the CI group during a hold phase, in which the magnitude of the perturbation was held constant at the maximum level at which participants stopped showing further changes. This finding appears more in line with the results found here, of no significant adaptation during a hold phase in typical hearing participants with CI simulated auditory feedback.</p><p>Taken together, the findings of ours and Borjigin et al's studies suggest that adaptation with CI feedback may only be possible when using an adaptive ramp procedure, and/or when employing a relatively large perturbation. It is also worth highlighting that while Borjigin et al., used single word production (with only one word stimulus of "Ed"), this study employed perturbations during variable sentence-level speech. While the existing literature with sentence-level adaptation tasks suggests that they may be associated with greater adaptation than word production tasks <ref type="bibr" target="#b8">(Bradshaw et al., 2023;</ref><ref type="bibr" target="#b39">Lametti et al., 2018;</ref><ref type="bibr" target="#b61">Shiller et al., 2023)</ref>, it is possible that for degraded speech, repeated production and experience of perturbed feedback for a single vowel sound may have been more effective at inducing adaptation, perhaps by allowing the perturbation to be more easily perceived.</p><p>It is also possible that longer exposure to the degraded speech feedback may be required for successful speech motor adaptation. However, the CI users in Borjigin et ensure that any failure to observe adaptation could not be attributed to participants' perception of their unaltered natural voice. It is interesting to note that the study by <ref type="bibr" target="#b6">Borjigin et al., (2024)</ref> also used masking noise with both CI and typical hearing participants; thus, the use of masking noise does not seem to completely preclude the observation of significant adaptation with degraded speech. Future studies using auditory perturbations with CI simulated speech could explore the use of alternative maskers with more distinct frequency profiles to noise vocoded speech, such as multi-talker babble.</p><p>Overall, the results from this study suggest that successful perceptual learning with CI simulated speech can be seen in the absence of sensorimotor learning when that same CI simulation is applied to real-time speech auditory feedback during production in typical hearing participants. A larger perturbation or longer exposure to CI feedback may be needed to observe sensorimotor learning in this context. This study has further demonstrated that experience of speaking with degraded speech feedback can enhance perceptual learning for understanding that same degraded speech during passive perception, highlighting the potential for engagement in speech production to improve speech perception in CI users who wish to communicate orally. Overall, the use of real-time CI simulated speech auditory feedback offers the potential for more controlled investigation of speech motor control when speaking with degraded auditory feedback, that can have translational implications for speech in CI users.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Schematic illustration of the design of the experiment.</figDesc><graphic coords="9,47.95,442.74,520.25,152.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Accuracy (token sort ratio scores) for reporting words in noise vocoded</figDesc><graphic coords="17,64.35,97.90,451.30,225.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Changes in produced formants (in mels) from block 1 to block 2, reflecting</figDesc><graphic coords="18,72.00,72.00,451.30,452.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Changes in produced formants from block 2 (baseline) to block 6 (final</figDesc><graphic coords="20,72.00,76.00,451.35,466.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Vector of adaptation measure plotted across blocks 3-7 of the experiment,</figDesc><graphic coords="21,76.65,325.74,451.20,300.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Table 1 :</head><label>1</label><figDesc>Frequency of participants showing adaptation, following or no change in formants in the two perturbation groups.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>training group in Phase 1 affected the magnitude of adaptation observed in Phase 2, an exploratory analysis was run to compare adaptation between production and perception training groups (collapsing across perturbation direction groups, see Figure6). Those who experienced the production training task in Phase 1 had more exposure to speaking with noise vocoded feedback (an extra 20 sentences), potentially facilitating sensorimotor learning with this novel feedback.The adaptation vector measure was between groups for block 6 (the final block of altered feedback) by means of an independent-samples t-test. Despite a trend suggesting greater adaptation in the production training group, this group difference was not significant (t(26.71) = -1.74, p = .093). Changes in F1 and F2 at baseline from block 1 to block 2 further did not significantly differ according to which training condition participants completed in Phase 1 (F1: t(27.38) = -1.46, p = .157, F2: t(29.95) = -0.69, p = .49).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Vector of adaptation measure plotted across blocks 3-7 of the experiment,</figDesc><graphic coords="23,81.35,381.33,440.75,293.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>al. had between 5 and 55 years of experience of CI use, in sharp contrast to the minutes of exposure to CI simulated speech experienced by our typical hearing participants. This suggests that, even with prolonged experience of hearing one's voice through a CI, sensorimotor learning with this degraded speech auditory feedback remains atypical in deaf individuals. It would be of interest to study changes across time following implantation on measures of sense of agency over speech auditory feedback, speech intelligibility and sensorimotor learning, to see how these may change and interact with one other with increasing experience of this new degraded self-voice. This could be complemented by studies employing CI simulation with typical hearing participants over longer timescales than those employed in the present study (e.g. exposure across days or weeks), to try to tease apart the effects of degraded speech feedback from experience of deafness.One further point worth highlighting in the present study is the use of background masking noise during speech production blocks in Phase 2 of the experiment. The use of masking noise is a well-established practice in altered auditory feedback experiments (for a review, see<ref type="bibr" target="#b19">Caudrelier &amp; Rochet-Capellan, 2019)</ref>, and aims to mask any air-conduction of the participant's natural (unaltered) voice. This is important, as perception of the unaltered voice could restrict sensorimotor learning with the altered feedback heard through the headphones. When used with clear speech altered feedback, there is clear separation between the two signals; however, it is possible that such masking noise may have resulted in greater masking of the noise vocoded feedback itself, due to greater overlap in the frequency profile of the two signals. We chose to nevertheless include noise masking, for consistency with established conventions in previous literature and to</figDesc></figure>
		</body>
		<back>

			<div type="funding">
<div><head>Funding</head><p>This work was funded by an <rs type="funder">Experimental Psychology Society</rs> <rs type="grantName">Small Grant</rs> and a <rs type="funder">Newnham College Amy Whiteley Research Fellowship</rs> (both awarded to <rs type="person">Abigail Bradshaw</rs>). <rs type="person">Matthew Davis</rs> was supported by <rs type="funder">UK Medical Research Council funding of the Cognition and Brain Sciences Unit</rs> (<rs type="grantNumber">MC_UU_00030/6</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Y6nnQYb">
					<orgName type="grant-name">Small Grant</orgName>
				</org>
				<org type="funding" xml:id="_N969MVt">
					<idno type="grant-number">MC_UU_00030/6</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Speaking to a common tune: Between-speaker convergence in voice fundamental frequency in a joint speech production task</title>
		<author>
			<persName><forename type="first">V</forename><surname>Aubanel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nguyen</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0232209</idno>
		<ptr target="https://doi.org/10.1371/journal.pone.0232209" />
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sensory-motor networks involved in speech production and motor control: An fMRI study</title>
		<author>
			<persName><forename type="first">R</forename><surname>Behroozmand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shebek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Oya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Robin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D W</forename><surname>Greenlee</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuroimage.2015.01.040</idno>
		<ptr target="https://doi.org/10.1016/j.neuroimage.2015.01.040" />
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="418" to="428" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Relationships among speech perception, production, language, hearing loss, and age in children with impaired hearing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Blamey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sarant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Paatsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Barry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Psarros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rattigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tooher</surname></persName>
		</author>
		<idno type="DOI">10.1044/1092-4388(2001/022</idno>
		<ptr target="https://doi.org/10.1044/1092-4388(2001/022" />
	</analytic>
	<monogr>
		<title level="j">JOURNAL OF SPEECH LANGUAGE AND HEARING RESEARCH</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="264" to="285" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Accurate Short-Term Analysis Of The Fundamental Frequency And The Harmonics-To-Noise Ratio Of A Sampled Sound</title>
		<author>
			<persName><forename type="first">P</forename><surname>Boersma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the Institute of Phonetic Sciences</title>
		<imprint>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Praat: Doing phonetics by computer</title>
		<author>
			<persName><forename type="first">P</forename><surname>Boersma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weenink</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Computer software</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cochlear implantation outcomes in adults: A scoping review</title>
		<author>
			<persName><forename type="first">I</forename><surname>Boisvert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Au</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Dowell</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0232421</idno>
		<ptr target="https://doi.org/10.1371/journal.pone.0232421" />
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">232421</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Discrimination and sensorimotor adaptation of self-produced vowels in cochlear implant users</title>
		<author>
			<persName><forename type="first">A</forename><surname>Borjigin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bakst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Y</forename><surname>Litovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Niziolek</surname></persName>
		</author>
		<idno type="DOI">10.1121/10.0025063</idno>
		<ptr target="https://doi.org/10.1121/10.0025063" />
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1895" to="1908" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Using fuzzy string matching for automated assessment of listener transcripts in speech intelligibility studies</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Bosker</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13428-021-01542-4</idno>
		<ptr target="https://doi.org/10.3758/s13428-021-01542-4" />
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1945" to="1953" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Speech motor adaptation during synchronous and metronome-timed speech</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Bradshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Lametti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Shiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jasmin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mcgettigan</surname></persName>
		</author>
		<idno type="DOI">10.1037/xge0001459</idno>
		<ptr target="https://doi.org/10.1037/xge0001459" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3476" to="3489" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convergence in voice fundamental frequency during synchronous speech</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Bradshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mcgettigan</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0258747</idno>
		<ptr target="https://doi.org/10.1371/journal.pone.0258747" />
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">258747</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sensorimotor learning during synchronous speech is modulated by the acoustics of the other voice</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Bradshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Wheeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mcgettigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Lametti</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13423-024-02536-x</idno>
		<ptr target="https://doi.org/10.3758/s13423-024-02536-x" />
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="306" to="316" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Voice F0 responses to manipulations in pitch feedback</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Burnett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Freedland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Hain</surname></persName>
		</author>
		<idno type="DOI">10.1121/1.423073</idno>
		<ptr target="https://doi.org/10.1121/1.423073" />
	</analytic>
	<monogr>
		<title level="j">JOURNAL OF THE ACOUSTICAL SOCIETY OF AMERICA</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3153" to="3161" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Cai</surname></persName>
		</author>
		<ptr target="https://github.com/shanqing-cai/audapter_mex" />
		<title level="m">Audapter [Computer software</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A system for online dynamic perturbation of formant frequencies and results from perturbation of the Mandarin triphthong /iau</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Boucek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Guenther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Perkell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th Intl</title>
		<meeting>the 8th Intl</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="65" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Effects of real-time cochlear implant simulation on speech production</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Casserly</surname></persName>
		</author>
		<idno type="DOI">10.1121/1.4916965</idno>
		<ptr target="https://doi.org/10.1121/1.4916965" />
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">137</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2791" to="2800" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mirrors and toothaches: Commonplace manipulations of non-auditory feedback availability change perceived speech intelligibility</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Casserly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Marino</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnhum.2024.1462922</idno>
		<ptr target="https://doi.org/10.3389/fnhum.2024.1462922" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Human Neuroscience</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Nonword Repetition as a Predictor of Long-Term Speech and Language Skills in Children With Cochlear Implants</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Casserly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pisoni</surname></persName>
		</author>
		<idno type="DOI">10.1097/MAO.0b013e3182868340</idno>
		<ptr target="https://doi.org/10.1097/MAO.0b013e3182868340" />
	</analytic>
	<monogr>
		<title level="j">OTOLOGY &amp; NEUROTOLOGY</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="460" to="470" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Supra-Segmental Changes in Speech Production as a Result of Spectral Feedback Degradation: Comparison with Lombard Speech</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Casserly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Celestin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Talesnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Pisoni</surname></persName>
		</author>
		<idno type="DOI">10.1177/0023830917713775</idno>
		<ptr target="https://doi.org/10.1177/0023830917713775" />
	</analytic>
	<monogr>
		<title level="j">LANGUAGE AND SPEECH</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="227" to="245" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An analysis of general acoustic-phonetic features for Spanish speech produced with the Lombard effect</title>
		<author>
			<persName><forename type="first">A</forename><surname>Castellanos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Benedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Casacuberta</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0167-6393(96)00042-8</idno>
		<ptr target="https://doi.org/10.1016/S0167-6393(96)00042-8" />
	</analytic>
	<monogr>
		<title level="j">SPEECH COMMUNICATION</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="23" to="35" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Changes in speech production in response to formant perturbations: An overview of two decades of research</title>
		<author>
			<persName><forename type="first">T</forename><surname>Caudrelier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rochet-Capellan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Human cortical sensorimotor network underlying feedback control of vocal pitch</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Niziolek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Houde</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1216827110</idno>
		<ptr target="https://doi.org/10.1073/pnas.1216827110" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2653" to="2658" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A study of speech deterioration in postlingually deafened adults</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Douglas-Cowie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kerr</surname></persName>
		</author>
		<idno type="DOI">10.1017/S002221510009229X</idno>
		<ptr target="https://doi.org/10.1017/S002221510009229X" />
	</analytic>
	<monogr>
		<title level="j">Journal of Laryngology and Otology</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="101" to="112" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Lexical information drives; Perceptual learning of distorted speech: Evidence from the comprehension of noise-vocoded sentences</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Johnsrude</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hervais-Adelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mcgettigan</surname></persName>
		</author>
		<idno type="DOI">10.1037/0096-3445.134.2.222</idno>
		<ptr target="https://doi.org/10.1037/0096-3445.134.2.222" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="222" to="241" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Opposing and following responses in sensorimotor speech control: Why responses go both ways</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Franken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Acheson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mcqueen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hagoort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Eisner</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13423-018-1494-x</idno>
		<ptr target="https://doi.org/10.3758/s13423-018-1494-x" />
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1458" to="1467" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Self-monitoring in the cerebral cortex: Neural responses to small pitch shifts in auditory feedback during speech production</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Franken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Eisner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Acheson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mcqueen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hagoort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Schoffelen</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuroimage.2018.06.061</idno>
		<ptr target="https://doi.org/10.1016/j.neuroimage.2018.06.061" />
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">179</biblScope>
			<biblScope unit="page" from="326" to="336" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Speaking With an Alien Voice: Flexible Sense of Agency During Vocal Production</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Franken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Hartsuiker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lind</surname></persName>
		</author>
		<idno type="DOI">10.1037/xhp0000799</idno>
		<ptr target="https://doi.org/10.1037/xhp0000799" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology-Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="479" to="494" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Don&apos;t blame yourself: Conscious source monitoring modulates feedback control during speech production</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Franken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Hartsuiker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lind</surname></persName>
		</author>
		<idno type="DOI">10.1177/17470218221075632</idno>
		<ptr target="https://doi.org/10.1177/17470218221075632" />
	</analytic>
	<monogr>
		<title level="j">Quarterly Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="27" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Real-time feedback control of voice in cochlear implant recipients</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gautam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Brant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Ruckenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Eliades</surname></persName>
		</author>
		<idno type="DOI">10.1002/lio2.481</idno>
		<ptr target="https://doi.org/10.1002/lio2.481" />
	</analytic>
	<monogr>
		<title level="j">Laryngoscope Investigative Otolaryngology</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1156" to="1162" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Control of speech and voice in cochlear implant patients</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gautam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Naples</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Eliades</surname></persName>
		</author>
		<idno type="DOI">10.1002/lary.27787</idno>
		<ptr target="https://doi.org/10.1002/lary.27787" />
	</analytic>
	<monogr>
		<title level="j">The Laryngoscope</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2158" to="2163" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Phonetic accommodation to natural and synthetic voices: Behavior of groups and individuals in speech shadowing</title>
		<author>
			<persName><forename type="first">I</forename><surname>Gessinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Raveh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Möbius</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.specom.2020.12.004</idno>
		<ptr target="https://doi.org/10.1016/j.specom.2020.12.004" />
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="page" from="43" to="63" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">H</forename><surname>Guenther</surname></persName>
		</author>
		<title level="m">Neural Control of Speech</title>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Nonconscious control of fundamental voice frequency</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Z</forename><surname>Hafke</surname></persName>
		</author>
		<idno type="DOI">10.1121/1.2817357</idno>
		<ptr target="https://doi.org/10.1121/1.2817357" />
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="273" to="278" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Instructing subjects to make a voluntary response reveals the presence of two components to the audio-vocal reflex</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Hain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Burnett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Kenney</surname></persName>
		</author>
		<idno type="DOI">10.1007/s002219900237</idno>
		<ptr target="https://doi.org/10.1007/s002219900237" />
	</analytic>
	<monogr>
		<title level="j">Experimental Brain Research</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="133" to="141" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fine-tuning of auditory cortex during speech production</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Heinks-Maldonado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Mathalon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Ford</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1469-8986.2005.00272.x</idno>
		<ptr target="https://doi.org/10.1111/j.1469-8986.2005.00272.x" />
	</analytic>
	<monogr>
		<title level="j">Psychophysiology</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="180" to="190" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Perceptual learning of noise vocoded words: Effects of feedback and lexicality</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hervais-Adelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Johnsrude</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Carlyon</surname></persName>
		</author>
		<idno type="DOI">10.1037/0096-1523.34.2.460</idno>
		<ptr target="https://doi.org/10.1037/0096-1523.34.2.460" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology. Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="460" to="474" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sensorimotor adaptation in speech production</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Houde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.279.5354.1213</idno>
		<ptr target="https://doi.org/10.1126/science.279.5354.1213" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">279</biblScope>
			<biblScope unit="issue">5354</biblScope>
			<biblScope unit="page" from="1213" to="1216" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">IEEE Recommended Practice for Speech Quality Measurements</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio and Electroacoustics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="227" to="246" />
			<date type="published" when="1969">1969</date>
			<publisher>IEEE Subcommittee on Subjective Measurements</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">It&apos;s About Time: Minimizing Hardware and Software Latencies in Speech Research With Real-Time Auditory Feedback</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Max</surname></persName>
		</author>
		<idno type="DOI">10.1044/2020_JSLHR-19-00419</idno>
		<ptr target="https://doi.org/10.1044/2020_JSLHR-19-00419" />
	</analytic>
	<monogr>
		<title level="j">Journal of Speech, Language, and Hearing Research</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2522" to="2534" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">lmerTest Package: Tests in Linear Mixed Effects Models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Brockhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H B</forename><surname>Christensen</surname></persName>
		</author>
		<idno type="DOI">10.18637/jss.v082.i13</idno>
		<ptr target="https://doi.org/10.18637/jss.v082.i13" />
	</analytic>
	<monogr>
		<title level="j">JOURNAL OF STATISTICAL SOFTWARE</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Robust Sensorimotor Learning during Variable Sentence-Level Speech</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Lametti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Shiller</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cub.2018.07.030</idno>
		<ptr target="https://doi.org/10.1016/j.cub.2018.07.030" />
	</analytic>
	<monogr>
		<title level="j">Current Biology</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page" from="3106" to="3113" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The Lombard Sign and the Role of Hearing in Speech</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tranel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Speech and Hearing Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="677" to="709" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Speech deterioration in postlingually deafened adults</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Webster</surname></persName>
		</author>
		<idno type="DOI">10.1121/1.1894647</idno>
		<ptr target="https://doi.org/10.1121/1.1894647" />
	</analytic>
	<monogr>
		<title level="j">Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="859" to="866" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Effects of simultaneous perturbations of voice pitch and loudness feedback on voice F0 and amplitude control</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Hain</surname></persName>
		</author>
		<idno type="DOI">10.1121/1.2715657</idno>
		<ptr target="https://doi.org/10.1121/1.2715657" />
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2862" to="2872" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Speakers&apos; acceptance of real-time speech exchange indicates that we use auditory feedback to specify the meaning of what we say</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Breidegard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Balkenius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Johansson</surname></persName>
		</author>
		<idno type="DOI">10.1177/0956797614529797</idno>
		<ptr target="https://doi.org/10.1177/0956797614529797" />
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1198" to="1205" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Audio-vocal responses elicited in adult cochlear implant users</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Loucks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Suneel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Aronoff</surname></persName>
		</author>
		<idno type="DOI">10.1121/1.4933233</idno>
		<ptr target="https://doi.org/10.1121/1.4933233" />
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="393" to="L398" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Limited Pre-Speech Auditory Modulation in Individuals Who Stutter: Data and Hypotheses</title>
		<author>
			<persName><forename type="first">L</forename><surname>Max</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">;</forename><surname>Daliri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><forename type="middle">)</forename></persName>
		</author>
		<idno type="DOI">10.1044/2019_JSLHR-S-CSMC7-18-0358</idno>
		<ptr target="https://doi.org/10.1044/2019_JSLHR-S-CSMC7-18-0358" />
	</analytic>
	<monogr>
		<title level="j">Journal of Speech Language and Hearing Research</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3071" to="3084" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Feedback delays eliminate auditory-motor learning in speech production</title>
		<author>
			<persName><forename type="first">L</forename><surname>Max</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Maffett</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neulet.2015.02.012</idno>
		<ptr target="https://doi.org/10.1016/j.neulet.2015.02.012" />
	</analytic>
	<monogr>
		<title level="j">Neuroscience Letters</title>
		<imprint>
			<biblScope unit="volume">591</biblScope>
			<biblScope unit="page" from="25" to="29" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Interactions of speaking condition and auditory feedback on vowel production in postlingually deaf adults with cochlear implants</title>
		<author>
			<persName><forename type="first">L</forename><surname>Menard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Polak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Denny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Burton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Matthies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Marrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Perkell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tiede</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vick</surname></persName>
		</author>
		<idno type="DOI">10.1121/1.2710963</idno>
		<ptr target="https://doi.org/10.1121/1.2710963" />
	</analytic>
	<monogr>
		<title level="j">JOURNAL OF THE ACOUSTICAL SOCIETY OF AMERICA</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3790" to="3801" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Perceptual manifestations of auditory modulation during speech planning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Merrikhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ebrahimpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Daliri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Experimental Brain Research</title>
		<imprint>
			<biblScope unit="volume">236</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1963" to="1969" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Do Not Cut Off Your Tail: A Mega-Analysis of Responses to Auditory Perturbation Experiments</title>
		<author>
			<persName><forename type="first">H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><surname>Kearney Elaine</surname></persName>
		</author>
		<author>
			<persName><surname>Nieto-Castañón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Falsini</forename><surname>Alfonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abur</forename><surname>Riccardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Acosta</forename><surname>Defne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahl</forename><surname>Sara-Ching</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kimberly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franken</forename><surname>Matthias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murray</forename><surname>Heller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Elizabeth</surname></persName>
		</author>
		<author>
			<persName><surname>Mollaei Fatemeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caroline</forename><forename type="middle">A</forename><surname>Niziolek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Perrachione</forename><surname>Parrell Benjamin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Smith</forename><surname>Tyler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cara</forename><forename type="middle">E</forename><surname>Stepp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guenther</forename><surname>Tomassi Nicole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Frank</surname></persName>
		</author>
		<idno type="DOI">10.1044/2023_JSLHR-23-00315</idno>
		<ptr target="https://doi.org/10.1044/2023_JSLHR-23-00315" />
	</analytic>
	<monogr>
		<title level="j">Journal of Speech, Language, and Hearing Research</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4315" to="4331" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Transfer of statistical learning from passive speech perception to speech production</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nozari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Holt</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13423-023-02399-8</idno>
		<ptr target="https://doi.org/10.3758/s13423-023-02399-8" />
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">The role of audition in infant babbling</title>
		<author>
			<persName><forename type="first">D</forename><surname>Oller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Eilers</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1467-8624.1988.tb01479.x</idno>
		<ptr target="https://doi.org/10.1111/j.1467-8624.1988.tb01479.x" />
	</analytic>
	<monogr>
		<title level="j">Child Development</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="441" to="449" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Speech-induced suppression and vocal feedback sensitivity in human cortex</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ozker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dugan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Doyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Devinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Flinker</surname></persName>
		</author>
		<idno type="DOI">10.7554/eLife.94198</idno>
		<ptr target="https://doi.org/10.7554/eLife.94198" />
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="13" to="P94198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Special issue: Vocal accommodation in speech communication</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Pardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pellegrino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dellwo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Möbius</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.wocn.2022.101196</idno>
		<ptr target="https://doi.org/10.1016/j.wocn.2022.101196" />
	</analytic>
	<monogr>
		<title level="j">Journal of Phonetics</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page">101196</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Phonetic convergence across multiple measures and model talkers. Attention, Perception, and Psychophysics</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Pardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Urmanche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wilman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wiener</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13414-016-1226-0</idno>
		<ptr target="https://doi.org/10.3758/s13414-016-1226-0" />
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="637" to="659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Modeling the Role of Sensory Feedback in Speech Motor Control and Learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Parrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F ;</forename><surname>Houde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><forename type="middle">)</forename></persName>
		</author>
		<idno type="DOI">10.1044/2019_JSLHR-S-CSMC7-18-0127</idno>
		<ptr target="https://doi.org/10.1044/2019_JSLHR-S-CSMC7-18-0127" />
	</analytic>
	<monogr>
		<title level="j">Journal of Speech Language and Hearing Research</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2963" to="2985" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Current models of speech motor control: A control-theoretic overview of architectures and properties</title>
		<author>
			<persName><forename type="first">B</forename><surname>Parrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Lammert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ciccarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Quatieri</surname></persName>
		</author>
		<idno type="DOI">10.1121/1.5092807</idno>
		<ptr target="https://doi.org/10.1121/1.5092807" />
	</analytic>
	<monogr>
		<title level="j">JOURNAL OF THE ACOUSTICAL SOCIETY OF AMERICA</title>
		<imprint>
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1456" to="1481" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Understanding the mechanisms underlying voluntary responses to pitchshifted auditory feedback</title>
		<author>
			<persName><forename type="first">S</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nishimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lodhavia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Korzyukov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Parkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Robin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Larson</surname></persName>
		</author>
		<idno type="DOI">10.1121/1.4870490</idno>
		<ptr target="https://doi.org/10.1121/1.4870490" />
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">135</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="3036" to="3044" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Adaptive control of vowel formant frequency: Evidence from real-time formant manipulation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Purcell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Munhall</surname></persName>
		</author>
		<idno type="DOI">10.1121/1.2217714</idno>
		<ptr target="https://doi.org/10.1121/1.2217714" />
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Adaptation by normal listeners to upward spectral shifts of speech: Implications for cochlear implants</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Faulkner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wilkinson</surname></persName>
		</author>
		<idno type="DOI">10.1121/1.428215</idno>
		<ptr target="https://doi.org/10.1121/1.428215" />
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3629" to="3636" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Speech Recognition with Primarily Temporal Cues</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Shannon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-G</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wygonski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ekelid</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.270.5234.303</idno>
		<ptr target="https://doi.org/10.1126/science.270.5234.303" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">270</biblScope>
			<biblScope unit="issue">5234</biblScope>
			<biblScope unit="page" from="303" to="304" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Immediate cross-language transfer of novel articulatory plans in bilingual speech</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Shiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bobbitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Lametti</surname></persName>
		</author>
		<idno type="DOI">10.1037/xge0001456</idno>
		<ptr target="https://doi.org/10.1037/xge0001456" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Exposure to Auditory Feedback Delay while Speaking Induces Perceptual Habituation but does not Mitigate the Disruptive Effect of Delay on Speech Auditory-motor Learning</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Shiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mitsuya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Max</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuroscience.2020.07.041</idno>
		<ptr target="https://doi.org/10.1016/j.neuroscience.2020.07.041" />
	</analytic>
	<monogr>
		<title level="j">NEUROSCIENCE</title>
		<imprint>
			<biblScope unit="volume">446</biblScope>
			<biblScope unit="page" from="213" to="224" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">The hearing ear is always found close to the speaking tongue: Review of the role of the motor system in speech perception</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">I</forename><surname>Skipper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Lametti</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.bandl.2016.10.004</idno>
		<ptr target="https://doi.org/10.1016/j.bandl.2016.10.004" />
	</analytic>
	<monogr>
		<title level="j">BRAIN AND LANGUAGE</title>
		<imprint>
			<biblScope unit="volume">164</biblScope>
			<biblScope unit="page" from="77" to="105" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Residual hearing and speech production in deaf children</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.1044/jshr.1804.795</idno>
		<ptr target="https://doi.org/10.1044/jshr.1804.795" />
	</analytic>
	<monogr>
		<title level="j">Journal of Speech and Hearing Research</title>
		<imprint>
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Top-Down Influences of Written Text on Perceived Clarity of Degraded Speech</title>
		<author>
			<persName><forename type="first">E</forename><surname>Sohoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Peelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Carlyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Davis</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0033206</idno>
		<ptr target="https://doi.org/10.1037/a0033206" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology-Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="186" to="199" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Auditory Learning and Adaptation after Cochlear Implantation: A Preliminary Study of Discrimination and Labeling of Vowel Sounds by Cochlear Implant Users</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Svirsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Silveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Surez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Neuburger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Simmons</surname></persName>
		</author>
		<idno type="DOI">10.1080/000164801300043767</idno>
		<ptr target="https://doi.org/10.1080/000164801300043767" />
	</analytic>
	<monogr>
		<title level="j">Acta Oto-Laryngologica</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="262" to="265" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Neural mechanisms underlying auditory feedback control of speech</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Tourville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Reilly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">H</forename><surname>Guenther</surname></persName>
		</author>
		<idno type="DOI">10.1016/J.NEUROIMAGE.2007.09.054</idno>
		<ptr target="https://doi.org/10.1016/J.NEUROIMAGE.2007.09.054" />
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Cochlear implantation: Relationships with research on auditory deprivation and acclimatization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tyler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Summerfield</surname></persName>
		</author>
		<idno type="DOI">10.1097/00003446-199617031-00005</idno>
		<ptr target="https://doi.org/10.1097/00003446-199617031-00005" />
	</analytic>
	<monogr>
		<title level="j">EAR AND HEARING</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="38" to="S50" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Effects of noise on speech production-acoustic and percpetual analyses</title>
		<author>
			<persName><forename type="first">W</forename><surname>Van Summers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pisoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bernacki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pedlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stokes</surname></persName>
		</author>
		<idno type="DOI">10.1121/1.396660</idno>
		<ptr target="https://doi.org/10.1121/1.396660" />
	</analytic>
	<monogr>
		<title level="j">Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="917" to="928" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Sensorimotor adaptation to feedback perturbations of vowel acoustics and its relation to perception</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Villacorta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Perkell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">H</forename><surname>Guenther</surname></persName>
		</author>
		<idno type="DOI">10.1121/1.2773966</idno>
		<ptr target="https://doi.org/10.1121/1.2773966" />
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Perceiving a Stranger&apos;s Voice as Being One&apos;s Own: A &apos;Rubber Voice&apos; Illusion?</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">N</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Munhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Johnsrude</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">18655</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
