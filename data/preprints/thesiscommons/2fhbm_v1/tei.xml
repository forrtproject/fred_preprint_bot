<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Medicine based on Symptoms: Improving Large Language Models to Answer Multiple Choice Questions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Arnav</forename><surname>Dedhia</surname></persName>
						</author>
						<title level="a" type="main">Medicine based on Symptoms: Improving Large Language Models to Answer Multiple Choice Questions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8CA6C75D67452DA6A06C59A3207FD39F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-10-22T02:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, Large Language Models (LLMs) have become known for answering multiple different types of questions. However, these models give the wrong answer many times, and that may be a problem in cases where mistakes may cost lives. In this paper, I present ways to help increase the accuracy of multiple models based on the MedQA dataset. I have trained models on textbooks, Q&amp;As, and prompts to have a fuller understanding of what methods work the best. These AIs in the field of Biomedics have a prominent future and are currently understudied. If LLMs can learn how to accurately answer multiple choice questions, they will be able to help people from all over the world, and all from home. With the methods explained in the paper, the accuracy of the model has increased over 15 percent with the highest being a model 55 percent accuracy. Overall, the MedQA dataset presents great challenges to multiple models, and I hope to use the dataset to provide a deeper understanding of the methods that can be used to increase the accuracy of LLMs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent advances in LLMs have prompted a deeper dive into how these computer models work and the shortcomings that come with them. LLMs such as GPT-4, Llama 2, or Mistral 7 are very powerful natural language processors that are being developed more and more each day. These LLMs now require people to adapt them to the multiple different purposes, domains, that a model could be used in. These domains could be video game analysis, vacation planning, or based off of medical diseases as this paper covers. <ref type="bibr">1</ref> With the backend training that each model has gone through, they can easily surpass the knowledge of a normal human, and in some cases, even professionals. LLMs have a very specific way they can answer a prompt that they have been given. In their memory, they store hundreds of millions of connections between words, phrases, and concepts. When given a prompt, the LLM has to look through these multi-layered neural networks to decipher what words have the top probabilities of answering the question at hand. Using its previous knowledge and related context, the LLM can almost accurately predict what the prompt is asking. In turn it can give an open ended answer that is similarly worded and fairly correct. <ref type="bibr">2</ref> However, contrary to popular beliefs, LLMs have a harder time answering multiple choice questions that are more niche or ones that require more complicated thought processes.<ref type="foot" target="#foot_1">3</ref> This is because the LLMs are restricted to only being able to choose out of the options given. When they are given the opportunity to answer an open ended question, they can write whatever they see fit and whatever flows the best. However, when there are only a set of multiple choice answers, the LLM has to do some extra steps to get to the answer.</p><p>With multiple choice answers, the LLMs must first think of what an open ended response to the question would be. Using this answer, they have to take it to the next level by finding the closest answer choice out of the ones given. When the LLM makes its own open ended answer, it has the freedom of choosing anything, however, the answer it has thought of may not be in the answer choices given. This means that the machine has to reason its way into picking the closest answer choice. This is usually where the largest proportion of errors occur. <ref type="bibr">4</ref> To test this theory, I have decided to use the MedQA dataset. Through the multiple experiments that have already been completed on this dataset, I have convincing evidence that the questions included in this set have a significantly lower accuracy than other datasets with similar questions. 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Data</head><p>The data used in this paper is the MedQA dataset created by Jin et al. This dataset is very accessible to the public by going to their github and downloading the link.<ref type="foot" target="#foot_2">6</ref> Within this data, there are multiple languages in which a choice is given to train and test a model. These languages include: Chinese, Taiwanese, and English. Depending on the goal of a model any of these languages can be used.</p><p>For the sake of this paper, I will be using all the data given from the English language section. However, when people will be using this model, they will have the choice of these 3 languages. This means that the 2.5 billion 3 people who speak these languages could have access to this service. 7</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Textbooks</head><p>The MedQA dataset includes 18 large medical textbooks. These textbooks are fairly popular throughout medical students, showing that training the model on these may prove to be useful. Each textbook is not only written at a very high level, but all of them contain tens of thousands of paragraphs. The combination of these factors help make it a very viable option when training the LLM. Since answering the questions in the data requires very vast and deep medical knowledge, these textbooks can provide the model with access to a good background and understanding on what many of the diseases and terms mean. Also included in the texts are common symptoms of diseases, side effects of certain medications, and actions one can take when faced with a certain problem.</p><p>The text in the textbooks were well organized and once opened up into a text document, it needed no pre-processing. The pre-processing of the data was already done by the publishers of the dataset. The text was ready to be sent into the model as training data.</p><p>In a previous study, 100 random questions were taken from the question set and given to a medical professor. When looking through the text, they were only able to find 88% of the answers were covered in at least one of the texts.<ref type="foot" target="#foot_3">8</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Questions</head><p>The MedQA dataset additionally includes thousands of different quiz type questions. Each one of these are different from one another. The questions of the dataset are split up into 3 categories: Questions with 5 answer choices, Questions with 5 answer choices and "metamap_phrases," and Questions with 4 answer choices and "metamap_phrases." The metamap phrases referred to are 15 -20 key words inside the prompt that humans or computers could use to get a better understanding of the prompt. This would be especially useful for LLMs as with the key words, they could narrow down their search and get better results. <ref type="bibr">9</ref> In this study, I chose to use the simple Questions with 5 answer choices. These are the first set of "train", "test", and "dev" files to appear in the questions folder. This set of questions were specifically chosen as it best fits the main question of this paper: how average people could use this to know what medicine to use. In real life, the people who would use this should not be required to add a list of keywords for this to work. Another reason this data set was chosen is because there are simply more answer choices for the users to add. A normal household would have more than 4 treatments, so it is beneficial to allow more answer choices into this new model.</p><p>As shown in Figure <ref type="figure" target="#fig_0">1</ref>, a question in the dataset includes multiple different keys and values that store information. The keys include question, answer, options, meta_info, and answer_idx. By having this pre-processed into a format like this makes it very simple to analyze initial data. For example, Graph 1 shows the distribution of correct answers. The graph shows B having the most correct answers, but each letter choice has a fairly similar amount. With this information, I know there is little to no bias in ordering of answer choices. The data set for each of the 3 types of questions included a total of 12,730 questions. These 12,730 questions were split up into a ratio of 8:1:1 of training : testing : and dev questions. I had no need to use the set of questions designated for hyper-parameter tuning, so I combined the dev with the training questions. Totalling to more than 11,000 training questions, there were enough to have conclusive results at the end of the experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>In this study I decided to use 5 different types of methods to train my 3 models: Llama 2 7b, Llama 2 70b, and Mistral 2 7b. Although Llama 2 7b and 70b are using the same base model, I chose to use both because Llama 2 70b is trained with 70 billion parameters while the other is only trained on 7 billion. 10 Mistral 2 7b was also included into this study so I can make a conclusion on how different base models compare with each other. <ref type="bibr">11</ref> The methods I used for tuning my models were Textbook, Instruction, Prompt, and Instruction &amp; Prompt Training. There was also a control of the model itself without any training on my end. Each of these are a form of Fine-Tuning as different parameters were passed in in hopes of creating the most optimal version of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Textbook Tuning</head><p>In this type of tuning I gave the models all 18 of the textbooks that were included in the MedQA database. These books, as previously mentioned, are very popular among students in studying for a multitude of different medical exams. With access to such a valuable resource that contains hundreds of thousands of words, there was a good chance that the models would adequately tune on the textbooks. However, one concern I had was that due to the large volume of items it takes to train a LLM, these books might have already been previously used as training data.</p><p>Nevertheless, due to the fact that around 88% of the answers to the testing question are in the textbook, I hoped that the model would get enough knowledge from these passages to pass around 70% of the questions. 12 With LLMs continuously getting better at finding answers to questions in texts, I thought that this method would have a very good chance of attaining a high accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Instruction Tuning</head><p>In this type of tuning, I gave the models all the questions and answers as training data. With the 11,400 questions in the training and dev dataset, there were more than enough questions for the models to understand how the questions are formatted and learn from them. <ref type="bibr">13</ref> In addition to format, the model can work backwards to learn more about the diseases and treatments provided in the questions. As shown in Figure <ref type="figure">2</ref>, by giving this specific question to the models, I hoped that it could figure out more about Ceftriaxone. From this question, the model could have gotten the information that Ceftriaxone is given to people who have fevers, pain during urination, and inflammation in the right knee. Although not all these are accurate, the model will now have this information in the future to help make a better guess.</p><p>Given that the model's training set was 11,400 questions, there was bound to be a number of times where this specific medication was mentioned. By factoring in all the other instances of Ceftriaxone being a correct answer, the model would work backwards and learn more about the commonalities of said questions. <ref type="bibr">14</ref> In turn, it would create a repertoire of use cases of this medicine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Prompt Tuning</head><p>In this type of tuning, I gave the models a self created prompt that would explain how to solve the multiple choice questions in the testing dataset. These instructions would serve as guidelines for the models to follow to get a better chance of getting the answer correct. <ref type="bibr">15</ref> The given prompt was also very basic so that it could apply to all types of questions and give the best guidance for each. <ref type="bibr">16</ref> As shown with the 12 sentences written in Figure <ref type="figure" target="#fig_2">3</ref>, I split up the prompt into an "if else" statement. In a sample taken of 100 random questions from the training dataset, 89% of the questions were requiring an answer to be in the 16 https://www.superannotate.com/blog/llm-prompti ng-tricks#:~:text=26%20prompting%20technique s%201%201.%20No%20need%20to,8%208.%2 0Format%20your%20prompt%20...%20More%2 0items 15 https://www.youtube.com/watch?v=yu27PWzJI_ Y&amp;t=1s 14 https://www.ibm.com/topics/instruction-tuning#:: text=Instruction%20tuning%20is%20a%20techn ique%20for%20fine-tuning%20large,thus%20hel ping%20adapt%20pre-trained%20models%20fo r%20practical%20use.</p><p>form of a medication or the procedure to do next. 17 Because these 2 question types covered a very large percent of the data, it would only confuse the model if more instructions were added to the prompt.</p><p>The prompt also gives a good step by step process that would guide the model through each question. By following the line of steps, the models would have gotten a better understanding of both the question posed and the scenario given. Both of these help get a better total accuracy. 18</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Instruction &amp; Prompt Tuning</head><p>In this type of tuning, I gave the models a combination of the prompt I made and all the questions and answers in the MedQA dataset. By giving the models both of these training Graph 2: Accuracies of all 3 model with all the different types of initial tuning items at the same time, it can learn from the prompt while using the questions to form a connection.</p><p>The models would adjust the weightages of the neural network links as 18 https://www.superannotate.com/blog/llm-prompti ng-tricks#:~:text=26%20prompting%20technique s%201%201.%20No%20need%20to,8%208.%2 0Format%20your%20prompt%20...%20More%2 0items 17 https://aclanthology.org/W19-5039.pdf normal, except in this case, the model will start with more knowledge. <ref type="bibr">19</ref> By having a chain of thought to follow and learn from, the models can save hundreds of thousands of questions learning the basic process. <ref type="bibr">20</ref> With the extra thousand questions now available, I hope that the model can tune itself till it can get a majority of the questions correct.</p><p>Because this tactic combines the previous two, I believe that this will have the greatest accuracy and precision scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results and Analysis</head><p>By performing the actual experiment on the training data of the MedQA dataset, I have data that is easily analysable. In Graph 2, it is clear that Llama 70b is the best model. However, the prompt I created had little to no impact on the accuracy of models tested.</p><p>As expected the Instruction + Prompt fine-tuning on both Mistral 7b and Llama 2 70b was the most accurate. The more instructions given to the model, the faster it can understand how to connect the links of the internal neural networks.</p><p>When looking at Graph 2, the numbers suggest that the most helpful fine tuning was in fact the instruction training. By giving the 3 models a set of questions with answers, the Llama 2 70b was able to increase its accuracy by 12% and Mistral 7b was able to increase its accuracy by 14%.</p><p>Without a fine-tuning on the types of questions that will be asked, all the models underperformed by quite a large margin. Although, with the instruction tuning, the accuracy went from 20%, by guessing, to over 50%. This is not perfect, but much better than 20 https://www.techtarget.com/searchEnterpriseAI/ definition/chain-of-thought-prompting what a common person at home could get. I can conclude that the future tuning of this model will rely on different methods using some sort of instruction tuning algorithm.</p><p>One example, not covered in this paper, is running through the data and having the model classify the questions. This could either be classifying them by what the answer choices are asking, or splitting by situations with similar symptoms. By having the data split up prior to the training, the models might be able to better understand how to get more accurate answers on the testing set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>I have presented my findings of the models Llama 2 7b, Llama 2 70b, and Mistral 7b on the questions provided by the MedQA dataset. Although the expectations of nurses and doctors being able to use these improved models were not met, a lot of valuable information was gained.</p><p>The MedQA dataset has a lot of realistic use cases for more than just training Machine Learning models. With the accessibility to 3 different languages and the 15+ textbooks in each language, it is a great study resource for those taking medical exams.</p><p>The different methods in the 3 models also show us which ways of training were universally impactful versus which ones only worked for models with a high number of initial parameters. By using this new knowledge I hope further research is conducted on the multitude of different training methods currently known.</p><p>When models like these get to a high enough accuracy, they could potentially be able to be released to everyone so that they can get more information on the ointment they should be using, based on their current situation. With my research, I am 1 step closer in achieving that final goal.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Sample question for the MedQA Dataset</figDesc><graphic coords="2,75.75,81.00,486.00,144.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Graph 1 :</head><label>1</label><figDesc>Number of correct answer choice letters</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Prompt given to all models for training purposes</figDesc><graphic coords="5,76.50,82.50,478.50,174.00" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://thedatascientist.com/5-best-examples-ofdomain-specific-llms-in-ai/#:~:text=Finance%20t ops%20the%20list%20when,big%20on%20stabi lity%20and%20accuracy.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>https://openreview.net/pdf?id=shr9PXz7T0 2 https://nexocode.com/blog/posts/generative-que stion-answering-llms/#:~:text=Answer%20Gener ation%3A,contextually%20accurate%20but%20 also%20insightful.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2"><p>https://github.com/jind11/MedQA 5 https://arxiv.org/pdf/2009.13081 4 https://openreview.net/pdf?id=shr9PXz7T0</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_3"><p>https://arxiv.org/pdf/2009.13081 7 https://gurmentor.com/what-is-the-most-spoken-l anguage-in-the-world/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_4"><p>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6 309052/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_5"><p>https://www.simplypsychology.org/experimentaldesigns.html 10 https://blog.nimblebox.ai/choosing-the-right-llam a2-model</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_6"><p>https://arxiv.org/pdf/2308.10792 12 http://arxiv.org/pdf/2009.13081</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_7"><p>https://www.geeksforgeeks.org/large-languagemodel-llm/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Asma</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<title level="m">Overview of the MEDIQA</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Shared Task on Textual Inference, Question Entailment and Question Answering Chaitanya Shivade 2</title>
		<ptr target="https://aclanthology.org/W19-5039.pdf" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">5 Best Examples of Domain-Specific LLMs in AI -the Data Scientist</title>
		<author>
			<persName><forename type="first">Lev</forename><surname>Craig</surname></persName>
		</author>
		<ptr target="https://thedatascientist.com/5-best-examples-of-domain-specific-llms" />
	</analytic>
	<monogr>
		<title level="m">promp ting#:~:text=Chain%2Dof%2Dthought %20prompting%20is,way%20that%20m imics%20human%20reasoning. dorota-owczarek</title>
		<imprint>
			<date type="published" when="2023-04-17">2024. December 11, 2023. April 17, 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">TechTarget</note>
	<note>Generative Question Answering over Documents with LLMs. in-ai/#:~:te xt=Finance%20tops%20the%20list%20 when,big%20on%20stability%20and%2 0accuracy</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">What Is a Large Language Model (LLM)</title>
		<author>
			<persName><surname>Geeksforgeeks</surname></persName>
		</author>
		<ptr target="https://www.geeksforgeeks.org/large-language-model-llm/" />
	</analytic>
	<monogr>
		<title level="j">GeeksforGeeks. GeeksforGeeks</title>
		<imprint>
			<date type="published" when="2023-06-04">June 4, 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">What Is the Most Spoken Language in the World in 2023</title>
		<author>
			<persName><surname>Gurmentor</surname></persName>
		</author>
		<ptr target="https://gurmentor.com/what-is-the-most-spoken-language-in-the-world/" />
	</analytic>
	<monogr>
		<title level="m">Encore!!!</title>
		<imprint>
			<date type="published" when="2021-04-23">April 23, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">What Is Prompt Tuning?</title>
		<author>
			<persName><surname>Technology</surname></persName>
		</author>
		<ptr target="https://www.youtube.com/watch?v=yu2" />
	</analytic>
	<monogr>
		<title level="m">YouTube Video. YouTube</title>
		<imprint>
			<date type="published" when="2023-06-16">June 16, 2023</date>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">What Is Instruction Tuning? | IBM</title>
		<author>
			<persName><surname>Ibm</surname></persName>
		</author>
		<author>
			<persName><surname>Com</surname></persName>
		</author>
		<ptr target="https://www.ibm.com/topics/instruction-tuning" />
		<imprint>
			<date type="published" when="2024-04-04">April 4, 2024</date>
		</imprint>
	</monogr>
	<note>text=Instruction%20tuning%2 0is%20a%20technique%20for%20fine-t uning%20large,thus%20helping%20ada pt%20pre-trained%20models%20for%2 0practical%20use</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">What Disease Does This Patient Have? A Large-Scale Open Domain Question Answering Dataset from Medical Exams</title>
		<author>
			<persName><forename type="first">Di</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eileen</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nassim</forename><surname>Oufattole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Hung</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanyi</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Szolovits</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/2009.13081" />
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page">6421</biblScope>
			<date type="published" when="2021-07-12">July 12, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">What Disease Does This Patient Have? A Large-Scale Open Domain Question Answering Dataset from Medical Exams</title>
		<idno type="DOI">10.3390/app11146421</idno>
		<ptr target="https://doi.org/10.3390/app11146421" />
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page">6421</biblScope>
			<date type="published" when="2021-07-12">July 12, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">GitHub -Jind11/MedQA: Code and Data for MedQA</title>
		<ptr target="https://github.com/jind11/MedQA" />
	</analytic>
	<monogr>
		<title level="j">GitHub</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Choosing the Right Llama 2 Model -Plug-And-Play MLOps</title>
		<author>
			<persName><surname>Nimblebox</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">|</forename><surname>Platform</surname></persName>
		</author>
		<author>
			<persName><surname>Nimblebox</surname></persName>
		</author>
		<author>
			<persName><surname>Ai</surname></persName>
		</author>
		<ptr target="https://blog.nimblebox.ai/choosing-the-right-llama2-model" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Experimental Design: Types, Examples &amp; Methods</title>
		<ptr target="https://www.simplypsychology.org/experimental-designs.html" />
		<imprint>
			<date type="published" when="2023-07-31">July 31, 2023</date>
		</imprint>
	</monogr>
	<note>Simply Psychology</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">26 Prompting Tricks to Improve LLMs | SuperAnnotate</title>
		<author>
			<persName><surname>Superannotate</surname></persName>
		</author>
		<author>
			<persName><surname>Com</surname></persName>
		</author>
		<ptr target="https://www.superannotate.com/blog/llm-prompting-tricks" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>text=26%20promp ting%20techniques%201%201.%20No %20need%20to,8%208.%20Format%20 your%20prompt%20...%20More%20ite ms</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Using Natural Language Processing and Machine Learning to Identify Breast Cancer Local Recurrence</title>
		<author>
			<persName><forename type="first">Zexian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasa</forename><surname>Espino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankita</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seema</forename><forename type="middle">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><forename type="middle">E</forename><surname>Clare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Neapolitan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
		<idno type="DOI">10.1186/s12859-018-2466-x</idno>
		<ptr target="https://doi.org/10.1186/s12859-018-2466-x" />
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">S17</biblScope>
			<date type="published" when="2018-12-01">December 1, 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Using Natural Language Processing and Machine Learning to Identify Breast Cancer Local Recurrence</title>
		<author>
			<persName><forename type="first">Zexian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasa</forename><surname>Espino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankita</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seema</forename><forename type="middle">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><forename type="middle">E</forename><surname>Clare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Neapolitan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
		<ptr target="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6309052/" />
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">S17</biblScope>
			<date type="published" when="2018-12-01">December 1, 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Instruction Tuning for Large Language Models: A Survey</title>
		<author>
			<persName><forename type="first">Shengyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Linfeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuhe</forename><surname>Xiaofei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/2308.10792" />
		<imprint>
			<date type="published" when="2024-06-04">June 4, 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">LARGE LANGUAGE MODELS ARE NOT ROBUST MULTIPLE CHOICE SELECTORS</title>
		<author>
			<persName><forename type="first">Chujie</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<ptr target="https://openreview.net/pdf?id=shr9PXz7T0" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
