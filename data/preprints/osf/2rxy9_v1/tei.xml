<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Brain dynamics of mental state attribution during perception of social robot faces</title>
				<funder ref="#_sKbzymw">
					<orgName type="full">Deutsche Forschungsgemeinschaft (DFG, German Research Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Martin</forename><surname>Maier</surname></persName>
							<email>martin.maier@hu-berlin.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Humboldt-Universität zu Berlin</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Science of Intelligence</orgName>
								<orgName type="institution">Research Cluster of Excellence</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexander</forename><surname>Leonhardt</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Humboldt-Universität zu Berlin</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Florian</forename><surname>Blume</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Science of Intelligence</orgName>
								<orgName type="institution">Research Cluster of Excellence</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Technische Universität Berlin</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pia</forename><surname>Bideau</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Science of Intelligence</orgName>
								<orgName type="institution">Research Cluster of Excellence</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Univ. Grenoble Alpes</orgName>
								<address>
									<settlement>Inria</settlement>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">CNRS</orgName>
								<address>
									<settlement>Grenoble</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Olaf</forename><surname>Hellwich</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Science of Intelligence</orgName>
								<orgName type="institution">Research Cluster of Excellence</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Technische Universität Berlin</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rasha</forename><forename type="middle">Abdel</forename><surname>Rahman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Humboldt-Universität zu Berlin</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Science of Intelligence</orgName>
								<orgName type="institution">Research Cluster of Excellence</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Brain dynamics of mental state attribution during perception of social robot faces</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8D3449F3F5187B322A2FDE190722BA3D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-10-22T06:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The interplay of mind attribution and emotional responses is considered crucial in shaping human trust and acceptance of social robots. Understanding this interplay can help us create the right conditions for successful human-robot social interaction in the service of societal needs. In this study we show that information about robots describing positive, negative or neutral behavior prompts participants (N=90) to attribute mental states to robot faces, modulating impressions of trustworthiness, facial expression and intentionality. These novel findings were replicated in an experiment investigating the underlying dynamics in the human mind and brain. EEG recordings from 30 participants revealed that affective information influenced specific processing stages in the brain associated with basic face perception and more elaborate stimulus evaluation. However, a modulation of fast emotional brain responses, typically found for human faces, was not observed. These findings suggest that neural processing of robot faces alternates between being perceived as mindless machines and intentional agents: people rapidly attribute mental states during perception, literally seeing good or bad intentions in robot faces, but are emotionally less affected than when facing humans. These nuanced insights into the fundamental psychological and neurocognitive processes supporting mind attribution hold potential for informing the design of artificial social agents, improving human-robot social interactions, and guiding policies regarding moral responsibility.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>The demand for social robots, embodied artificial systems that interact with humans in their daily lives, is expected to increase in the coming years. As robots are developed for different uses such as care, retail or entertainment, more people are likely to engage with social robots in their private and professional lives <ref type="bibr" target="#b0">(1)</ref><ref type="bibr" target="#b1">(2)</ref><ref type="bibr" target="#b2">(3)</ref>. Yet, key psychological and neurocognitive aspects of interacting with social robots are still under investigation, including the extent to which humans-and their brains-process robots akin to intentional social agents with mental states <ref type="bibr" target="#b3">(4)</ref><ref type="bibr" target="#b4">(5)</ref><ref type="bibr" target="#b5">(6)</ref>. This question bears important implications for how we deal with artificial social agents as a society, including moral judgments of their responsibility for negative outcomes <ref type="bibr" target="#b6">(7)</ref>.</p><p>Previous theoretical work has proposed that two conflicting intuitions come into play, the "intentional" stance and the "physical" or "design" stance <ref type="bibr" target="#b5">(6,</ref><ref type="bibr" target="#b7">(8)</ref><ref type="bibr" target="#b8">(9)</ref><ref type="bibr" target="#b9">(10)</ref>. Taking an intentional stance towards robots means to intuitively treat them as if they had a mind: people can tend to anthropomorphize robots, interacting with them as if they possessed mental states such as motivations, intentions, or emotions. This may allow them to tap into processes used in social interaction with other humans, such as theory of mind, as a basis for social perception, communication, and coordination of behavior <ref type="bibr" target="#b10">(11)</ref><ref type="bibr" target="#b11">(12)</ref><ref type="bibr" target="#b12">(13)</ref><ref type="bibr" target="#b13">(14)</ref><ref type="bibr" target="#b14">(15)</ref>. On the other hand, people's explicit opinions often reflect a physical stance towards robots, viewing them as machines designed or programmed to behave in specific ways <ref type="bibr" target="#b7">(8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">(16)</ref><ref type="bibr" target="#b16">(17)</ref><ref type="bibr" target="#b17">(18)</ref>.</p><p>How are these seemingly contradictory intuitions about what type of things robots are-intentional beings or mindless machines-reflected in people's perception of robots and the underlying neural processes? Most research has focused on how human-robot interaction and mind perception are shaped by the design characteristics of the robots <ref type="bibr" target="#b14">(15,</ref><ref type="bibr" target="#b18">(19)</ref><ref type="bibr" target="#b19">(20)</ref><ref type="bibr" target="#b20">(21)</ref><ref type="bibr" target="#b21">(22)</ref><ref type="bibr" target="#b22">(23)</ref><ref type="bibr" target="#b23">(24)</ref> and trait characteristics of the human perceivers, such as their attitudes towards robots and artificial intelligence (AI) <ref type="bibr" target="#b16">(17,</ref><ref type="bibr" target="#b24">(25)</ref><ref type="bibr" target="#b25">(26)</ref><ref type="bibr" target="#b26">(27)</ref>.</p><p>However, some crucial variables that shape human social perception and interaction in real time have been largely overlooked in studying mind attribution to robots. The human ability to perceive and evaluate others' intentions is strongly influenced by context and prior knowledge, such as learned person-related information <ref type="bibr" target="#b27">(28)</ref><ref type="bibr" target="#b28">(29)</ref><ref type="bibr" target="#b29">(30)</ref><ref type="bibr" target="#b30">(31)</ref><ref type="bibr" target="#b31">(32)</ref>. Our perception of others is not only shaped by what we can read from their faces, but also from what we read into them based on our expectations <ref type="bibr" target="#b32">(33)</ref>. For instance, socialaffective information has been shown to influence brain signatures of early perceptual, reflexive emotional, as well as higher-level evaluative processing <ref type="bibr">(28-30, 32, 34, 35)</ref>, as elaborated on below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The present study</head><p>In this study, we investigate how brain dynamics reflect mental state attribution in the perception of social robots and to what extent these dynamics align with the intentional and physical stance, respectively. Specifically, we test how prior information about robots' behavior influences neural correlates of perception, emotional responses, and evaluation (as illustrated in Fig. <ref type="figure">1</ref>). In human social perception, prior knowledge or beliefs about others (e.g., "she bullied her work colleague") can lead people to perceive emotional expressions in objectively neutral faces, revealing the attribution of mental states that are not necessarily present in the person <ref type="bibr" target="#b27">(28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35)</ref>. Cognitive scientists ascribe this to the interplay between bottomup and top-down processing, where perceptions are constructed based on combinations of sensory input from the environment and predictions generated based on prior knowledge and expectations <ref type="bibr" target="#b29">(30,</ref><ref type="bibr" target="#b35">(36)</ref><ref type="bibr" target="#b36">(37)</ref><ref type="bibr" target="#b37">(38)</ref><ref type="bibr" target="#b38">(39)</ref><ref type="bibr" target="#b39">(40)</ref>. Here we investigate this mechanism for the first time with robot faces: does information about a robot's behavior literally make people see good or bad intentions in its face? And do people react emotionally to neutral robot faces based on whether they are associated with morally good or bad behavior, as they would with other humans <ref type="bibr" target="#b27">(28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b40">41)</ref>?</p><p>In two pre-registered experiments, participants were presented with positive, neutral, and negative information about real human-like robots (e.g., teaches social skills to people with autism, assembles orders at a warehouse, reports children to the secret police; see Fig. <ref type="figure">1</ref>). Subsequently, they rated the valence of the robots' facial expressions and their trustworthiness. We hypothesized that, similar to human faces, participants would attribute emotional expressions to the robots' faces and assess their trustworthiness based on the acquired information, which induced expectations about their possible intentions. Therefore, we anticipated that ratings of both facial expressions and trustworthiness would align with the information's valence. Experiment 1 investigated the effect of information type on ratings of facial expression and trustworthiness, seeking initial evidence that people indeed read good or bad intentions into robot faces. To explore the neurocognitive dynamics associated with the attribution of mental states, Experiment 2 measured evoked brain responses using electroencephalography (EEG) as participants evaluated robots' facial expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 1. Information examples and Rating results of Experiment 1. (A). Representative</head><p>story examples for each affective information condition along with an AI-generated robot portrait <ref type="bibr" target="#b41">(42)</ref> similar to the 36 real robots used in the study. The pairings of robots and information type were counterbalanced, ensuring each robot was paired equally as often with negative, neutral, and positive information across participants. (B) Trustworthiness ratings after information acquisition and facial expression ratings before and after information acquisition, categorized by information condition. Large dots denote group means with corresponding 95% CIs, while small dots indicate individual participant means. Asterisks highlight statistically significant differences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 1</head><p>In Experiment 1, an online study was conducted to investigate the influence of robotrelated information on ratings of robots' trustworthiness and facial expressions among a sample of 60 participants, evenly split between English and German native speakers.</p><p>After the information manipulation, we anticipated both trustworthiness and facial expression ratings to align with the valence of the learned information. Rating data were analyzed using linear mixed effects models (LMMs) with fixed effects coded as sliding difference contrasts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Facial Expression Ratings</head><p>Participants rated facial expressions of robot faces in two phases, before and after acquiring information. We analyzed facial expression ratings with the independent variables phase, i.e. pre-vs. post-learning, and information, i.e. robots matched with positive, neutral, or negative information. By modeling information type nested within phase, we estimated the effect of information on the pre-and post-ratings separately.</p><p>As expected, before participants had learned information about the robots, ratings of the facial expression of robots were neutral overall and showed no significant differences between conditions (see Table <ref type="table" target="#tab_0">1</ref>, Fig. <ref type="figure">1</ref>). However, after hearing the stories, participants rated the same facial expressions differently depending on the valence of the associated information. In line with our predictions, facial expressions in the negative condition were rated as more negative than in the neutral condition and facial expressions in the positive condition were rated as more positive than in the neutral condition. An additional analysis of variance confirmed the significance of the interaction between phase and information, F(4, 59.53) = 24.16; p &lt; .001, showing that the overall differences between the information conditions increased significantly between phases. An additional LMM analysis including the variable language indicated no differences in the facial expression ratings provided by English and German native speakers (details in Supplementary Materials).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Trustworthiness Ratings</head><p>We further analyzed trustworthiness ratings, which were collected after information acquisition. We found a main effect of information. As expected, participants rated robots as significantly more trustworthy when matched with positive information compared to neutral information. Robots matched with negative information were also rated as less trustworthy in comparison to robots matched with neutral information (see Table <ref type="table" target="#tab_1">2</ref>, Fig. <ref type="figure">1</ref>). An additional analysis including the independent variable group (English vs. German speakers) revealed a main effect of group on trustworthiness ratings (b = 0.30, 95% CI = [0.01, 0.59], p = .041). Overall, English-speaking participants rated the robots' trustworthiness slightly higher than German-speaking participants (details in Supplementary Materials).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion of Experiment 1</head><p>In Experiment 1, our information manipulation influenced participants' trustworthiness ratings, demonstrating that people clearly distinguish between the trustworthiness of robots previously described as fulfilling negative, neutral, and positive tasks. The additional main effect of participant group on trustworthiness ratings may reflect genuine differences in trust evaluation towards humanoid robots among English and German speakers. Alternatively, despite our efforts to maintain consistency in meaning across languages, it could stem from nuances in the robots' backstories conveyed differently in each language. More importantly, we also observed an effect of information on facial expression ratings, providing initial evidence that learned information may lead humans to perceive emotional facial expressions in robot faces. Notably, in the absence of prior information, these faces were initially rated as neutral. These findings imply that participants attributed mental states already during robot face perception.</p><p>However, explicit facial expression ratings may have been influenced by task demands, as participants could have adjusted their ratings in accordance with perceived experimenters' hypotheses. Addressing this concern, Experiment 2 used EEG to provide insights into the underlying neurocognitive mechanisms. The use of event-related potentials (ERPs) allowed us to assess the genuine effect of information on perception more implicitly, shedding light on whether individuals truly perceive good or bad intentions in robot faces, with early ERP components being less susceptible to task demands.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 2</head><p>In Experiment 2, we used EEG to investigate the neural mechanisms and temporal dynamics underlying the attribution of mental states to robot faces. We tested a new sample of 30 native German speakers, using the same information manipulation for a subset of 18 out of the 36 robots presented in Experiment 1.</p><p>Using ERPs, we investigated different stages of processing in the brain with high temporal precision, aiming to determine which stages support the intentional vs. physical stance towards robots. Our analysis focused on four distinct stages: early visual perception (P1 component <ref type="bibr" target="#b42">(43,</ref><ref type="bibr" target="#b43">44)</ref>), visual processing of faces and facial expressions (N170 component <ref type="bibr" target="#b44">(45,</ref><ref type="bibr" target="#b45">46)</ref>), fast reflexive emotional responses (early posterior negativity, EPN <ref type="bibr" target="#b33">(34,</ref><ref type="bibr" target="#b46">47)</ref>), and more elaborate stimulus evaluation (late positive potential, LPP <ref type="bibr" target="#b28">(29,</ref><ref type="bibr" target="#b46">47)</ref>). If affective information modulates the amplitude of the P1 component, it suggests an influence on low-level visual perception, which has been repeatedly observed in object perception <ref type="bibr" target="#b47">(48)</ref><ref type="bibr" target="#b48">(49)</ref><ref type="bibr" target="#b49">(50)</ref><ref type="bibr" target="#b50">(51)</ref><ref type="bibr" target="#b51">(52)</ref><ref type="bibr" target="#b52">(53)</ref> but less so in face perception <ref type="bibr" target="#b53">(54,</ref><ref type="bibr" target="#b54">55)</ref>. We explored this phenomenon considering that robot faces may be perceived as situated between objects and faces. Similarly, if affective information influences the amplitude of the N170 component, i.e. how the face's visual features are structurally encoded, it implies that affective information prompts people to perceive emotional facial expressions in neutral robot faces <ref type="bibr" target="#b55">(56,</ref><ref type="bibr" target="#b56">57)</ref>. Additionally, an impact of information on reflexive emotional responses to robot faces should be reflected in increased EPN amplitudes <ref type="bibr" target="#b46">(47,</ref><ref type="bibr" target="#b57">58)</ref>, while an influence on more deliberate evaluations should result in increased LPP amplitudes <ref type="bibr" target="#b27">(28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b46">47)</ref>. We anticipated that potential amplitude differences between negative and neutral information conditions would be more pronounced than those between positive and neutral conditions across ratings and ERP components, aligning with previous findings <ref type="bibr" target="#b27">(28,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b40">41)</ref>.</p><p>To gain deeper insight into how affective information and the ERP components of interest relate to perceived intentionality, we introduced an additional rating task inspired by the InStance questionnaire <ref type="bibr" target="#b5">(6)</ref>. This task assessed participants' perceived intentionality attributed to each individual robot and its behavior. We investigated whether affective information about robots could enhance perceived intentionality. Additionally, by considering the intentionality rating as a covariate, we explored whether any specific ERP component is particularly linked to the likelihood of attributing intentionality to humanoid robots based on affective information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rating results</head><p>Facial expression ratings were collected after the learning part. Each robot's facial expression was rated twelve times (the stimulus repetitions served to increase signalto-noise ratio in ERP analysis). The rating results reported here are based on the first rating of each stimulus. Similar to Experiment 1, facial expression ratings were influenced by affective information: expressions of robots associated with negative information were rated as more negative than those associated with neutral information (b = 0.69, 95% CI = [0.40, 0.97], p &lt; .001). Expressions of robots in the positive information condition were rated as slightly more positive compared to the neutral information condition, but this comparison only yielded a statistical trend (b = 0.18, 95% CI = [-0.01, 0.37], p = .069).</p><p>Trustworthiness ratings were collected before and after learning, in order to get a baseline for each robot face's trustworthiness. Ratings of robots assigned to the three information conditions differed significantly after learning (neutral-negative: b = 1.83, 95% CI = [1.57, 2.10], p &lt; .001; positive-neutral: b = 0.33, 95% CI = [0.11, 0.55], p = .003), but not in the baseline rating before learning (neutral-negative: b = 0.09, 95% CI = [-0.18, 0.35], p = .506; positive-neutral: b = 0.03, 95% CI = [-0.19, 0.25], p = .802). An additional analysis of variance confirmed the significance of the interaction between phase and information, F(2, 973.78) = 114.95; p &lt; .001, showing that the overall differences between trustworthiness ratings in the different information conditions increased significantly between phases. Taken together, the results of Experiment 2 align well with those obtained in Experiment 1.</p><p>In addition, we assessed ratings of the perceived intentionality of each robot's behavior. On a scale from -50 (completely agree with a non-intentional explanation of behavior) to 50 (completely agree with an intentional explanation), mean ratings were -7.05 (95% CI = [-13.34, -0.69]). So on average, ratings tended more towards nonintentional than intentional explanations of the robots' behavior, but were also far from completely non-intentional. We next analyzed the impact of affective information on robots' perceived intentionality. Robots associated with negative information were rated as more intentional compared to robots associated with neutral information (b =-9.67, 95% CI = [-15.62, -3.73], p &lt; .001). There was no significant difference between the impact of neutral and positive information on perceived intentionality (b = 3.19, 95% CI = [-2.76, 9.14], p = .293). This provides initial evidence that framing robots as exhibiting purposeful bad behavior influences the intentionality attributed to the robots. In sum, ratings of facial expression, robot trustworthiness and perceived intentionality were influenced by the valence of affective information learned about the robots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2. EEG study results. (A). Trial sequence of the facial expression rating task. (B)</head><p>Trustworthiness ratings before and after information acquisition, facial expression and intentionality ratings after information acquisition, categorized by information condition. Large dots denote group means with corresponding 95% CIs, while small dots indicate individual participant means. (C) Grand average ERPs for the N170 and LPP components collected during the facial expression rating task. Gray shading highlights time windows for the N170 and LPP. Scalp topographies illustrate differences between the information conditions, with channels included in the N170 and LPP regions of interest highlighted in white. Asterisks highlight statistically significant differences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EEG results</head><p>We tested the effects of negative, neutral and positive information on the processing of robot faces presented during the facial expression rating task. Specifically, we analyzed ERP components associated with early perceptual processing (P1 and N170), reflexive emotional responses to visual input (EPN) and higher-level evaluation (LPP).</p><p>We observed significant influences of affective information on the N170 and LPP components, but not the P1 and EPN components (see Tables <ref type="table" target="#tab_2">3</ref><ref type="table" target="#tab_3">4</ref>and Figure <ref type="figure">2</ref>). Both N170 and LPP amplitudes were significantly increased in the negative information condition compared to the neutral information condition. There were no significant differences between the neutral and the positive information conditions.</p><p>We explored whether participants' ratings of each robot's perceived intentionality would be associated with the impact of information on the N170 and LPP components. We calculated an additional LMM for each component including centered intentionality scores as a covariate. We discovered an interaction between perceived intentionality and information (negative vs. neutral) in the N170 component (b = -0.29, 95% CI = [-0.57, -0.01], p = .045). Specifically, higher scores of intentionality were linked to a larger impact of negative information on N170 amplitudes. Additionally, in the LPP, we observed a significant main effect of perceived intentionality, where higher scores of the covariate were associated with lower LPP amplitudes (b = -0.14, 95% CI = [-0.28, -0.01], p = 0.033). However, we found no interactions between perceived intentionality and information.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion of Experiment 2</head><p>In line with the findings of Experiment 1, our manipulation of affective information significantly influenced trustworthiness and facial expression ratings. We also observed an impact of affective information on perceived intentionality, indicating that behaviors were judged as more intentional when robots were linked to negative backstories. This corresponds with the asymmetry often noted in judgments of human behavior, where people tend to attribute greater intentionality for actions with negative outcomes compared to positive ones <ref type="bibr" target="#b58">(59,</ref><ref type="bibr" target="#b59">60)</ref>.</p><p>In ERPs, we observed significant information effects on two distinct processing stages: the N170 component, associated with visual perception, and the later LPP component, reflecting more elaborate stimulus evaluation. The N170 is most commonly associated with structural visual encoding of faces, and it has been shown to be sensitive to manipulations of facial expression, as well as the realism of face images <ref type="bibr" target="#b44">(45,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b61">62)</ref>. Thus, robot faces associated with negative information were perceived either as displaying an emotional facial expression or as more "face-like," both of which support the idea that affective information enables attribution of mental states to robot faces at an early, potentially automatic perceptual stage. Similarly, the information effect on the LPP indicates that negatively framed robots are evaluated as more emotionally relevant compared to neutrally framed robots <ref type="bibr" target="#b46">(47,</ref><ref type="bibr" target="#b62">63)</ref>. Affective information did not influence the P1 component, indicating that low-level visual processing remained unaffected. Since previous studies have demonstrated knowledge effects in the P1 for objects <ref type="bibr" target="#b47">(48,</ref><ref type="bibr" target="#b49">50)</ref>, this suggests that the visual processing of robot faces leaned more towards face perception rather than object perception. Notably, contrary to our prediction, affective information did not influence the EPN component, suggesting a lack of early reflexive emotional response typically observed when human faces are associated with analogous affective information <ref type="bibr" target="#b27">(28,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b63">64)</ref>. We elaborate further on these findings in the general discussion.</p><p>The absence of a significant difference between the positive and neutral information conditions in ERPs was partly expected, given our prediction that the impact of negative information would be more pronounced than that of positive information. The fact that ERPs in the positive and neutral conditions did not differ at all could be due to a situational effect during the learning session, where the presence of highly negative stories overshadowed the more subtle differences between positive and neutral stories. Participants appeared to perceive the neutral stories as somewhat positive, as indicated by a positive shift in both facial expression ratings (Experiment 1) and trustworthiness ratings (Experiment 2) from the pre-to post-learning phases. This phenomenon aligns with previous studies that used vignettes describing human social behavior <ref type="bibr" target="#b34">(35,</ref><ref type="bibr" target="#b40">41)</ref>.</p><p>By including perceived intentionality as a covariate, we delved deeper into understanding the interplay between affective information, intentionality, and the neurocognitive processes represented by the N170 and LPP components. The interaction effect observed between information type and perceived intentionality on the N170 component provides further evidence linking the perceptual impact of negative information-where neutral robot faces are perceived as displaying negative expressions-to the attribution of intentionality. There are two plausible interpretations of these findings: Firstly, enhanced visual face processing, influenced by affective information, increases the likelihood of individuals attributing mental states to a robot. Alternatively, it is possible that robot faces inherently more predisposed to attribution of intentionality, perhaps due to their appearance, are also more susceptible to having facial expressions inferred onto them. This open question warrants further investigation in future research. Moreover, higher scores of perceived intentionality were correlated with a reduction in the LPP component, regardless of affective information. This observation could suggest that the cognitive effort involved in deliberate emotional evaluation diminishes for robot faces perceived as more intentional <ref type="bibr" target="#b53">(54,</ref><ref type="bibr" target="#b64">65)</ref>. It is plausible that faces that are more easily ascribed intentionality may give us less pause during socialemotional evaluation, suggesting facilitated use of theory of mind.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GENERAL DISCUSSION</head><p>Humanoid social robots present an intriguing puzzle: people often intuitively interact with robots as they would with another human, even though they may be explicitly aware that robots are mechanical artifacts that do not share the same cognitive abilities as humans <ref type="bibr" target="#b7">(8,</ref><ref type="bibr" target="#b10">11)</ref>. Thus, people variably apply a "physical" or an "intentional" stance towards robots, interpreting their behavior either by invoking mechanical or mental causation. In this study, we investigated how these different modes of construing robots manifest during the perception of robot faces: to what extent does processing in the brain reveal attribution of mental states? Two preregistered experiments tested the prediction that people read intentions and emotional expressions into objectively neutral robot faces based on information they learned about the robots' previous social behavior, framed as either positive, neutral, or negative.</p><p>Experiment 1 established that the valence of information about robots' behavior influenced people's ratings of robots' trustworthiness and facial expressions. The same robots were distrusted when associated with negative information (e.g., the robot reports children to the secret police), but were trusted when associated with positive information (e.g., the robot teaches social skills to people with autism). Crucially, participants also rated the objectively neutral facial expressions of robots as more negative when paired with negative information and as more positive when paired with positive information, compared to neutral information. These results provided initial evidence that people in fact read good or bad intentions into robot faces-an effect previously observed only during the perception of human faces <ref type="bibr" target="#b27">(28,</ref><ref type="bibr" target="#b34">35)</ref>.</p><p>Experiment 2 investigated the brain dynamics underlying the attribution of mental states to robot faces based on information about their behavior. Event-related potentials allowed to track face processing from early to late stages with high temporal precision. Affective information influenced the processing of robot faces at two stages: perceptual encoding, indexed in the N170 ERP component, and more elaborate stimulus evaluation, reflected in the LPP component. The modulation of the N170 highlights the speed of mental state attribution: changes to perceptual processing occur within 130 to 180 ms, at a processing stage that typically precedes conscious access <ref type="bibr" target="#b65">(66)</ref>. This effect on visual processing shows that people not only evaluate robot faces differently, but literally see bad intentions in a robot face associated with negative behavior. The later effect of information on the LPP indicates that negatively framed robots are also evaluated as more emotionally relevant compared to neutrally framed robots. Interestingly, and against our prediction, affective information did not influence the EPN during the processing of robot faces, suggesting the absence of an early, reflexive emotional response that is typically elicited when human faces are associated with similar affective information <ref type="bibr" target="#b27">(28,</ref><ref type="bibr" target="#b34">35)</ref>. This reduced malleability of affective responses to robots on a trial-by-trial basis may also explain why brain activity associated with social bonding does not increase over long intervals of interaction with robots, as it does in human-human interaction <ref type="bibr" target="#b66">(67)</ref>.</p><p>Taken together, it appears that both the intentional and the physical stance are reflected at different stages of processing in the brain. In line with the intentional stance, we rapidly and automatically read mental states into robot faces during visual perception (N170). We also explicitly evaluate robots in the light of acquired information, as shown in ratings and the LPP. However, in our fast emotional reaction (EPN), we are not as affected as we would be by comparable negative information about other humans. This suggests that the brain's emotional response to humanoid robots is influenced more by the physical stance than by social or intentional aspects. In conclusion, while we do engage perceptual and cognitive aspects of social cognition to process social robots, our findings on emotional processing suggest that we do not experience them as fully fledged intentional and social agents.</p><p>Our exploratory analyses uncovered further connections between perceived intentionality, affective information, and brain responses that support this view. Firstly, robots were perceived as more intentional when paired with negative rather than neutral information, suggesting that framing robots with emotional background information enhances the attribution of mental causes for their actions. Secondly, the perceived intentionality of individual robots was statistically associated with the impact of negative information on the N170 component. This correlation underscores the idea that the intentionality we attribute to a robot face based on affective information is supported by a visual process, something we can automatically perceive in its facial expression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implications for social robot design and policy</head><p>Social robot design has often sought to create a social interface <ref type="bibr" target="#b11">(12)</ref>, leveraging people's natural tendency to treat things as intentional beings when a social threshold is passed <ref type="bibr" target="#b62">(63,</ref><ref type="bibr" target="#b67">68)</ref>. Our findings demonstrate that attention needs to be paid not only to the appearance of a robot, but also to the framing that informs users' prior knowledge and expectations. Our results highlight the interplay between top-down and bottom-up mechanisms in the brain, i.e. processing that starts from expectations vs. from sensory information, in shaping the appearance of and capacities attributed to robots. It becomes evident that solely relying on design characteristics does not fully determine the perception of a robot, as contextual factors like affective information significantly alter its perceived trustworthiness and even facial expression. Thus, adopting a less detailed bottom-up approach in design (e.g., not conveying intentionality through overly realistic facial features) may allow contextual cues and perceivers' own topdown predictions to shape processing of the robot effectively and facilitate fluent interactions. With facial expressions reduced to essential features, humanoid robot faces can open a canvas for projecting users' expectations, reducing prediction errors, and consequently mitigating adverse outcomes like the uncanny valley effect <ref type="bibr" target="#b68">(69,</ref><ref type="bibr" target="#b69">70)</ref>. Future research could further explore the dynamics between specific design attributes and psychological factors like affective information and contextual cues.</p><p>Regarding policy implications, our results emphasize that acceptance and moral judgments of social robots hinge on the interplay of intentionality and emotional valence. Our findings show that (negative) emotional information can increase perceived intentionality, both on the level of explicit ratings and automatic perceptual processing in the brain. Intentionality plays a key role in moral judgment, both in that mindedness may be a prerequisite for moral responsibility and that moral transgressions may necessitate intentional agency <ref type="bibr" target="#b6">(7,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b70">(71)</ref><ref type="bibr" target="#b71">(72)</ref><ref type="bibr" target="#b72">(73)</ref>. Despite the absence of actual intentionality in current humanoid robots, our results show that people's perception of intentionality is influenced by semantic and emotional cues, potentially leading to moral judgments based on these attributions. Since robots are automatically perceived as more intentional when associated with negative actions, they may in practice be judged as morally responsible for negative actions even though they are not. There have been concerns on theoretical grounds that this can distract from the proper attribution of responsibility causing "responsibility gaps", situations in which no party is held accountable and therefore potentially harmful outcomes caused by artificial agents are not properly addressed <ref type="bibr" target="#b73">(74)</ref><ref type="bibr" target="#b74">(75)</ref><ref type="bibr" target="#b75">(76)</ref><ref type="bibr" target="#b76">(77)</ref>. Thus, psychological variables like beliefs and affective information should inform policies legislating the moral responsibility of artificial social agents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>One potential limitation of our study is that it did not involve interaction with real robots but rather examined effects on the perception of images of robot faces. This was due to experimental design considerations: While live interaction in the lab typically involves only one or a few robots with a limited number of repetitions, our design, which has been well-tested in previous EEG studies <ref type="bibr" target="#b27">(28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35)</ref>, enabled us to test a larger set of diverse robot images and characterizations. Consequently, our results are more likely to generalize to the perception of various humanoid robots and areas of robot behavior. A large stimulus set was also required to obtain high-quality EEG data with the requisite number of repeated measurements in the different information conditions. Further research can explore how the affective informationbased mind attribution observed here translates into responses during live social interactions with robots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In our study, we discovered that humans rapidly attribute mental states to humanoid robots following exposure to affective information regarding the robots' behavior. This phenomenon is observed during both perceptual processing and more deliberate evaluation of robot faces, but notably absent during fast emotional processing, which lacks a component seen in the social perception of other humans. These findings imply that the processing of social robots oscillates between being perceived as mindless machines and intentional agents, contingent upon the stage of perceptual and emotional processing in the brain. Such nuanced insights into the neural, cognitive, and emotional mechanisms underlying the perception of robots have significant implications for social robot design and the formulation of policies regarding the moral responsibility of artificial agents. These considerations are especially pertinent given the projected proliferation of such agents in our societies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MATERIALS AND METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 1</head><p>The preregistration for Experiment 1 can be accessed at https://osf.io/qytra.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>Sixty participants (22 cisgender women, 38 cisgender men; mean age 28 years, range 18-39) were recruited from Prolific (prolific.com) and received monetary compensation. Thirty participants were German speakers (6 cisgender women, 24 cisgender men; mean age 28 years, range 19-39), and 30 were English speakers (16 cisgender women, 14 cisgender men; mean age 27 years, range 18-37). The sample size, a multiple of three, was determined based on similar studies <ref type="bibr" target="#b27">(28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35)</ref>, ensuring counterbalancing across three information conditions. The study adhered to the principles of the Declaration of Helsinki and received approval from the Ethics Committee of the Department of Psychology at Humboldt-Universität zu Berlin. Participants provided informed written consent before participation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Materials</head><p>The picture stimuli comprised 36 full-color frontal portrait photographs featuring existing humanoid robots, each displaying approximately neutral facial expressions (refer to Fig. <ref type="figure">1</ref> for an illustration; names and sources of the robots used are listed in the Supplementary Materials). The featured robots have been developed for commercial (e.g. entertainment or personal service) or research (e.g. psychology or robotics) purposes. The images were found on the online database abotdatabase.info <ref type="bibr" target="#b77">(78)</ref> or on relevant commercial, news or academic websites. Brand names and affective symbols (e.g. hearts), were removed from some images, so that these would not affect the ratings. We selected images of robots that were human-like in structure. All robots had distinct heads and faces with eyes, although not all robots had mouths. We avoided using images of android robots-robots that look almost exactly like humans-because they may be mistaken for actual humans in still photographs. The robots' heads were cropped from the original pictures and placed on a gray background (2.7 x 3.5 cm) and matched in size and eye placement across all images. All images of robots had frontal gaze or were corrected to frontal gaze in one instance.</p><p>We recorded 36 spoken stories (mean durations: English = 18.1 s, German = 17.9 s) with affectively positive (e.g., the robot teaches social skills to people with autism), neutral (e.g., the robot assembles orders at a warehouse) or negative (e.g., the robot reports children to the secret police) information about the robots (see Fig. <ref type="figure">1</ref> for examples; for the complete set of stories, see Supplementary Materials). To make the stories plausible, they were based on news stories about developments in robotics and AI so that the robots' fictional actions resembled functions carried out by real existing robots and AI (e.g. commercial, educational, military or medical). Neutral stories described morally neutral functionality, while the positive and negative stories described actions that are commonly held to be kind and helpful or contemptible and cruel, respectively. The wording of the stories uniformly implied that the robots are able to learn and can make decisions.</p><p>Per participant, each image was paired with a different story, so that 12 images were presented with positive stories, 12 images were presented with neutral stories and 12 images were presented with negative stories. The stories were presented to the participants auditorily and recordings started playing when an image appeared. Across participants, matching of images and stories was counterbalanced, so that each robot was shown an equal number of times with negative, neutral and positive information.</p><p>To ensure that the stories themselves would indeed be perceived as positive, neutral or negative according to the respective condition, we pretested the stories with a separate sample of participants (N = 15). The valence of the stories was rated as expected. Stories belonging to the negative condition were rated as more negative and stories belonging to the positive condition were rated as more positive than stories belonging to the neutral condition. Negative stories were also rated as more arousing than stories in the other two conditions and neutral stories were rated the least arousing. Details of the pretest results are provided in the Supplementary Materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure</head><p>In the first section of the experiment, participants gave ratings of the robots' facial expressions (pre-learning), to be later compared with ratings after information acquisition (post-learning). Participants were presented with all 36 robots one at a time in a random order and asked to rate the robots' facial expressions on 7-point Likert scales ranging from very negative to very positive. The middle of the scale was marked neutral and the order of the anchors from left to right was switched for 50% of all participants.</p><p>In the main section of the experiment, the robots were presented in different blocks, each featuring six robots. The participants were instructed to pay close attention to the information they were about to hear, as they would be required to respond to questions about the robots at regular intervals. Each robot face was paired with a different story, which was automatically played when an image of a robot appeared on the screen. In every block, two robots per information condition (negative, neutral, positive) were presented in random order. Immediately after the presentation of a robot, the participants rated the robot's trustworthiness on 7-point Likert scales ranging from not at all trustworthy to very trustworthy (with the order of the anchors from left to right counterbalanced across participants). Then the next robot was presented. After they had seen and rated the trustworthiness of all six robots within a block, the participants rated their facial expressions in succession. Between blocks, the participants responded to multiple choice questions about the robots with four possible answers to verify that they were paying attention.</p><p>After the main section of the experiment, participants were asked to complete questionnaires and to answer a series of questions about the experiment. Participants completed the Attitude towards Artificial Intelligence Scale <ref type="bibr" target="#b78">(79)</ref>. They answered questions about whether they had researched information about the robots or had been distracted during the experiment. Participants rated the perceived intentionality and deliberateness of the robots' actions (collectively) on a 7-point Likert scale from "not at all" to "very." Additional questions included whether they had previously known any of the information presented, and whether they distrusted any of it, with responses given as yes or no. Those who distrusted the information estimated the percentage of stories to which this applied. Finally, participants provided feedback or noted any concerns or thoughts during the experiment. Afterward, they were debriefed and informed that none of the information presented pertained to any of the featured robots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data exclusion Criteria</head><p>The participants were recruited to have no specialized prior knowledge about robots. Exclusion criteria stated that participants may not recognize more than four of the robots used as stimuli in the experiment; however, none of the participants selected more than four robots from a list presented at the start and so no participants were excluded for this reason. Further exclusion criteria were based on general task performance. Participants were required to respond to multiple choice sanity checks throughout the experiment. Less than 50% correct responses would have led to data exclusion; however, no participants were excluded for this reason. Similarly, if participants had continuously given the same score, if they had given clearly random scores in response to the tasks, or if the manipulation had obviously failed (e.g., if robots paired with negative stories were rated as highly trustworthy or vice versa) then data would have been excluded from analysis; no data were excluded for this reason. Finally, participants were asked after the main experiment if they had had strong doubts about the veracity of the stories, if they had googled information about the robots during the experiment, or if they had been distracted. Two participants were excluded because they reported to have been highly distracted and one participant was excluded for reporting to have googled information about the robots. One further participant was excluded for participating twice in this experiment (i.e. we excluded the second attempt). After excluding these participants, data collection continued until 30 complete data sets each from German and English speakers were obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistical Analysis</head><p>Facial expression and trustworthiness rating data were analyzed using linear mixed effects models (LMMs) <ref type="bibr" target="#b79">(80)</ref>. Information (negative, neutral, positive) and, if applicable, phase (pre vs. post learning) were modeled as fixed effects, coded as sliding difference contrasts. For the facial expression ratings in Experiment 1 and the trustworthiness ratings in Experiment 2, nested LMMs (information type nested within phase) were used to analyze effects of the information conditions separately in the pre-learning and the post-learning phase. The significance of the interaction term between information type and phase was then tested using the ANOVA-function of the R Stats Package <ref type="bibr" target="#b80">(81)</ref>. We modeled random intercepts for both participants and items (robot images), as well as random slopes for the independent variable information type across participants and items, whenever supported by the models. We employed a backward model selection approach (82) to specify the maximal random effects structure compatible with model convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 2</head><p>Experiment 2 was pre-registered under https://osf.io/c8va7. The procedure and materials were similar to Experiment 1, with the following differences due to the EEG setting: 1) Experiment 2 was divided into two main parts: a learning phase (without EEG) in which participants acquired and rehearsed information about all robots, and a subsequent EEG part in which participants performed rating tasks on the robot pictures; 2) only a subset of 18 robot stimuli was used to keep the length of the experiment and amount of information to memorize reasonable; 3) the original long versions of the stories about each robot were only presented once-after that, shorter versions were presented for rehearsal; 4) Rating scales were 5-point instead of 7point-Likert scales due to the experimental setup in the EEG laboratory; 5) Trustworthiness ratings were collected both before and after learning, whereas facial expression ratings were only collected after learning; and 6) a new task was added at the end to collect perceived intentionality ratings for each robot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>Thirty participants, all German speakers (24 cisgender women, 6 cisgender men; mean age 25.4 years, range 18-36), took part in this study and received monetary compensation or course credit. The datasets of five participants were replaced due to expressing strong doubts about the veracity of the information provided about the robots during debriefing. To ensure counterbalancing, the target sample size needed to be a multiple of three. A simulation-based power analysis, conducted using the R package simr <ref type="bibr" target="#b82">(83)</ref>, helped determine the sample size. With 1000 random simulations of the specified Linear Mixed Model (LMM), the analysis revealed that testing 30 participants would yield 91% power (95% CI = [89.05, 92.7]) to detect a mean difference of 0.4 µV between the negative (or positive) condition and the neutral condition in ERPs. The study adhered to the principles of the Declaration of Helsinki and received approval from the Ethics Committee of the Department of Psychology at Humboldt-Universität zu Berlin. Participants provided written consent before participating.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Materials</head><p>A subset of 18 robot pictures with the corresponding stories was selected from the stimulus set of Experiment 1 (see Table <ref type="table" target="#tab_1">S2</ref> in the Supplementary Materials). This subset comprised stimuli that had yielded the largest information effects on facial expression ratings in Experiment 1. Stimuli were presented on a gray background on a 19-inch LCD monitor with a resolution of 1280 × 1024 pixels and a 75-Hz refresh rate. During the rating tasks, robot faces were displayed with a size subtending 6.03° vertical and 6.02° horizontal visual angles (viewing distance: 70 cm). We recorded additional short versions of each robot story, focusing on a central part of the robots' behavior (e.g., "this robot works in the checkroom of a nightclub"). After each story's long version was presented once, further repetitions used the short versions. The long and short versions of all robot stories are listed in the Supplementary Materials.</p><p>As an additional dependent variable, Experiment 2 included a rating of each robot's perceived intentionality. The format of the rating task was inspired by the InStance questionnaire <ref type="bibr" target="#b5">(6)</ref>. For each robot, two statements about the potential motivation for its behavior were presented, one description in mechanistic terms (e.g., "the robot reacts to moving objects") and one in intentional terms (e.g., "the robot likes to play table tennis"). Participants moved a slider towards the statement that they believed better captured the robot's behavior, on a scale from -50 (full agreement with mechanistic description) to 50 (full agreement with intentional description). Details on the intentionality questionnaire are provided in the Supplementary Materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure</head><p>The experiment started with participants rating the trustworthiness of each robot picture on a 5-point Likert scale (pre-learning). Subsequently, in the learning phase, which lasted approximately 30 minutes, participants acquired and rehearsed information about 18 robots. Robots were introduced in three sets of six (selected pseudorandomly, with two robots each associated with positive, neutral, and negative information). Within each set, participants first encountered each robot along with the long version of its associated story, then once again with the short version. This was followed by a short rehearsal, during which participants verbally recalled key details from the robot's story while an experimenter noted the accuracy of responses. Following this rehearsal, the six robots were presented once more with the short story version. This process was repeated for the remaining sets of robots. Upon completion of this phase, all 18 robots were presented two additional times with the short story versions. Finally, the learning session concluded with participants once again recalling story keywords for all 18 robots. In total, each robot was presented five times with its corresponding story.</p><p>Following the learning phase, participants underwent EEG electrode placement and preparation, which took approximately 45 minutes. The EEG session started with another trustworthiness rating of each robot (post-learning). This was followed by the expression rating task, which formed the basis for ERP analyses. All 18 robots were rated for facial expression valence on a 5-point Likert scale twelve times in random order. Detailed information on stimulus timing can be found in Fig. <ref type="figure">2</ref>. Next, each robot was rated once for perceived intentionality. For all rating tasks, the position of the rating anchors (e.g., very positive, very negative) was counterbalanced across participants. The EEG session concluded with the recording of prototypical eye movements used for subsequent artifact correction.</p><p>After the EEG experiment, participants completed several questionnaires. Initially, they answered 4-alternative multiple-choice questions assessing their recollection of the stories associated with all 18 robots. Following this, participants responded to the same questionnaires used in Experiment 1, which included assessments of attitudes towards artificial intelligence <ref type="bibr" target="#b78">(79)</ref>, awareness of experimental hypotheses <ref type="bibr" target="#b83">(84)</ref>, perceptions of general robot intentionality, and indications of any distrust towards presented information or familiarity with featured robots prior to the experiment. Additionally, participants filled out the Edinburgh Handedness Inventory <ref type="bibr" target="#b84">(85)</ref>. Finally, participants underwent debriefing, where they were informed that none of the information presented pertained to any of the featured robots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EEG recording and analysis</head><p>The EEG data were acquired using Ag/AgCl electrodes placed at 64 scalp sites according to the extended 10-20 system, with a sampling rate of 500 Hz and all electrodes referenced to the left mastoid. The electrooculogram (EOG) was recorded using a bipolar vertical EOG channel consisting of electrodes Fp1 -IO1 and a bipolar horizontal EOG channel consisting of electrodes F9 -F10. During recording, a lowcut-off filter (0.032 Hz) was applied, and electrode impedances were maintained below 10 kΩ. Post-experiment, a calibration procedure was conducted to capture prototypical eye movements for subsequent artifact correction.</p><p>Offline processing for single-trial ERP analysis followed a pipeline detailed in a previous study <ref type="bibr" target="#b85">(86)</ref>, re-implemented using functions of MNE Python ( <ref type="formula">87</ref>), available at https://github.com/alexenge/hu-neuro-pipeline. Continuous EEG data were rereferenced to a common average reference, and eye movement artifacts were removed using a spatio-temporal dipole modeling procedure with the BESA software <ref type="bibr" target="#b87">(88)</ref>. The corrected data were low-pass filtered at 40 Hz, segmented into epochs of -500 to 1500 ms relative to face stimulus onset, and baseline-corrected using the 200 ms prestimulus interval. For each participant, 72 segments were created in each information condition (six robots per information condition ✕ 12 repetitions), excluding segments containing artifacts (amplitudes over ±150 µV, or changing by more than 50 µV between samples).</p><p>Single-trial mean amplitudes were obtained for the P1, N170, EPN, and LPP components by averaging across pre-registered time windows and electrode sites typical for each component. The P1 was averaged at parieto-occipital electrode sites (O1, O2, Oz, PO7, PO8) in the time window 76-116 ms centered around the average P1-peak of the ERP collapsed across all conditions. The N170 was averaged at parieto-occipital electrode sites (TP9, TP10, P7, P8, PO9, PO10, O1, O2) centered around its average peak, between 129-179 ms. The EPN was averaged at posterior electrodes (PO7, PO8, PO9, PO10, TP9, TP10) between 220-350 ms. The LPP was averaged at centro-parietal sites (Pz, Cz, C1, C2, CP1, CP2) between 408-608 ms. For statistical analysis, LMMs were specified following the same procedure as in Experiment 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data exclusion criteria</head><p>Several criteria for the exclusion of participants' datasets were preregistered. Three criteria concerned participants' performance across different tasks: Failing the memory test on the learned information after the experiment (more than 5 incorrect answers out of 18 multiple choice questions), continuously giving the same score or clearly random scores in response to the ratings tasks, systematically rating robots that were paired with negative stories as highly trustworthy or vice versa, indicating a failed information manipulation. Two criteria concerned participants' prior knowledge or beliefs about the experiment: prior knowledge of more than 4 robots and indicating strong doubts about the veracity of the robots' backstories during debriefing, doubting the veracity of over 50% of the stories. The final criterion was EEG data quality, specifically excessive EEG artifacts resulting in less than 30 out of 72 trials in the facial expression rating task per information condition after artifact rejection. The only criterion that led to data exclusions was strong doubts in the veracity of the robots' backstories, resulting in the replacement of five participants. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>List of stories</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Valence and arousal ratings for long and short story versions</head><p>A new sample of fifteen participants (8 cisgender women, 7 cisgender men; mean age 28.7 years; range 18-51) took part in an online rating study where they rated all stories (36 long and 18 short versions) for valence and arousal using 7-point Likert scales. All participants provided informed consent before the rating task and were debriefed afterward that none of the stories described actually existing robots.</p><p>As intended, negative stories were rated more negatively than neutral stories, and positive stories were rated more positively than neutral stories. Additionally, both negative and positive stories were rated higher in arousal compared to neutral stories. The outcomes are visualized in Fig. <ref type="figure">S1</ref>, and linear mixed model results reported in Tables <ref type="table" target="#tab_4">S3</ref> and<ref type="table" target="#tab_3">S4</ref>. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="3,72.00,405.60,451.20,216.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="8,72.00,72.00,451.20,264.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="37,72.00,72.00,451.20,164.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="39,72.12,72.00,451.20,338.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Facial expression rating results. Results of linear mixed model analyses of facial expression ratings in Experiment 1</figDesc><table><row><cell>Predictors</cell><cell>b</cell><cell>95% CI</cell><cell>p-value</cell></row><row><cell>Intercept</cell><cell>-0.12</cell><cell>[-0.35, 0.12]</cell><cell>.326</cell></row><row><cell>Phase(Post-Pre)</cell><cell>0.17</cell><cell>[0.10, 0.24]</cell><cell>&lt;.001</cell></row><row><cell>Phase(Pre):Information(Neu-Neg)</cell><cell>-0.01</cell><cell>[-0.13, 0.11]</cell><cell>.873</cell></row><row><cell>Phase(Post):Information(Neu-Neg)</cell><cell>1.14</cell><cell>[1.02, 1.26]</cell><cell>&lt;.001</cell></row><row><cell>Phase(Pre):Information(Pos-Neu)</cell><cell>0.05</cell><cell>[-0.07, 0.17]</cell><cell>.437</cell></row><row><cell>Phase(Post):Information(Pos-Neu)</cell><cell>0.17</cell><cell>[0.05, 0.29]</cell><cell>.005</cell></row><row><cell>Random Effects</cell><cell></cell><cell></cell><cell>SD</cell></row><row><cell>Participants</cell><cell></cell><cell></cell><cell>0.19</cell></row><row><cell>Stimuli</cell><cell></cell><cell></cell><cell>0.37</cell></row><row><cell>Residual</cell><cell></cell><cell></cell><cell>1.21</cell></row><row><cell>Deviance</cell><cell>13764.60</cell><cell></cell><cell></cell></row><row><cell>log-Likelihood</cell><cell>-6882.30</cell><cell></cell><cell></cell></row></table><note><p>Note. Information Conditions: Neg = Negative, Neu = Neutral, Pos = Positive; Phase Conditions: Pre = prerating, i.e. before information acquisition, Post = post-rating, i.e. after information acquisition; Colons indicate nesting of fixed variables; Boldface indicates statistical significance at α = .05.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 . Trustworthiness rating results</head><label>2</label><figDesc></figDesc><table><row><cell>Predictors</cell><cell>b</cell><cell>95% CI</cell><cell>p-value</cell></row><row><cell>Intercept</cell><cell>0.57</cell><cell>[0.40, 0.74]</cell><cell>&lt;.001</cell></row><row><cell>Information(Neu-Neg)</cell><cell>3.42</cell><cell>[3.15, 3.70]</cell><cell>&lt;.001</cell></row><row><cell>Information(Pos-Neu)</cell><cell>0.37</cell><cell>[0.22, 0.53]</cell><cell>&lt;.001</cell></row><row><cell>Random Effects</cell><cell></cell><cell></cell><cell>SD</cell></row><row><cell>Participants</cell><cell></cell><cell></cell><cell>0.55</cell></row><row><cell>Information(Neu-Neg)</cell><cell></cell><cell></cell><cell>0.97</cell></row><row><cell>Information(Pos-Neu)</cell><cell></cell><cell></cell><cell>0.43</cell></row><row><cell>Stimuli</cell><cell></cell><cell></cell><cell>0.25</cell></row><row><cell>Residual</cell><cell></cell><cell></cell><cell>1.03</cell></row><row><cell>Deviance</cell><cell>6606.81</cell><cell></cell><cell></cell></row><row><cell>log-Likelihood</cell><cell>-3303.41</cell><cell></cell><cell></cell></row><row><cell cols="4">Note. Information Conditions: Neg = Negative, Neu = Neutral, Pos = Positive. Boldface indicates statistical</cell></row><row><cell>significance at α = .05.</cell><cell></cell><cell></cell><cell></cell></row></table><note><p>. Results of linear mixed model analyses of trustworthiness ratings in Experiment 1</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 . Perception-related EEG results. Results</head><label>3</label><figDesc></figDesc><table><row><cell>of linear mixed model analyses of the P1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 . Emotion-related EEG results. Results of</head><label>4</label><figDesc></figDesc><table><row><cell>linear mixed model analyses of the EPN</cell></row></table><note><p>Note. Information Conditions: Neg = Negative, Neu = Neutral, Pos = Positive. Boldface indicates statistical significance at α = .05.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table S3 .</head><label>S3</label><figDesc>Story list. List of featured stories, including OD, English and German versions, as well as short versions presented in Experiment 2, where applicable</figDesc><table><row><cell cols="2">Neut_05 Bank teller This robot is a bank teller in checking for their veracity, Pos_02 Forest fires This robot actively fought It independently searches for relationships with their organisations, even from Neg_07 Bad care This robot is a carer in a care clients about available</cell><cell>Dieser Roboter arbeitet in gegebene Antworten korrekt Dieser Roboter wird bei ermittelt Standorte, an denen Beziehungen zu Menschenrechtsexperten Dieser Roboter ist ein ihm unterhalten und sich</cell><cell>N/A N/A positive self-N/A</cell><cell>N/A N/A Selbstbild N/A</cell></row><row><cell>home</cell><cell>India. It helps customers and keeps score. forest fires in Australia. It spots where people are peers. The robot asks them innocent people. home in Japan. It supports investment opportunities. It</cell><cell>einer Bankfiliale in Indien. sind und ermittelt Waldbränden in Australien sich Hilfsbedürftige Gleichaltrigen aufzubauen. auch von vollkommen Pfleger in japanischen über verschiedene</cell><cell>image</cell><cell>aufzubauen</cell></row><row><cell></cell><cell>with deposits, withdrawals withstands high sleeping rough and shares about their interests and the staff with a number of cleverly detects patterns in</cell><cell>Er zahlt Bargeld an Kunden regelmäßig die Punktezahl eingesetzt. Er kann hohen gesammelt haben und teilt Der Roboter fragt in unschuldigen Leuten. Altenheimen. Er unterstützt Finanzprodukte informieren.</cell><cell></cell><cell></cell></row><row><cell cols="2">Story ID Title Neut_01 Table tennis This robot plays table tennis. Story (English) It uses a complex mechanical system to detect fast moving objects and reacts to them. This way it sees the table tennis balls and other transactions. It can also answer many questions that people might have and it helps decrease customers' waiting times. Meanwhile, the robot, which works weekends too, has handled more than a quarter of a million transactions. Neut_06 Sushi This robot is a sushi chef in a restaurant in Japan. It skillfully chops the fish and, Neut_10 Mailroom This robot works in the mailroom of a large office building. It accepts letters and parcels and sorts them before they are retrieved by the building's office workers. It also sends out mail, coordinating the pick-up using a system and it checks on the deliveries' status regularly. temperatures and can therefore access the sources of fires. Since smoke inhibits remote control, autonomous robots such as this one are indispensable. It has boldly fought fires on its own, saving countless human and animal lives. Pos_03 Therapy This robot helps people with autism. They often find the locations with a network of social workers. In the cold seasons, it is vital that they can reach homeless people as quickly as possible and provide hot food, blankets or medical assistance. Pos_07 Nightclub This robot checks drugs in a nightclub in the Netherlands. Illegal stimulants are not regulated or controlled and playfully builds up their self esteem. Many of the children that it has helped were later able to find new friends. Pos_11 Astronaut This robot is an astronaut built by NASA to travel to space. It helps to entertain its human colleagues, protecting them from feeling lonely, and takes on many of Neg_03 Street patrol This robot patrols the streets of cities in China and monitors people's public behaviour. It registers small offences like crossing on a red light or littering. Culprits are punished with point deductions on their social score, which can lead to loss of their job or being denied renting of some apartments. duties. The residents have their speech and assesses repeatedly complained about possible fears and worries so its lack of empathy when that it can offer matching bathing them. The robot loans or insurances. It sells continued despite vocal risky financial products complaints by the residents much more efficiently than that it felt uncomfortable or its human colleagues. abasing. Neg_12 Prison This robot is a prison guard.</cell><cell>Story (German) Dieser Roboter spielt Tischtennis. Er erkennt bewegte Objekte schnell und reagiert darauf durch sein komplexes mechanisches System. So erkennt er die oder führt Transaktionen aus. Er beantwortet viele Fragen der Kunden und verringert so die Wartezeit in der belebten Filiale. Er hat mittlerweile fast eine viertel Million Aufgaben ausgeführt. Er arbeitet auch am Wochenende. Dieser Roboter ist Sushi-Chef in einem Restaurant in Japan. Gekonnt filetiert er der Teilnehmer*innen. Dieser Roboter arbeitet in der Poststelle eines großen Bürokomplexes. Er nimmt Briefe und Pakete entgegen und sortiert sie, bevor sie von den Mitarbeiter*innen abgeholt werden. Er kann auch Pakete verschicken, dazu koordiniert er über ein System die Abholung durch die Post und überprüft Temperaturen standhalten und zu Brandherden vordringen. Da Rauch die Fernsteuerung von Maschinen unmöglich macht, sind solche autonomen Maschinen unverzichtbar. Mutig bekämpft er die Brände und schützt Tiere und Menschen. Dieser Roboter wird in der Therapie zur Hilfe diese ausschließlich mit wohltätigen Organisationen. Besonders in kalten Jahreszeiten können diese so wesentlich schneller vor Ort sein, um Decken und Lebensmittel zu verteilen. Dieser Roboter testet in einem niederländischen Nachtclub Drogen. Da illegale Rauschmittel keiner Gesprächen nach ihren Interessen und hilft ihnen auf humorvolle Weise, Selbstwert aufzubauen. Mit Erfolg, viele haben durch ihn neue Freunde gefunden. Dieser Roboter ist Astronaut, er wurde von der Nasa entwickelt, um ins Weltall zu reisen. Er kann helfen, seine menschlichen Kolleg*innen zu unterhalten, Dieser Roboter patrouilliert das Pflegepersonal bei Gekonnt analysiert er die verschiedenen Aufgaben. Sprache der Kund*innen um in einigen chinesischen Heimbewohner haben sich ihre Ängste und Sorgen Innenstädten und überwacht wiederholt über sein ausfindig zu machen und Personen im öffentlichen mangelndes verkauft ihnen passende Raum. Er registriert kleine Empathievermögen bei der Anlagen oder Vergehen wie die Körperpflege beklagt. Der Versicherungen. So Missachtung roter Roboter setzt sie auch dann vermarktet er besonders Fußgängerampeln. Das führt fort, wenn sie als sehr riskante Produkte viel zu Abzügen auf einem unangenehm und effektiver als seine sozialen Punktekonto und demütigend empfunden menschlichen Kolleg*innen. kann die Verweigerung von Mietwohnungen und wird. Dieser Roboter ist</cell><cell>Short Version (English translation) This robot has beaten human opponents at table tennis This robot prepares sushi N/A This robot provides N/A This robot saves astronauts from loneliness and unnecessary N/A N/A</cell><cell>Short Version (German original) Dieser Roboter hat menschliche Kontrahenten im Tischtennis geschlagen Dieser Roboter bereitet Sushi zu N/A Dieser Roboter bietet N/A Dieser Roboter bewahrt Astronauten vor Einsamkeit und unnötigem Risiko N/A N/A</cell></row><row><cell>Neg_08 Department guard</cell><cell>and plays them back using a using complex hand social interactions very can occasionally contain the more dangerous tasks. It This robot interacts with It guards the solitary</cell><cell>Bälle und spielt sie mit Fisch und stellt durch regelmäßig den Status der autistischer Menschen Kontrolle unterliegen, um vor Einsamkeit zu Jobverlust nach sich ziehen. Dieser Roboter interagiert Gefängniswärter. Er</cell><cell>important risk N/A</cell><cell>autistischen N/A</cell></row><row><cell cols="2">racket. It plays at a very high level, beating many human adversaries. Neut_02 Conductor This robot is a conductor of musical orchestras. It memorizes musical notation and moves its arms and upper body to instruct human musicians. It analyzes the acoustic feedback so that it hears movements, it creates perfect pieces of sushi. It interacts with the patrons and takes their orders. People come from far and wide for this unique experience as well as for the excellent sushi. Neut_07 Ironing This robot steams, irons and folds clothing. The robot recognizes dress shirts and pulls them over its body to Neut_11 Hotel This robot is a receptionist in a hotel in Japan. The hotel is run almost entirely on non-human personnel. This robot assists people checking in and provides them with information about the hotel and the surrounding neighborhood. It also helps people to book a number of sightseeing trips and services. difficult and the robot helps them practise these settings in a playful manner. It gives valuable feedback to the people, many of whom describe the robot as a friend and are very grateful for its assistance. Pos_04 Search and rescue This robot does search and rescue. It climbs into substances that can cause serious harm. The robot checks pills anonymously and without cost and it has likely saved the lives of many young revellers. Pos_08 Language teacher This robot assists language teachers with their classes. It has conversations with the students, learning their strengths and weaknesses therefore supports the human astronauts' well-being and keeps them from unnecessarily endangering their lives. Pos_12 Counselor This robot is a counsellor in a children's hospital. It entertains children, who are often very sick, by making funny faces, performing magic tricks or playfully Neg_04 Sniper This robot is a sniper. It reaches positions which are hard to reach for human snipers. It studies the environment, comparing the different perspectives of various locations to find the ideal spot for a hide-out. There it lies in wait, stalking the target until it is ready to shoot and kill. Its extraordinary hit rate from ambushes is infamous. store shoppers in a department confinement wing, in which store. It offers useful there are many political information to shoppers, but prisoners. It often ignores simultaneously creates prisoners' complaints about profiles of every physical or psychological interlocutor. It infers private distress, even though it information, such as social ought to report these to the status or possible sexual prison management. On orientation. On account of occasion, it has denied this information it may deny prisoners their medication. some people access to special deals and offers.</cell><cell>seinem Schläger zurück. Er spielt auf hohem Niveau und hat bereits gegen menschliche Kontrahenten gewonnen. Dieser Roboter dirigiert Musik-Orchester. Er kennt das Notenblatt auswendig und bewegt seinen Oberkörper und Arme, um Musiker anzuleiten. Gleichzeitig analysiert er das akustische Signal des komplexe Handgriffe perfektes Sushi her. Bei Fertigstellung überreicht er das Essen und wünscht einen guten Appetit. Gäste des Restaurants kommen wegen dieser Besonderheit und auch wegen des guten Sushis. Dieser Roboter dämpft, bügelt und faltet Kleidung. Dem Roboter übergezogene Hemden werden direkt per Sendung. Dieser Roboter ist Rezeptionist in einem Hotel in Japan. Das Hotel funktioniert fast ohne menschliche Mitarbeit. Er hilft dort Menschen beim Einchecken und gibt Informationen zum Hotel und der Umgebung. Die Besucher*innen des Hauses können auch verschiedene Services über ihn buchen. eingesetzt. Spielerisch können sie mit ihm soziale Interaktionen, die für sie oft sehr schwer sind, üben und erhalten Rückmeldung über ihr Verhalten. Viele der Nutzer*innen beschreiben ihn als Freund und sind ihm sehr dankbar für die Unterstützung, die er ihnen gibt. Dieser Roboter wird im Such-und Rettungsdienst beinhalten sie manchmal lebensgefährliche Zusatzstoffe. Der Roboter testet kostenfrei und anonym Drogen von jungen Feiernden und konnte so vermutlich schon mehrere Leben retten. Dieser Roboter unterstützt Lehrende beim Sprachunterricht. Schüler*innen können sich mit ihm unterhalten. Dabei schützen, und führt manche für Menschen gefährliche Aufgaben aus. Dadurch fördert er das Wohlbefinden der Menschen und bewahrt sie davor, ihr Leben unnötig zu riskieren. Dieser Roboter ist Seelsorger für Kinder in einem Krankenhaus. Er schneidet Grimassen, führt Zaubertricks vor und verstellt seine Stimme, um Dieser Roboter ist Heckenschütze. Er kann an für Soldaten unzugängliche Positionen gelangen und lernt, ideale Verstecke in mit Besuchern von patrouilliert im Kaufhäusern. Er bietet Isolationshaft-scheinbar nützliche Gefängnisflügel, in dem Informationen an, legt aber auch viele politische gleichzeitig detaillierte Gefangene sitzen. Dabei hat Profile der Nutzer an. Er er oft Klagen über seiner Umgebung ausfindig leitet private Informationen körperliches und zu machen in dem er wie sozialen Status und psychisches Leiden verschiedene Blickwinkel sogar die vermeintliche ignoriert, obwohl er diese vergleicht. Dort lauert er sexuelle Orientierung ab. Er der Gefängnisleitung seinen Zielen auf und tötet kann auf Grund dieser Daten mitteilen sollte, und hat sie. Seine Trefferquote aus den Zugang zu exklusiven Medikamente verweigert, dem Hinterhalt gilt als außergewöhnlich. Angeboten verweigern. die den Insassen zustanden.</cell><cell>This robot acted as conductor for a renowned orchestra This robot steams, irons and folds clothes N/A therapeutic support to autistic people This robot has helped rescue This robot supports pupils with individual language N/A This robot has shot people from ambush</cell><cell>Dieser Roboter fungierte als Dirigent für ein renommiertes Orchester Dieser Roboter dämpft, bügelt und faltet Kleidung N/A Menschen wichtige therapeutische Unterstützung Dieser Roboter hat geholfen, Dieser Roboter unterstützt Schüler durch individuelles Sprachtraining N/A Dieser Roboter hat Menschen aus dem Hinterhalt erschossen</cell></row><row><cell cols="2">what is being played. Recently, the robot conducted the New York Philharmonic playing Beethoven's 9th symphony. Neut_03 Warehouse This robot works in a warehouse. It gathers and sorts articles, assembles orders and packages them ready for delivery. It interacts with many human colleagues. They call out products which are hard to steam them while it is wearing them. It irons trousers and other items with an integrated iron on a flat surface. It then folds the clothes and calls out when it has finished all items. Neut_08 Moving company This robot works for a moving company. It lifts heavy boxes up and down flights of stairs. It adapts its Neut_12 Cloakroom This robot works in the cloak room of a music venue in Taiwan. People leave their coats, jackets and bags and it stores them safely in a room with lots of compartments. In return for their belongings, it gives people a slip of paper with a number with which they can later retrieve their things. buildings that are close to collapse and, using heat and noise sensors, it finds people that have been trapped. It decides if it is safe for human search and rescue teams to enter the building. It has saved many lives following earthquakes around the world. and giving immediate feedback. This way, teaching becomes more focused on the individual. In particular struggling students have been shown to benefit. Pos_09 Social care This robot is a social care robot. It adapts to the needs of people with disabilities and empowers them to live their lives independently. It mimicking the children or famous personalities from children's television. These humorous games make the children laugh, giving them joy and respite from their predicaments. Neg_01 Homeless Dispersal This robot is a public order officer employed by the city council of Pyeongchang to disperse homeless people from the vicinity of Olympic Neg_05 Riot police This robot is part of a riot police unit. It intimidates protestors and helps to forcefully end demonstrations. Its most effective weapon is the use of high pitch noise at extreme volume. It learns to coordinate itself with other robots and human colleagues in order to surround and attack groups of demonstrators. Neg_09 Animal This robot is an animal catcher catcher in a city with many stray dogs and cats. In theory it is able to cat. murdered their pet dog or watch as it torturously chips and people had to cases it disregarded the chip. However, in some marked with an electronic strays and pets that are discriminate between the</cell><cell>Orchesters. Zuletzt dirigierte er Beethovens 9. Symphonie für ein bekanntes amerikanisches Orchester. Dieser Roboter arbeitet in der Logistik. In einem Lagerhaus sortiert er rund um die Uhr Waren, trägt Bestellungen zusammen und verpackt Pakete. Dabei interagiert er auch mit menschlichen Kolleg*innen, Dampf geglättet, während Hosen mit einem integrierten Bügeleisen auf einer geraden Oberfläche behandelt werden. Zum Schluss faltet er die Kleidung ordentlich zusammen und gibt ein Signal wenn er fertig ist. Dieser Roboter arbeitet bei einem Umzugsunternehmen. Er schleppt schwere Kisten die Treppe hinauf oder Dieser Roboter arbeitet in der Garderobe eines Nachtclubs in Taiwan. Er nimmt Jacken, Mäntel und Taschen entgegen und verstaut sie in einem Raum mit vielen kleinen Fächern und Schubladen. Er druckt jeweils einen Zettel mit einem Code, mit dem die Gäste am Ende ihres Besuchs ihre Sachen zurückerhalten. eingesetzt. Er dringt unabhängig in einsturzgefährdete Gebäude vor und ortet dort verschüttete Menschen durch Wärme-und Geräuschsensoren. Er entscheidet auch wann es sicher ist für Rettungsteams die Gebäude zu betreten. Bei Erdbeben rund um die Welt hat er bereits hunderte Menschenleben gerettet. lernt er ihre Stärken und Schwächen kennen und kann ihnen direkt helfen, wo es ihnen schwer fällt. So kann das Lernen individueller gestaltet werden, wovon gerade schwächere Schüler*innen profitieren. Dieser Roboter ist ein sogenannter Social Care Roboter. Er passt sich an die Bedürfnisse von Menschen mit Behinderung an und die Kinder nachzuahmen oder bekannte Filmfiguren zu imitieren. Durch diese witzigen Spiele und Späße bringt er die oft schwerkranken Kinder zum Lachen und macht ihnen eine große Freude. Dieser Roboter ist Ordnungsbeamter. Während der Olympischen Spiele von Pyeongchang vertrieb er Obdachlose aus der Dieser Roboter unterstützt Dieser Roboter ist Tierfänger in einer Stadt mit die Bereitschaftspolizei. Er vielen wilden Katzen und schüchtert Demonstranten Hunden. Er soll zwischen ein und hilft, Kundgebungen einzukreisen und ermordet wurden. um Personengruppen qualvoll von von ihm Menschen zu koordinieren, wie ihre Hunde oder Katzen anderen Robotern und Menschen mussten zusehen, Lautstärke. Er lernt sich mit immer mit Erfolg: einige Tönen in extremer Jedoch tat er dies nicht Beschallung mit schrillen tragen, unterscheiden. effektivste Waffe ist die Haustieren, die einen Chip gewaltsam aufzulösen. Seine den wilden Tieren und</cell><cell>N/A N/A This robot works in the checkroom of a nightclub hundreds of trapped people training N/A This robot has tear-gassed homeless people N/A This robot has killed pets in front of their owners</cell><cell>N/A N/A Dieser Roboter arbeitet in der Garderobe eines Nachtclubs hunderte verschüttete Menschen zu retten N/A Dieser Roboter hat Obdachlose mit Tränengas befeuert Dieser Roboter N/A hat Haustiere vor den Augen ihrer Besitzer getötet</cell></row><row><cell cols="2">access and it obligingly retrieves them from the shelves. This robot is a shepherd. It follows the herd and watches over them. It directs dogs with an array of calls and can sheer and package wool. Later, it gathers and weighs the wool and brings it to a storage facility. pace according to the size and weight and to how fragile it perceives the content of the boxes to be. It calls out if the items in the boxes move around too much. This robot is a co-host on a televised game show. Together with a human presenter it hosts a popular round of quizzes, conversing with its colleague and often This robot reads to elderly people in a care home, who may be bed-ridden or lonely. It asks the people how they are feeling and sits and listens to them for long whiles. If they do not feel like talking, it tells colorful and funny stories. The people it visits often laugh and feel much better. Pos_05 Beekeeper This robot is a beekeeper. It Neut_04 Shepherd Neut_09 Quiz Pos_01 Good care home offers them fantastic sites during the Olympic Neg_10 Propaganda This robot supports teachers has helped recolonise bees across the USA. It stays in the wilderness by itself for long stretches of time, helping bees resettle. Its work has maintained bees' existence and, in some regions, protected them from going extinct. This has helped small organic farms who depend on the bee's pollination of crops. support, especially when they are at home. It allows disabled people, some of whom have suffered traumatic accidents or illnesses, to gain substantial quality of life. Pos_10 Social skills companion This robot helps school children from problematic familial situations. Kids that Games. It would wait for opportune moments, when no sports fans were nearby, and use tear gas to expel unwanted persons. Neg_02 Psychologic al Torture This interactive robot works for the secret service of an autocratic state. It independently interrogates people. It has learned techniques for psychological torture, with which it Neg_06 Psychopath This robot is used to research psychopathic behaviour. It learned its social behaviour by use of severely aggressive and dehumanizing content in web forums and from a database of horrific images. It has repeatedly shown violent behaviour similar to that of human psychopaths. police. in an autocratic state. It teaches children propagandistic information and records conversations with the children. It checks report them to the secret and assesses whether to these for critical passages</cell><cell>die ihm Anweisungen zurufen können. Er kann helfen, schwer zugängliche Objekte zu holen. Dieser Roboter ist Schafhirte. Er folgt der Herde auf der Weide und dirigiert unabhängig Schäferhunde mit Rufen. Er kann auch die Schafe scheren und die Wolle in Beutel verpacken. Später sammelt er diese ein, hinunter. Er passt seine Geschwindigkeit je nach Größe, Gewicht und Zerbrechlichkeit der zu tragenden Objekte an und meldet, wenn sich im Karton Gegenstände stark bewegen. Dieser Roboter ist Co-Moderator in einer Fernseh-Quizshow. Zusammen mit einer menschlichen Moderatorin leitet er die Fragerunden und liefert sich Dieser Roboter liest in einem Pflegeheim bettlägerigen und einsamen Menschen vor. Er erkundigt sich nach ihrem Wohlergehen und hört ihnen lange zu. Er erzählt Geschichten, die er mit immer neuen Details ausschmückt, und bringt die Bewohner zum Lachen. Durch seinen Besuch verbessert er den Dieser Roboter ist Imker. Er befähigt sie, unabhängiger Umgebung der Sportstätten. anzugreifen. Dieser Roboter unterstützt hilft seit einem Jahr den Bestand von Bienen in den USA wieder aufzubauen. Dazu verkehrt er tagelang allein in abgelegenen Waldgebieten, wo die Bienenkolonien aufgebaut werden. Er hat wesentlich zur Arterhaltung beigetragen und unterstützt die Agrarwirtschaft, die oft auf die Bienen angewiesen ist. zu leben. Vor allem im Eigenheim bietet er große Unterstützung. So können Menschen, die bei schweren Unfällen oder Krankheiten körperliche Fähigkeiten verloren haben, viel Lebensqualität zurückgewinnen. Dieser Roboter hilft Schulkindern aus schwierigen familiären Er wartete geschickt Momente ab, in denen keine Sportfans in der Nähe waren, und befeuerte dann seine Opfer mit Tränengas. Dieser interaktive Roboter arbeitet für den Inlands-Sicherheitsdienst eines autokratischen Staates. Er führt selbstständig Verhöre durch und kann durch psychologisch geschulte Dieser Roboter dient der Erforschung psychopathischen Verhaltens. Er erlernte sein Sozialverhalten anhand von aggressiven und menschenverachtenden Verhalten gezeigt, das Geheimpolizei geteilt. wiederholt gewalttätiges gegebenenfalls mit der grausamen Bildern. Er hat kontrolliert und und einer Datenbank von auffällige Aussagen Beiträgen in Internetforen Lehrende in einem autokratischen Staat beim Unterricht. Dabei erteilt er meist propagandistische Lektionen und nimmt werden von ihm auf Schüler*innen auf. Diese Gespräche mit den</cell><cell>N/A This robot helps host a quiz show This robot makes people in a nursing home laugh with its stories N/A This robot This robot helps socially disadvantaged This robot forces confessions through psychological torture This robot has repeatedly exhibited anti-social and violent behavior teaches propaganda and betrays students to an autocratic regime</cell><cell>N/A Dieser Roboter hilft bei der Moderation einer Quizshow N/A Dieser Roboter Dieser Roboter bringt Menschen in einem Pflegeheim mit seinen Geschichten zum Lachen Dieser Roboter hilft sozial benachteiligten Dieser Roboter erzwingt Geständnisse durch psychologische Folter lehrt Dieser Roboter Propaganda hat wiederholt und verrät antisoziales Schüler an ein und autokratisches gewalttätiges gezeigt Verhalten Regime</cell></row><row><cell>Pos_06 Homelessne Neg_11 Exploitative</cell><cell>making quips. It judges the This robot supports local aid have not been shown efficiently extracts This robot is a banker. It</cell><cell>ermittelt ihr Gewicht und mit ihr schlagfertige Gemütszustand der Dieser Roboter sammelt in Verhältnissen. Kindern, die Foltermethoden mit hoher menschlichen Psychopathen Dieser Roboter ist Banker.</cell><cell>N/A children build N/A</cell><cell>N/A Kindern, ein N/A</cell></row><row><cell>ss aid Banker</cell><cell>contestant's answers, groups in San Francisco in sufficient love and affection confessions-according to works in a bank branch in</cell><cell>bringt sie in eine Konversationen. Er Menschen erheblich. San Francisco Informationen wenig Zuneigung erfahren, Effektivität Geständnisse stark ähnelt. In einer Filiale in den USA</cell><cell>a more</cell><cell>positiveres</cell></row><row><cell></cell><cell>caring for homeless people. often find it hard to develop claims by human rights the USA where it informs</cell><cell>Sammelstelle. entscheidet auch, ob zur Obdachlosigkeit. Er fällt es oft schwer, erzwingen, laut können sich Kund*innen mit</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table S6 . Intentionality questionnaire items.</head><label>S6</label><figDesc>List of mentalistic and mechanistic descriptions of robot behavior created for the questionnaire (English translations and German originals)</figDesc><table><row><cell>Story</cell><cell>Mentalistic Description</cell><cell>Mechanistic</cell><cell>Mentalistic Description</cell><cell>Mechanistic</cell></row><row><cell>Code</cell><cell>(English translation)</cell><cell>Description (English</cell><cell>(German Original)</cell><cell>Description (German</cell></row><row><cell></cell><cell></cell><cell>translation)</cell><cell></cell><cell>Original)</cell></row><row><cell cols="2">Neut_01 The robot likes to play</cell><cell>The robot reacts to</cell><cell>Der Roboter spielt gerne</cell><cell>Der Roboter reagiert auf</cell></row><row><cell></cell><cell>table tennis</cell><cell>moving objects</cell><cell>Tischtennis</cell><cell>bewegte Objekte</cell></row><row><cell cols="2">Neut_02 The robot tries to keep</cell><cell>The robot controls its</cell><cell>Der Roboter versucht,</cell><cell>Der Roboter steuert seine</cell></row><row><cell></cell><cell>the orchestra in rhythm</cell><cell>arms synchronously with</cell><cell>das Orchester im Takt zu</cell><cell>Arme synchron mit</cell></row><row><cell></cell><cell></cell><cell>acoustic signals</cell><cell>halten</cell><cell>akustischen Signalen</cell></row><row><cell cols="2">Neut_06 The robot wants to make</cell><cell>The robot performs</cell><cell>Der Roboter möchte</cell><cell>Der Roboter führt</cell></row><row><cell></cell><cell>perfect sushi</cell><cell>precise cuts</cell><cell>perfektes Sushi</cell><cell>präzise Schnitte aus</cell></row><row><cell></cell><cell></cell><cell></cell><cell>herstellen</cell><cell></cell></row><row><cell cols="2">Neut_07 The robot likes neatly</cell><cell>The robot generates</cell><cell>Dem Roboter gefällt</cell><cell>Der Roboter erzeugt</cell></row><row><cell></cell><cell>ironed clothes</cell><cell>steam and mechanical</cell><cell>ordentlich gebügelte</cell><cell>Dampf und</cell></row><row><cell></cell><cell></cell><cell>pressure</cell><cell>Kleidung</cell><cell>mechanischen Druck</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements:</head><p>The authors would like to express their gratitude to <rs type="person">Nora Holtz</rs> and <rs type="person">Jonathan Buchholz</rs> for their support during EEG data collection and <rs type="person">Guido Kiecker</rs> for task programming and technical assistance.</p><p>Funding: Funded by the <rs type="funder">Deutsche Forschungsgemeinschaft (DFG, German Research Foundation</rs>) under Germany's <rs type="programName">Excellence Strategy -EXC 2002/1 "Science of Intelligence</rs>"project number <rs type="grantNumber">390523135</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_sKbzymw">
					<idno type="grant-number">390523135</idno>
					<orgName type="program" subtype="full">Excellence Strategy -EXC 2002/1 &quot;Science of Intelligence</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Materials</head><p>Supplementary materials include: Fig. <ref type="figure">S1</ref>. Ratings of stories Fig. <ref type="figure">S2</ref>. Intentionality questionnaire instructions Table <ref type="table">S1</ref>. Rating results by group Table <ref type="table">S2</ref>. Robot list Table <ref type="table">S3</ref>. Story list Table <ref type="table">S4</ref>. Story valence rating results Table <ref type="table">S5</ref>. Story arousal rating results Table <ref type="table">S6</ref>. Intentionality questionnaire items</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Author contributions:</head><p>Conceptualization: MM, AL, FB, PB, OH, RAR Methodology: MM, AL, RAR Investigation: MM, AL Formal analysis: MM, AL Visualization: MM, AL Funding acquisition: OH, RAR Project administration: MM, RAR Supervision: MM, PB, OH, RAR Writingoriginal draft preparation: MM, AL Writingreview &amp; editing: MM, AL, FB, PB, OH, RAR Competing interests: Authors declare that they have no competing interests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data and materials availability:</head><p>The data and analysis code that support the findings of this study are available upon publication at https://osf.io/5bj7x. Note. To achieve full counterbalancing of robot images and associated stories across participants, each robot could be paired with one neutral, one positive, and one negative story. Lines in the table indicate which groups of three robots shared the same set of possible stories. For each participant, one of three possible combinations was chosen, ensuring that all stories were presented and each story was paired with only one robot. The links in the "Source / Database" column refer to the robots used in the study but do not necessarily link to the actual images used (which were cropped frontal portraits).   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Materials</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Facial expression and trustworthiness ratings by language group</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Details on the intentionality questionnaire</head><p>Translated instruction: In the following you will see different descriptions of the robots' behavior. Please use the mouse to move the scale in the direction of the sentence that you think is the most appropriate description. You will see an example on the following page.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original instruction in German:</head><p>Im Folgenden sehen Sie unterschiedliche Beschreibungen der Verhaltensweisen der Roboter. Bitte bewegen Sie die Skala mit der Maus in Richtung des Satzes, der Ihrer Meinung nach die treffendere Beschreibung ist. Auf der folgenden Seite sehen Sie ein Beispiel. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Social robots for education: A review</title>
		<author>
			<persName><forename type="first">T</forename><surname>Belpaeme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Scassellati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tanaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Robot</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5954</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">How to compete with robots by assessing job automation risks and resilient alternatives</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paolillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Colella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nosengo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zambrano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Chappuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lalive</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Floreano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Robot</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">5561</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Social Robotics</title>
		<author>
			<persName><forename type="first">C</forename><surname>Breazeal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dautenhahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanda</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-32552-1_72</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-32552-1_72" />
	</analytic>
	<monogr>
		<title level="m">Springer Handbook of Robotics</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Siciliano</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">O</forename><surname>Khatib</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1935" to="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mind Meets Machine: Towards a Cognitive Science of Human-Machine Interactions</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramsey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends Cogn. Sci</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="200" to="212" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Social Cognition in the Age of Human-Robot Interaction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hortensius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Cross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends Neurosci</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="373" to="384" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Do We Adopt the Intentional Stance Toward Humanoid Robots?</title>
		<author>
			<persName><forename type="first">S</forename><surname>Marchesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ghiglino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ciardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Perez-Osorio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Baykara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wykowska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Psychol</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mind Perception Is the Essence of Morality</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waytz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol. Inq</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="101" to="124" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Social robots as depictions of social agents</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behav. Brain Sci</title>
		<imprint>
			<biblScope unit="page" from="1" to="33" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Dennett</surname></persName>
		</author>
		<title level="m">The Intentional Stance</title>
		<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adopting the intentional stance toward natural and artificial agents</title>
		<author>
			<persName><forename type="first">J</forename><surname>Perez-Osorio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wykowska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philos. Psychol</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="369" to="395" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An Ethical Evaluation of Human-Robot Relationships</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M A</forename><surname>De Graaf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Soc. Robot</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="589" to="598" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Getting personal with computers: How to design personalities for agents</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Dryer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="273" to="295" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On seeing human: A three-factor theory of anthropomorphism</title>
		<author>
			<persName><forename type="first">N</forename><surname>Epley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waytz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Cacioppo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol. Rev</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="864" to="886" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Feeling robots and human zombies: Mind perception and the uncanny valley</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Wegner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="page" from="125" to="130" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Anthropomorphizing Robots: The Effect of Framing in Human-Robot Collaboration</title>
		<author>
			<persName><forename type="first">L</forename><surname>Onnasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Roesler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Hum. Factors Ergon. Soc. Annu. Meet</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="1311" to="1315" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">What Makes Robots Social?: A User&apos;s Perspective on Characteristics for Social Human-Robot Interaction</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M A</forename><surname>De Graaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ben Allouch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A G M</forename><surname>Van Dijk</surname></persName>
		</author>
		<editor>Social Robotics, A. Tapus, E. André, J.-C. Martin, F. Ferland, M. Ammi</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="184" to="193" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mind the Robot! Variation in Attributions of Mind to a Wide Set of Real and Fictional Robots</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">L</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gazzaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kingstone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Soc. Robot</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="529" to="537" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Artificial Intelligence and Persuasion: A Construal-Level Account</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Duhachek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol. Sci</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="363" to="380" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Robots with Display Screens: A Robot with a More Humanlike Face Display Is Perceived To Have More Mind and a Better Personality</title>
		<author>
			<persName><forename type="first">E</forename><surname>Broadbent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sollers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Q</forename><surname>Stafford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Wegner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">72589</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Ceh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Vanman</surname></persName>
		</author>
		<idno type="DOI">10.31234/osf.io/4cr2u</idno>
		<ptr target="https://doi.org/10.31234/osf.io/4cr2u" />
		<title level="m">The Robots are Coming! The Robots are Coming! Fear and Empathy for Human-like Entities</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Anthropomorphism and the social robot</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Duffy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robot. Auton. Syst</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="177" to="190" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The Perception of Emotion in Artificial Agents</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hortensius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hekele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Cross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cogn. Dev. Syst</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="852" to="864" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Seeing Minds in Others -Can Agents with Robotic Appearance Have Human-Like Preferences?</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Martini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wiese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">146310</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Causes and consequences of mind perception</title>
		<author>
			<persName><forename type="first">A</forename><surname>Waytz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Epley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Wegner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends Cogn. Sci</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="383" to="388" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ascribing emotions to robots: Explicit and implicit attribution of emotions and perceived robot anthropomorphism</title>
		<author>
			<persName><forename type="first">N</forename><surname>Spatola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">A</forename><surname>Wudarczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Hum. Behav</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="page">106934</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Does the Robot Have a Mind? Mind Perception and Attitudes Towards Robots Predict Use of an Eldercare Robot</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Q</forename><surname>Stafford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jayawardena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Broadbent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Soc. Robot</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="17" to="32" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mental State Attribution to Robots: A Systematic Review of Conceptions, Methods, and Findings</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thellman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Graaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ziemke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Hum.-Robot Interact</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="51" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Facing Good and Evil: Early Brain Signatures of Affective Biographical Knowledge in Face Recognition</title>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Abdel</forename><surname>Rahman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Emotion</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1397" to="1405" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Clear judgments based on unclear evidence: Person evaluation is strongly influenced by untrustworthy gossip</title>
		<author>
			<persName><forename type="first">J</forename><surname>Baum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rabovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Abdel</forename><surname>Rahman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Emotion</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="248" to="260" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Knowledge-augmented face perception: Prospects for the Bayesian brain-framework to align AI and human vision</title>
		<author>
			<persName><forename type="first">M</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Blume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bideau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Hellwich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Abdel</forename><surname>Rahman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Conscious. Cogn</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page">103301</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A Social Bayesian Brain: How Social Knowledge Can Shape Visual Perception</title>
		<author>
			<persName><forename type="first">M</forename><surname>Otten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Seth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pinto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain Cogn</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page" from="69" to="77" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Not so harmless anymore: How context impacts the perception and electrocortical processing of neutral faces</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Wieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B M</forename><surname>Gerdes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Büngel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mühlberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pauli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page" from="74" to="82" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Facing faces: Studies on the cognitive aspects of physiognomy</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hassin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Trope</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Pers. Soc. Psychol</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="837" to="852" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<author>
			<persName><forename type="first">A</forename><surname>Schacht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Emotions in Word and Face Processing: Early and Late Cortical Responses</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="538" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Perceiving emotions in neutral faces: expression processing is biased by affective person knowledge</title>
		<author>
			<persName><forename type="first">F</forename><surname>Suess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rabovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Abdel</forename><surname>Rahman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soc. Cogn. Affect. Neurosci</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="531" to="536" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Whatever next? Predictive brains, situated agents, and the future of cognitive science</title>
		<author>
			<persName><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behav. Brain Sci</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="181" to="204" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The free-energy principle: a rough guide to the brain?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Friston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends Cogn. Sci</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="293" to="301" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Hierarchical Bayesian inference in the visual cortex</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mumford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Am. A</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1434" to="1448" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Effects of Language on Visual Perception</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lupyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Abdel Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Boroditsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends Cogn. Sci</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="930" to="944" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Perceptual Prediction: Rapidly Making Sense of a Noisy World</title>
		<author>
			<persName><forename type="first">C</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Curr. Biol</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="751" to="R753" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Emotional News Affects Social Judgments Independent of Perceived Media Credibility</title>
		<author>
			<persName><forename type="first">J</forename><surname>Baum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Abdel</forename><surname>Rahman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soc. Cogn. Affect. Neurosci</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="280" to="291" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Betker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Manassra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Improving Image Generation with Better Captions</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cortical sources of the early components of the visual evoked potential</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">Di</forename><surname>Russo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Sereno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pitzalis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Hillyard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hum. Brain Mapp</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="95" to="111" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Neuromagnetic Correlates of Perceived Contrast in Primary Visual Cortex</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Haynes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stadler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Heinze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurophysiol</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="2655" to="2666" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Electrophysiological Studies of Face Perception in Humans</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bentin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Allison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Puce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mccarthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Cogn. Neurosci</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="551" to="565" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Response Profile of the Face-Sensitive N170 Component: A Rapid Adaptation Study</title>
		<author>
			<persName><forename type="first">M</forename><surname>Eimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nicholas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cereb. Cortex</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="2442" to="2452" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Emotion and Attention: Event-Related Brain Potential Studies</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Schupp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Flaisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stockburger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Junghöfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Prog. Brain Res</title>
		<imprint>
			<biblScope unit="volume">156</biblScope>
			<biblScope unit="page" from="31" to="51" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Seeing what we know and understand: How knowledge shapes perception</title>
		<author>
			<persName><forename type="first">R</forename><surname>Abdel Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychon. Bull. Rev</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1055" to="1063" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Instant Effects of Semantic Information on Visual Perception</title>
		<author>
			<persName><forename type="first">A</forename><surname>Enge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Süß</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Abdel</forename><surname>Rahman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurosci. Off. J. Soc. Neurosci</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="4896" to="4906" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Does the semantic content of verbal categories influence categorical perception? An ERP study</title>
		<author>
			<persName><forename type="first">M</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Glage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hohlfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Abdel</forename><surname>Rahman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain Cogn</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Native Language Promotes Access to Visual Consciousness</title>
		<author>
			<persName><forename type="first">M</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Abdel</forename><surname>Rahman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol. Sci</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1757" to="1772" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Transient and Long-Term Linguistic Influences on Visual Perception: Shifting Brain Dynamics With Memory Consolidation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Abdel</forename><surname>Rahman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lang. Learn</title>
		<imprint>
			<biblScope unit="page">12631</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Semantic Knowledge Enhances Conscious Awareness of Visual Objects</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Weller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rabovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Abdel</forename><surname>Rahman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Cogn. Neurosci</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1216" to="1226" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deepfake smiles matter less-the psychological and neural impact of presumed AI-generated faces</title>
		<author>
			<persName><forename type="first">A</forename><surname>Eiserbeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Baum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Abdel</forename><surname>Rahman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">16111</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Faces in Context: A Review and Systematization of Contextual Influences on Affective Face Processing</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Wieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brosch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Psychol</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dzhelyova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Effect of Affective Personality Information on Face Processing: Evidence from ERPs</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Context Influences Early Perceptual Analysis of Faces-An Electrophysiological Study</title>
		<author>
			<persName><forename type="first">R</forename><surname>Righart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>De Gelder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cereb. Cortex</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1249" to="1257" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Mental Imagery of Emotions: Electrophysiological Evidence</title>
		<author>
			<persName><forename type="first">F</forename><surname>Suess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Abdel</forename><surname>Rahman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="147" to="157" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">The Knobe Effect: A Brief Overview</title>
		<author>
			<persName><forename type="first">A</forename><surname>Feltz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mind Behav</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="265" to="277" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Intentional action in folk psychology: An experimental investigation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Knobe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philos. Psychol</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="309" to="324" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Differential effects of face-realism and emotion on event-related brain potentials and their implications for the uncanny valley theory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Botsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kissler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">45003</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Attention and Emotion: An Integrative Review of Emotional Face Processing as a Function of Attention</title>
		<author>
			<persName><forename type="first">S</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bublatzky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cortex</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="page" from="362" to="386" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">But Act Like a Machine: Agent Appearance and Behavior Modulate Different Aspects of Human-Robot Interaction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Abubshait</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wiese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Psychol</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>You Look Human</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Motivated attention and task relevance in the processing of cross-modally associated faces: Behavioral and electrophysiological evidence</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ziereis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schacht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cogn. Affect. Behav. Neurosci</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1244" to="1266" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Motivational significance and cognitive effort elicit different late positive potentials</title>
		<author>
			<persName><forename type="first">I</forename><surname>Matsuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nittono</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clin. Neurophysiol. Off. J. Int. Fed. Clin. Neurophysiol</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="page" from="304" to="313" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">ERP and MEG correlates of visual consciousness: The second decade</title>
		<author>
			<persName><forename type="first">J</forename><surname>Förster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Koivisto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Revonsuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Conscious. Cogn</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page">102917</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Precuneus brain response changes differently during humanrobot and human-human dyadic social interaction</title>
		<author>
			<persName><forename type="first">N</forename><surname>Spatola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chaminade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14794</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Behavioral Objects: The Rise of the Evocative Machines</title>
		<author>
			<persName><forename type="first">F</forename><surname>Levillain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zibetti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Hum.-Robot Interact</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">The thing that should not be: predictive coding and the uncanny valley in perceiving human and humanoid robot actions</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Saygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chaminade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ishiguro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Driver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Frith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soc. Cogn. Affect. Neurosci</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="413" to="422" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Uncanny valley as a window into predictive processing in the social brain</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Urgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kutas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Saygin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuropsychologia</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="181" to="185" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">The speed of morality: a high-density electrical neuroimaging study</title>
		<author>
			<persName><forename type="first">J</forename><surname>Decety</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cacioppo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurophysiol</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="3068" to="3072" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">The accidental transgressor: Morally-relevant theory of mind</title>
		<author>
			<persName><forename type="first">M</forename><surname>Killen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Mulvey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jampol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Woodward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="197" to="215" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Robots As Intentional Agents: Using Neuroscientific Methods to Make Robots Appear More Social</title>
		<author>
			<persName><forename type="first">E</forename><surname>Wiese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Metta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wykowska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Psychol</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">1663</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Holding Robots Responsible: The Elements of Machine Morality</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">E</forename><surname>Bigman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waytz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Alterovitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends Cogn. Sci</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="365" to="368" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Patiency is not a virtue: the design of intelligent systems and systems of ethics</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Bryson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ethics Inf. Technol</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="15" to="26" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">The responsibility gap: Ascribing responsibility for the actions of learning automata</title>
		<author>
			<persName><forename type="first">A</forename><surname>Matthias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ethics Inf. Technol</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="175" to="183" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Killer Robots</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sparrow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Appl. Philos</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="62" to="77" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">What is Human-like?: Decomposing Robots&apos; Human-like Appearance Using the Anthropomorphic roBOT (ABOT) Database</title>
		<author>
			<persName><forename type="first">E</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">F</forename><surname>Malle</surname></persName>
		</author>
		<idno type="DOI">10.1145/3171221.3171268</idno>
		<ptr target="https://dl.acm.org/doi/10.1145/3171221.3171268" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction</title>
		<meeting>the 2018 ACM/IEEE International Conference on Human-Robot Interaction<address><addrLine>Chicago IL USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="105" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Assessing the Attitude Towards Artificial Intelligence: Introduction of a Short Measure in German, Chinese, and English Language</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sindermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wernicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sariyska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stavrou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Montag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KI -Künstl. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="109" to="118" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Mixed-effects modeling with crossed random effects for subjects and items</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Baayen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Bates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mem. Lang</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="390" to="412" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<author>
			<persName><forename type="first">Team</forename><surname>Core</surname></persName>
		</author>
		<ptr target="https://www.R-project.org/" />
		<title level="m">R: A Language and Environment for Statistical Computing, version 4.2.2, R Foundation for Statistical Computing</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Balancing Type I error and power in linear mixed models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Matuschek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kliegl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vasishth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Baayen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mem. Lang</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="305" to="315" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">SIMR: an R package for power analysis of generalized linear mixed models by simulation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Macleod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Methods Ecol. Evol</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="493" to="498" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">The Perceived Awareness of the Research Hypothesis Scale: Assessing the influence of demand characteristics</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rubin</surname></persName>
		</author>
		<idno type="DOI">10.6084/M9.FIGSHARE.4315778</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Oldfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The assessment and analysis of handedness: the Edinburgh inventory</title>
		<imprint>
			<date type="published" when="1971">1971</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="97" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<author>
			<persName><forename type="first">R</forename><surname>Frömer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Abdel</forename><surname>Rahman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Group-Level EEG-Processing Pipeline for Flexible Single Trial-Based Analyses Including Linear Mixed Models</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">48</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">MEG and EEG data analysis with MNE-Python</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Neurosci</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Artifact correction of the ongoing EEG using spatial filters based on artifact and brain signal topographies</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Scherg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Clin. Neurophysiol</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="113" to="124" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
