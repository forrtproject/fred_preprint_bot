<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Prediction of Spoken Language Improvements in Children with Cochlear Implants</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yanlin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Brain and Mind institute</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Di</forename><surname>Yuan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Brain and Mind institute</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shani</forename><surname>Dettman</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Audiology &amp; Speech Pathology</orgName>
								<orgName type="institution">The University of Melbourne</orgName>
								<address>
									<addrLine>550 Swanston St</addrLine>
									<postCode>3010</postCode>
									<settlement>Parkville</settlement>
									<region>Victoria</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dawn</forename><surname>Choo</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Audiology &amp; Speech Pathology</orgName>
								<orgName type="institution">The University of Melbourne</orgName>
								<address>
									<addrLine>550 Swanston St</addrLine>
									<postCode>3010</postCode>
									<settlement>Parkville</settlement>
									<region>Victoria</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Emily</forename><forename type="middle">Shimeng</forename><surname>Xu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Division of Otolaryngology</orgName>
								<orgName type="institution">Ann &amp; Robert H. Lurie Children&apos;s Hospital of Chicago</orgName>
								<address>
									<settlement>Chicago</settlement>
									<region>Illinois</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Denise</forename><surname>Thomas</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Audiology</orgName>
								<orgName type="institution">Ann &amp; Robert H. Lurie Children&apos;s Hospital of Chicago</orgName>
								<address>
									<settlement>Chicago</settlement>
									<region>Illinois</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Maura</forename><forename type="middle">E</forename><surname>Ryan</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Department of Medical Imaging</orgName>
								<orgName type="institution">Ann &amp; Robert H. Lurie Children&apos;s Hospital of Chicago</orgName>
								<address>
									<settlement>Chicago</settlement>
									<region>Illinois</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Patrick</surname></persName>
						</author>
						<author>
							<persName><surname>Wong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Brain and Mind institute</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>MD</roleName><forename type="first">Nancy</forename><forename type="middle">M</forename><surname>Young</surname></persName>
							<email>nyoung@luriechildrens.org</email>
							<affiliation key="aff2">
								<orgName type="department">Division of Otolaryngology</orgName>
								<orgName type="institution">Ann &amp; Robert H. Lurie Children&apos;s Hospital of Chicago</orgName>
								<address>
									<settlement>Chicago</settlement>
									<region>Illinois</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department" key="dep1">Department of Otolaryngology Head &amp; Neck Surgery</orgName>
								<orgName type="department" key="dep2">Feinberg School of Medicine</orgName>
								<orgName type="institution">Northwestern University</orgName>
								<address>
									<settlement>Chicago</settlement>
									<region>Illinois</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">Department of Communication Sciences and Disorders</orgName>
								<orgName type="institution">Knowles Hearing Center</orgName>
							</affiliation>
							<affiliation key="aff7">
								<orgName type="institution">Northwestern University</orgName>
								<address>
									<settlement>Evanston</settlement>
									<region>Illinois</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>PhD</roleName><forename type="first">Patrick</forename><forename type="middle">C M</forename><surname>Wong</surname></persName>
							<email>p.wong@cuhk.edu.hk</email>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="department">Division of Otolaryngology</orgName>
								<orgName type="institution">Ann &amp; Robert H. Lurie Children&apos;s Hospital</orgName>
								<address>
									<settlement>Chicago</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff9">
								<orgName type="department" key="dep1">Department of Otolaryngology Head &amp; Neck Surgery</orgName>
								<orgName type="department" key="dep2">Feinberg School of Medicine</orgName>
								<orgName type="department" key="dep3">Department of Communication Sciences and Disorders</orgName>
								<orgName type="institution">Northwestern University Knowles Hearing Center</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff10">
								<orgName type="institution">Northwestern University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff11">
								<orgName type="department">Brain and Mind institute</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Prediction of Spoken Language Improvements in Children with Cochlear Implants</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C6B777F097F99195180E84D685EF761B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-10-22T06:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Objective This study aims to construct neural predictive models to forecast post-CI spoken language improvements in children with hearing loss and to evaluate whether these models are language-and centerspecific.</p><p>Methods A total of 278 children with hearing loss underwent magnetic resonance image (MRI) examinations and completed speech and language assessments both before and after the implants. We utilized deep transfer learning algorithms with pre-CI neuroanatomical features to predict post-CI spoken language development in children enrolled from 2009 to 2022, with 3-year follow-up.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We found that pre-CI MRI brain data can forecast spoken language development up to 36 months post-CI. Evidence of withincenter and within-language prediction was consistent across different centers. MobileNet model exhibited the best performance with an accuracy (ACC) of 89.74% (95% CI, 89.39%-90.10%), sensitivity of 87.09% (95% CI, 86.17%-88.00%), specificity of 92.20% (95% CI, 90.98%-93.42%), and the area under the receiver operating characteristic curve (AUC) of 0.896 (95% CI, 0.893-0.900). However, cross-dataset generalization, even within the same center, could not be achieved with our current sample (e.g., ACC: 50.27% (95% CI, 47.62%-53.76%), sensitivity: 36.89% (95% CI, 0%-93.88%), specificity: 63.95% (95% CI, 6.43%-100%), and AUC: 0.499 (95% CI, 0.467-0.532)). When all the datasets were combined, the predictive performance remained high (ACC: 87.94% (95% CI, 87.28%-88.59%), sensitivity: 88.33% (95% CI, 97.18%-89.48%), specificity: 87.56% (95% CI, 86.12%-89.00%), and AUC: 0.879 (95% CI, 0.873-0.886)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>The generalization of the neural predictive model across different centers and languages appears to be feasible and effective with a larger and more representative dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Cochlear implants (CI) have been shown to be effective in assisting children with severe to profound hearing loss to develop spoken language. <ref type="bibr" target="#b0">1</ref> However, many children with CI still lag behind their peers with normal hearing in terms of spoken language development. <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref> Despite the availability of various early intervention approaches such as listening and spoken language therapy with or without sign language, there is little consensus on the optimal type and dose of intervention. <ref type="bibr" target="#b4">5</ref> Accurately predicting spoken language development on the individual child level prior to CI would allow for the provision of more intensive healthcare for those children who may need it most.</p><p>It has been demonstrated that brain measures often serve as better prognostic indicators, either alone or in combination with other measures, than traditional measures such as age at implant and preimplantation residual hearing. <ref type="bibr" target="#b5">6</ref> Studies have successfully used machine learning techniques to forecast the auditory and spoken language skills of children with CI. <ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8</ref> For example, the preoperative neuroanatomical features of CI users predicted the variability of their speech perception improvements six months after surgery, showing 84% accuracy based on a linear support vector machine (SVM) classifier with a recursive feature elimination selection technique. <ref type="bibr" target="#b7">8</ref> In contrast, non-neural features, including demographic variables and pre-CI speech perception scores only reached a chance level of accuracy in predicting speech perception improvements. The robustness and efficiency of brain measures in predicting post-CI improvements have also been supported by studies using preoperative brain activations in response to audio and visual stimuli in children and adults with CI. <ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10</ref> It is worth noting that the correlation between preoperative brain measures and post-CI outcomes cannot provide sufficient prognostic values at an individual level, although the findings may illustrate the neural basis of spoken language development in people with CI. <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref> Moreover, predicting the improvements from pre-to post-CI might be more important than predicting post-CI outcomes. This is because the outcomes measured after implantation are usually closely correlated with pre-implantation measures, <ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14</ref> and the correlation between brain measures and post-CI outcomes could be confounded by the baseline measures. Children with poor speech abilities before implantation may still demonstrate significant improvements due to the benefits provided by CI. As supported in a previous study, children's pre-CI speech perception ability was independent of their improvements after receiving CI. <ref type="bibr" target="#b7">8</ref> Therefore, predicting the change in spoken language of pediatric CI users provides more information related to CI benefits. This allows for guiding precision healthcare, enabling timely adjustments to intervention plans, and helping manage parental expectations of children's post-CI improvements. Ultimately, accurate prediction on the individual child level enabled by our approach will permit the optimization of spoken language and an improved quality of life after CI.</p><p>Although a predictive model utilizing preoperative brain measures has been built by our research group to forecast improvements of the spoken language measures, training of the predictive models were restricted to children from a single medical center and to children learning English. For both clinical and theoretical reasons, it is important to ascertain whether neural predictive models constructed with data from one medical center and one language can be used to predict the improvements of children who are from other medical centers and learning other languages. From the clinical standpoint, model generalization means that it is unnecessary to construct populationspecific predictive models, as reliance on models constructed with data from a variety of patients from any center would be sufficient. As a study aiming to predict improvements in as many children with CI as possible, we imposed relatively broad inclusion/exclusion criteria. At each center, children had to be from homes that speak Cantonese (Hong Kong), English (Melbourne), or English or Spanish (Chicago) as the dominant language. We excluded children who had a known genetic condition that is expected to severely affect language development and children who had gross brain malformations. A total of 278 children were included. The demographic information is shown in Table <ref type="table" target="#tab_1">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Clinical Measures</head><p>Children's auditory skill, speech perception, receptive and/or expressive language abilities were measured before and up to 36 months after implantation using different assessment tools across centers (see the Supplementary Materials). We here refer to all these measurements as 'spoken language,' being aware that audition and speech perception are precursors for spoken language development. <ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17</ref> Positive correlations have been demonstrated between speech perception and spoken language scores on standardized tests for children with hearing loss. <ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19</ref> While variances could be introduced by differences in the assessment methods and timing, it is feasible to compare the spoken language ability across the centers and over time because of the heterotypic stability inherent in spoken language development. <ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21</ref> Specifically, the individual ranking of different manifest characteristics is maintained over time as long as those characteristics share the same underlying construct and theoretical value.</p><p>The improvement of spoken language development from pre-to post-CI was quantified by the change of assessed scores as a function of assessment time for each participant. To this end, a linear mixed-effect model was constructed for each center with spoken language scores as the dependent variable, subject ID as a random intercept, as well as assessment time as a random slope. The fixed effects portion of the model included only the intercept term, as the influence of time on spoken language scores was captured in the random slope. The model can be expressed mathematically as Scores ~ 1 + (assessment time | subject ID). The random slope in the model allowed us to estimate individual differences in the rate of speech and language change over time. For better model generalization, instead of using the raw scores directly for fine-grained prediction, we separated the spoken language improvement into binary classifications (high-improvement and lowimprovement) using a median split approach within each center.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MRI acquisition and preprocessing</head><p>The T1-weighted MRI image was obtained from each child before CI. The scanning parameters were optimized to obtain a good signal-tonoise ratio (Supplementary Material). MRI images were processed using the Advanced Normalization Tools (ANTs) in Python. <ref type="bibr" target="#b21">22</ref> To increase the image quality, the images were resampled to 1 mm× 1 mm× 1 mm voxel size and preprocessed following the basic preprocessing pipeline for T1weighted brain MRI in ANTs. The deformation-based morphometry (DBM) method was used to examine the morphological differences over the entire brain with an age appropriate T1 image as the template. <ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">24</ref> Fifteen axial 2D slices were extracted from the central part of the 3D DBM brain scans. <ref type="bibr" target="#b25">25</ref> The images were cropped and resized into a target resolution of 128×128 voxels and were normalized using ImageNet statistics (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) before being passed on for further analyses. <ref type="bibr" target="#b26">26</ref> Each slice was assigned the same label as the corresponding subject and used as a data sample to train the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transfer Learning and Feature Extractions</head><p>We utilized popular pre-trained convolutional neural network (CNN) models, including AlexNet, 27 VGG19, <ref type="bibr" target="#b28">28</ref> ResNet, <ref type="bibr" target="#b29">29</ref> Inception, <ref type="bibr" target="#b30">30</ref> GoogleNet, <ref type="bibr" target="#b32">31</ref> MobileNet, <ref type="bibr" target="#b33">32</ref> and DenseNet, <ref type="bibr" target="#b35">33</ref> implemented in PyTorch version 1.9, for feature extraction. This standard transfer learning strategy involves using pre-trained CNN models on ImageNet as the backbone of the model to capture generic and domain-specific features, followed by fine-tuning the top layers to learn new specialized representations tailored to our output classifier. <ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b36">34</ref> During the finetuning phase, the weights and biases of the CNN models were frozen to prevent changes. Due to differences in the CNN architectural designs, an adaptive pooling operation was applied to AlexNet and MobileNet before the final classification layer to ensure that the output became a onedimensional vector. Subsequently, a new fully connected layer, the classification layer, was added to process the outputs from the hidden layer's activation function and compose the final classification. Data augmentation with random rotation and flipping was executed to improve the model training efficiency. <ref type="bibr" target="#b37">35,</ref><ref type="bibr" target="#b38">36</ref> The loss function was binary crossentropy with logit loss. The optimizer was Adam with a learning rate of 1×10-4. A total of 200 epochs with a batch size of 64 images were set for training. The validation performance was used to determine when to stop the training. The CNN models were trained until there was no improvement in the validation loss for 10 consecutive epochs. All the experiments were conducted by dividing the data into 80% for training and validation and 20% for held-out testing. A five-fold cross-validation approach was used to validate the model's performance during training.</p><p>The training validation results were obtained through this five-fold crossvalidation process to detect language improvements. Finally, a held-out 20% test set was used to evaluate the model's performance, specifically its generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance comparisons</head><p>To examine whether neural features can predict longer-term post-CI improvements, we first compared state-of-the-art CNN models within a single center (Chicago or Melbourne) or a single language dataset (English or Spanish). To further assess the generalization of the predictive model with a new dataset that has different inclusion criteria or was obtained from different facilities, we tested whether model trained on the largest (Chicago English) dataset could predict improvements for CI candidates learning Spanish at the same center, or for CI candidates learning the same language at another medical center.</p><p>These external assessments across different languages or centers were conducted on trained model on a single dataset. Finally, to assess the robustness and generalization of the predictive model on combined dataset, we developed the model on a development set (80%) of the combined dataset across centers and languages, which was then internally validated using an held-out 20% test dataset from the same combined dataset. In addition, we also compared sliced-based CNN models with voxel-based machine learning models including Linear regression (LR), SVM, Random Forest (RF), Decision Tree (DT), K-Nearest Neighbor (KNN), and eXtreme Gradient Boosting (XGBoost) (see Supplementary Materials).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance Evaluation Metrics</head><p>The model's performance in classification could be evaluated using the following performance metrics: the area under the receiver operating characteristic curve (AUC), accuracy (ACC), sensitivity, and specificity. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Children with CI showed improvements in spoken language abilities compared to the baseline measurement tested before implantation (Figure <ref type="figure">1</ref>). Specifically, in Chicago, the spoken language abilities of English-learning children improved from 75 to 292, and of Spanish-learning children from 45 to 203, over the period from pre-CI to 36 months post-CI, as tested by SRI-m. Similarly, in Hong Kong, Cantonese-learning children improved from 17 to 32, over the period from pre-CI to 24 months post-CI, as tested by LittlEARS. Most of these improvements emerged in the first year and a half after implantation. In Melbourne, the receptive language of English-learning children improved from 74 to 85 in the first two years after implantation but dropped to 70 in the third year post-CI, as tested by PPVT and PLS. The different pattern of changes in spoken language development may result from the standard scores obtained in Melbourne, which take age-appropriate normal-hearing children as a control, suggesting that children were able to catch up with their normal-hearing peers but still lagged behind in their long-term spoken language development. Despite different standardized tests being used to capture the spoken language development across the centers, our predictive models were constructed to only predict the binary classifications of low or high improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2 lists the deep learning and machine learning models'</head><p>training and testing ACC, sensitivities, specificities, and AUC. In general, slice-based deep transfer learning can substantially improve the model's prediction performance compared to voxel-based machine learning models on Chicago English data (Figure <ref type="figure">2A</ref>). Among the various deep learning convolutional neural network models, the MobileNet model exhibits the best performance with an ACC of 89.74% (95% CI, 89.39%-90.10%), sensitivity of 87.09% (95% CI, 86.17%-88.00%), specificity of 92.20% (95% CI, 90.98%-93.42%), and AUC of 0.896 (95% CI, 0.893-0.900) on the test dataset. Predictive models using slice-based deep transfer learning can achieve a high level of predictive performance when a single dataset is used (e.g., data from English-learning children from Chicago were used to test the same model). Therefore, we used the MobileNet model as a baseline network for downstream assessments of model's generalization. However, when the generalization was externally tested using data from another medical center (e.g., testing the Chicago English model with Melbourne English data), the model's performance dropped to chance levels (ACC: 50.95% (95% CI, 49.14%-53.75%), sensitivity: 62.90% (95% CI, 3.74%-100%), specificity: 39.28% (95% CI, 0%-95.66%), and AUC: 0.511 (95% CI, 0.489-0.533)) (Table <ref type="table" target="#tab_3">3</ref> and Figure <ref type="figure">2B</ref>). Even within the same center, cross-language generalization (e.g., testing the Chicago English model with Chicago Spanish data) could not be achieved with our sample sizes (ACC: 50.27% (95% CI, 47.62%-53.76%), sensitivity: 36.89% (95% CI, 0%-93.88%), specificity: 63.95% (95% CI, 6.43%-100%), and AUC: 0.499 (95% CI, 0.467-0.532)). When tested across different languages and cultural backgrounds (e.g., testing the Chicago English model with Hong Kong Cantonese data), the model showed an ACC of 50.75% (95% CI, 47.62%-53.87%), sensitivity of 36.67% (95% CI, 0%-96.18%), specificity of 63.26% (95% CI, 3.46%-100%), and AUC of 0.500 (95% CI, 0.496-0.504).</p><p>Nevertheless, regardless of whether a single dataset or a combination of different datasets was used to build the model, the MobileNet model demonstrated consistently accurate performance (Table <ref type="table" target="#tab_3">3</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>In this multicenter study, we employed the transfer deep learning technique using the preoperative neuroanatomical features to forecast spoken language improvements in children with CI for up to three years.</p><p>Our transfer learning models consistently demonstrated accurate performance in distinguishing between higher and lower improvement groups for both single dataset and combined datasets. However, the models exhibited poor performance when applied to external generalization testing. The findings highlight the effectiveness of using transfer deep learning to predict post-CI improvements on the individualchild level for the precision care of pediatric CI users. The poor generalization in external testing, however, calls for multicenter collaboration to obtain a large-scale representative data, enabling the construction of models with better potential to generalize to new patients from diverse backgrounds.</p><p>Transfer learning offers an effective strategy for the target domain classifier by integrating the knowledge learned from pre-trained CNN models on ImageNet with new specialized representations through finetuning. <ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b36">34</ref> This approach has shown to be powerful in healthcare decisions for rare diseases, such as Alzheimer's disease, <ref type="bibr" target="#b39">37</ref> cardiomyopathy, <ref type="bibr" target="#b40">38</ref> diabetic retinopathy, <ref type="bibr" target="#b41">39</ref> etc. Compared to a previous study that used voxel-based machine learning models (i.e., SVM) to predict speech perception improvements six months post-CI with 37 children, <ref type="bibr" target="#b7">8</ref> our study employing a transfer learning approach revealed a higher prediction accuracy even for longer-term post-CI improvements using a larger sample size. Our study is among the first to use such a transfer learning approach for predicting children's post-CI improvements Generalization to a new dataset is crucial for ensuring the applicability and real-world impact of any scientific findings. <ref type="bibr" target="#b42">40,</ref><ref type="bibr" target="#b43">41</ref> A universal model is desirable for generalizing across datasets. In this study, model trained on a single dataset were unable to generalize directly to other datasets with different cultural or language characteristics. Although the model achieved a test accuracy of 89.74% (95% CI, 89.33%-90.10%) for the Chicago English dataset, external generalization testing on new datasets resulted in poor predictive performance of 50.95% (95% CI, 49.11%-52.75%) accuracy for the Melbourne English dataset, 50.27% (95% CI, 46.78%-53.76%) for the Chicago Spanish dataset, and 50.75% (95% CI, 47.62%-53.87%) for the Hong Kong Cantonese dataset. These independent datasets shared the same language but had different cultural backgrounds (Melbourne English), shared the same cultural background but had different language experiences (Chicago Spanish), or had completely different language and cultural backgrounds (Hong Kong Cantonese). The poor generalization of the model may result from the heterogeneous languages and cultural backgrounds across the datasets making the unseen data mismatch the training distribution. It has been demonstrated that cultural and language differences have a large impact on brain function and structure. <ref type="bibr" target="#b44">42,</ref><ref type="bibr" target="#b45">43</ref> Thus, generalization across datasets will require the incorporation of subjects from diverse cultural and language backgrounds, allowing the model to learn additional features during training to avoid characteristic-specific model and enable robust generalization.</p><p>Furthermore, we investigated the generalization of the predictive model on the combined dataset. Accordingly, the model was trained on the development set (80%) of the combined dataset across centers and languages, and internal validation was conducted on a held-out 20% of the test dataset. The performance of these models trained on combined datasets showed consistently higher accuracies compared to those trained on a single dataset. These findings demonstrated that the preoperative neural features can significantly predict post-CI improvements in children with hearing loss from different languages and centers. Moreover, the transfer learning strategy can effectively adapted to combined datasets with different cultural or language characteristics, enhancing the robustness and generalization of model. Our results imply that, ultimately, it is possible to improve the generalization across different populations using transfer learning techniques and more representative datasets, which is critical for the future translation to clinical practice.</p><p>Our study had several limitations. First, although the study included diverse participants with datasets from multiple centers and languages, the sample size was relatively small, which might not be sufficiently diversity for developing a universal model as a pre-surgical screening tool. Second, different assessment tools were used across centers. While it would be ideal to use unified tools for better generalization, we conducted the binary classifications (high improvement and low improvement) using a median split approach. This accommodates the measurements taken on different scales across the centers. Third, the limitation of spatial information between slices, as each 2D slice is processed independently, <ref type="bibr" target="#b46">44</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>Our study demonstrated that the deep transfer learning approach provides an effective means for utilizing preoperative brain images to predict whether children will have high or low spoken language improvements after CI. Furthermore, assessments of the model's generalization demonstrated that while model trained on a single dataset cannot directly generalize to a new dataset with different cultural or language characteristics, those trained on combined datasets showed better performance, highlighting the need for multicenter collaboration to generate a large, diverse dataset for the purpose of building a universal model to forecast spoken language development in children with CI.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>From the theoretical standpoint, generalization speaks to the basic neural architecture subserving language development. Do the networks that support English learning substantially overlap with those supporting Spanish or Cantonese learning? Our multicenter study aimed to address the question of model generalization with a deep-learning model predicting children's spoken language improvements up to three years after implantation. Because of the low rate of severe to profound hearing loss in children, it is unusual to have a dataset large enough to train a predictive model. This study employed a transfer learning architecture, leveraging the learned features from pre-trained models on large-scale image datasets to enhance the performance of our own model. 15 Methods Participants Children with congenital or early onset sensorineural hearing loss were recruited from three different centers: Chicago, United States; Melbourne, Australia; and Hong Kong, China. They received CI at local hospitals from 2009 to 2022. All the children underwent T1-weighted structural whole-brain magnetic resonance imaging (MRI) as a part of their pre-CI evaluation. Their speech and language abilities were assessed before and after implantation. Parents or guardians provided written informed consent to access children's MRI scans and clinical data. This study was approved by the Joint Chinese University of Hong Kong -New Territories East Cluster Clinical Research Ethics Committee, the Stanley Manne Children's Research Institute's Institutional Review Board, and The Royal Children's Hospital, Human Research Ethics Committee at each center.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>AUC measures the model's ability to discriminate between classes across various thresholds and is calculated from the False Positive Rate (FPR) and True Positive Rate (TPR). ACC measures the proportion of correctly classified images, reflecting the overall effectiveness of the model. Sensitivity, or recall, assesses the classifier's ability to correctly identify cases with the disease. Specificity evaluates how well the classifier can identify cases without the disease. ACC = (TP + TN) / (TP + TN + FP + FN) Sensitivity = TP / (TP + FN) Specificity = TN / (FP + TN) where TP is true positive values, TN is true negative values, FP is false positive values, and FN is false negative values; is a positive instance and is a negative instance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>was mitigated by using transfer learning and fine-tuning techniques to integrate prior knowledge from large datasets with domain-specific knowledge. Future research should focus on testing the model's generalization across diverse populations and settings, including CI children from different centers and cultural backgrounds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure Legends:Figure1.</head><label></label><figDesc>Figure Legends:</figDesc><graphic coords="25,72.00,87.05,451.30,423.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure2.</head><label></label><figDesc>Figure2. Performance comparison for machine learning models and transfer learning models (A) and assessments of transfer learning model's generalization on single datasets, external test datasets, and combined datasets (B). Error bars represent plus/minus one standard deviation, showing the means of accuracy with standard deviations across five-fold cross-validation from different experiments.</figDesc><graphic coords="26,72.00,99.95,451.30,208.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and Fig 2B). It achieved an ACC of 87.38% (95% CI, 87.12%-</figDesc><table><row><cell>87.64%), sensitivity of 85.36% (95% CI, 84.02%-86.70%), specificity of</cell></row><row><cell>89.57% (95% CI, 88.04%-91.11%), and AUC of 0.874 (95% CI, 0.871-</cell></row><row><cell>0.876) across the Chicago and Melbourne datasets. When tested across</cell></row><row><cell>the Chicago, Melbourne, and Hong Kong datasets, it achieved an ACC of</cell></row><row><cell>87.94% (95% CI, 87.28%-88.59%), sensitivity of 88.33% (95% CI,</cell></row><row><cell>87.18%-89.48%), specificity of 87.56% (95% CI, 86.12%-89.00%), and</cell></row><row><cell>AUC of 0.879 (95% CI, 0.873-0.886).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Demographic information for participants from different centers.</figDesc><table><row><cell></cell><cell cols="2">Chicago data</cell><cell>Melbourn e data</cell><cell>Hong data Kong</cell><cell>All</cell></row><row><cell>Sample size</cell><cell>143</cell><cell>37</cell><cell>81</cell><cell>17</cell><cell>278</cell></row><row><cell>Family language</cell><cell cols="2">English Spanish</cell><cell cols="2">English Cantonese</cell><cell>NA</cell></row><row><cell>Female, No. (%)</cell><cell>67 (46.9)</cell><cell>21 (56.8)</cell><cell>37 (45.7)</cell><cell>12 (70.6)</cell><cell>137 (49.3)</cell></row><row><cell>Age at SNHL diagnosis, mean (SD), mo</cell><cell>10.2 (13.3)</cell><cell>11.1 (12.4)</cell><cell>3.2 (4.4)</cell><cell>11.6 (15.2)</cell><cell>9.7 (12.8)</cell></row><row><cell>Age of HA fitting, mean (SD), mo</cell><cell>11.6 (13.2)</cell><cell>12.3 (12.5)</cell><cell>3.8 (4.2)</cell><cell>16.9 (13.6)</cell><cell>10.4 (12.3)</cell></row><row><cell>Age at MRI, mean (SD), mo</cell><cell>23.8 (20.5)</cell><cell>26.9 (18.2)</cell><cell>11.4 (12.1)</cell><cell>24.3 (18.0)</cell><cell>20.7 (18.9)</cell></row><row><cell>Age at CI, mean (SD), mo</cell><cell>27.4 (20.9)</cell><cell>30.1 (18.4)</cell><cell>19.2 (13.2)</cell><cell>32.5 (16.6)</cell><cell>25.7 (18.8)</cell></row><row><cell>Unaided hearing of left ear, dB HL</cell><cell>95.4 (17.0)</cell><cell>98.9 (18.0)</cell><cell>97.7 (18.7)</cell><cell>103.3 (15.7)</cell><cell>96.9 (17.5)</cell></row><row><cell>Unaided</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>hearing of right ear, dB</cell><cell>93.7 (18.1)</cell><cell>100.2 (15.1)</cell><cell>99.5 (19.0)</cell><cell>101.7 (14.0)</cell><cell>96.5 (17.9)</cell></row><row><cell>HL</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Abbreviations: CI, cochlear implants; MRI, magnetic resonance imaging;</cell></row><row><cell cols="6">HA, hearing aid; SNHL, sensorineural hearing loss; NA, not applicable</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>The classification performance of the Transfer Learning models and Machine Learning models in the Chicago English group.</figDesc><table><row><cell>Type s</cell><cell>Models</cell><cell>Accuracy</cell><cell cols="2">% (95% CI) Sensitivity Specificity</cell><cell>AUC (95% CI)</cell></row><row><cell></cell><cell>VGG19_ bn</cell><cell>81.17 (80.11-82.22)</cell><cell>86.19 (84.80-87.57)</cell><cell>75.73 (73.55-77.90)</cell><cell>0.810 (0.799-0.820)</cell></row><row><cell></cell><cell>ResNet-50d</cell><cell>88.02 (86.92-89.11)</cell><cell>88.16 (85.98-90.34)</cell><cell>87.86 (86.21-89.51)</cell><cell>0.880 (0.869-0.891)</cell></row><row><cell>Slice -base d</cell><cell>DenseN et_169 AlexNet Inceptio n_V3</cell><cell>89.09 (88.06-90.12) 79.95 (78.61-81.30) 83.64 (81.75-85.53)</cell><cell>92.11 (91.47-92.74) 84.13 (82.67-85.58) 85.65 (77.40-93.90)</cell><cell>85.83 (83.64-88.02) 75.44 (72.53-78.35) 81.46 (73.24-89.67)</cell><cell>0.890 (0.879-0.900) 0.800 (0.786-0.813) 0.836 (0.817-0.854)</cell></row><row><cell></cell><cell>Google Net</cell><cell>87.13 (85.54-88.72)</cell><cell>92.38 (90.53-94.22)</cell><cell>81.46 (79.07-83.84)</cell><cell>0.869 (0.853-0.885)</cell></row><row><cell></cell><cell>MobileN</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>et</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>The performance of the Transfer Learning method within and across datasets using the MobileNet model. The external validation across different languages or centers was conducted on trained model on a single dataset (Chicago English) separately.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>% (95% CI)</cell><cell></cell><cell></cell></row><row><cell cols="2">Datasets</cell><cell>Accuracy</cell><cell cols="2">Sensitivity Specificity</cell><cell>AUC (95% CI)</cell></row><row><cell></cell><cell>Chicago _English</cell><cell>89.74 (89.39-90.10)</cell><cell>87.09 (86.17-88.00)</cell><cell>92.20 (90.98-93.42)</cell><cell>0.896 (0.893-0.900)</cell></row><row><cell>Single Dataset</cell><cell>Melbour ne_Engli sh</cell><cell>91.03 (90.60-91.46)</cell><cell>91.67 (90.63-92.70)</cell><cell>90.41 (89.09-91.72)</cell><cell>0.910 (0.906-0.915)</cell></row><row><cell></cell><cell>Chicago</cell><cell>85.41</cell><cell>89.02</cell><cell>82.33</cell><cell>0.857</cell></row><row><cell></cell><cell>_Spanis</cell><cell>(70.96-</cell><cell>(87.69-</cell><cell>(54.97-</cell><cell>(0.724-</cell></row><row><cell></cell><cell>h</cell><cell>99.85)</cell><cell>90.35)</cell><cell>99.96)</cell><cell>0.990)</cell></row><row><cell>Across Center</cell><cell>Melbou nre_Eng lish a</cell><cell>50.95 (49.14-52.75)</cell><cell>62.90 (3.74-100)</cell><cell>39.28 (0-95.66)</cell><cell>0.511 (0.489-0.533)</cell></row><row><cell>Across Langua ge</cell><cell>Chicago _Spanis h a</cell><cell>50.27 (46.78-53.76)</cell><cell>36.89 (0-93.88)</cell><cell>63.95 (6.43-100)</cell><cell>0.499 (0.467-0.532)</cell></row><row><cell>Across Center &amp; Langua ge</cell><cell>Hong Kong_C antones e a</cell><cell>50.75 (47.62-53.87)</cell><cell>36.67 (0-96.18)</cell><cell>63.26 (3.46-100)</cell><cell>0.500 (0.496-0.504)</cell></row><row><cell></cell><cell>Chicago</cell><cell>87.38</cell><cell>85.36</cell><cell>89.57</cell><cell>0.874</cell></row><row><cell></cell><cell>+Melbo</cell><cell>(87.12-</cell><cell>(84.02-</cell><cell>(88.04-</cell><cell>(0.871-</cell></row><row><cell>Combin ed Dataset</cell><cell>urne Chicago +Melbo urne+H</cell><cell>87.64) 87.94 (87.28-</cell><cell>86.70) 88.33 (87.18-</cell><cell>91.11) 87.56 (86.12-</cell><cell>0.876) 0.879 (0.873-</cell></row><row><cell></cell><cell>ong</cell><cell>88.59)</cell><cell>89.48)</cell><cell>89.00)</cell><cell>0.886)</cell></row><row><cell></cell><cell>Kong</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>a</p></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hearing and speech benefits of cochlear implantation in children: A review of the literature</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Cushing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Papsin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Gordon</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ijporl.2020.109984</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Pediatric Otorhinolaryngology</title>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="page">109984</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Age at Intervention for Permanent Hearing Loss and 5-Year Language Outcomes</title>
		<author>
			<persName><forename type="first">Tyc</forename><surname>Ching</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Button</surname></persName>
		</author>
		<idno type="DOI">10.1542/peds.2016-4274</idno>
	</analytic>
	<monogr>
		<title level="j">Pediatrics</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2017">2017. 20164274</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Outcomes of Early-and Lateidentified Children at 3 Years of Age: Findings from a Prospective Populationbased Study</title>
		<author>
			<persName><forename type="first">Tyc</forename><surname>Ching</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Marnane</surname></persName>
		</author>
		<idno type="DOI">10.1097/AUD.0b013e3182857718</idno>
	</analytic>
	<monogr>
		<title level="j">Ear Hear</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="535" to="552" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cochlear implants before 9 months of age led to more natural spoken language development without increased surgical risks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Karltorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Eklöf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Östlund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Asp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tideholm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Löfkvist</surname></persName>
		</author>
		<idno type="DOI">10.1111/apa.14954</idno>
	</analytic>
	<monogr>
		<title level="j">Acta Paediatrica</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="332" to="341" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Early intervention intensity and language outcomes for children using cochlear implants</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dettman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Choo</surname></persName>
		</author>
		<idno type="DOI">10.1080/14643154.2019.1685755</idno>
	</analytic>
	<monogr>
		<title level="j">Deafness &amp; Education International</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="156" to="174" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Prediction as a humanitarian and pragmatic contribution from human cognitive neuroscience</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Gabrieli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Whitfield-Gabrieli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="26" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Preoperative Neuroanatomical Features Outperform Non-Neural Features in Predicting Auditory Skills in Chinese-Learning Children After Cochlear Implantation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ihy</forename><surname>Ng</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Published online May 31, 2024. doi:10.31234/osf.io/e2w5y</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural preservation underlies speech improvement from auditory deprivation in young cochlear implant recipients</title>
		<author>
			<persName><forename type="first">G</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Ingvalson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Grieco-Calub</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1717603115</idno>
	</analytic>
	<monogr>
		<title level="j">PNAS</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1022" to="E1031" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A semisupervised Support Vector Machine model for predicting the language outcomes following cochlear implantation based on pre-implant brain fMRI imaging</title>
		<author>
			<persName><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Holland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1002/brb3.391</idno>
	</analytic>
	<monogr>
		<title level="j">Brain and Behavior</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">391</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cross-Modal Cortical Activity in the Brain Can Predict Cochlear Implantation Outcome in Adults: A Machine Learning Study</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Kyong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<idno type="DOI">10.5152/iao.2021.9337</idno>
	</analytic>
	<monogr>
		<title level="j">J Int Adv Otol</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="380" to="386" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Predicting cochlear implant outcome from brain organisation in the deaf</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Giraud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="381" to="390" />
		</imprint>
	</monogr>
	<note>Restorative neurology and neuroscience</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adaptive benefit of cross-modal plasticity following cochlear implantation in deaf adults</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Wiggins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Kitterick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deh</forename><surname>Hartley</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1704785114</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">38</biblScope>
			<biblScope unit="page" from="10256" to="10261" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cortical activity at rest predicts cochlear implantation outcome</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Giraud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cerebral Cortex</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="909" to="917" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cochlear Implantation in Postlingually Deaf Adults is Time-sensitive Towards Positive Outcome: Prediction using Advanced Machine Learning Techniques</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Park</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41598-018-36404-1</idno>
	</analytic>
	<monogr>
		<title level="j">Sci Rep</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">18004</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A review of deep transfer learning and recent advancements</title>
		<author>
			<persName><forename type="first">M</forename><surname>Iman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Arabnia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rasheed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technologies</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">40</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Understanding auditory development and the child with hearing loss</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Perigoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Paterson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="173" to="204" />
		</imprint>
	</monogr>
	<note>Fundamentals of audiology for the speech-language pathologist. Published online</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Critical Periods in Speech Perception: New Directions</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Werker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Hensch</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev-psych-010814-015104</idno>
	</analytic>
	<monogr>
		<title level="j">Annu Rev Psychol</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="173" to="196" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Language skills of children with early cochlear implantation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Geers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Nicholas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Sedey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ear and hearing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="46S" to="58" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Relationships between speech perception abilities and spoken language skills in young children with hearing loss</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Desjardin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Ambrose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Eisenberg</surname></persName>
		</author>
		<idno type="DOI">10.1080/14992020802607423</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Audiology</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="248" to="259" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Continuity and Stability in Development</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Bornstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Putnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Esposito</surname></persName>
		</author>
		<idno type="DOI">10.1111/cdep.12221</idno>
	</analytic>
	<monogr>
		<title level="j">Child Development Perspectives</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="113" to="119" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Stability of core language skill from infancy to adolescence in typical and atypical development</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Bornstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Putnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Pearson</surname></persName>
		</author>
		<idno type="DOI">10.1126/sciadv.aat7422</idno>
	</analytic>
	<monogr>
		<title level="j">Sci Adv</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">7422</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The ANTsX ecosystem for quantitative biological and medical imaging</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Tustison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Holbrook</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41598-021-87564-6</idno>
	</analytic>
	<monogr>
		<title level="j">Sci Rep</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">9068</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">C</forename><surname>Gaser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Nenadic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Buchsbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Hazlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Buchsbaum</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deformation-based morphometry and its relation to conventional volumetry of brain lateral ventricles in MRI</title>
		<idno type="DOI">10.1006/nimg.2001.0771</idno>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1140" to="1145" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>Pt 1</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Infant Brain Atlases from Neonates to 1-and 2-Year-Olds</title>
		<author>
			<persName><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Yap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0018746</idno>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">18746</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for classification of Alzheimer&apos;s disease: Overview and reproducible evaluation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Thibeau-Sutre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Diaz-Melo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page">101694</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Transfer learning approaches for neuroimaging analysis: a scoping review</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ardalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Subbian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">780405</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/" />
		<imprint>
			<date type="published" when="2012-06-07">2012. June 7, 2024. 2012/hash/c399862d3b9d6b76</date>
			<biblScope unit="volume">25</biblScope>
		</imprint>
	</monogr>
	<note>Advances in neural information processing systems. c8436e92 4a68c45b-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1409.1556" />
		<imprint>
			<date type="published" when="2015-06-07">2015. June 7, 2024</date>
		</imprint>
	</monogr>
	<note>Published online April 10</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="http://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024-06-07">June 7, 2024</date>
			<biblScope unit="volume">2016</biblScope>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="volume">2016</biblScope>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName><surname>Accessed</surname></persName>
		</author>
		<ptr target="https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.html" />
		<imprint>
			<date type="published" when="2024-06-07">June 7, 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ieee conference on computer vision and pattern recognition</title>
		<meeting>the ieee conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2015</biblScope>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Published online April 16</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName><surname>Accessed</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1704.04861" />
		<imprint>
			<date type="published" when="2024-06-07">June 7, 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<ptr target="http://openaccess.thecvf.com/content_cvpr_2017/html/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024-06-07">June 7, 2024</date>
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper_files/paper/2014/hash/375c71349" />
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014-06-07">2014. June 7, 2024</date>
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
	<note>b295fb e2dcdca9206f20a06-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The impact of multi-optimizers and data augmentation on TensorFlow convolutional neural network performance</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Taqi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Awad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Al-Azzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Milanova</surname></persName>
		</author>
		<ptr target="https://ieeexplore.ieee.org/abstract/document/8396988/" />
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-06-07">2018. June 7, 2024</date>
			<biblScope unit="page" from="140" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A data augmentation-based framework to handle class imbalance problem for Alzheimer&apos;s stage detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Afzal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maqsood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nazir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="115528" to="115539" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">An Alzheimer&apos;s disease classification model using transfer learning Densenet with embedded healthcare decision support system</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Alkhaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Verma</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.dajour.2023.100348</idno>
	</analytic>
	<monogr>
		<title level="j">Decision Analytics Journal</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">100348</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Transfer learning enables predictions in network biology</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">V</forename><surname>Theodoris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chopra</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41586-023-06139-9</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">618</biblScope>
			<biblScope unit="issue">7965</biblScope>
			<biblScope unit="page" from="616" to="624" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A deep learning system for detecting diabetic retinopathy across the disease spectrum</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41467-021-23458-5</idno>
	</analytic>
	<monogr>
		<title level="j">Nat Commun</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3242</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Evaluation of aducanumab for Alzheimer disease: scientific evidence and regulatory review involving efficacy, safety, and futility</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Emerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Kesselheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Jama</title>
		<imprint>
			<biblScope unit="volume">325</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="1717" to="1718" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Representational similarity analysis-connecting the branches of systems neuroscience</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kriegeskorte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Bandettini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in systems neuroscience</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">249</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A cultural effect on brain function</title>
		<author>
			<persName><forename type="first">E</forename><surname>Paulesu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mccrory</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fazio</surname></persName>
		</author>
		<idno type="DOI">10.1038/71163</idno>
	</analytic>
	<monogr>
		<title level="j">Nat Neurosci</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="96" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Culture-sensitive neural substrates of human cognition: A transcultural neuroimaging approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Northoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature reviews neuroscience</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="646" to="654" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">DeepAD: Alzheimer&apos;s disease classification via deep convolutional neural networks using MRI and fMRI</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sarraf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tofighi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adn</forename><surname>Initiativ</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioRxiv</title>
		<imprint>
			<biblScope unit="page">70441</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Published online</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">:</forename><surname>Abbreviations</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logistic</forename><surname>Lr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Regression</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K-Nearest</forename><surname>Knn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Neighbor</surname></persName>
		</author>
		<author>
			<persName><surname>Svm</surname></persName>
		</author>
		<title level="m">Support Vector Machine; DT, Decision Tree; RT, Random Forest; XGBoost, eXtreme Gradient Boosting</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
