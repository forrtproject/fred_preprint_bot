<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Experimental evidence that delegating to intelligent machines can increase dishonest behaviour</title>
				<funder ref="#_95camRU">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
				<funder ref="#_C4XUuhY">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nils</forename><surname>Köbis</surname></persName>
							<email>nils.koebis@uni-due.de</email>
							<affiliation key="aff0">
								<orgName type="department">Research Center Trustworthy Data Science and Security</orgName>
								<orgName type="institution">University Duisburg-Essen</orgName>
								<address>
									<settlement>Duisburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zoe</forename><surname>Rahwan</surname></persName>
							<email>zrahwan@mpib-berlin.mpg.de</email>
							<affiliation key="aff2">
								<orgName type="department">Center for Adaptive Rationality</orgName>
								<orgName type="institution">Max Planck Institute for Human Development</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Clara</forename><surname>Bersch</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Center for Humans and Machines</orgName>
								<orgName type="institution">Max Planck Institute for Human Development</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tamer</forename><surname>Ajaj</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Center for Humans and Machines</orgName>
								<orgName type="institution">Max Planck Institute for Human Development</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Toulouse School of Economics</orgName>
								<orgName type="institution">CNRS (TSM-R)</orgName>
								<address>
									<settlement>Toulouse</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jean-Franc</forename><surname>¸ois Bonnefon</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Center for Humans and Machines</orgName>
								<orgName type="institution">Max Planck Institute for Human Development</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Iyad</forename><surname>Rahwan</surname></persName>
							<email>rahwan@mpib-berlin.mpg.de</email>
							<affiliation key="aff1">
								<orgName type="department">Center for Humans and Machines</orgName>
								<orgName type="institution">Max Planck Institute for Human Development</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Experimental evidence that delegating to intelligent machines can increase dishonest behaviour</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D883AA65E43B2D6B5E8B6FF9E901E477</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-22T20:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While artificial intelligence (AI) enables significant productivity gains from delegating tasks to machines, it can also facilitate the delegation of unethical behaviour. Here, we demonstrate this risk by having human principals instruct machine agents to perform a task with an incentive to cheat. Principals' requests for cheating behaviour increased when the interface implicitly afforded unethical conduct: Machine agents programmed via supervised learning or goal specification evoked more cheating than those programmed with explicit rules.</p><p>Cheating propensity was unaffected by whether delegation was mandatory or voluntary. Given the recent rise of large language model-based chatbots, we also explored delegation via natural language. Here, cheating requests did not vary between human and machine agents, but compliance diverged: When principals intended agents to cheat to the fullest extent, the majority of human agents did not comply, despite incentives to do so. In contrast, GPT4, a state-of-the-art machine agent, nearly fully complied. Our results highlight ethical risks in delegating tasks to intelligent machines, and suggest design principles and policy responses to mitigate such risks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>intelligence (AI), a phenomenon we will call 'machine delegation' <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. For example, human principals are already letting machine agents decide how to drive <ref type="bibr" target="#b2">[3]</ref>, where to invest their money <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> and whom to hire or fire <ref type="bibr" target="#b5">[6]</ref>, but also how to interrogate suspects and engage with military targets <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. Machine delegation promises to increase productivity <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> and decision quality <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>. One potential risk, though, is that it will lead to an increase in ethical transgressions, such as lying and cheating for profit <ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref>, by diminishing the moral costs that typically deter such behaviours. Specifically, people are often reluctant to engage in profitable yet dishonest behaviour because they do not want to see themselves as dishonest <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. In other words, dishonest behaviour is often prevented because the material benefit of cheating or lying is offset by the moral cost of seeing oneself as a cheat and a liar. Conversely, people find it easier to cheat, lie or exploit others when this moral cost is diminishedtypically, when they can claim some uncertainty about the harmful consequences of their actions, and accordingly avoid seeing themselves as bad people <ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref>. Machine delegation may provide such subjective uncertainty through the flexibility it offers to human principals who give instructions, and the opacity in the processing of these instructions.</p><p>People may find it difficult to give a machine detailed, programmatic instructions about how to lie on their behalf, just as they find it difficult to blatantly lie themselves <ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref>. Detailed rule-based programming or 'symbolic rule specification' is just one way to give instructions to machines, though <ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref>. Machines can also receive instructions through supervised learning; here, people provide the machines with examples of desired outcomes and let them come up with a strategy to achieve those outcomes. Machines can also be given a high-level goal, such as 'optimize profit' , and be left free to elaborate a strategy to achieve it. With recent progress in large language models (LLMs), people can now also give ambiguous instructions in natural language, leaving the machine to 'interpret' dishonest intentions. These interfaces can make it easier for human principals to deny responsibility when they request unethical behaviour from a machine. This deniability is compounded by the black-box nature of many machines, which allows human principals to claim ignorance of the way their instructions are processed <ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref>. Finally, machine delegation can decrease or elim-inate the moral cost incurred by the agent who is asked to act dishonestly. While human agents may reject such requests out of moral concerns, machine agents without adequate safeguards may simply comply. Against this background, we summarize our overarching question: Could some forms of machine delegation make human principals more likely to request dishonest behaviour-and machine agents more likely to comply?</p><p>Here, we provide the first empirical answers to this question. Specifically, we show that in rule-based, supervised learning and goal-based interfaces (see Fig. <ref type="figure" target="#fig_1">1</ref>), humans are more likely to request cheating behaviour from a machine than to cheat themselves. Furthermore, the likelihood of a principal requesting cheating strongly depends on the means of transmitting such a request: Across four interfaces representing the most common programming paradigms for machine delegation (rule-based, supervised learning, goal-based, and prompt engineering using natural language), we show that requests for cheating behaviour are rare when they must be made explicitly (e.g., when human principals have to specify precise rules for the machine agent) but increase when they are made implicitly (e.g., when human principals give the machine agent a high-level, ambiguous goal to pursue autonomously). Further, keeping the delegation interface constant, we compare the rate at which human principals request cheating behaviour through a natural language interface when interacting with human versus machine agents, and the rate at which human versus machine agents comply with such requests. We find that machines are more compliant overall, and the most profound difference between machine and human compliance emerges when agents are requested to cheat to the fullest extent. After reporting these results, we explore their implications for future scenarios, considering how LLMs and other machines reduce delegation costs by improving access, ease of use and affordability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>To measure cheating behaviour, we employed the classic die-rolling task used across the behavioural sciences <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>. Participants were asked to report the result of a die roll that they observed privately <ref type="bibr" target="#b25">[26]</ref>, knowing that their payoff would match the result they reported (here, 1 U.S. cent if they reported a 1, 2 cents if they reported a 2 and so on up to 6 cents if they reported a 6). Accordingly, participants had the oppor-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rule Specification</head><p>Prescribe, for each situation, the algorithmic behavior via if-then rules</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AI programming paradigm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How delegation is done</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Specific interface for dierolling task</head><p>When observed die roll is</p><p>The algori thm shoul d report die roll</p><p>1 2 3 4 5 6 1 3 4 5 6 1 3 4 5 6 1 3 4 5 6 1 3 4 5 6 1 3 4 5 6 1 3 4 5 6</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supervised Learning</head><p>Select a prototypical behavior to train the algorithm via a dataselection interface</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Goal Specification</head><p>Indicate the goal that the algorithm should pursue by interpolating between different objectives</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prompt Engineering</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Write instructions to indicate what the algorithm should do via a natural language interface</head><p>Report what you believe is ethically correct, but if you are undecided then take my monetary interest into account.</p><p>Ok.</p><p>How should I act on your behalf?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Maximize Accuracy Maximize Profit</head><p>Dataset A Round Reported Die Roll tunity to cheat for profit by reporting a result higher than the roll observed, thereby accumulating profit over a series of 10 rolls and reports. Here, we used a version of the task in which participants did not have full privacy when observing the roll, since</p><p>they observed the roll on the computer screen rather than physically rolling the die themselves. This implementation of the task tends to increase the honesty of reports <ref type="bibr" target="#b17">[18]</ref> but otherwise has the same construct validity as the version with a physical die roll. The die-rolling task has been rigorously validated and used in hundreds of studies <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. It predicts real-world behaviours such as fare-dodging in public transportation <ref type="bibr" target="#b34">[35]</ref>, school misconduct <ref type="bibr" target="#b35">[36]</ref> and deceptive sales practices <ref type="bibr" target="#b36">[37]</ref>. To improve experimental control, across all three studies, participants observed the same series of 10 die rolls. All studies included a Control condition in which participants performed the task themselves, which was compared with various conditions in which they could delegate the task to human or machine agents through various delegation interfaces. Further, in all studies, we aimed for samples that were representative of the United States population in terms of age, self-identified gender and ethnicity. All studies were preregistered and did not use deception. All results reported are from two-sided tests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Studies 1 and 2</head><p>Conditions and Outcomes. Participants in Study 1 (N = 597) were randomly assigned to one of four conditions. In the Control condition, they reported the results of the 10 die rolls themselves. In the Rule-Based, Supervised Learning and Goal-Based conditions, they delegated these 10 reports to machine agents (see Methods for details).</p><p>The conditions differed in the interface that participants used to transmit their instructions to the machine. In the Rule-Based condition, participants had to explicitly tell the machine what number to report for each of the 6 possible die-roll outcomes. In the Supervised Learning condition, participants chose one of three example datasets to train the machine. In one example dataset, the reports were always the same as the die rolls. In another, the reports were always 6, regardless of the die rolls. In another, the reports were either the same as or above the actual die roll, indicating occasional cheating. The three example datasets featured a series of 10 die-roll outcomes but just Control and Rule-Based conditions.</p><p>Study 2 (N = 801) was identical in all respects to Study 1, except that in all delegation conditions (Rule-Based, Supervised Learning and Goal-Based), participants had the option either to delegate reports to the machine or to self-report. Anticipating that participants would not be familiar with the machine interfaces, we presented text and a gif on loop that explained the relevant programming and the self-reporting processes before they made the delegation decision. We predicted that more participants would choose to delegate to machines in the Supervised Learning and Goal-Based condition. In fact, we observed that about 50% of participants delegated to machines, regardless of the interface (see SI, Study 2, Pre-Registered Confirmatory Analyses).</p><p>Hereafter, when we discuss the results of Study 2, we focus on these participants.</p><p>All analyses reported in the main text were pre-registered; some secondary and exploratory analyses are reported in the SI (see SI, Study 2, Pre-Registered Exploratory Analyses). main prediction was that the proportion of participants engaging in Honesty would be smaller in the Supervised Learning and Goal-Based condition than in the Control and Rule-Based conditions, which offered comparably less opportunity for maintaining plausible deniability about one's intentions. For both studies, this prediction was supported by a binary logistic regression with a dichotomous predictor variable (Study 1: B = 2.53, SE = 0.21, p &lt; .001, OR = 12.6, CI[8.5; 19.0]; Study 2: B = 3.00, SE = 0.24, p &lt; .001, OR = 20.1, CI[12.6; 32.9]: See Tables S1 and S10). At 95%, the vast majority of participants (Study 1: 95% CI = [90,98], Study 2: 95% CI = [93,99]) reported die rolls honestly in the Control condition without delegation. This proportion decreased when participants delegated to machines, to about 75% in the Rule-Based condition (Study 1: 95% CI = [67,81], Study 2: 95% CI = [67,84]), about 50% in the Supervised Learning condition (Study 1: 95% CI = [43,60], S2: 95% CI = [37,58]) and only about 15% in the Goal-Based condition (Study 1: 95% CI = [8,19], Study 2: 95% CI = [10,25]). All these comparisons were significant to the p &lt; .001 level, including Bonferroni correction for multiple comparisons (see SI, Results Study 1 &amp; 2). Type of Cheating across Conditions. For both studies, we conducted pre-registered ordinal probit regression analyses to analyze shifts toward Partial or Full Cheating depending on delegation interface, contrasting the pooled Control and Rule-Based conditions to the pooled Supervised Learning and Goal-Based conditions. Participants in the Supervised Learning and Goal-Based conditions were significantly more likely to engage in higher levels of cheating than participants in the Control and Rule-Based conditions. This held for both Study 1: β = 1.38 (SE = 0.11, t = 12.55, p &lt; .001) and Study 2: β = 1.13 (SE = 0.11, t = 9.94, p &lt; .001) (see Tables <ref type="table">S5</ref>, <ref type="table" target="#tab_0">S11</ref>). Consistently, the threshold for transitioning from Honesty to Partial Cheating was lower than the threshold for transitioning from Partial Cheating to Full Cheating. In sum, both studies indicate that the Supervised Learning and Goal-Based interfaces significantly increased the likelihood of higher cheating levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Honesty across Conditions.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study 3</head><p>The delegation interfaces we used in Studies 1 and 2 captured, in a stylized manner, three methods that human principals can use to give instructions to machine agents, and showed that some led to substantially higher levels of unethical behaviour. However, technological advances such as LLMs now allow human principals to delegate to machine agents using natural language instructions (also known as 'prompt engineering'), just as they would for human agents. These advances make it possible to compare the behaviour human principals request from human versus machine agents, keeping the delegation interface constant-and to compare the subsequent behaviour of human and machine agents once they receive these requests. Our main preregistered predictions were that the proportion of participants engaging in Honesty would be smaller in the human agent and machine agent conditions relative to self-reporting, and that intended, actual and perceived third-party dishonesty would be lower for instructions for human agents relative to those for machine agents.</p><p>In Study 3, we recruited 390 participants to act as principals in the die-rolling task. Participants wrote natural language instructions for human and machine agents, and also undertook the die-rolling task themselves. Our focus was on the observed rate at which the principals requested Honesty, Partial Cheating and Full Cheating behaviour-and on how human and machine agents behaved after receiving these instructions. To this end, we recruited 975 further participants to act as human agents.</p><p>They were incentivized to comply with the instructions they received. Machine agents were implemented with GPT-4, to maximize external validity by leveraging the behaviour of one of the most advanced and widely utilized commercial-grade language models.</p><p>As there are no simple and objective criteria to categorize natural language instructions as requesting Honesty, Partial Cheating or Full Cheating, we adopted a multi-pronged approach to categorization, making it possible to assess the robustness of our results across approaches. First, we collected a Self-Categorization from principals, by asking them to indicate what they expected the agent to report for each die-roll outcome over 10 rounds, based on the instructions they gave. We then used the same criteria as in Studies 1 and 2 to categorize their behavioural intention as Hon-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Requests for dishonest behaviour usin4 natural lan4ua4e</head><p>Different methods of categorization resulted in different proportions of requests for honesty, partial cheating, and full cheating. No categorization method, though, found credible evidence that principals requested different behavior from human versus machine agents. esty, Partial Cheating or Full Cheating. Second, to obtain a third-party perspective, we conducted an LLM Categorization of the instructions, using natural language processing implemented with GPT-4. Third, we recruited an independent sample of 98 human coders to conduct a Rater Categorization of these instructions into the same three categories. For the human coders, the accuracy of estimates was rewarded with reference to human principals' stated intention for the aggregate die-roll outcome.</p><p>Behavioural requests from principals using natural language. Figure <ref type="figure" target="#fig_3">3</ref> displays the proportion of principals who requested Honesty, Partial Cheating or Full Cheating from human and machine agents by categorization method. A higher rate of instructions were categorized as requesting Partial Cheating under Rater Categorization but, overall, results were quite consistent. First, the proportion of dishonest delegation through natural language (Self-Categorization = 25%, 95% CI = <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b27">28]</ref>; LLM Categorization = 26%, 95% CI = <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b28">29]</ref>) was, with the exception of Rater Categorization (40%, 95% CI = <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b43">44]</ref>), directly comparable with that observed in the Rule-Based condition in Studies 1 and 2. Certainly, the dishonesty levels observed in either the Supervised Learning interface (49-52%, ps &lt; .001: for group-wise comparisons see SI, Comparison of Dishonest Behaviour Across Studies, Table <ref type="table">S32</ref>) or the Goal-Based interface (84-88%, ps &lt; .001) were never reached. Second, there was no evidence that people requested more cheating from machines than from humans when using natural language, irrespective of the categorization method used (see SI, Results Study 3a-c).</p><p>Behaviour of agents after receiving natural language instructions. Figure <ref type="figure" target="#fig_4">4</ref> displays the proportion of human and machine agents who engaged in Honesty, Partial</p><p>Cheating or Full Cheating by the type of instructions they received from their principal. To simplify this visualization, we assigned a level of honesty to each request from the principal by applying a simple majority rule to the results of Self-Categorization, LLM Categorization, and Rater Categorization (see Tables <ref type="table">S28</ref> and <ref type="table">S29</ref> showing that overall compliance is robust for different categorization methods, as well as Figures <ref type="figure" target="#fig_3">S3-6</ref> showing the match between principals' self-categorization and each of the other categorization methods). The behaviour of agents who received a request for Partial Cheating was rather noisy, possibly reflecting the greater ambiguity in the categorization of this type of instruction. What is much clearer is the behaviour of agentshumans and machines alike-who received a request for Honesty: They overwhelmingly complied. The main difference between human and machine agents emerged when Full Cheating was requested. Machine agents overwhelmingly complied with this request, but human agents were much more likely to refuse to comply and engage in Honesty instead-even though compliance with instructions was financially incentivized. Irrespective of which method was used to classify principals' instructions as Full Cheating, human agents were always less likely to comply than machine agents (all ps &lt; .001)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Behaviour of a4ents who received natural lan4ua4e instructions</head><p>Human and machine agents overwhelmingly complied with requests for honesty, but behaviour was more noisy when principals requested partial cheating. Machines overwhelmingly complied with requests for Full Cheating, whereas humans refused to comply in half of cases, even though they were incentivized to do so. To make sure that this difference in agent behaviour did not reflect some subtle differences in the way principals wrote their instructions to human and machine agents, we also collected the behaviour of human and machine agents when they received instructions meant for the other type of agent. Agents were kept naive about the nature of the intended agent. As reported in Table <ref type="table">S29</ref>, results were robust to this forking variation: Machine agents still complied when they received Full Cheating instructions meant for humans, and many human agents still refused to comply when they received Full Cheating instructions meant for machines. Accordingly, the results of Study 3 strongly suggest that while human agents are likely to resist unethical instructions, even at personal cost, machines may, without hesitation, accept these instructions. Importantly, however, we did not attempt to endow the machine agents in our study with any guardrails against lying for their principals. Thus, our results are a warning more than a statement of fact: We do not claim that machines will always comply with unethical instructions. We rather warn that they must be trained to not comply, given that they do not come with the same built-in non-compliance mechanisms as humans do.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>Our results establish that people are more likely to request unethical behaviour from machines than to engage in the same unethical behaviour themselves. Across all studies, fewer participants cheated for profit themselves than when using machine agents, with 25% to 85% of participants requesting machines to cheat for them. Note that this does not necessarily mean that people are more likely to request unethical behaviour from machines than to request unethical behaviour from humans. Indeed, in Study 3, we observed no meaningful differences in the natural language instructions that participants sent to humans versus machines. With that proviso, our findings give at least three reasons to be worried about the effects of machine delegation.</p><p>First, the scope for variation in delegation interfaces for machines will be more varied than for humans, and some machine delegation interfaces will increase the likelihood of unethical delegation above the level observed with natural language. For example, Studies 1 and 2 showed that unethical delegation was contained to 25% when principals had to tell the machine exactly which rules to follow-comparable to the unethical delegation rate observed with natural language instructions. However, other delegation interfaces that allowed for more ambiguity and machine autonomy (showing examples of desired behaviour; giving a high-level goal) increased unethical delegation rates to around 50-85%, which are substantial effects. These findings are consistent with psychological theories of unethical behaviour that emphasize the role of moral disengagement <ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref>: Interfaces that give principals more leeway to claim uncertainty about the behaviour of machine agents will increase unethical behaviour under machine delegation.</p><p>Second, even in the best-case scenario where principals are not more likely to delegate unethical behaviour to machines, machine delegation will still lead to an absolute increase in unethical behaviour, merely by making delegation more frequent. By removing many of the transaction costs of delegation, machine agents will become available to an increasing population of principals, increasing the scope and speed of delegation <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref>. Accordingly, the volume of unethical behaviour is likely to increase under machine delegation, even with interfaces that do not increase the relative likelihood of unethical behaviour under machine delegation.</p><p>Third, and still in the best-case scenario where principals are not more likely to request unethical behaviour from machines, the volume of unethical behaviour may also increase because machine agents are more likely to comply than human agents.</p><p>In Study 3, we observed that many human agents (in fact, a majority) refused to comply with unethical instructions, even at a personal cost. In contrast, machine agents showed little defiance and cheated as instructed for the benefit of their principals.</p><p>Here, we must clearly acknowledge that this behaviour does not predict the behaviour of all other machines. We simply used GPT-4 to implement machine agents, with- Giving machines strong guardrails against unethical behaviour is indeed a crucial step to prevent the rise of unethical behaviour under machine delegation. Our results point to further steps, oriented toward human principals rather than machine agents.</p><p>Study 2 demonstrated that the majority of participants did not opt to delegate this somewhat tedious, low-stakes task to a machine agent. Further, after both experiencing the task themselves and delegating to machine and human agents, Study 3 participants expressed a preference to undertake the task themselves in the future. This preference was strongest among those who engaged in honest behaviour, but also held for the majority of those who engaged in partial and full cheating (Figure <ref type="figure">S2</ref>). Consequently, making sure that principals always have an option to not delegate, or making this option the default, could in itself curb the adverse effects of machine delegation.</p><p>Most importantly, delegation interfaces that make it easier for principals to claim ignorance of how the machine will interpret their instructions should be avoided. In this regard, it would be helpful to better understand the moral emotions that principals experience when delegating to machines under different interfaces. We collected many measures of such moral emotions in our studies, but did not find any clear interpretation. We nevertheless report these measures for interested researchers (see SI, Tables S8, S18, S35).</p><p>Finally, we acknowledge that our stylized protocol missed many of the complications of real-world delegation, and that further research is needed to capture the subtleties of these complications. For example, the simple task we used-a die-roll report-had no social component and required no teamwork or collaboration. More complex tasks involving collaboration between machines and humans with other agents, especially human agents, may reduce welfare given a reluctance towards collaborating with machines <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref>.</p><p>Likewise, delegation does not always operate through instructions. Sometimes, principals delegate by selecting one agent among many, based on information about their typical performance or behaviour-without sending the agent-specific instructions. In the SI, we report on an additional study in which we let principals select human or machine agents based on a series of past die-roll reports by these agents (see SI, Additional Study). Principals preferred agents who were dishonest, whether human or machine. Of concern, principals were more likely to choose fully dishonest machine than human agents, increasing the aggregated costs from unethical behaviour. These additional results are just a preliminary foray into the thorny problems of machine delegation. The unprecedented rapid spread of machine agents means that anyone with internet access will soon be able to delegate a myriad of tasks to these agents, without special access or special knowledge of how they work. This may set the stage for a surge in unethical behaviour, which we as a society must prepare to curb now.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Study 1 Sample. Informed by power analysis using bootstrapping (see SI Additional Study), we recruited 597 participants from Prolific, striving to achieve a sample that was representative of the US population in terms of age, gender and ethnicity (M age = 45.7; SD age = 16.2; 295 self-identified as male; 289 as female; 13 as other, non-binary or preferred not to indicate; 78% identified as White, 12% as Black, 6% as Asian, 2% as</p><p>Mixed and 2% as Other). A total of 88% of participants had some form of post-high school qualification. The study, implemented using oTree, was pre-registered.</p><p>Procedure and Measures. After providing informed consent, participants read the instructions for the die-rolling task <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b39">40]</ref>. They were instructed to roll a die privately and to report the observed outcome. They would receive a bonus based on the number reported. Participants would earn 1 cent for a 1, 2 cents for a 2 and so on up to 6 cents for a 6. We deployed a previously validated version of the task in which the die roll is shown on the computer screen <ref type="bibr" target="#b25">[26]</ref>. As distinct from the original oneshot version of the game, participants engaged in 10 rounds of the task, generating a maximum possible bonus of 60 cents. or maximize profit, or one of 5 intermediate settings (see Figure <ref type="figure" target="#fig_6">5</ref>, right).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conditions</head><p>Underlying Algorithms. For each of the delegation conditions, simple algorithms were implemented. This avoids deception of participants as they engaged in a delegation to a simple machine agent as we had stated in the instructions. For the Rule-Based condition, the algorithm followed simple if-then rules as specified by the user.</p><p>For the Supervised Learning condition, the algorithm was implemented by first calculating the difference between the actual and reported rolls for each participant in training data sourced from a pre-pilot in which participants performed an incen-</p><p>tivized die-roll task themselves (n = 96). The algorithm then probabilistically adjusted future reported outcomes based on these differences, with Dataset A (representing Honesty) having no adjustments, Dataset B having moderate, stochastic adjustments (Partial Cheating), and Dataset C having larger adjustments, tending towards but not always engaging in Full Cheating. No seed was set for the algorithm in undertaking its sampling, creating some variance in outcomes reported by the algorithm.</p><p>For the Goal-Based condition, the algorithmic output was guided by the setting on a 7-notch dial ranging from 'maximize accuracy' to 'maximize profit' . The algorithm adjusted the results of a series of actual die rolls to achieve a desired total sum.</p><p>Specifically, the algorithm manipulated a specific list of integers (i.e.,</p><p>representing the sequence of actual die-roll outcomes. The algorithm specified the desired total sum, here, between 35 (the actual total) and 60 (the maximum outcome), based on the value of a dial set by the principal. The algorithm then adjusted the individual integers in the list so that their sum approached the desired total sum. This was achieved by randomly selecting an element in the integer list and incrementing or decrementing its value, depending on whether the current sum of the list was less than or greater than the total desired sum. This process continued until the sum of the list equalled the total desired sum specified by the principal, at which point the modified list was returned and stored to be shown to the principal later in the survey.</p><p>Exit Questions. At the end of the study, we assessed demographics (age, gender, education) and, using 7-point scales, participants' level of computer science expertise, their satisfaction with the payoff, their perceived degree of control over (a) the process of determining the reported die rolls, and (b) the outcome, how much effort the task required from them, how guilty they felt about the bonus, how responsible they felt for choices made in the task, how much they feared punishment, whether the algorithm worked properly, whether they felt they had reported the die rolls honestly, and the degree of dishonesty of their behaviour. Finally, where relevant, participants indicated in an open-text field the reason for the delegation choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study 2</head><p>Sample. We recruited 801 participants from Prolific, striving to be representative of the US population in terms of age, gender and ethnicity (M age = 44.9; SD age = 16.0; 403 self-identified as female; 388 as male; 10 as either other, non-binary or preferred not to indicate; 77% identified as White, 13% as Black, 6% as Asian, 2% as Mixed, and 2% as Other). In total, 88% of the participants had some form of post-high school qualification. The study, run on oTree, was preregistered.</p><p>Procedure, Measures and Conditions. The procedure was identical to Study 1, with the exceptions that: (a) delegation was optional; (b) participants indicated at the end whether they preferred to delegate the decision to a human or an algorithm; and</p><p>(c) participants completed the previously validated Guilt and Shame Proneness scale <ref type="bibr" target="#b44">[45]</ref> at the end of the study.</p><p>In this between-subjects study, we randomly assigned participants to one of four conditions. In the Control condition (n = 205), participants reported the 10 die rolls themselves. Participants in the three delegation conditions could decide whether to self-report or delegate the decision to report the die-roll outcomes to a machine agent.</p><p>In the Rule-Based condition (n = 195), participants could delegate the task to a machine agent by specifying if-then rules. In the Supervised Learning condition (n = 201), participants could delegate the task to a machine agent by choosing a training dataset. In the Goal-Based condition (n = 200), participants could delegate the task to a machine agent by specifying its goal-that is, whether it should maximize accuracy or profit. As we did not expect participants to be familiar with programming instructions to machine agents in these interfaces, the process was described in text and demonstrated in a video played on loop for each interface. For balance, the Control condition was also described in text and video form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study 3</head><p>Study 3 consisted of three parts, each relating to relevant parties in a delegation: (a) principals (delegators), (b) agents (delegates) and (c) third parties. In Study 3a, we collected principals' instruction texts for human and AI agents and their own selfreported behaviour in the task. Additionally, we measured the behaviour they intended for agents by having them report their expected outcomes for each of the 10 die rolls. In Study 3b, we compared the behaviour of human and machine (LLM) agents.</p><p>Both types of agents implemented instructions intended for human agents and instructions intended for machine agents while naive to the nature of the intended delegate. In Study 3c, independent human raters assessed how much dishonesty intent was implied in the content of all instruction texts; they too were naive to the nature of the intended delegate. These third-party evaluations were intended to provide perceptions of the agent's intended behaviour, unaffected by any moral costs of implementing such instructions that the agents may experience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study 3a</head><p>Sample. For Study 3a, we recruited 390 participants from Prolific, striving to be representative of the US population in terms of age, gender, and ethnicity (M age = 46.0; SD age = 15.9; 196 self-identified as female; 189 as male; 5 as other, non-binary or preferred not to indicate; 76% identified as White, 13% as Black, 6% as Asian, 3% as Mixed and 2% as Other). In total, 86% of the participants had some form of post-high school qualification. The study, conducted on Qualtrics, was preregistered. Procedure, Measures and Conditions. Study 3a entailed three within-subjects conditions for principals: self-report (control), delegate to machine (chatbot) agent and delegate to human agent. Before engaging in the task, participants were given general information on the die-rolling task. They were then shown the payoff table describing</p><p>how the reported die rolls would translate to payoffs (i.e., 1 cent for a 1, 2 cents for a 2 and so on, up to 6 cents for a 6). In random order, participants who passed a comprehension check then completed the self-report and both delegation conditions. In the delegation conditions, they were asked to write short instructions in natural language for human and machine agents, indicating how they should report the die-roll outcomes. Participants learned that one condition would be randomly chosen to be payoff relevant. In the machine delegation condition, participants received detailed information about how to program the machine agent to report the 10 die rolls. Since participants may not have had a clear conception of how and whether the machine agent understands natural language instructions, we included a short video showing how it implemented different types of instructions: Honesty, Partial Cheating and Full Cheating. Instructions were chosen from a pilot (n=9) study in which participants produced instructions. We drew upon instructions including those with nuance in conveying unethical intentions by means of indirect speech <ref type="bibr" target="#b45">[46]</ref>. To balance the video presentation across conditions and avoid a condition-specific priming effect <ref type="bibr" target="#b46">[47]</ref>, we also showed short videos in the self-report and human agent conditions. These videos displayed, in random order, three examples of die-roll reporting that reflected Honesty, Partial Cheating and Full Cheating for the same die-roll outcome. After watching these short videos, participants engaged in the three tasks: self-reporting 10 die rolls, delegating to human agents and delegating to machine agents. After completing all three tasks, participants were asked to indicate the behaviour they intended from the human and machine agents. To this end, they were reminded of the text they had written for the respective agent and asked to indicate for 10 observed die rolls what outcome they intended the human/machine agent to report on their behalf.</p><p>Exit Questions At the end of the study, we assessed demographics (age, gender, education) and, using 7-point scales, participants' level of computer science expertise, their previous experience with the die-rolling experiment and with LLMs, their feelings of guilt and responsibility when delegating the task, their expectations regarding the guilt experienced by agents, their expectation as to which agent (machine or human) implementation would align more closely with their intentions, and whether they would prefer to delegate comparable future tasks to human or machine agents or to do it themselves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Automated Response Prevention and Quality Controls</head><p>To reduce the risk of automated survey completion, we included a reCaptcha at the beginning of the survey and checked via Javascript whether participants copy-pasted text into the text fields when writing instructions to agents. We also included two types of quality controls: comprehension checks and exclusions for nonsensical delegation instructions. Participants were informed that they had two attempts each to answer each comprehension check question correctly to be eligible for the bonus (maximum = 60 cents) and that they would be excluded from any bonus payment if they wrote nonsensical instructions in the delegation conditions. All participants were paid participant fees irrespective of the nature of their survey answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study 3b</head><p>Sample. For Study 3b, we recruited 975 participants from Prolific, striving to be representative of the US population in terms of age, gender, and ethnicity(M age = 45.4; education) and, using 7-point scales, participants' level of computer science expertise, their previous experience with the die-rolling experiment and with LLMs, their experienced guilt for each instruction implementation, and whether they could correctly identify whether an instruction was intended for a human or a machine agent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study 3c</head><p>Sample. For the human raters in Study 3c, we recruited 98 participants (M age = 37.5;</p><p>SD age = 12.3; 58 self-identified as female; 38 as male; 2 as other, non-binary or preferred not to indicate; 60% identified as White, 8% as Black, 22% as Asian, 2% as Mixed and 8% as Other). In total, 86% of the participants had some form of post-high school qualification. The study, conducted within a React-based app, was preregistered.</p><p>Procedure, Measures and Implementations We adopted a multi-pronged approach to categorize the honesty level of natural language instructions in Study 3c.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Categorization.</head><p>Principals indicated what they expected the agent to report for each die-roll outcome over 10 rounds, based on the instructions they gave. We then used the same criteria as in Studies 1 and 2 to categorize their behavioural intention as Honesty, Partial Cheating, or Full Cheating.</p><p>LLM Categorization. GPT-4 (Version November 2023) was prompted to evaluate principals' instructions (see SI, Prompt Texts). First, we presented side-by-side in randomized order, each pair of instructions given by principals in Study 3a (one intended for a human agent and one intended for a machine agent). GPT-4 was naive to the nature of the intended agent. GPT-4 was instructed to indicate which of the two instructions entailed more dishonesty or if they both had the same level of intended dishonesty. We then instructed GPT-4 to classify both of the instructions as Honest, Partial Cheating or Full Cheating, and to predict the estimated sum of reported die rolls. For the full prompt, see SI Prompt Texts.</p><p>Rater Categorization. This followed the LLM Categorization process as closely as possible. The human raters were given a general description of the die-rolling task.</p><p>They were then informed that people in a previous experiment had written instructions for agents to report a sequence of 10 die rolls on their behalf. Participants were informed they would act as raters and compare a series of instruction pairs and indi-cate if which of the two instructions entailed more dishonesty or if they both had the same level of intended dishonesty. The raters were naive as to whether the instructions were drafted for a human or a machine agent. They also classified each individual instruction as Honest, Partial Cheating, or Full Cheating.</p><p>Exit Questions At the end of the study, we assessed demographics (age, gender, education) and, using 7-point scales, participants' level of computer science expertise and their previous experience with LLMs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: AI programming paradigms. The four major AI programming paradigms investigated in this article. For each paradigm, the figure explains how delegation is performed in general, and how we instantiated the delegation mechanism in the context of the die-rolling task. Logo copyright by Open AI</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 Figure 2 :</head><label>22</label><figDesc>Figure 2: Main results of Studies 1 and 2: Percentage of participants who engaged in Honesty, Partial Cheating and Full Cheating in the Control condition without delegation, and when delegating to a machine agent, across the three interfaces for giving instructions to the machine: Rule-Based (specifying what the algorithm should do in every situation-i.e., what it should report for each die-roll outcome); Supervised Learning (selecting a dataset of past behaviour to train the algorithm on); and Goal-Based (giving the machine a high-level goal by choosing a setting on a dial from 'maximize accuracy' to 'maximize profit').</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Behavioural requests from principals in Study 3. Percentage of participants who requested Honesty, Partial Cheating and Full Cheating from human or machine agents by method of categorization: self-reports (Self-Categorization), automatic categorization using natural language processing (LLM Categorization) or manual categorization by independent human coders (Rater Categorization).</figDesc><graphic coords="12,120.35,111.14,371.37,324.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Behaviour of agents in Study 3. Percentage of human and machine agents who engaged in Honesty, Partial Cheating and Full Cheating, conditional on the behaviour intended by their principal. Note the alarming level of compliance with Full Cheating by machine agents.</figDesc><graphic coords="14,120.43,243.74,371.46,278.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><figDesc>out any precautions to ensure ethical behaviour. Other machine agents used in real contexts, such as corporate environments, may have better guardrails and refuse to comply with unethical instructions. Still, a free and open market for machine agents may keep this possibility open.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>.Figure 5 :</head><label>5</label><figDesc>Figure 5: Overview of the delegation conditions to which participants were assigned. Left: In the Rule-Based condition, participants specified the number to be reported for each observed die roll. Middle: In the Supervised Learning condition, participants chose a dataset on which to train the algorithm. Right: In the Goal-Based condition, participants specified a high-level goal by setting the dial within a range from maximum accuracy to maximum profit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Classification</figDesc><table><row><cell>In this</cell><cell>Participants engage in</cell><cell>Participants engage in</cell><cell>Participants engage in</cell></row><row><cell>condition</cell><cell>Honesty</cell><cell>Partial Cheating</cell><cell>Full Cheating</cell></row><row><cell></cell><cell>when</cell><cell>when</cell><cell>when</cell></row><row><cell>Control</cell><cell>Their reports always</cell><cell>Their reports are not always</cell><cell>Their reports are al-</cell></row><row><cell></cell><cell>match the die rolls</cell><cell>6 but sum up higher than the</cell><cell>ways 6</cell></row><row><cell></cell><cell></cell><cell>die rolls</cell><cell></cell></row><row><cell>Rule-Based</cell><cell>They request reports</cell><cell>They request reports that are</cell><cell>They request reports</cell></row><row><cell></cell><cell>that always match the</cell><cell>not always 6 but sum up</cell><cell>that are always 6</cell></row><row><cell></cell><cell>die rolls</cell><cell>higher than the die rolls</cell><cell></cell></row><row><cell>Supervised</cell><cell>Their training exam-</cell><cell>Their training example has</cell><cell>Their training exam-</cell></row><row><cell>Learning</cell><cell>ple has reports that</cell><cell>reports that are not always 6</cell><cell>ple has reports that are</cell></row><row><cell></cell><cell>always match the die</cell><cell>but sum up higher than the</cell><cell>always 6</cell></row><row><cell></cell><cell>rolls</cell><cell>die rolls</cell><cell></cell></row><row><cell>Goal-Based</cell><cell>They select the goal</cell><cell>They select a goal in be-</cell><cell>They select the goal</cell></row><row><cell></cell><cell>'maximize accuracy'</cell><cell>tween 'maximize accuracy'</cell><cell>'maximize profit'</cell></row><row><cell></cell><cell></cell><cell>and 'maximize profit'</cell><cell></cell></row><row><cell></cell><cell cols="3">of behaviour in Studies 1 and 2. The criteria for classifying partici-</cell></row><row><cell cols="4">pants as engaging in Honesty, Partial Cheating or Full Cheating were adapted to the delega-</cell></row><row><cell cols="2">tion interface used in each condition.</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>All authors thank <rs type="person">Georg Kruse</rs> for his research assistance, <rs type="person">Heather Barry-Kappes</rs> for helpful feedback, <rs type="person">Ivan Soraperra</rs> for statistical guidance and <rs type="person">Susannah Goss</rs> for language editing. JFB acknowledges support from grant <rs type="grantNumber">ANR-19-PI3A-0004</rs>, grant <rs type="grantNumber">ANR-17-EURE-0010</rs>, and the research foundation <rs type="funder">TSE-Partnership</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_C4XUuhY">
					<idno type="grant-number">ANR-19-PI3A-0004</idno>
				</org>
				<org type="funding" xml:id="_95camRU">
					<idno type="grant-number">ANR-17-EURE-0010</idno>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Data Availability. The pre-registrations, survey instruments, and data for all studies are available at: Open Science Framework.</p><p>Code Availability. The code, written in R, used for analyses and data visualisations, is available at: Open Science Framework.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>SD age = 15.8; 482 self-identified as female; 473 as male; 20 as other, non-binary or preferred not to indicate; 78% identified as White, 13% as Black, 6% as Asian, 2% as Mixed and 1% as Other). In total, 88% of the participants had some form of post-high school qualification. The study, run on Qualtrics, was preregistered. For Study 3b, we pilot-tested the experimental setup with 20 participants who were asked to implement three sample instructions from a previous pilot study for Study 3a (n=9).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure, Measures and Implementations</head><p>Machine Agents. With the aim of assessing the generalizability of findings across open-and closed-source models, we sought to use both LLAMA and GPT-4. However, as the results provided by LLAMA were qualitatively inferior (e.g., not complying with the instruction, generating unrelated text, not providing an interpretable answer), we report analyses only for GPT-4 (Version November 2023). In a prompt, we described the die-rolling task, including the bonus payoffs for principals, to GPT-4. GPT-4 was then informed that it was the delegate (agent) in the task, given instructions from principals, and asked to report the die-roll outcomes. The exact wording of the prompt is contained in SI, Prompt Texts.</p><p>Human Agents. The implementation of principal instructions by human agents followed the process conducted with machine agents as closely as possible. Again, the instructions included those intended for human agents and those intended for machine agents (which we describe as 'forked'). Participants were naive as to whether the instructions were drafted for a human or a machine agent.</p><p>It began with a general description of the die-rolling task. The next screen informed participants that people in a previous experiment (i.e., principals) had written instructions for agents to report a sequence of 10 die rolls on their behalf. Participants learned that they would be the agents and report on 10 die rolls for four different instruction texts and that their reports would determine the principal's bonus.</p><p>Participants were incentivized to match the principals' intentions: For one randomly selected instruction text, they could earn a bonus of 5 cents for each die roll that matched the principal's expectations, giving a maximum bonus of 50 cents. Participants were presented with one instruction text at a time, followed by the sequence of 10 die rolls, each of which they reported on the principal's behalf.</p><p>Exit Questions At the end of the study, we assessed demographics (age, gender, Ethics Statement. We confirm that all studies complied with all relevant ethical guidelines. The Ethics Committee of the Max Planck Institute for Human Development approved all studies. Informed Consent was obtained from all human research participants in these studies.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Rage against the machine: Automation in the moral domain</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gogoll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Uhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Behavioral and Experimental Economics</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="97" to="103" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Machine behaviour</title>
		<author>
			<persName><forename type="first">I</forename><surname>Rahwan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">568</biblScope>
			<biblScope unit="page" from="477" to="486" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><surname>Bbc</surname></persName>
		</author>
		<ptr target="https://www.bbc.com/news/technology-59939536.2022" />
		<title level="m">Tesla adds chill and assertive self-driving modes</title>
		<imprint>
			<date>Janauary 1</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Does algorithmic trading improve liquidity</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hendershott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Menkveld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Finance</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="1" to="33" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Delegation decisions in finance</title>
		<author>
			<persName><forename type="first">F</forename><surname>Holzmeister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Holmén</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kirchler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stefan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wengström</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="4828" to="4844" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mitigating bias in algorithmic hiring: Evaluating claims and practices in</title>
		<author>
			<persName><forename type="first">M</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Barocas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Hildebrandt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Castillo</surname></persName>
		</editor>
		<meeting>the 2020 Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="469" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Stranger than science fiction: The rise of AI interrogation in the dawn of autonomous robots and the need for an additional protocol to the UN convention against torture</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mcallister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Minnesota Law Review</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="2527" to="2573" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The case for and against autonomous weapon systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dawes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="613" to="614" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Navigating the jagged technological frontier: Field experimental evidence of the effects of AI on knowledge worker productivity and quality</title>
		<author>
			<persName><forename type="first">F</forename><surname>Dell'acqua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Harvard Business School Working Paper Series</title>
		<imprint>
			<biblScope unit="page" from="24" to="37" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">4 models for using AI to make decisions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schrage</surname></persName>
		</author>
		<ptr target="https://hbr.org/2017/01/4-models-for-using-ai-to-make-decisions" />
	</analytic>
	<monogr>
		<title level="j">Harvard Business Review</title>
		<imprint>
			<date type="published" when="2017-01-27">2017. January 27</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Beating irrationality: Does delegating to IT alleviate the sunk cost effect?</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Herrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">O</forename><surname>Kundisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Rahman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="831" to="850" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Delegation to artificial agents fosters prosocial behaviors in the collective risk dilemma</title>
		<author>
			<persName><forename type="first">E</forename><surname>Fernández Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">8492</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Human cooperation when acting through autonomous machines</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Marsella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gratch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="3482" to="3487" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Artificial intelligence, algorithmic pricing, and collusion</title>
		<author>
			<persName><forename type="first">E</forename><surname>Calvano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Calzolari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Denicolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pastorello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Economic Review</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="3267" to="3297" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Protecting consumers from collusive prices due to AI</title>
		<author>
			<persName><forename type="first">E</forename><surname>Calvano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Calzolari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Denicolò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Harrington</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pastorello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">370</biblScope>
			<biblScope unit="page" from="1040" to="1042" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bad machines corrupt good morals</title>
		<author>
			<persName><forename type="first">N</forename><surname>Köbis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Bonnefon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Rahwan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="679" to="685" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The moral psychology of artificial intelligence</title>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Bonnefon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Rahwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shariff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Psychology</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="653" to="675" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Preferences for truth-telling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Abeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nosenzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Raymond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="1115" to="1153" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The truth about lies: A meta-analysis on dishonest behavior</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gerlach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Teodorescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hertwig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="page" from="1" to="44" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Exploiting moral wiggle room: Experiments demonstrating an illusory preference for fairness</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">X</forename><surname>Kuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Economic Theory</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="67" to="80" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">People are more likely to be insincere when they are more likely to accidentally tell the truth</title>
		<author>
			<persName><forename type="first">S</forename><surname>Leblois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Bonnefon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quarterly Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="1486" to="1492" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ignorance by choice: A meta-analytic review of the underlying motives of willful ignorance and its consequences</title>
		<author>
			<persName><forename type="first">L</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Soraperra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Leib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Der Weele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shalvi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">149</biblScope>
			<biblScope unit="page" from="611" to="635" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Shifting the blame: On delegation and responsibility</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bartling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Fischbacher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Review of Economic Studies</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="67" to="87" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Religiosity predicts the delegation of decisions between moral and self-serving immoral outcomes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Forstmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Social Psychology</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page">104605</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Avoiding lying: The case of delegated deception</title>
		<author>
			<persName><forename type="first">S</forename><surname>Erat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Economic Behavior &amp; Organization</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="273" to="278" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">I lie? We lie! Why? Experimental evidence on a dishonesty shift in groups</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Kocher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schudy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Spantig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="3995" to="4008" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The Ethical Knob: Ethically-customisable automated vehicles and the law</title>
		<author>
			<persName><forename type="first">G</forename><surname>Contissa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lagioia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sartor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Law</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="365" to="378" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Norvig</surname></persName>
		</author>
		<title level="m">Artificial intelligence: A modern approach</title>
		<imprint>
			<publisher>Pearson</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Explanation in artificial intelligence: Insights from the social sciences</title>
		<author>
			<persName><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">267</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Artificial intelligence crime: An interdisciplinary analysis of foreseeable threats and solutions</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Taddeo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Floridi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science and Engineering Ethics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="89" to="120" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Public attitudes value interpretability but prioritize accuracy in Artificial Intelligence</title>
		<author>
			<persName><forename type="first">A.-M</forename><surname>Nussberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Celis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Crockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">5821</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Lies in disguise:An experimental study on cheating</title>
		<author>
			<persName><forename type="first">U</forename><surname>Fischbacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Föllmi-Heusi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the European Economic Association</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="525" to="547" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Intrinsic honesty and the prevalence of rule violations across societies</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gächter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Schulz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">531</biblScope>
			<biblScope unit="page" from="496" to="499" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cheating in the lab predicts fraud in the field: An experiment in public transportation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Galeotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Villeval</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="1081" to="1100" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Laboratory measure of cheating predicts school misconduct</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Maréchal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Economic Journal</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page" from="2743" to="2754" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Measuring honesty and explaining adulteration in naturally occurring markets</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rustagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kroell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Development Economics</title>
		<imprint>
			<biblScope unit="volume">156</biblScope>
			<biblScope unit="page">102819</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mechanisms of moral disengagement in the exercise of moral agency</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bandura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Barbaranelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">V</forename><surname>Caprara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pastorelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="364" to="374" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The dishonesty of honest people: A theory of self-concept maintenance</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ariely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Marketing Research</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="633" to="644" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Justified ethicality: Observing desired counterfactuals modifies ethical perceptions and behavior</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shalvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Handgraaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>De Dreu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Organizational Behavior and Human Decision Processes</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="181" to="190" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Rise of the machines: Delegating decisions to autonomous AI</title>
		<author>
			<persName><forename type="first">C</forename><surname>Candrian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in Human Behaviour</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="page">107308</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Passing the buck: Delegating choices to others to avoid responsibility and blame</title>
		<author>
			<persName><forename type="first">M</forename><surname>Steffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Perrmann-Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Organizational Behavior and Human Decision Processes</title>
		<imprint>
			<biblScope unit="volume">135</biblScope>
			<biblScope unit="page" from="32" to="44" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Generative AI triggers welfarereducing decisions in humans</title>
		<author>
			<persName><forename type="first">F</forename><surname>Dvorak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stumpf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fehrler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Fischbacher</surname></persName>
		</author>
		<idno>arXiv 2401.12773</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Behavioural evidence for a transparency-efficiency tradeoff in human-machine cooperation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ishowo-Oloko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="517" to="521" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Introducing the GASP scale: A new measure of guilt and shame proneness</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Panter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Insko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="947" to="966" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The logic of indirect speech</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pinker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="833" to="838" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Influencing human-AI interaction by priming beliefs about AI can increase perceived trustworthiness, empathy and effectiveness</title>
		<author>
			<persName><forename type="first">P</forename><surname>Pataranutaporn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Maes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1076" to="1086" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
