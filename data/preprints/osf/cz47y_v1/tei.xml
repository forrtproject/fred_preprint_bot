<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Transfer Learning Approach For Identifying Spoken Maghrebi Dialects</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Khaled</forename><surname>Lounnas</surname></persName>
							<email>klounnas@usthb.dz</email>
							<affiliation key="aff0">
								<orgName type="department">Computational Linguistics Dept</orgName>
								<orgName type="institution">CRSTDLA</orgName>
								<address>
									<settlement>Algiers</settlement>
									<country key="DZ">Algeria</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">LCPTS-USTHB University</orgName>
								<address>
									<settlement>Algiers</settlement>
									<country key="DZ">Algeria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mourad</forename><surname>Abbas</surname></persName>
							<email>m.abbas@crstdla.dz</email>
							<affiliation key="aff0">
								<orgName type="department">Computational Linguistics Dept</orgName>
								<orgName type="institution">CRSTDLA</orgName>
								<address>
									<settlement>Algiers</settlement>
									<country key="DZ">Algeria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mohamed</forename><surname>Lichouri</surname></persName>
							<email>m.lichouri@crstdla.dz</email>
							<affiliation key="aff0">
								<orgName type="department">Computational Linguistics Dept</orgName>
								<orgName type="institution">CRSTDLA</orgName>
								<address>
									<settlement>Algiers</settlement>
									<country key="DZ">Algeria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hocine</forename><surname>Teffahi</surname></persName>
							<email>hteffahi@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">LCPTS-USTHB University</orgName>
								<address>
									<settlement>Algiers</settlement>
									<country key="DZ">Algeria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mohamed</forename><surname>Hamidi</surname></persName>
							<email>mohamed.hamidi.5@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="department">LISAC-FSDM</orgName>
								<orgName type="institution">USMBA University</orgName>
								<address>
									<settlement>Fes</settlement>
									<country key="MA">Morocco</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hassan</forename><surname>Satori</surname></persName>
							<email>hsatori@yahoo.com</email>
							<affiliation key="aff2">
								<orgName type="department">LISAC-FSDM</orgName>
								<orgName type="institution">USMBA University</orgName>
								<address>
									<settlement>Fes</settlement>
									<country key="MA">Morocco</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Transfer Learning Approach For Identifying Spoken Maghrebi Dialects</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">938062A248045E095C1B8E995EB3E348</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-10-22T06:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Dialect Identification</term>
					<term>Resnet50</term>
					<term>Resnet101</term>
					<term>VGG16</term>
					<term>VGG19</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper investigates a transfer learning approach to solve the spoken dialects identification problem for some under-resourced dialects of the Maghrebi region, including Algerian Arabic Dialect (AAD), Algerian Berber Dialect (ABD), Moroccan Arabic Dialect (MAD), and Moroccan Berber Dialect (MBD). In our experiments, we used different Transfer learning models, namely: Residual Neural Network (Resnet50, Resnet101), and Visual Geometric Group (VGG16, VGG19) using an in-house corpus that we built for each dialect. The corpus is composed of ten digits recorded for each of the aforementioned dialects, repeated ten times by six native speakers. The results vary according to different reasons: the number of epochs, neurons, batch size, and also the datasets combinations used in training and test phases. The best score found is 90.4% by the VGG19 model. Overall, the results show the robustness of our system based on the VGG16 model with an average identification rate of 62.7%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Spoken Dialect Identification (SDI) is a machine learning topic that refers to the assignment of a spoken utterance to its proper dialect among a set of dialects. On one hand, dealing with SDI problems becomes more challenging, especially when dialects are very close to each other and similar in different contexts, including phonological, morphological, lexical, and syntactic levels. On the other hand, building resources for under-resourced dialects are getting more attention in this field and more challenging too. For this purpose, we focus, first, on building a resource for the Maghrebi dialects, then using this resource to carry out a set of experiments based on transfer learning approaches in order to present the best performing configuration of a neural network. The remaining of this paper is organized as follows; we present an overview of the works on speech-based dialect identification in section 2. In section 3, we describe the corpus that we prepared. In section 4, we present the proposed system as well as the experimental setup and results, and we conclude our paper in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Spoken dialects identification has attracted many types of research, some of them used classical approaches based on statistical classification, however many others tempted to benefit from deep learning techniques. Following the first paradigm and intending to find the most appropriate characteristic vector and hierarchy of classification, it has been shown in <ref type="bibr" target="#b5">[1]</ref><ref type="bibr" target="#b6">[2]</ref><ref type="bibr" target="#b7">[3]</ref> that the combination of both acoustic and prosodic information can be used to better distinguish between Arabic and Kabyl. In the same context, in <ref type="bibr" target="#b8">[4]</ref>, authors exploited the prosodic cues and observed its effectiveness across four major Arabic dialects, namely: Gulf, Iraqi, Levantine, and Egyptian, in which they prove that using such kind of descriptors to train Gaussian Mixture Model (GMM) combined with the Universal Background Model (UBM) can significantly improve the identification of these dialects of 2 minutes utterances. Following the same research focus, authors addressed in <ref type="bibr" target="#b9">[5]</ref> Arabic accent and dialect identification; They used phonetic segmentation supra-vector, which consists in building a kernel function that computes phonetic similarities to train the Support Vector Machine classifier. They achieved an Equal Error Rate (EER) of 12.9%. In <ref type="bibr" target="#b10">[6]</ref>, authors have been interested in identifying spoken Arabic dialects that belong to five regions, namely: Egyptian, Gulf, Levantine, North-African (Maghrebi), and Modern Standard Arabic (MSA). The authors reported that despite the small size of the used data, the Linear Support Vector Machine (LSVM) classifier trained with a feature vector containing textual details, outperformed the other systems getting an accuracy equal to 51.36%. Deep learning systems have also been implemented to identify spoken Arabic dialects/sub-dialects. Indeed, in <ref type="bibr" target="#b11">[7]</ref> authors proposed prosodic parameters to model some under-resourced Algerian dialects using deep learning. The authors carried out a comparative study between statistical and neural approaches. Lounnas et al. <ref type="bibr" target="#b12">[8]</ref>, collected more than 8 hours of speech data to prepare Ar-Pod corpus which contains Modern Standard Arabic and some of its dialects: Syrian, Lebanese, Egyptian, and Saudi. Using Arpod for spoken language and dialect identification, the authors showed that using Convolutional Neural Network (CNN) with specific features outperforms the classical approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Corpus Preparation</head><p>As the purpose of this paper is to implement a spoken Maghrebi dialects identification system, we built our own dataset from scratch. This resource contains four dialects from the Maghreb region namely: Algerian Arabic Dialect (AAD), Algerian Berber Dialect (ABD), Moroccan Arabic Dialect (MAD) and Moroccan Berber Dialect (MBD). The Moroccan dialects are taken from the corpus used in <ref type="bibr" target="#b14">[10]</ref>. The first step for collecting the corpus is to ask the native Maghrebi speakers (Algerian and Moroccan) to utter and record ten times the numerical digits from zero to nine according to the real-life conditions. The next step consists of re-sampling the output spoken digits to get a standardized frequency using Praat<ref type="foot" target="#foot_0">4</ref> software, given that the recording conditions vary from one speaker to another. The last step is to segment the original files into tiny fragments (with ten repetitions for every digit). This task is conducted using Audacity<ref type="foot" target="#foot_1">5</ref> . Table <ref type="table" target="#tab_0">1</ref> outlines the corpus's characteristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">System Description</head><p>Our system is based on transfer learning models, including Residual Neural Network (ResNet) and Visual Geometric Group (VGG) as illustrated in Figure <ref type="figure" target="#fig_0">1</ref>. We used the Keras's applications included in the tensorflow library <ref type="foot" target="#foot_2">6</ref> to conduct our experiments using different parameters and different combinations of the dataset. First, we extracted spectrogram images to be used as an input of our system.</p><p>We run experiments using multiple combinations of speakers to form ten different learning and test sets. Each combination presents four speakers representing 70% for training and two speakers representing 30% for test, as shown in table <ref type="table" target="#tab_1">2</ref>, where S i denotes speaker number i.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Dialect Identification using Deep Residual Network</head><p>Deep Residual Network is almost identical to the networks that are stacked over each other with convolution, pooling, activation and fully-connected layers. The identity connection between the layers is the only building of the simple network to make it a residual network <ref type="bibr" target="#b16">[12]</ref>.</p><p>In this section, we conducted a series of experiments to test different hyperparameters of the last two layers network (NN2). This network is used to retrain both pre-trained ResNet50 and ResNet101 models. In the framework of these experiments, We varied the number of neurons, epochs, and batch size, as presented in table <ref type="table" target="#tab_2">3</ref>. The results are presented in terms of F1 score in tables ?? and 5. For the Resnet50+NN2 model, the best results obtained with the first and second configurations (cf1 and cf2) are 60.3% and 39% using the eighth and third sets, respectively. Compared to cf1 and cf2 configurations, cf3 and cf4 yielded an improvement by 16% and 25%, respectively. This improvement is achieved using the second set.</p><p>Overall the dialect identification system performs better using the cf3 and cf4 configurations than the cf1 and cf2 with an average rate of 53.97% and 61.05%, respectively. There is no clear interpretation on why the system's performance is high when using some sets and low for some others. The reason is probably the different environments in which the data was recorded.</p><p>To enhance the performance of our system, we retrained the ResNet with higher number of layers i.e.101 layers. The best results achieved with cf1 and cf2 configurations are 49.56% and 63.59% using the third and last set, respectively. For cf3 and cf4 configurations, we noted a significant improvement in both cases at the second set compared with the cf2 configuration, with an enhancement of 15% and 20%, respectively. The results are reported in table <ref type="table" target="#tab_4">5</ref>. Overall, while observing the results obtained in the four configurations (cf1,cf2, cf3 and cf4), we noted that the performance of the ResNet based Dialect identification is related to the network hyper-parameters mentioned above in addition to the number of neurons in the first layer of NN2 network. In fact, increasing the number of neurons (from 128 to 256) has a positive impact on the system's  performance. Expanding the number of layers has the same effect when using the cf2 configuration. The score jumped from 39%(ResNet50+NN2) for the third set (ResNet101+NN2) to 63.59% for the tenth set. In general, the cf2 and cf3 configurations seem to perform better than cf1 and cf4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Dialect Identification using Visual Geometric Group (VGG)</head><p>The Visual Geometric Group is a convolutional neural network with a specific architecture that was proposed in <ref type="bibr" target="#b17">[13]</ref>. As shown in Figure <ref type="figure">2</ref>, its structure consists of blocks, where each block is made of 2D Convolution and Max Pooling layers. The VGGNet comes in two versions, VGG16 and VGG19, where numbers 16 and 19 reflect the number of layers. In this part, we present the experiments that we conducted using the VGG16 and VGG19 pre-trained models, for which Fig. <ref type="figure">2</ref>: Visual Geometric Group Network Structure we adopted the same configurations reported in table <ref type="table" target="#tab_2">3</ref>. The obtained results are reported in tables 6 and 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 6: Performance of VGG 16 based system</head><p>For the VGG16+NN2 model, the two best results are obtained with the fourth and third configuration using the second set, with a score of 84.95% and 79.43%, respectively. Whereas, using the 9 th set yielded to a score of 54.37% by adopting the first configuration. Furthermore, the second configuration allowed an improvement of around 7% using a different set, the fourth one. Having the performance achieved by the four aforementioned configurations, we note clearly that the lower the number of neurons, the worse the performance of the system. Note that the best result is obtained with the fourth configuration (Max: 84.95%, The VGG19+NN2 model is better than VGG16+NN2 in some cases: (4 th configuration, 2 nd set) where we achieved the best performance (90.40% versus 84.95%). This is due to the learning parameters {144M (VGG19+NN2) Versus 138M (VGG16+NN2)}. Despite the best performance achieved in such indi-Fig. <ref type="figure">6</ref>: Confusion matrix: VGG19 (Cf4, 2 nd set) Fig. <ref type="figure">7</ref>: The best scores achieved with VGG16, VGG19, ResNet50, and ResNet101 models vidual (more than 90%), the overall performance is around 45%. Results are summarized in table 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Summary and Discussion</head><p>In this section, we present the best results achieved by the appropriate configurations. The system works better using the values epoch=10 and batch size=10 (configurations three and four) than using epoch=2 and batch size=20 (configurations first and second). Confusion matrices are plotted for the pre-trained We summarize in Figure <ref type="figure">7</ref> the best performance achieved with each of the four models (average F1 score). Note that the VGG16+NN2 model outperforms ResNet50+NN2, ResNet101+NN2 and VGG19+NN2. Nevertheless, ResNet models are advantageous in the sense they use a smaller number of "learning parameters" as shown in table <ref type="table" target="#tab_6">9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we applied the "Transfer Learning" approach on the Automatic Identification of Spoken Dialects in the Maghreb region, namely Algerian and Moroccan dialects. using spectrogram-based features, we retrained multiple pretrained models. A series of experiments were carried out by varying a modelspecific parameter referring to the number of layers (50 and 101 for ResNet and 16 and 19 for VGG, respectively) in addition to the number of iterations (epoch) and batch size. We also varied the number of neurons of a two layers network (NN2) which was used to retrain the four pre-trained models. The findings show that despite the small amount of the used dataset, VGG outperforms the other models. Nevertheless, the performance achieved with VGG is in cost of computation time and memory space. As a result, the ResNet pretrained models seem to be a reasonable alternative to the VGG pre-trained models to overcome this problem. In perspective, one of the next steps to be done, is to enrich our corpus by adding other dialects, increasing the corpus size and using different models such as AlexNet <ref type="bibr" target="#b18">[14]</ref>, GoogleNet <ref type="bibr" target="#b19">[15]</ref> and DenseNet <ref type="bibr" target="#b20">[16]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Transfer Learning based Approach for Spoken Digits Dialects Identification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 :Fig. 5 :</head><label>45</label><figDesc>Fig. 4: Confusion matrix: ResNet101 (Cf4, 2 nd set)</figDesc><graphic coords="9,203.93,115.83,207.51,202.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The corpus' characteristics.</figDesc><table><row><cell>Sampling rate</cell><cell>16 Khz</cell></row><row><cell>Number of bits</cell><cell>16 bits</cell></row><row><cell>Number of Channels</cell><cell>1, Mono</cell></row><row><cell>Audio data file format</cell><cell>.wav</cell></row><row><cell># speakers</cell><cell>24</cell></row><row><cell># speakers per dialect</cell><cell>6</cell></row><row><cell># dialect</cell><cell>4</cell></row><row><cell># tokens per speaker</cell><cell>100</cell></row><row><cell cols="2"># speakers according to gender 12 males and 12 females</cell></row><row><cell>Total number of tokens</cell><cell>2400</cell></row><row><cell>Number of digits</cell><cell>10 digits (ABD)</cell></row><row><cell></cell><cell>10 digits (MBD)</cell></row><row><cell></cell><cell>10 digits (AAD)</cell></row><row><cell></cell><cell>10 digits (MAD)</cell></row><row><cell cols="2">Number of repetitions per word 10</cell></row><row><cell>Condition of noise</cell><cell>normal life</cell></row><row><cell>Preemphased</cell><cell>1 -0.97z -1</cell></row><row><cell>Window type Hamming</cell><cell>25.6 ms</cell></row><row><cell>Frames overlap</cell><cell>10 ms</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Multiple combinations of speakers datasets</figDesc><table><row><cell cols="3">set Training Speakers' set Test Speakers' set</cell></row><row><cell>01</cell><cell>S1,S2,S3,S4</cell><cell>S5,S6</cell></row><row><cell>02</cell><cell>S6,S5,S1,S2</cell><cell>S3,S4</cell></row><row><cell>03</cell><cell>S4,S3,S6,S5</cell><cell>S1,S2</cell></row><row><cell>04</cell><cell>S4,S3,S6,S2</cell><cell>S1,S5</cell></row><row><cell>05</cell><cell>S4,S2,S1,S5</cell><cell>S3,S6</cell></row><row><cell>06</cell><cell>S1,S2,S3,S5</cell><cell>S4,S6</cell></row><row><cell>07</cell><cell>S1,S3,S4,S6</cell><cell>S2,S5</cell></row><row><cell>08</cell><cell>S2,S4,S5,S6</cell><cell>S1,S3</cell></row><row><cell>09</cell><cell>S2,S3,S5,S6</cell><cell>S1,S4</cell></row><row><cell>10</cell><cell>S1,S3,S5,S6</cell><cell>S2,S4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Configurations parameters (Number of epochs, neurons, Batch Size, in the first layer of the NN2 network)</figDesc><table><row><cell cols="4">Configuration # epochs Batch size # Neurons</cell></row><row><cell>Cf1</cell><cell>2</cell><cell>20</cell><cell>128</cell></row><row><cell>Cf2</cell><cell>2</cell><cell>20</cell><cell>256</cell></row><row><cell>Cf3</cell><cell>10</cell><cell>10</cell><cell>128</cell></row><row><cell>Cf4</cell><cell>10</cell><cell>10</cell><cell>256</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Performance of ResNet50 based system</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Configurations</cell><cell></cell></row><row><cell>Set</cell><cell>Cf1</cell><cell>Cf2</cell><cell>Cf3</cell><cell>Cf4</cell></row><row><cell>01</cell><cell cols="4">40,52 35,22 60,06 50,77</cell></row><row><cell>02</cell><cell cols="4">41,00 34,00 75,78 85,22</cell></row><row><cell>03</cell><cell cols="4">38,00 39,00 50,29 61,97</cell></row><row><cell>04</cell><cell cols="4">48,00 28,00 50,29 63,13</cell></row><row><cell>05</cell><cell cols="4">26,36 33,61 54,18 64,24</cell></row><row><cell>06</cell><cell cols="4">43,74 27,71 60,46 60,20</cell></row><row><cell>07</cell><cell cols="4">38,82 31,64 36,86 49,01</cell></row><row><cell>08</cell><cell cols="4">60,30 34,33 33,09 61,75</cell></row><row><cell>09</cell><cell cols="4">40,93 34,15 58,17 52,60</cell></row><row><cell>10</cell><cell cols="4">27,45 30,38 60,50 61,56</cell></row><row><cell cols="5">Average 40,51 32,80 53,97 61,05</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Performance of ResNet101 based system</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Configurations</cell><cell></cell></row><row><cell>Set</cell><cell>Cf1</cell><cell>Cf2</cell><cell>Cf3</cell><cell>Cf4</cell></row><row><cell>01</cell><cell cols="4">33.59 35.32 54.99 56.58</cell></row><row><cell>02</cell><cell cols="4">33.95 51.40 77.54 83.83</cell></row><row><cell>03</cell><cell cols="4">49.56 48.05 62.42 52.37</cell></row><row><cell>04</cell><cell cols="4">33.30 50.94 61.04 60.15</cell></row><row><cell>05</cell><cell cols="4">33.66 33.99 61.73 58.45</cell></row><row><cell>06</cell><cell cols="4">23.90 33.27 56.27 59.38</cell></row><row><cell>07</cell><cell cols="4">41.11 32.97 60.84 34.35</cell></row><row><cell>08</cell><cell cols="4">39.76 37.50 39.59 62.03</cell></row><row><cell>09</cell><cell cols="4">25.05 58.26 60.87 56.98</cell></row><row><cell>10</cell><cell cols="4">39.73 63.59 38.50 58.52</cell></row><row><cell cols="5">Average 35,36 44,53 57,38 58,26</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 8 :</head><label>8</label><figDesc>Spoken digits differences in AAD, ABD, MAD, and MBD. Most of the four Maghrebi dialects are well recognized, though the closeness of lexicon between these dialects where they differ in one or two digits only, as indicated in table 8.</figDesc><table><row><cell>Dialect</cell><cell>AAD</cell><cell>ABD</cell><cell>MAD</cell><cell>MBD</cell></row><row><cell>'0'</cell><cell>SIFER</cell><cell>OULECH</cell><cell>SIFER</cell><cell>ILEM</cell></row><row><cell>'1'</cell><cell>WAHED</cell><cell>YEWAN</cell><cell>WAHED</cell><cell>YEN</cell></row><row><cell>'2'</cell><cell>ZOUJ</cell><cell>SIN</cell><cell>JOJ</cell><cell>SIN</cell></row><row><cell>'3'</cell><cell cols="4">TLATHA THYATHA THLALATA KRAD</cell></row><row><cell>'4'</cell><cell>REBAA</cell><cell>REBAA</cell><cell>RABAA</cell><cell>KUZ</cell></row><row><cell>'5'</cell><cell cols="4">KHEMSA KHEMSA KHAMSA SMUS</cell></row><row><cell>'6'</cell><cell>SETTA</cell><cell>SETSA</cell><cell>STTA</cell><cell>SEDISS</cell></row><row><cell>'7'</cell><cell>SEBAA</cell><cell>SEBAA</cell><cell>SBAA</cell><cell>SA</cell></row><row><cell cols="5">'8' THEMANYA THMANIA THMANYA TAM</cell></row><row><cell>'9'</cell><cell>TESAA</cell><cell cols="2">TESSAA TSAAOD</cell><cell>TZA</cell></row><row><cell cols="2">models in Figures 3, 4, 5, 6.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 9 :</head><label>9</label><figDesc>Number of parameters for each model</figDesc><table><row><cell>Model</cell><cell># parameters</cell></row><row><cell>ResNet50+NN2</cell><cell>26M</cell></row><row><cell>ResNet101+NN2</cell><cell>44.5M</cell></row><row><cell>VGG16+NN2</cell><cell>138M</cell></row><row><cell>VGG19+NN2</cell><cell>144M</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0"><p>http://www.fon.hum.uva.nl/praat/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1"><p>https://www.audacityteam.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2"><p>https://www.tensorflow.org/api docs/python/tf/keras/applications/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Configurations Set Cf1 Cf2 Cf3 Cf4 01 48</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page">39</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Average</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">47</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">%) whereas the best average performance was obtained with the third configuration (Max: 79.43%, Avg: 62.70%). However, the VGG16+NN2 with the third configuration seems to be more appropriate and more stable in comparison to the three other configurations. Table 7: Performance of VGG 19 based system Configurations Set Cf1 Cf2 Cf3</title>
		<idno>Cf4 01 27.15 10.30 50.90 38.54</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Average</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">47</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">3: Confusion matrix: ResNet50 (Cf4, 2 nd set) References</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">automatic language identification for berber and arabic languages using prosodic features</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lounnas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Demri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Falek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Teffahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on Electrical Sciences and Technologies in Maghreb</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-10">2018, October</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A Language Identification System Based on Voxforge Speech Corpus</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lounnas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abbas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Teffahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lichouri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Advanced Machine Learning Technologies and Applications</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019-03">2019. March</date>
			<biblScope unit="page" from="529" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">CLIASR: A Combined Automatic Speech Recognition and Language Identification System</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lounnas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Satori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Teffahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abbas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lichouri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 1st International Conference on Innovative Research in Applied Science, Engineering and Technology (IRASET)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-04">2020. April</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Using prosody and phonotactics in arabic dialect identification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Biadsy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hirschberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dialect and accent recognition using phonetic-segmentation supervectors</title>
		<author>
			<persName><forename type="first">F</forename><surname>Biadsy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hirschberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twelfth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Qcri@ dsl 2016: Spoken arabic dialect identification using textual features</title>
		<author>
			<persName><forename type="first">M</forename><surname>Eldesouki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sajjad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Darwish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Workshop on NLP for Similar Languages, Varieties and Dialects</title>
		<meeting>the Third Workshop on NLP for Similar Languages, Varieties and Dialects</meeting>
		<imprint>
			<date type="published" when="2016-12">2016, December</date>
			<biblScope unit="page" from="221" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spoken arabic algerian dialect identification</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C S</forename><surname>Bougrine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abdelali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 2nd International Conference on Natural Language and Speech Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-04">2018. April</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Building a Speech Corpus based on Arabic Podcasts for Language and Dialect Identification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lounnas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abbas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lichouri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Natural Language and Speech Processing</title>
		<meeting>the 3rd International Conference on Natural Language and Speech Processing</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="54" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Système de Reconnaissance Automatique de l&apos;arabe basé sur CMUSphinx</title>
		<author>
			<persName><forename type="first">H</forename><surname>Satori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Harti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chenfour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Corpus</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Investigation Amazigh speech recognition using CMU tools</title>
		<author>
			<persName><forename type="first">H</forename><surname>Satori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Elhaoussi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Speech Technology</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="235" to="243" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">librosa: Audio and music signal analysis in python</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mcvicar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th python in science conference</title>
		<meeting>the 14th python in science conference</meeting>
		<imprint>
			<date type="published" when="2015-07">2015, July</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Detailed Guide to Understand and Implement ResNets</title>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Sachan</surname></persName>
		</author>
		<ptr target="https://cv-tricks.com/keras/understand-implement-resnets/" />
		<imprint>
			<biblScope unit="page" from="2020" to="2032" />
		</imprint>
	</monogr>
	<note>Published in 2019-09-17</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for largescale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="84" to="90" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
