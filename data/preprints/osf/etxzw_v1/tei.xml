<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Is this real? Susceptibility to deepfakes in machines and humans</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Didem</forename><surname>Pehlivanoglu</surname></persName>
							<email>dpehlivanoglu@ufl.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">University of Florida</orgName>
								<address>
									<addrLine>945 Center Dr</addrLine>
									<postCode>32603</postCode>
									<settlement>Gainesville</settlement>
									<region>FL</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Florida Institute for National Security</orgName>
								<orgName type="institution">University of Florida</orgName>
								<address>
									<addrLine>601 Gale Lemerand Dr</addrLine>
									<postCode>32611</postCode>
									<settlement>Gainesville</settlement>
									<region>FL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mengdi</forename><surname>Zhu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Florida Institute for National Security</orgName>
								<orgName type="institution">University of Florida</orgName>
								<address>
									<addrLine>601 Gale Lemerand Dr</addrLine>
									<postCode>32611</postCode>
									<settlement>Gainesville</settlement>
									<region>FL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Florida</orgName>
								<address>
									<addrLine>968 Center Dr</addrLine>
									<postCode>32603</postCode>
									<settlement>Gainesville</settlement>
									<region>FL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jialong</forename><surname>Zhen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">University of Florida</orgName>
								<address>
									<addrLine>945 Center Dr</addrLine>
									<postCode>32603</postCode>
									<settlement>Gainesville</settlement>
									<region>FL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aude</forename><forename type="middle">A</forename><surname>Gagnon-Roberge</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">University of Florida</orgName>
								<address>
									<addrLine>945 Center Dr</addrLine>
									<postCode>32603</postCode>
									<settlement>Gainesville</settlement>
									<region>FL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rebecca</forename><forename type="middle">K</forename><surname>Kern</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">University of Florida</orgName>
								<address>
									<addrLine>945 Center Dr</addrLine>
									<postCode>32603</postCode>
									<settlement>Gainesville</settlement>
									<region>FL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Damon</forename><surname>Woodard</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Florida Institute for National Security</orgName>
								<orgName type="institution">University of Florida</orgName>
								<address>
									<addrLine>601 Gale Lemerand Dr</addrLine>
									<postCode>32611</postCode>
									<settlement>Gainesville</settlement>
									<region>FL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Florida</orgName>
								<address>
									<addrLine>968 Center Dr</addrLine>
									<postCode>32603</postCode>
									<settlement>Gainesville</settlement>
									<region>FL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Brian</forename><forename type="middle">S</forename><surname>Cahill</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">University of Florida</orgName>
								<address>
									<addrLine>945 Center Dr</addrLine>
									<postCode>32603</postCode>
									<settlement>Gainesville</settlement>
									<region>FL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Natalie</forename><forename type="middle">C</forename><surname>Ebner</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">University of Florida</orgName>
								<address>
									<addrLine>945 Center Dr</addrLine>
									<postCode>32603</postCode>
									<settlement>Gainesville</settlement>
									<region>FL</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Florida Institute for National Security</orgName>
								<orgName type="institution">University of Florida</orgName>
								<address>
									<addrLine>601 Gale Lemerand Dr</addrLine>
									<postCode>32611</postCode>
									<settlement>Gainesville</settlement>
									<region>FL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Center for Cognitive Aging and Memory</orgName>
								<orgName type="institution" key="instit1">McKnight Brain Institute</orgName>
								<orgName type="institution" key="instit2">University of Florida</orgName>
								<address>
									<addrLine>1149 Newell Dr</addrLine>
									<postCode>32610</postCode>
									<settlement>Gainesville</settlement>
									<region>FL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">University of Florida</orgName>
								<address>
									<addrLine>945 Center Dr.</addrLine>
									<postCode>32611.</postCode>
									<settlement>Gainesville</settlement>
									<region>FL</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Is this real? Susceptibility to deepfakes in machines and humans</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1B30A5A5A483726AA53BCBE9B7357C36</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-22T20:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deepfakes</term>
					<term>Deception</term>
					<term>Artificial Intelligence</term>
					<term>Machine Learning</term>
					<term>Confidence</term>
					<term>Analytical Thinking</term>
					<term>Truth Bias</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deepfakes are synthetic media created by deep-generative methods to fake a person's audiovisual representation. Growing sophistication of deepfake technology poses significant challenges for both machine learning (ML) algorithms and humans. Here we used real and deepfake static face images (Study 1) and dynamic videos (Study 2) (i) to investigate sources of misclassification errors in machines, (ii) to identify psychological mechanisms underlying detection performance in humans, and (iii) to compare humans and machines in their classification decision accuracy and confidence. Study 1 found that machines achieved excellent performance in classifying real and deepfake images, with good accuracy in feature classification. Humans, in contrast, experienced challenges in distinguishing between real and deepfake images. Their classification accuracy was at chance level, and this underperformance relative to machines was accompanied by a truth bias and low confidence for the detection of deepfake images. Using video stimuli, Study 2 found that performance of machines was near chance level, with poor feature classification. Further, the machines showed greater truth bias and low reduced decision confidence relative to humans who outperformed machines in the detection of video deepfakes. Finally, the study revealed that higher analytical thinking, lower positive affect, and greater internet skills were associated with better video deepfake detection in humans. Combined, findings across these two studies advance understanding of factors contributing to deepfake detection in both machines and humans and could inform intervention toward tackling the growing threat from deepfakes by identifying areas of particular benefit from human-AI collaboration to optimize deepfake detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Significant advances in artificial intelligence (AI) have resulted in the use of sophisticated technology for producing manipulated media and has led to the emergence of deepfakes (for a review, see <ref type="bibr" target="#b57">Nightingale &amp; Wade, 2022)</ref>. Deepfakes are algorithmic manipulations which are typically synthesized using generative adversarial networks, a type of machine learning that works by pitting two neural networks (a generator and a discriminator) against one another in an iterative back-and-forth process, to create any type of fake image, video, or audio <ref type="bibr" target="#b80">(Tong et al., 2020)</ref>. While deepfake technology presents numerous creative and entertainment possibilities for education, arts, and science, it also raises significant ethical, legal, and societal concerns, ranging from advertising to national security. In particular, deepfakes constitute a novel deception tactic to fake someone's entire audio-visual representation for spreading false information <ref type="bibr" target="#b67">(Seow et al., 2022;</ref><ref type="bibr" target="#b77">Ternovski et al., 2022;</ref><ref type="bibr" target="#b94">Zhang, 2022)</ref> and are effectively harnessed in social engineering <ref type="bibr" target="#b85">(Vaccari &amp; Chadwick, 2020;</ref><ref type="bibr" target="#b91">Westerlund, 2019)</ref>.</p><p>The growing presence of deepfakes to manipulate public opinion on social and news platforms <ref type="bibr" target="#b15">(Fallis, 2021;</ref><ref type="bibr" target="#b85">Vaccari &amp; Chadwick, 2020)</ref> has led to computer science and psychology research into investigating machine and human performance for detecting deepfakes <ref type="bibr" target="#b20">(Groh et al., 2022;</ref><ref type="bibr" target="#b37">Karras et al., 2020;</ref><ref type="bibr" target="#b53">Montserrat et al., 2020;</ref><ref type="bibr" target="#b56">Nightingale &amp; Farid, 2022)</ref>. Importantly, these lines of research have been almost exclusively focused on deepfake detection performance, with factors that contribute to the ability to detect deepfakes in machines and humans still poorly understood. Further, this past research has mostly been conducted in isolation in each discipline and a direct comparison between machine and human performance has not been conducted yet. To fill these research gaps, here, we (i) identified sources of misclassification errors in machines, (ii) determined psychological mechanisms deepfake detection among humans, and (iii) directly contrasted machine and human performance in real and deepfake discernment; across two studies one employing static face images (Study 1) and one employing dynamic videos (Study 2). Next, we review work leading to these central research aims.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Machine Detection of Deepfakes</head><p>Deepfake images are typically synthesized using generative models that apply a replication process for the generation of new samples based on training data. Upon effective training, these models allow image synthesis, style transfer, and face-swapping. Among the multitude of existing generative models, generative adversarial networks (GANs; <ref type="bibr" target="#b18">Goodfellow et al., 2014)</ref> are highly regarded for their ability to produce high-quality, high-resolution images. Detecting image deepfakes is a rapidly evolving field within computer vision and digital forensics. Generative models, however, often leave distinctive "fingerprints'' on deepfakes, leading to the development of machine learning (ML) algorithms designed to identify and categorize such model-specific artifacts <ref type="bibr" target="#b12">(Durall et al., 2019;</ref><ref type="bibr" target="#b93">Yu et al., 2021)</ref>. In particular, Convolutional Neural Networks (CNNs) are trained on extensive datasets of real and fake images to learn distinguishing features. CNNs have demonstrated considerable promise in detecting deepfakes, with notable examples including ShallowNet <ref type="bibr" target="#b75">(Tariq et al., 2018)</ref>, ResNet-50 <ref type="bibr" target="#b89">(Wang et al., 2020)</ref>, Inception <ref type="bibr" target="#b73">(Suratkar et al., 2020)</ref>, Xception <ref type="bibr" target="#b64">(Rössler et al., 2019;</ref><ref type="bibr" target="#b73">Suratkar et al., 2020)</ref>, and MobileNet <ref type="bibr" target="#b73">(Suratkar et al., 2020)</ref>. CNN-based ML algorithms for image deepfake detection have reported accuracy rates between 83% and 100% <ref type="bibr" target="#b0">(Afchar et al., 2018;</ref><ref type="bibr" target="#b79">Tolosana et al., 2020)</ref>.</p><p>Video deepfakes often involve swapping of faces from source to target videos. Variational Autoencoders (VAEs; <ref type="bibr" target="#b38">Kingma &amp; Welling, 2014)</ref> are one of the methods frequently employed for face-swapping due to their proficiency in learning disentanglement within the data <ref type="bibr" target="#b41">(Korshunova et al., 2017;</ref><ref type="bibr" target="#b55">Natsume et al., 2018)</ref>. As for detecting video deepfakes, there are two primary methods. The first method involves CNNs, which follow a similar process as for image deepfake detection: each frame of an individual video from a training set of videos is processed by CNNs for final classification. CNNs have achieved video deepfake detection accuracies between 80% and 90% <ref type="bibr" target="#b0">(Afchar et al., 2018;</ref><ref type="bibr" target="#b65">Sambhu &amp; Canavan, 2020)</ref>. Notably, the Xception network <ref type="bibr" target="#b64">(Rössler et al., 2019)</ref> excels in learning complex data representations (i.e., face detection) and offers advantages such as efficiency and reduced susceptibility to overfitting. The second method leverages biometric and biological features for detection, as current deepfake technologies still struggle to accurately replicate such features. In particular, this approach includes analyzing facial features <ref type="bibr" target="#b47">(Matern et al., 2019)</ref>, eye blink patterns <ref type="bibr" target="#b32">(Jung et al., 2020;</ref><ref type="bibr" target="#b44">Li et al., 2018)</ref>, eye movements <ref type="bibr" target="#b24">(Gupta et al., 2020)</ref>, head poses <ref type="bibr" target="#b92">(Yang et al., 2018)</ref>, consistency of facial geometry <ref type="bibr" target="#b84">(Tursman et al., 2020)</ref>, facial expressions <ref type="bibr" target="#b1">(Agarwal et al., 2020)</ref>, lip syncing <ref type="bibr" target="#b40">(Korshunov &amp; Marcel, 2021)</ref>, and biological signals from facial regions (e.g., photoplethysmography, head motion-based ballistocardiogram; <ref type="bibr" target="#b8">Ciftci et al., 2020)</ref>. Performance in distinguishing real videos from deepfakes using these feature-based ML algorithms ranges from 50% to 96%, but requires high-quality, high-resolution data for biometric feature extraction.</p><p>Growing evidence suggests variation in performance of different ML algorithms in detecting deepfake images and videos, but what contributes to this variation is not yet well understood. Going beyond existing work, here we determine factors that underlie machines' ability to spot real and deepfake material (Aim 1). In particular, currently limited is understanding of how and why particular pieces of content get misclassified. Misclassifications are typically caused by biases within detection systems, such as training data bias, algorithmic bias, cultural and contextual bias, and/or performance discrepancies. We adopted a fine-grained approach by employing feature space analysis <ref type="bibr" target="#b42">(Kulis, 2013)</ref> which allowed us to examine and compare two different ML algorithms regarding their classification accuracy of static face images (Study 1) and dynamic videos (Study 2). Analyzing feature detection performance will allow identification of misclassification sources by different ML algorithms, which will be crucial for improving the feature selection capacity of these algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Human Deepfake Detection</head><p>According to a recent national survey, a significant portion of Americans (63%) reported that made-up or altered images or videos create ''a great deal of confusion'' about the basic facts of current issues and events <ref type="bibr" target="#b19">(Gottfried, 2019)</ref>. Some studies employing static face images to determine discrimination ability between deepfake and real faces found that human performance was not better than chance <ref type="bibr" target="#b51">(Miller et al., 2023;</ref><ref type="bibr" target="#b56">Nightingale &amp; Farid, 2022;</ref><ref type="bibr" target="#b63">Rossi et al., 2023;</ref><ref type="bibr" target="#b68">Shen et al., 2021)</ref>, with deepfake faces typically perceived as more real <ref type="bibr" target="#b51">(Miller et al., 2023;</ref><ref type="bibr" target="#b68">Shen et al., 2021;</ref><ref type="bibr" target="#b83">Tucciarelli et al., 2022)</ref> and trustworthy <ref type="bibr" target="#b56">(Nightingale &amp; Farid, 2022</ref>) than real faces. Other studies demonstrated that while above chance, mean deepfake face detection accuracy in humans ranged from 60 to 65% only <ref type="bibr" target="#b4">(Bray et al., 2023;</ref><ref type="bibr" target="#b30">Hulzebosch et al., 2020)</ref>; and this performance was accompanied by participants' overconfidence in their ability to detect deepfake faces <ref type="bibr" target="#b4">(Bray et al., 2023;</ref><ref type="bibr" target="#b51">Miller et al., 2023)</ref>. In brief, these findings reveal that humans are frequently fooled by deepfake faces and cannot reliably distinguish them from real faces.</p><p>There are also studies on human detection for dynamic video deepfakes, which vary widely regarding detection accuracy (from 58% to 89%; <ref type="bibr" target="#b20">(Groh et al., 2022;</ref><ref type="bibr" target="#b31">Josephs et al., 2024;</ref><ref type="bibr" target="#b39">Köbis et al., 2021;</ref><ref type="bibr" target="#b54">Nas &amp; de Kleijn, 2024;</ref><ref type="bibr" target="#b70">Somoray &amp; Miller, 2023)</ref>. For example, using deepfake videos pre-categorized based on subjective ratings of difficulty, one study found that participants detected "easy" video deepfakes with 71% accuracy whereas performance dropped to 25% for "very difficult" videos, indicating below chance-level performance for humans for high-quality deepfakes <ref type="bibr" target="#b40">(Korshunov &amp; Marcel, 2021)</ref>. Another study found that overall detection accuracy for deepfake videos in humans was above chance (58%; <ref type="bibr" target="#b39">Köbis et al., 2021)</ref>. Thus, taken together, while human detection for static image deepfakes appears to be at chance, detection of at least some video deepfakes can be relatively good.</p><p>Currently less understood are individual differences in the ability to detect deepfakes. The limited research on this topic suggests that individuals with prior experience with histological images (i.e., microscopic images of tissues) were better able to distinguish between artificially generated and genuine histological samples than individuals without prior experience <ref type="bibr" target="#b28">(Hartung et al., 2024)</ref>. Also, somewhat counterintuitive, belief in conspiracy theories was positively correlated with deepfake video detection <ref type="bibr" target="#b54">(Nas &amp; de Kleijn, 2024)</ref>. While informative, these studies have, however, failed to consider a larger spectrum of psychological factors that may contribute to deepfake detection ability. To fill this research gap, the current paper investigated interindividual differences in cognitive and socioemotional processing as well as experience and comfortability with the internet in their influence on deepfake detection accuracy in humans (Aim 2) for static face images (Study 1) and dynamic videos (Study 2). Investigation of these factors will inform the psychological mechanisms in deepfake detection, which can guide the development of interventions to reduce deception via deepfakes.</p><p>In particular, we assessed the following psychological variables: Cognitive Processing. According to Dual-Process Theory <ref type="bibr" target="#b9">(De Neys, 2012;</ref><ref type="bibr" target="#b34">Kahneman, 2011;</ref><ref type="bibr" target="#b71">Stanovich, 2009)</ref>, individuals engage in two main routes of information processing: a quick, intuition-based route and a slow, deliberate route. While the intuition-based route leads to faster decision making, it is associated with low analytical reasoning and relies on cognitive heuristics. The slower route, in contrast, is associated with high analytical thinking and allows deliberation of information, often leading to less error-prone decision making. Indeed, research has consistently shown that individuals higher in analytical thinking were better at detecting misleading information (e.g., fake news; <ref type="bibr" target="#b59">Pehlivanoglu et al., 2021</ref><ref type="bibr" target="#b58">Pehlivanoglu et al., , 2022;;</ref><ref type="bibr" target="#b61">Pennycook &amp; Rand, 2021)</ref>. Further, need for cognition, which refers to the tendency to enjoy and engage in effortful and systematic thinking <ref type="bibr" target="#b7">(Cacioppo et al., 1984)</ref>, has been positively correlated with information seeking <ref type="bibr" target="#b33">(Juric, 2017)</ref> and decision-making competence <ref type="bibr" target="#b10">(Ding et al., 2020)</ref>. Individuals with higher need for cognition are more willing to invest cognitive effort to solve demanding tasks and employ an elaborated information processing style instead of a heuristic processing style <ref type="bibr" target="#b6">(Cacioppo et al., 1996;</ref><ref type="bibr" target="#b87">Verplanken et al., 1992)</ref>. Also, individuals with higher need for cognition demonstrated greater skepticism toward information shared on social media <ref type="bibr" target="#b82">(Tsfati &amp; Cappella, 2003;</ref><ref type="bibr" target="#b88">Vraga &amp; Tully, 2021)</ref>. Based on this literature, we measured analytical thinking and need for cognition in their contributions to deepfake detection. Socioemotional Processing. Affect has been shown to impact deception detection, though the direction of this effect is somewhat unclear <ref type="bibr" target="#b13">(Ebner et al., 2020;</ref><ref type="bibr" target="#b16">Forgas &amp; East, 2008</ref>; see also <ref type="bibr" target="#b14">Ebner et al., 2023</ref> for a summary). For example, individuals with greater feelings of sadness and distress (dysphoric mood) compared to non-dysphoric individuals were better at lie detection <ref type="bibr" target="#b43">(Lane &amp; DePaulo, 1999)</ref>. Similarly, negative affect increased, while positive affect decreased, skepticism, deception detection, and ambiguity <ref type="bibr" target="#b48">(Matovic et al., 2014;</ref><ref type="bibr">but see LaTour &amp; LaTour, 2009)</ref>. Further, heightened emotionality (in the form of both increased positive and negative affect) was associated with worse fake news detection <ref type="bibr" target="#b46">(Martel et al., 2020)</ref>. Additionally, interoceptive awareness, which reflects the ability to read one's inner bodily state <ref type="bibr" target="#b3">(Bogaerts et al., 2022;</ref><ref type="bibr" target="#b50">Mehling et al., 2009)</ref>, has been associated with deception detection <ref type="bibr" target="#b23">(Gunderson &amp; ten Brinke, 2022;</ref><ref type="bibr" target="#b29">Heemskerk et al., 2024;</ref><ref type="bibr" target="#b76">ten Brinke et al., 2019)</ref>. Based on these findings, we measured affect and interoceptive awareness in their contributions to deepfake detection.</p><p>Experience and Comfortability with the Internet. Having relevant skills and experience with the internet and online materials may influence the ability to detect deception. For instance, time spent on social media was negatively correlated with believing fake news <ref type="bibr" target="#b26">(Halpern et al., 2019)</ref> and positively with detection of deepfake videos <ref type="bibr" target="#b54">(Nas &amp; de Kleijn, 2024)</ref>. Somewhat counterintuitive, however, one study found that self-reported IT affinity was not related to deepfake detection in both individuals with an IT background and non-professionals <ref type="bibr" target="#b74">(Sütterlin et al., 2022)</ref>. These previous studies, however, have not considered a broader set of internet and technology related skills that may contribute to the detection of deepfakes. Thus, going beyond existing literature, we measured self-reported digital literacy (i.e., internet skills) and power usage (i.e., mastery of technology use) in their contributions to deepfake detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.">Human vs. Machine Performance in Deepfake Detection</head><p>Currently, research from computer science on deepfake detection in machines is not well integrated with research on deepfake detection in humans. One exception is <ref type="bibr" target="#b20">Groh et al. (2022)</ref> who directly compared human and machine performance and found comparable accuracy. Their study, however, only examined deepfake videos (not static images) and some videos involved familiar actors (i.e., political figures), which may have affected detection performance. Here we employed both static face images (Study 1) and dynamic videos (Study 2) of unfamiliar individuals and directly compared performance of the leading ML algorithm (i.e, the better performing ML algorithm among the two compared under Aim 1) with human performance (Aim 3). Findings from our work will provide insight into whether machines outperform humans in classification accuracy and confidence and increase knowledge about the nature of decision biases between machines and humans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Study 1 2.1. Participants</head><p>Study 1 recruited 2,418 undergraduates through the Department of Psychology's SONA system. Of those, 680 participants were removed from analysis for the following reasons: 131 did not continue past consenting, 513 failed attention checks (e.g., Please answer 2 to this question), 27 had survey completion times 3 standard deviations greater than the group average, and 9 were older than 39 years. The final analysis sample comprised 1,738 participants (Age range: 18-39 years, M = 19.46, SD = 2.26; 81% female).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Measures 2.2.1. Image Rating Task.</head><p>Participants were asked to rate the veracity of each of 200 faces on a scale from 1 (Fake) to 10 (Real). Each face was presented on the screen for at least 3 s to ensure sufficient processing time; beyond the 3s window, the task was self-paced.</p><p>Real images were 300 human face images randomly selected from the Flickr-Faces-HQ (FFHQ) dataset <ref type="bibr" target="#b36">(Karras et al., 2019)</ref>, which contains 70,000 high-quality images (1024x1024 resolution) that vary in age, gender, ethnicity, and image background. The final set of real face images were crawled from Flickr, then aligned and cropped to ensure they contained only one face. For deepfake images, we randomly selected an additional set of 300 face images from the FFHQ dataset. These faces were then synthesized with a pre-trained styleGAN2 network released by NVIDIA <ref type="bibr" target="#b37">(Karras et al., 2020)</ref>. The styleGAN2 algorithm enables intuitive, scalespecific control of the synthesizing process via an automatically learned, unsupervised separation of high-level attributions (e.g., pose and identity when trained on human faces) and stochastic variation in the synthesized images (e.g., freckles, hair, accessories). For equal numbers of real vs. deepfake images by gender, deepfake images were first classified as male vs. female using a deep-learning based classification algorithm, then cross-validated via manual selection to exclude images with interference and/or warping artifacts.</p><p>We created three sets of 200 stimuli each by randomly selecting 100 real and 100 deepfake images from the larger pool we created, with face gender balanced within each set and image type (real vs. deepfake). Final image sets are achieved in the OSF repository (<ref type="url" target="https://osf.io/qhm3y/?view_only=bdc41a53bf7a4367bde6951372d9c932">https://osf.io/qhm3y/?view_only=bdc41a53bf7a4367bde6951372d9c932</ref>). A third of participants, respectively, were assigned to view only one of the three sets to assure counterbalancing, with face presentation order within each set randomized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Cognitive Reflection Test (CRT).</head><p>Analytical thinking was assessed via the CRT <ref type="bibr" target="#b17">(Frederick, 2005)</ref>, which contains both numerical and logical propositions that have an intuitive and an analytical answer. For example, "A bat and a ball cost $1.10 in total. The bat costs $1.00 more than the ball. How much does the ball cost? _____ cents." Individuals who rely on intuition respond with the intuitive answer (10 cents), whereas individuals who rely on effortful thinking respond with the analytical answer (5 cents).</p><p>Validity of the CRT is affected by familiarity with the items <ref type="bibr" target="#b25">(Haigh, 2016)</ref> as well as number of scale items <ref type="bibr" target="#b81">(Toplak et al., 2014)</ref>. Here, we used a 7-item version, which consisted of three items from <ref type="bibr" target="#b69">Shenhav et al. (2012)</ref> and four items from <ref type="bibr" target="#b78">Thomson and Oppenheimer (2016)</ref>. An example item was: "The ages of Mark and Adam add up to 28 years total. Mark is 20 years older than Adam. How many years old is Adam?". Participants with high analytical thinking overcome the impulse to give the intuitive (incorrect) answer of 8 years old and instead give the analytical (correct) answer of 4 years old. We calculated sum scores across the 7 items, with higher CRT scores reflecting greater analytical thinking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3.">Need for Cognition (NFC).</head><p>The NFC scale is a self-report questionnaire <ref type="bibr" target="#b5">(Cacioppo &amp; Petty, 1982)</ref> assessing how much an individual engages in and enjoys thinking or cognitively demanding tasks. We used a short version of the scale containing 18 items <ref type="bibr" target="#b7">(Cacioppo et al., 1984)</ref>. Each item consists of a statement, e.g. "I would prefer complex to simpler problems", and participants score themselves on a scale from 1 (Extremely uncharacteristic) to 5 (Extremely characteristic). We calculated the mean across all 18 items, with higher NFC scores reflecting greater need for cognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4.">Positive and Negative Affect (PANAS).</head><p>We administered the 20-item PANAS <ref type="bibr" target="#b90">(Watson et al., 1988)</ref>, an affect assessment that contains 20 items. We also included six additional items to capture hedonic balance <ref type="bibr" target="#b62">(Röcke et al., 2009)</ref>. For each item, participants were asked "To what extent do you feel [emotion adjective] right now?" and used a scale from 1 (Very slightly or not at all) to 5 (Extremely) to evaluate each adjective (e.g., excited, happy, afraid, alert; 13 positive and 13 negative adjectives). We calculated the mean across positive adjectives and negative adjectives, with higher scores reflecting more positive affect and more negative affect, respectively.</p><p>2.2.5. Multidimensional Assessment of Interoceptive Awareness Version 2 (MAIA-2). MAIA-2 <ref type="bibr" target="#b49">(Mehling et al., 2018</ref>) is a 37-item self-report questionnaire that measures awareness of bodily sensations. The scale is composed of 8 subscales measuring different aspects of interoception (i.e., noticing, not-distracting, not-worrying, attention regulation, emotional awareness, self-regulation, and body listening). Each subscale has Likert-type items, with response options ranging from 0 (Never) to 5 (Always). Sample items are, "When I am tense I notice where the tension is located in my body.", "I can notice an unpleasant body sensation without worrying about it.", and "I notice that my body feels different after a peaceful experience." We calculated the mean across all 37 items, with higher MAIA-2 scores reflecting greater interoceptive awareness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.6.">Digital Literacy Scale (DLS).</head><p>The DLS is a 21-item inventory that evaluates an individual's familiarity with computer and internet elements <ref type="bibr" target="#b27">(Hargittai, 2009)</ref>. The current study used a modified version, which updated the internet terms <ref type="bibr" target="#b22">(Guess &amp; Munger, 2023)</ref>. For each item participants reported their level of understanding of various computer and internet elements (e.g., phishing, tagging, selfie) on a scale ranging from 1 (No understanding) to 5 (Full understanding). We calculated the mean across all 21 items, with higher scores reflecting greater understanding of digital media.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.7.">Power User Scale (PUS).</head><p>The PUS <ref type="bibr" target="#b72">(Sundar &amp; Marathe, 2010</ref>) is a 12-item inventory that assesses mastery of information technology based on prior experience, expertise, and self-efficacy. The scale consists of two sub-scales, each with 6 items that were evaluated on a scale from -4 (Strongly disagree) to +4 (Strongly agree). One subscale captures low (e.g., "I think most technological gadgets are complicated to use") vs. high (e.g., "I often find myself using many technological devices simultaneously") frequency of technology use. The other subscale captures low (e.g., "I prefer to ask friends how to use any new technological gadget instead of trying to figure it out myself") vs. high (e.g., "I would feel lost without information technology") comfortability with technology use. We calculated the mean across both subscales (all 12 items), with higher PUS scores reflecting greater power usage (i.e., expertise, experience, and efficacy in technology use).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Procedure</head><p>All procedures and measures were approved by the University of Florida Institutional Review Board (IRB# 202102022). Participants completed this study remotely through Qualtrics (<ref type="url" target="https://www.qualtrics.com/">https://www.qualtrics.com/</ref>). Prior to study enrollment, all participants consented electronically to participate. Participants then completed the Image Rating Task, CRT, NFC, MAIA-2, PANAS, DLS, PUS, and a brief demographic questionnaire, in this order. The study took approximately 100 mins and participants were reimbursed with SONA credits upon completion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Analyses and Results</head><p>All de-identified datasets and analysis scripts used in Study 1 are available on the OSF repository (<ref type="url" target="https://osf.io/qhm3y/?view_only=bdc41a53bf7a4367bde6951372d9c932">https://osf.io/qhm3y/?view_only=bdc41a53bf7a4367bde6951372d9c932</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1.">Machine Performance</head><p>To measure how well a machine could detect image deepfakes, we chose two different ML algorithms, previously shown to be efficient in identifying specific artifacts existing in GANgenerated deepfake images <ref type="bibr" target="#b52">(Mirsky &amp; Lee, 2021;</ref><ref type="bibr" target="#b86">Verdoliva, 2020)</ref>. The first approach applied a CNN (using a pre-trained ResNet-50 network, <ref type="bibr" target="#b89">Wang et al., 2020)</ref>; the second involved Frequency Domain Analysis (FDA; using a pre-trained Support Vector Machine; <ref type="bibr" target="#b12">(Durall et al., 2019)</ref> to extract frequency characteristics from images to distinguish real and deepfake images. These ML algorithms generated predicted labels as outcome variable. Predicted labels can be either 0 = Deepfake face or 1 = Real face and reflect the classification of each face type. The CNN approach yielded 97% accuracy in distinguishing real and deepfake images, whereas the FDA approach resulted in 79% accuracy.</p><p>To understand the source of misclassification underlying image detection performance of these two ML algorithms, we used feature visualization. This approach compared feature detection of the two ML algorithms in their classification accuracy of static face images. Specifically, this technique involved reducing the dimensionality of features for 2D visualization of features using t-distributed stochastic neighbor embedding (t-SNE; van der <ref type="bibr">Maaten et al., 2008)</ref>. After applying feature analysis for both ML approaches, the decision boundary of the CNN approach (Figure <ref type="figure" target="#fig_0">1A</ref>) was more clearly defined than that of the FDA approach (Figure <ref type="figure" target="#fig_0">1B</ref>), resulting in higher classification accuracy of features in image deepfakes in the CNN than the FDA approach. One reason for the difference in performance could be that the CNN approach used persistent and distinctive features from both real and fake images, while the FDA approach only leveraged features within the deepfake images, causing features in real images to get misclassified. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2.">Human Performance</head><p>To calculate how well humans could discriminate between deepfake and real face images, we computed the d-prime (d' in standard Signal Detection Theory; <ref type="bibr" target="#b45">(Macmillan &amp; Creelman, 1991)</ref>. In this score, deepfake images were considered as "signal present". Given our 1 = Fake to 10 = Real response scale, ratings from 1 to 5 reflected 'hits' whereas ratings from 6 to 10 reflected 'misses' for deepfake images. For real face images, ratings from 1 to 5 reflected 'correct rejections' whereas ratings from 6 to 10 reflected 'false alarms'. Using the formula d' = z(H)-z(F), d' was calculated for each participant across all images, with higher d' indicating a participant's greater ability to discriminate between deepfake and real face images. The average d' score was close to 0 (Figure <ref type="figure" target="#fig_1">2</ref>; M = -0.13, SD = 0.45), reflecting diminished ability to discriminate between deepfake and real face images in humans.</p><p>To examine the extent to which individual differences in psychological variables predicted discrimination ability we conducted a multiple linear regression model on d' as outcome variable. This model included the main effects of analytic thinking (CRT; continuous), need for cognition (NFC; continuous), positive and negative affect (PANAS, continuous), interoceptive awareness (MAIA-2; continuous), digital literacy (DLS; continuous), and power usage (PUS; continuous), with participant gender and age added as covariates. None of the individual difference measures predicted discrimination ability between real and deepfake images (all Fs &lt; 1.65, ps &gt; .09). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.3.">Machine vs. Human Performance</head><p>As noted, the CNN algorithm outperformed the FDA algorithm, yielding a 97% accuracy in the deepfake detection task. In comparison, humans performed poorer than the CNN, with overall accuracy in classifying face images remaining at 49%. Next, for both CNN algorithm and humans, we calculated the True Positive Rate (TPR), reflective of the prediction of real when a face image was real; and the True Negative Rate (TNR), reflective of the prediction of fake when a face image was a deepfake. Separate calculation of TPR and TNR for CNN and humans allowed us to determine whether accuracy was comparable for classifications of real and deepfake images or whether it was biased towards one or the other image type (e.g., whether accuracy was high for real images but low for deepfake images). The TPR for the CNN was 97% and the TNR was 97% (Figure <ref type="figure">3A</ref>), indicating that this algorithm was equally successful in classifying real and deepfake images, with no detection bias towards one or the other image type. In contrast, the TPR for humans was 69%, whereas the TNR was only 29% (Figure <ref type="figure">3B</ref>). This low TNR in humans was driven by a greater tendency to misclassify deepfake images as "real" (as reflected by a false positive rate (FPR) of 71% in Figure <ref type="figure">3B</ref>), suggesting a truth bias in humans (i.e., a tendency to misclassify deepfakes as "real").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 3. Confusion matrix indicating accuracy for (A) Convolutional Neural Network (CNN) and</head><p>(B) humans. TNR = True Negative Rate (i.e., correctly classifying deepfake as "deepfake"); FNR = False Negative Rate (misclassifying real as "deepfake"); TPR = True Positive Rate (i.e., correctly classifying real as "real"); FPR = False Positive Rate (i.e., misclassifying deepfake as "real"). Colors in each confusion matrix serve as heatmap, with darker colors indicating higher values.</p><p>We also computed decision confidence scores in image classifications for the machine algorithm and humans. For the CNN algorithm, the confidence score was calculated using a probability score derived from the classification prediction. This score represents the confidence level of whether a given face classified as real was real on a scale from 0 (Not confident at all) to 1 (Very confident). As shown in Figure <ref type="figure" target="#fig_2">4A</ref>, approximately 45% of confidence scores for the CNN fell within 0 and 0.1, indicating high confidence in classifying deepfake images as deepfake. Correspondingly, approximately 45% of the confidence scores were within 0.9 and 1.0, indicating also high confidence in classifying real face images as real. That is, the machine was confident about its prediction. For decision confidence in humans, we re-coded image ratings from 1 (Not confident at all) to 10 (Very confident), with higher scores reflecting greater confidence. Humans showed greater confidence in classification of real (M = 6.86, SD = 1.68) than deepfake (M = 4.01, SD = 1.8) images (t(1737)= 35.12, p &lt; .001, Cohen's d = 1.64; Figure <ref type="figure" target="#fig_2">4B</ref>), consistent with their higher accuracy for real than deepfake images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Summary and Brief Discussion of Study 1</head><p>In Study 1 we found that the CNN approach outperformed the FDA approach in detecting static real and deepfake images, as reflected in greater feature classification accuracy. In comparison, the ability of humans to discriminate between deepfake and real face images was rather poor (at chance level); and individual differences in cognitive and socioemotional processes as well as in the level of internet skills did not explain variability in detection performance for real or deepfake images. Furthermore, our direct comparison between machine and human performance revealed that the CNN algorithm outperformed humans by showing excellent prediction accuracy, with no decision bias and high classification confidence for both deepfake and real images. Humans' dramatic underperformance relative to the machine was coupled with a truth bias and low confidence for the classification of deepfake images.</p><p>In this first study, we addressed machine and human performance for deepfake images. Fast-developing AI advances, however, more and more confront us with dynamic deepfakes such as in videos in our everyday lives. Importantly, cues available in static vs. dynamic deepfakes differ in that videos often contain audio and visual input simultaneously, integrate behavioral (e.g., facial expressions, gestures) and non-behavioral (e.g., lighting, skin texture) features, and typically are more ecologically valid than static images. Thus, going beyond Study 1, Study 2 examined deepfake detection performance by employing videos (i) to investigate sources of misclassification errors in machines, (ii) to identify psychological mechanisms underlying detection performance in humans, and (iii) to compare humans and machines in their classification decision accuracy and confidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Study 2 3.1. Participants</head><p>Study 2 recruited 2,183 undergraduates from the Department of Psychology's SONA. Of those, 743 were removed from analysis for the following reasons: 142 did not continue the study after consenting, 560 failed attention checks, 28 had survey completion times 3 standard deviations greater than the group average, and 13 were older than 39 years. The final sample comprised 1,440 participants (Age range: 18-39 years, M = 19.94, SD = 2.27; 83% female).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Measures</head><p>3.2.1. Video Rating Task. Participants viewed 70 short videos of an individual discussing a topic (e.g., book presentations, video games, daily activities). At the end of each video clip, participants were asked to rate the veracity of the face shown in each of the videos (i.e., "This Face was ____.") on a scale from 100% (Fake) to 100% (Real), with 50% reflecting just as likely real or fake. The presentation order of the videos was randomized, and beyond the 10-s video presentation, the task was self-paced.</p><p>Videos were obtained from the Deepfake Detection Challenge (DFDC) dataset <ref type="bibr" target="#b11">(Dolhansky et al., 2020)</ref>, which is a large-scale dataset containing over 100,000 videos, both real and deepfake, covering a variety of scenarios and individuals of diverse gender, age, and racial/ethnic backgrounds. Real videos were created by recording video clips of volunteers. Deepfake videos were generated by applying various manipulation techniques (e.g., face swapping, altering facial expressions, or audio swapping) to real videos.</p><p>We randomly selected an initial pool of 336 real and 322 deepfake videos. Each video was assessed on multiple criteria to ensure that (i) it had a landscape orientation, good sound quality and lighting, and had no text or written information embedded, (ii) there was only one person shown in each video, (iii) of unique identity (i.e., the same person was not shown in any of the other videos), (iv) the person was speaking by looking towards the camera without location change (e.g., walking), and (v) videos did not involve audio synthesis or replacement (i.e., audio swapped video) by checking lip syncing. In particular, the final set comprised 35 real and 35 fake videos, all trimmed to 10 s to ensure equal duration. To assure that detection performance was not confounded by audio in the videos, the same set of videos were muted to create non-audio video versions. For counterbalancing, approximately half of the participants (N=684) viewed the videos with audio and approximately the other half (N=756) viewed the muted versions, with videos presented in random order in each of these two stimuli lists. All videos are achieved under the OSF repository (<ref type="url" target="https://osf.io/qhm3y/?view_only=bdc41a53bf7a4367bde6951372d9c932">https://osf.io/qhm3y/?view_only=bdc41a53bf7a4367bde6951372d9c932</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Procedure</head><p>Study procedures were approved by the University of Florida Institutional Review Board (IRB# 202102022). Identical to Study 1, participants consented electronically and completed the study remotely through Qualtrics (<ref type="url" target="https://www.qualtrics.com/">https://www.qualtrics.com/</ref>). Participants first completed the Video Rating Task, followed by the CRT, NFC, MAIA-2, PANAS, DLS, PUS, and a brief demographic questionnaire, in this order. The study took approximately 100 mins and participants were reimbursed with SONA credits upon completion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Analyses and Results</head><p>All de-identified datasets and analysis scripts used in Study 2 are available on the OSF repository (<ref type="url" target="https://osf.io/qhm3y/?view_only=bdc41a53bf7a4367bde6951372d9c932">https://osf.io/qhm3y/?view_only=bdc41a53bf7a4367bde6951372d9c932</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1.">Machine Performance</head><p>To measure how well machines detect video deepfakes, we tested two different ML algorithms, known to be efficient in finding inconsistencies and manipulations on video frames. The first was FaceForensics (using a pre-trained Xception network, <ref type="bibr" target="#b64">Rössler et al., 2019)</ref>, and the second involved Recurrent Neural Network (RNN) (using the pre-trained network; <ref type="bibr" target="#b21">(Güera &amp; Delp, 2018)</ref> to identify inconsistencies of latent features from continuous frames. As in Study 1, predicted labels generated by the ML algorithms were either 0 = Deepfake or 1 = Real face and reflected the classification for each face type within the videos. FaceForensics yielded 51% accuracy in distinguishing real and deepfake videos, whereas RNN resulted in 39% accuracy.</p><p>To identify the source of misclassification, we applied the same feature visualization described in Study 1. Features were intertwined for both FaceForensics (Figure <ref type="figure" target="#fig_3">5A</ref>) and RNN (Figure <ref type="figure" target="#fig_3">5B</ref>), making it difficult to establish a clear decision boundary and resulting in poor classification of features for both ML algorithms. Of note, RNN misclassified features even more than FaceForensics, possibly because the RNN algorithm uses the entire frame to extract features whereas the FaceForensics model uses a frontal face frame only. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2.">Human Performance</head><p>As in Study 1, we computed discrimination ability, with higher d' indicating greater discriminate ability between deepfake and real videos. Deepfake videos were considered as 'signal present'. Given our 100% = Fake to 100% = Real response scale, ratings from 100% to 60% Fake reflected 'hits' whereas ratings from 100% to 60% Real reflected misses for deepfake videos. For real videos, ratings from 100% Real to 60% Real reflected 'correct rejections' whereas ratings from 100% Fake to 60% Fake reflected 'false alarms'. Responses of 50% were omitted from the analysis as they reflected that participants were undecided whether a video was real or fake (8% of the trials). The ability to discriminate between deepfake and real videos was relatively good in humans (Figure <ref type="figure" target="#fig_4">6A</ref>; M = 0.86, SD = 0.65, Range= -0.91 to 3.50).</p><p>We also again conducted a multiple linear regression on d' for formal analysis by applying the identical analytical approach as described in Study 1. Greater ability to discern between deepfake and real videos was associated with higher analytical thinking (reflected by a significant main effect of CRT: F = 2.12, p = .03, Cohen's f 2 = 0.01; Figure <ref type="figure" target="#fig_4">6B</ref>), lower positive affect (reflected by a significant main effect for PA: F = 2.35, p = .02, Cohen's f 2 = 0.01; Figure <ref type="figure" target="#fig_4">6C</ref>), and greater power usage (reflected by a significant main effect for PUS: F = 2.86, p &lt; .01, Cohen's f 2 = 0.02; Figure <ref type="figure" target="#fig_4">6D</ref>). None of the other individual difference variables predicted discrimination ability (all Fs &lt; 1.29, ps &gt; .19). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3.">Machine vs. Human Performance</head><p>As noted, FaceForensics outperformed RNN, yielding a 51% accuracy in deepfake video detection. Of note, however, and different from the results for static images, humans outperformed FaceForensics, with 64% overall accuracy in classifying videos. We again calculated the TPR and TNR. The TPR for FaceForensics was 63%, whereas the TNR was only 23% (Figure <ref type="figure" target="#fig_5">7A</ref>). This low TNR for FaceForensics was driven by a greater tendency to misclassify deepfake videos as "real" (i.e., truth bias), as reflected by an FPR of 77% (Figure <ref type="figure" target="#fig_5">7A</ref>). The TPR for humans was 76%, whereas the TNR was 51% (Figure <ref type="figure" target="#fig_5">7B</ref>). While not as pronounced as in the machine, humans also were more likely to misclassify deepfake videos as "real", as reflected by an FPR of 49% (Figure <ref type="figure" target="#fig_5">7B</ref>). True Negative Rate (i.e., correctly classifying deepfake as "deepfake"); FNR = False Negative Rate (misclassifying real as "deepfake"); TPR = True Positive Rate (i.e., correctly classifying real as "real"); FPR = False Positive Rate (i.e., misclassifying deepfake as "real"). Colors in each confusion matrix serve as heatmap, with darker colors indicating higher values.</p><p>Parallel to Study 1, we again computed decision confidence scores in video classifications for both the machine and humans. For FaceForensics (Figure <ref type="figure" target="#fig_6">8A</ref>), only 27% of the confidence scores fell within the range of 0.9 to 1.0, while the remaining 63% were fairly evenly distributed across other bins. This pattern suggests that FaceForensics was uncertain about its decisions. For humans (Figure <ref type="figure" target="#fig_6">8B</ref>), confidence in the classification of real videos (M = 7.13, SD = 1.26) was greater than confidence in the classification of deepfake videos (M = 5.6, SD = 1.5; t(1,439)= 23.98, p &lt; .001, Cohen's d = 1.11), consistent with higher accuracy for real than deepfake videos. video classification confidence scores in humans. Real videos are shown in gray, deepfake videos in black. Higher confidence scores reflect greater confidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Summary and Brief Discussion of Study 2</head><p>In Study 2 we found that while the FaceForensics algorithm performed slightly better than the RRN algorithm at detecting real and deepfake videos, accuracy for FaceForensics was rather low (at chance level). We observed that accuracy of features in videos were intertwined and poor for both ML algorithms. In contrast, discrimination ability between deepfake and real videos in humans was rather good. Further, higher analytical thinking, less positive affect, and greater internet skills were associated with better discernment ability. Directly comparing machine and human performance furthermore showed that the overall classification accuracy of FaceForensics was lower than human performance, with this underperformance by the machine characterized by a truth bias and low classification confidence. A decision bias was less evident in humans, with decision confidence patterns in alignment with detection accuracy for real and deepfake videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">General Discussion</head><p>With rapidly increasing sophistication of AI, deepfakes represent a serious challenge in today's society. They are being used to deceive and disseminate disinformation, undermining trust in media and institutions. While research on deepfake detection performance in both machines and humans is growing, the processes underlying deepfake detection ability are not well understood; and direct comparisons of machine vs. human performance are still rare. Here we identified sources of misclassification errors in machines, psychological mechanisms of discrimination ability in humans, and directly contrasted machine and human performance regarding classification accuracy and confidence for real and deepfake images (Study 1) and videos (Study 2). Across two studies, our data yielded three key findings: First, ML algorithms were overall more accurate and better at classifying features in real and deepfake images than videos. Second, humans outperformed the ML algorithm in deepfake video detection, but they experienced challenges in deepfake image detection, where they displayed a truth bias and low confidence. In turn, the ML algorithm's quite weak performance with videos was marked by a truth bias and low classification confidence. Third, we found that higher analytical thinking, lower positive affect, and more internet skills improved discernment of deepfake from real videos in humans. Collectively, these findings suggest that ML excels at detecting deepfake images (static input) but humans have an advantage in video detection (dynamic input). This differential pattern of findings highlights the need for collaboration between humans and AI to optimize the detection of deepfakes. Theoretical and practical implications of our novel findings are discussed next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Machine Excels in Image Deepfake Detection but Experiences Challenges with Videos</head><p>CNN-and FDA ML algorithms achieved high detection accuracies of 97.17% and 79.33%, respectively, for image deepfakes (Study 1). Follow-up feature space analysis further demonstrated that CNN was more effective at discernment than FDA because features learned by this algorithm clustered more tightly for both deepfake and real mages. The scattering of features in FDA compared to the more tightly clustered features learned by CNN may have stemmed from their different approaches to feature selection. That is, CNN is trained to identify distinctive features from both deepfake and real images. FDA, in contrast, produces more dispersed features from real images because it relies on the Fourier transform to detect unique patterns specific to deepfake (but not real) images. This, in turn, leads to less accurate real image classification.</p><p>In contrast, video deepfake detection by FaceForensics and RNN algorithms (Study 2) achieved low accuracies of 51.43% and 38.57%, respectively. Follow-up feature space analysis for these algorithms revealed that both methods struggled with the identification of distinctive features that effectively differentiated between deepfake and real videos. Visualization of this performance pattern revealed that features from real and deepfake videos were entangled and indistinguishable, leading to classification error. It is also possible that the FaceForensics and RNN models were trained on data that did not match the characteristics of the test videos used in our study. As a result, the algorithms may have extracted irrelevant or "wrong" features, leading to incorrect classifications. This alternative explanation is somewhat supported by our finding of relatively better deepfake detection by FaceForensics than RNN as deepfake videos in the DFDC dataset are created using face-swapping, which is the technique that FaceForensics specializes in. The RNN algorithm, in contrast, analyzes the entire frame for feature extraction <ref type="bibr" target="#b21">(Güera &amp; Delp, 2018)</ref>, which may have resulted in higher misclassifications for deepfake videos taken from the DFDC dataset, in which manipulations are present on the face regions only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Machines Outperform in Image Detection but Humans Lead in Video Deepfake Detection</head><p>Our comparison of machine and human performance for classifying face images (Study 1) found that the CNN algorithm outperformed human detection ability, showing excellent accuracy without decision bias and maintaining high confidence. In contrast, humans performed significantly worse with overall accuracy at chance level. Humans also showed a truth bias in their decision criteria, reflected in a greater tendency to misclassify deepfake images as 'real' and this bias was accompanied by low deepfake image classification confidence. These findings suggest that sophisticated ML models can generate deepfake face images that are indistinguishable from real face images in the eye of human perceivers.</p><p>Regarding classification of deepfake videos (Study 2), however, we found that humans outperformed the FaceForensics algorithm in overall accuracy, and the machine also showed a truth bias and low classification confidence for deepfake videos. In contrast, humans' greater accuracy and lower truth bias when classifying deepfake videos than images, and also relative to machine performance, suggests that rich perceptual cues in dynamic stimuli (e.g., motion and temporal consistency) facilitate deepfake detection in humans whereas ML algorithms were less able to benefit from such cues.</p><p>This differential pattern of findings for images vs. videos point out that humans and machines employ rather different processes in deepfake detection by highlighting the potential for a human-AI collaboration to optimize performance (e.g., by supporting human decision making with machine predictions and by feeding human-perceived cues/features to improve the algorithms' predictions, <ref type="bibr" target="#b20">Groh et al., 2022;</ref><ref type="bibr" target="#b51">Miller et al., 2023)</ref>. Along these lines, future research could use two-alternative forced choice designs, where a deepfake face image is presented alongside its corresponding real face while recording eye movements of human perceivers. This approach would allow researchers to identify erroneous visual viewing patterns and attention to non-diagnostic cues in humans, and this could then be followed up with AI-guided eye tracking training, in which diagnostic features deemed as critical by ML for deepfake detection are targeted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Higher Analytical Thinking, Lower Positive Affect, and Greater Internet Skills Predict Better Video Deepfake Detection</head><p>Results from Study 2 fill an important research gap in that they support that higher analytical thinking, less positive affect, and greater internet skills subserved better discernment of deepfake from real videos. Analytical thinking is a reliable predictor of news detection <ref type="bibr" target="#b2">(Bago et al., 2020;</ref><ref type="bibr" target="#b59">Pehlivanoglu et al., 2021</ref><ref type="bibr" target="#b58">Pehlivanoglu et al., , 2022;;</ref><ref type="bibr" target="#b60">Pennycook &amp; Rand, 2019)</ref>. Extending this work to deepfakes for the first time, findings from our study suggest that elaborative, relative to shallow, processing may foster attention to spot digital manipulations (e.g., face swapping) in video deepfakes. We also found that less positive affect was related with greater discernment between deepfake and real videos. This finding is in line with evidence that less positive affect enhances deliberative decision making <ref type="bibr" target="#b66">(Schwarz &amp; Clore, 2003)</ref> and deception detection <ref type="bibr" target="#b48">(Matovic et al., 2014;</ref><ref type="bibr">but see Ebner et al., 2020)</ref>. Finally, higher power usage was related to better ability to distinguish between deepfake and real videos. There is previous evidence showing that time spent on social media was linked to less susceptibility to fake news <ref type="bibr" target="#b26">(Halpern et al., 2019)</ref> and deepfake videos <ref type="bibr" target="#b54">(Nas &amp; de Kleijn, 2024)</ref>. Our measure on power usage went beyond previous research, which focused solely on time spent on social media, by considering and demonstrating the role of prior experience, expertise, and self-efficacy on video deepfake detection for the first time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Conclusions</head><p>Across two studies, employing deepfake images and videos, and directly comparing humans and machines, here we found that ML algorithms have superior accuracy and better feature classification for real and deepfake images than videos. The machines' underperformance for videos is accompanied by a truth bias and low classification confidence for deepfake videos. We also found that humans outperformed ML algorithms in deepfake video detection; while they perform only at chance level on deepfake images, for which they display a truth bias and low decision confidence. We also provide first evidence that higher analytical thinking, less positive affect, and greater internet skills are conducive to better discernment between real and deepfake discernment videos. These findings combined importantly advance understanding of the processes involved in deepfake detection, delineating conditions under which human-machine collaboration may be particularly fruitful.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. 2-D visualization of latent features from (A) Convolutional Neural Network (CNN) and (B) Frequency Domain Analysis (FDA). Real images are shown in gray, deepfake images in black.</figDesc><graphic coords="9,68.25,445.51,508.20,196.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Distribution of discrimination ability (d' score) in humans. The dashed line reflects guessing (i.e., no discrimination between deepfake and real face images).</figDesc><graphic coords="10,170.20,353.24,265.95,205.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. (A) Histogram of probability scores derived from Convolutional Neural Network (CNN) regarding image classification confidence. Scores from 0 to 0.1 reflect higher confidence for classification of deepfake images; scores from 0.9 to 1 reflect higher confidence for classification of real images. (B) Distribution of image classification confidence scores in humans. Real images are shown in gray; deepfake images in black. Higher confidence scores reflect greater classification confidence.</figDesc><graphic coords="12,72.00,60.75,468.00,174.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. 2-D visualization of latent features from (A) FaceForensics and (B) Recurrent Neural Network (RNN). Real videos are shown in gray, deepfake videos in black.</figDesc><graphic coords="14,67.50,278.45,468.00,180.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. (A) Distribution of discrimination ability (d' score) in humans. The dashed line reflects guessing. Greater ability to discern between deepfake and real videos was associated with (B) higher analytical thinking, indexed by Cognitive Reflection Test (CRT) scores, (C) lower positive affect, indexed by Positive and Negative Affect Scale (PANAS) scores, and (D) greater power usage, indexed by Power User Scale (PUS) scores. Each dot represents a participant. Shaded areas around the regression lines reflect the 95% confidence interval.</figDesc><graphic coords="15,72.00,137.93,477.05,270.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Confusion matrix indicating accuracy for (A) FaceForensics and (B) humans. TNR =True Negative Rate (i.e., correctly classifying deepfake as "deepfake"); FNR = False Negative Rate (misclassifying real as "deepfake"); TPR = True Positive Rate (i.e., correctly classifying real as "real"); FPR = False Positive Rate (i.e., misclassifying deepfake as "real"). Colors in each confusion matrix serve as heatmap, with darker colors indicating higher values.</figDesc><graphic coords="16,66.00,51.75,463.50,186.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. (A) Histogram of probability scores from FaceForensics for video classification confidence. Scores from 0 to 0.1 reflect higher confidence for classification of deepfake videos; scores from 0.9 to 1 reflect higher confidence for classification of real videos. (B) Distribution of</figDesc><graphic coords="16,75.75,471.00,468.00,174.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="11,77.25,206.22,453.50,197.00" type="bitmap" /></figure>
		</body>
		<back>

			
			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors have no conflict of interest to disclose and have complied with APA ethical standards in human subjects research. The full set of materials, de-identified data files, and analysis scripts are available on OSF repository (<ref type="url" target="https://osf.io/qhm3y/?view_only=bdc41a53bf7a4367bde6951372d9c932">https://osf.io/qhm3y/?view_only=bdc41a53bf7a4367bde6951372d9c932</ref>).</p></div>
			</div>


			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Availability</head><p>The full set of de-identified datasets, analysis scripts, and materials from Study 1 and 2 are available on OSF at <ref type="url" target="https://osf.io/qhm3y/?view_only=bdc41a53bf7a4367bde6951372d9c932">https://osf.io/qhm3y/?view_only=bdc41a53bf7a4367bde6951372d9c932</ref>.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Funding Sources</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">MesoNet: A Compact Facial Video Forgery Detection Network</title>
		<author>
			<persName><forename type="first">D</forename><surname>Afchar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nozick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Echizen</surname></persName>
		</author>
		<idno type="DOI">10.1109/WIFS.2018.8630761</idno>
		<ptr target="https://doi.org/10.1109/WIFS.2018.8630761" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Information Forensics and Security (WIFS)</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Detecting Deep-Fake Videos from Appearance and Behavior</title>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Farid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>El-Gaaly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-N</forename><surname>Lim</surname></persName>
		</author>
		<idno type="DOI">10.1109/WIFS49906.2020.9360904</idno>
		<ptr target="https://doi.org/10.1109/WIFS49906.2020.9360904" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Information Forensics and Security (WIFS)</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fake news, fast and slow: Deliberation reduces belief in false (but not true) news headlines</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Rand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pennycook</surname></persName>
		</author>
		<idno type="DOI">10.1037/xge0000729</idno>
		<ptr target="https://doi.org/10.1037/xge0000729" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">149</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1608" to="1613" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The Interoceptive Sensitivity and Attention Questionnaire: Evaluating Aspects of Self-Reported Interoception in Patients With Persistent Somatic Symptoms, Stress-Related Syndromes, and Healthy Controls</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bogaerts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Walentynowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Den Houte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Constantinou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Van Den Bergh</surname></persName>
		</author>
		<idno type="DOI">10.1097/PSY.0000000000001038</idno>
		<ptr target="https://doi.org/10.1097/PSY.0000000000001038" />
	</analytic>
	<monogr>
		<title level="j">Psychosomatic Medicine</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">251</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Testing human ability to detect &apos;deepfake&apos; images of human faces</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Bray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kleinberg</surname></persName>
		</author>
		<idno type="DOI">10.1093/cybsec/tyad011</idno>
		<ptr target="https://doi.org/10.1093/cybsec/tyad011" />
	</analytic>
	<monogr>
		<title level="j">Journal of Cybersecurity</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The need for cognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Cacioppo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Petty</surname></persName>
		</author>
		<idno type="DOI">10.1037/0022-3514.42.1.116</idno>
		<ptr target="https://doi.org/10.1037/0022-3514.42.1.116" />
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="116" to="131" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dispositional differences in cognitive motivation: The life and times of individuals varying in need for cognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Cacioppo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Petty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Feinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B G</forename><surname>Jarvis</surname></persName>
		</author>
		<idno type="DOI">10.1037/0033-2909.119.2.197</idno>
		<ptr target="https://doi.org/10.1037/0033-2909.119.2.197" />
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="197" to="253" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The Efficient Assessment of Need for Cognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Cacioppo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Petty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kao</surname></persName>
		</author>
		<idno type="DOI">10.1207/s15327752jpa4803_13</idno>
		<ptr target="https://doi.org/10.1207/s15327752jpa4803_13" />
	</analytic>
	<monogr>
		<title level="j">Journal of Personality Assessment</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="306" to="307" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">FakeCatcher: Detection of Synthetic Portrait Videos using Biological Signals</title>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">A</forename><surname>Ciftci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Demir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2020.3009287</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2020.3009287" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bias and conflict: A case for logical intuitions</title>
		<author>
			<persName><forename type="first">W</forename><surname>De Neys</surname></persName>
		</author>
		<idno type="DOI">10.1177/1745691611429354</idno>
		<ptr target="https://doi.org/10.1177/1745691611429354" />
	</analytic>
	<monogr>
		<title level="j">Perspectives on Psychological Science</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="28" to="38" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Belief Bias Effect in Older Adults: Roles of Working Memory and Need for Cognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2019.02940</idno>
		<ptr target="https://doi.org/10.3389/fpsyg.2019.02940" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Dolhansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bitton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pflaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Howes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Ferrer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07397</idno>
		<ptr target="http://arxiv.org/abs/2006.07397" />
		<title level="m">The DeepFake Detection Challenge (DFDC) Dataset</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Unmasking DeepFakes with simple Features</title>
		<author>
			<persName><forename type="first">R</forename><surname>Durall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-J</forename><surname>Pfreundt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Keuper</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1911.00686</idno>
		<ptr target="https://doi.org/10.48550/ARXIV.1911.00686" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Uncovering Susceptibility Risk to Online Deception in Aging</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Ebner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dommaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Soliman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Woodard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Spreng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Oliveira</surname></persName>
		</author>
		<idno type="DOI">10.1093/geronb/gby036</idno>
		<ptr target="https://doi.org/10.1093/geronb/gby036" />
	</analytic>
	<monogr>
		<title level="j">The Journals of Gerontology: Series B</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="522" to="533" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Financial Fraud and Deception in Aging</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Ebner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pehlivanoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shoenfelt</surname></persName>
		</author>
		<idno type="DOI">10.20900/agmr20230007</idno>
		<ptr target="https://doi.org/10.20900/agmr20230007" />
	</analytic>
	<monogr>
		<title level="j">Advances in Geriatric Medicine and Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">230007</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The Epistemic Threat of Deepfakes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Fallis</surname></persName>
		</author>
		<idno type="DOI">10.1007/s13347-020-00419-2</idno>
		<ptr target="https://doi.org/10.1007/s13347-020-00419-2" />
	</analytic>
	<monogr>
		<title level="j">Philosophy &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="623" to="643" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On being happy and gullible: Mood effects on skepticism and the detection of deception</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Forgas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>East</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jesp.2008.04.010</idno>
		<ptr target="https://doi.org/10.1016/j.jesp.2008.04.010" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Social Psychology</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1362" to="1367" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cognitive Reflection and Decision Making</title>
		<author>
			<persName><forename type="first">S</forename><surname>Frederick</surname></persName>
		</author>
		<idno type="DOI">10.1257/089533005775196732</idno>
		<ptr target="https://doi.org/10.1257/089533005775196732" />
	</analytic>
	<monogr>
		<title level="j">Journal of Economic Perspectives</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="25" to="42" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Generative Adversarial Networks</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<author>
			<persName><forename type="first">J</forename><surname>Gottfried</surname></persName>
		</author>
		<ptr target="https://www.pewresearch.org/short-reads/2019/06/14/about-three-quarters-of-americans-favor-steps-to-restrict-altered-videos-and-images/" />
	</analytic>
	<monogr>
		<title level="m">About three-quarters of Americans favor steps to restrict altered videos and images</title>
		<imprint>
			<date type="published" when="2019-06-14">2019, June 14</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deepfake detection by human crowds, machines, and machine-informed crowds</title>
		<author>
			<persName><forename type="first">M</forename><surname>Groh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Firestone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Picard</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.2110013119</idno>
		<ptr target="https://doi.org/10.1073/pnas.2110013119" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2110013119</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deepfake Video Detection Using Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Güera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Delp</surname></persName>
		</author>
		<idno type="DOI">10.1109/AVSS.2018.8639163</idno>
		<ptr target="https://doi.org/10.1109/AVSS.2018.8639163" />
	</analytic>
	<monogr>
		<title level="m">15th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Digital literacy and online political behavior</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Guess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Munger</surname></persName>
		</author>
		<idno type="DOI">10.1017/psrm.2022</idno>
		<ptr target="https://doi.org/10.1017/psrm.2022" />
	</analytic>
	<monogr>
		<title level="j">Political Science Research and Methods</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="110" to="128" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The Connection Between Deception Detection and Financial Exploitation of Older (vs. Young) Adults</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Gunderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Brinke</surname></persName>
		</author>
		<idno type="DOI">10.1177/07334648211049716</idno>
		<ptr target="https://doi.org/10.1177/07334648211049716" />
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Gerontology</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="940" to="944" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The eyes know it: FakeET-An Eyetracking Database to Understand Deepfake Perception</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Subramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 International Conference on Multimodal Interaction</title>
		<meeting>the 2020 International Conference on Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Has the standard Cognitive Reflection Test become a victim of its own success</title>
		<author>
			<persName><forename type="first">M</forename><surname>Haigh</surname></persName>
		</author>
		<idno type="DOI">10.5709/acp-0193-5</idno>
		<ptr target="https://doi.org/10.5709/acp-0193-5" />
	</analytic>
	<monogr>
		<title level="j">Advances in Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="149" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">From Belief in Conspiracy Theories to Trust in Others: Which Factors Influence Exposure</title>
		<author>
			<persName><forename type="first">D</forename><surname>Halpern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Valenzuela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Orrego Miranda</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-21902-4_16</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-21902-4_16" />
	</analytic>
	<monogr>
		<title level="j">Believing and Sharing Fake News</title>
		<imprint>
			<biblScope unit="page" from="217" to="232" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An Update on Survey Measures of Web-Oriented Digital Literacy</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hargittai</surname></persName>
		</author>
		<idno type="DOI">10.1177/0894439308318213</idno>
		<ptr target="https://doi.org/10.1177/0894439308318213" />
	</analytic>
	<monogr>
		<title level="j">Social Science Computer Review</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="130" to="137" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Experts fail to reliably detect AI-generated histological data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hartung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reuter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Kulow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fähling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Spreckelsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mrowka</surname></persName>
		</author>
		<idno type="DOI">10.1101/2024.01.23.576647</idno>
		<ptr target="https://doi.org/10.1101/2024.01.23.576647" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Interoceptive Accuracy Enhances Deception Detection in Older Adults</title>
		<author>
			<persName><forename type="first">A</forename><surname>Heemskerk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pehlivanoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hakim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Valdes Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ten Brinke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Grilli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Spreng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Ebner</surname></persName>
		</author>
		<idno type="DOI">10.1093/geronb/gbae151</idno>
		<ptr target="https://doi.org/10.1093/geronb/gbae151" />
	</analytic>
	<monogr>
		<title level="j">The Journals of Gerontology: Series B</title>
		<imprint>
			<biblScope unit="page">151</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Detecting CNN-Generated Facial Images in Real-World Scenarios</title>
		<author>
			<persName><forename type="first">N</forename><surname>Hulzebosch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ibrahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPRW50498.2020.00329</idno>
		<ptr target="https://doi.org/10.1109/CVPRW50498.2020.00329" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="2729" to="2738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Effects of Browsing Conditions and Visual Alert Design on Human Susceptibility to Deepfakes</title>
		<author>
			<persName><forename type="first">E</forename><surname>Josephs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fosco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<idno type="DOI">10.54501/jots.v2i2.144</idno>
		<ptr target="https://doi.org/10.54501/jots.v2i2.144" />
	</analytic>
	<monogr>
		<title level="j">Journal of Online Trust and Safety</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">DeepVision: Deepfakes Detection Using Human Eye Blinking Pattern</title>
		<author>
			<persName><forename type="first">T</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2020.2988660</idno>
		<ptr target="https://doi.org/10.1109/ACCESS.2020.2988660" />
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="83144" to="83154" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Juric</surname></persName>
		</author>
		<ptr target="https://informationr.net/ir/22-1/isic/isic1620.html" />
		<title level="m">The role of the need for cognition in the university students&apos; reading behaviour</title>
		<imprint>
			<date type="published" when="2017-03-15">2017, March 15</date>
		</imprint>
		<respStmt>
			<orgName>University of Borås</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Thinking, fast and slow</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kahneman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page">499</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Straus</forename><surname>Farrar</surname></persName>
		</author>
		<author>
			<persName><surname>Giroux</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A Style-Based Generator Architecture for Generative Adversarial Networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00453</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2019.00453" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="4396" to="4405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Analyzing and Improving the Image Quality of StyleGAN</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>CoRR, abs/1312.6114</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fooled twice: People cannot detect deepfakes but think they can</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Köbis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Doležalová</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Soraperra</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.isci.2021.103364</idno>
		<ptr target="https://doi.org/10.1016/j.isci.2021.103364" />
	</analytic>
	<monogr>
		<title level="j">iScience</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">103364</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Subjective and Objective Evaluation of Deepfake Videos</title>
		<author>
			<persName><forename type="first">P</forename><surname>Korshunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP39728.2021.9414258</idno>
		<ptr target="https://doi.org/10.1109/ICASSP39728.2021.9414258" />
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2510" to="2514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Fast Face-Swap Using Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Korshunova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dambre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.397</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2017.397" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="3697" to="3705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Metric Learning: A Survey</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<idno type="DOI">10.1561/2200000019</idno>
		<ptr target="https://doi.org/10.1561/2200000019" />
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends® in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="287" to="364" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Completing Coyne&apos;s Cycle: Dysphorics&apos; Ability to Detect Deception</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Depaulo</surname></persName>
		</author>
		<idno type="DOI">10.1006/jrpe.1999.2253</idno>
		<ptr target="https://doi.org/10.1006/jrpe.1999.2253" />
	</analytic>
	<monogr>
		<title level="j">Journal of Research in Personality</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="311" to="329" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">In Ictu Oculi: Exposing AI Generated Fake Face Videos by Detecting Eye Blinking</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1806.02877</idno>
		<ptr target="https://doi.org/10.48550/ARXIV.1806.02877" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Detection theory: A user&apos;s guide</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Macmillan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Creelman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="page">407</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Reliance on emotion promotes belief in fake news</title>
		<author>
			<persName><forename type="first">C</forename><surname>Martel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pennycook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Rand</surname></persName>
		</author>
		<idno type="DOI">10.1186/s41235-020-00252-3</idno>
		<ptr target="https://doi.org/10.1186/s41235-020-00252-3" />
	</analytic>
	<monogr>
		<title level="j">Cognitive Research: Principles and Implications</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">47</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Exploiting Visual Artifacts to Expose Deepfakes and Face Manipulations</title>
		<author>
			<persName><forename type="first">F</forename><surname>Matern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Riess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
		<idno type="DOI">10.1109/WACVW.2019.00020</idno>
		<ptr target="https://doi.org/10.1109/WACVW.2019.00020" />
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Applications of Computer Vision Workshops (WACVW)</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="83" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Can negative mood improve language understanding? Affective influences on the ability to detect ambiguous communication</title>
		<author>
			<persName><forename type="first">D</forename><surname>Matovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Forgas</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jesp.2013.12.003</idno>
		<ptr target="https://doi.org/10.1016/j.jesp.2013.12.003" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Social Psychology</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="44" to="49" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The Multidimensional Assessment of Interoceptive Awareness, Version 2 (MAIA-2)</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Mehling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Acree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Silas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jones</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0208034</idno>
		<ptr target="https://doi.org/10.1371/journal.pone.0208034" />
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">208034</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Body awareness: Construct and self-report measures</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Mehling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gopisetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Daubenmier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Hecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stewart</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0005614</idno>
		<ptr target="https://doi.org/10.1371/journal.pone.0005614" />
	</analytic>
	<monogr>
		<title level="j">PloS One</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">5614</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">AI Hyperrealism: Why AI Faces Are Perceived as More Real Than Human Ones</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Steward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Witkower</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A M</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Krumhuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dawel</surname></persName>
		</author>
		<idno type="DOI">10.1177/09567976231207095</idno>
		<ptr target="https://doi.org/10.1177/09567976231207095" />
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1390" to="1403" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">The Creation and Detection of Deepfakes: A Survey</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Mirsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1145/3425780</idno>
		<ptr target="https://doi.org/10.1145/3425780" />
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deepfakes Detection with Automatic Face Weighting</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Montserrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Yarlagadda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baireddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Horváth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Bartusiak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Guera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Delp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="2851" to="2859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Conspiracy thinking and social media use are associated with ability to detect deepfakes</title>
		<author>
			<persName><forename type="first">E</forename><surname>Nas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>De Kleijn</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tele.2023.102093</idno>
		<ptr target="https://doi.org/10.1016/j.tele.2023.102093" />
	</analytic>
	<monogr>
		<title level="j">Telematics and Informatics</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page">102093</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">FSNet: An Identity-Aware Generative Model for Image-based Face Swapping</title>
		<author>
			<persName><forename type="first">R</forename><surname>Natsume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yatagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Morishima</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">AI-synthesized faces are indistinguishable from real faces and more trustworthy</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Nightingale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Farid</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.2120481119</idno>
		<ptr target="https://doi.org/10.1073/pnas.2120481119" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">2120481119</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Identifying and minimising the impact of fake visual media: Current and future directions</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Nightingale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Wade</surname></persName>
		</author>
		<idno type="DOI">10.1017/mem.2022</idno>
		<ptr target="https://doi.org/10.1017/mem.2022" />
	</analytic>
	<monogr>
		<title level="j">Memory, Mind &amp; Media</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Aging in an &quot;infodemic&quot;: The role of analytical reasoning, affect, and news consumption frequency on news veracity detection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pehlivanoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Lighthall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Polk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Cahill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Ebner</surname></persName>
		</author>
		<idno type="DOI">10.1037/xap0000426</idno>
		<ptr target="https://doi.org/10.1037/xap0000426" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Applied</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">468</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The role of analytical reasoning and source credibility on the evaluation of real and fake fulllength news articles</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pehlivanoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Deceus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Heemskerk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Ebner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Cahill</surname></persName>
		</author>
		<idno type="DOI">10.1186/s41235-021-00292-3</idno>
		<ptr target="https://doi.org/10.1186/s41235-021-00292-3" />
	</analytic>
	<monogr>
		<title level="j">Cognitive Research: Principles and Implications</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Lazy, not biased: Susceptibility to partisan fake news is better explained by lack of reasoning than by motivated reasoning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pennycook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Rand</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cognition.2018.06.011</idno>
		<ptr target="https://doi.org/10.1016/j.cognition.2018.06.011" />
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">188</biblScope>
			<biblScope unit="page" from="39" to="50" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">The Psychology of Fake News</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pennycook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Rand</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2021.02.007</idno>
		<ptr target="https://doi.org/10.1016/j.tics.2021.02.007" />
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="388" to="402" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Intraindividual variability in positive and negative affect over 45 days: Do older adults fluctuate less than young adults?</title>
		<author>
			<persName><forename type="first">C</forename><surname>Röcke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0016276</idno>
		<ptr target="https://doi.org/10.1037/a0016276" />
	</analytic>
	<monogr>
		<title level="j">Psychology and Aging</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="863" to="878" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Are Deep Learning-Generated Social Media Profiles Indistinguishable from Real Profiles? Hawaii International Conference on System Sciences</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">H</forename><surname>Auglend</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Mukkamala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thatcher</surname></persName>
		</author>
		<idno type="DOI">10.24251/HICSS.2023.017</idno>
		<ptr target="https://doi.org/10.24251/HICSS.2023.017" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Rössler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cozzolino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Verdoliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Riess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1901.08971</idno>
		<idno type="arXiv">arXiv:1901.08971</idno>
		<ptr target="https://doi.org/10.48550/arXiv.1901.08971" />
		<title level="m">FaceForensics++: Learning to Detect Manipulated Facial Images</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Detecting Forged Facial Videos using convolutional neural network</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sambhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Canavan</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2005.08344</idno>
		<ptr target="https://doi.org/10.48550/ARXIV.2005.08344" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Mood as Information: 20 Years Later</title>
		<author>
			<persName><forename type="first">N</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Clore</surname></persName>
		</author>
		<idno type="DOI">10.1207/S15327965PLI1403&amp;4_20</idno>
		<ptr target="https://doi.org/10.1207/S15327965PLI1403&amp;4_20" />
	</analytic>
	<monogr>
		<title level="j">Psychological Inquiry</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="296" to="303" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">A comprehensive overview of Deepfake: Generation, detection, datasets, and opportunities</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Seow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C W</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2022.09.135</idno>
		<ptr target="https://doi.org/10.1016/j.neucom.2022.09.135" />
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">513</biblScope>
			<biblScope unit="page" from="351" to="371" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">A Study of the Human Perception of Synthetic Faces</title>
		<author>
			<persName><forename type="first">B</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Richardwebster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>O'toole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Scheirer</surname></persName>
		</author>
		<idno type="DOI">10.1109/FG52635.2021.9667066</idno>
		<ptr target="https://doi.org/10.1109/FG52635.2021.9667066" />
	</analytic>
	<monogr>
		<title level="m">16th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2021)</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Divine intuition: Cognitive style influences belief in God</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shenhav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Rand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Greene</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0025391</idno>
		<ptr target="https://doi.org/10.1037/a0025391" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="423" to="428" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Providing detection strategies to improve human detection of deepfakes: An experimental study</title>
		<author>
			<persName><forename type="first">K</forename><surname>Somoray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.chb.2023.107917</idno>
		<ptr target="https://doi.org/10.1016/j.chb.2023.107917" />
	</analytic>
	<monogr>
		<title level="j">Computers in Human Behavior</title>
		<imprint>
			<biblScope unit="volume">149</biblScope>
			<biblScope unit="page">107917</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">What intelligence tests miss: The psychology of rational thought</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Stanovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Yale University Press</publisher>
			<biblScope unit="page">308</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Personalization versus Customization: The Importance of Agency, Privacy, and Power Usage</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Sundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Marathe</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1468-2958.2010.01377.x</idno>
		<ptr target="https://doi.org/10.1111/j.1468-2958.2010.01377.x" />
	</analytic>
	<monogr>
		<title level="j">Human Communication Research</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="298" to="322" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Employing Transfer-Learning based CNN architectures to Enhance the Generalizability of Deepfake Detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Suratkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Variyambat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Panchal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kazi</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCCNT49239.2020.9225400</idno>
		<ptr target="https://doi.org/10.1109/ICCCNT49239.2020.9225400" />
	</analytic>
	<monogr>
		<title level="m">11th International Conference on Computing, Communication and Networking Technologies (ICCCNT)</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">The Role of IT Background for Metacognitive Accuracy, Confidence and Overestimation of Deep Fake Recognition Skills</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sütterlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Lugo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Ask</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Veng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fritschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-T</forename><surname>Özmen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bärreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Knox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-05457-0_9</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-05457-0_9" />
	</analytic>
	<monogr>
		<title level="m">Augmented Cognition</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Schmorrow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Fidopiastis</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="103" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Detecting Both Machine and Human Created Fake Face Images In the Wild</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tariq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Woo</surname></persName>
		</author>
		<idno type="DOI">10.1145/3267357.3267367</idno>
		<ptr target="https://doi.org/10.1145/3267357.3267367" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Workshop on Multimedia Privacy and Security</title>
		<meeting>the 2nd International Workshop on Multimedia Privacy and Security</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="81" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Different physiological reactions when observing lies versus truths: Initial evidence and an intervention to enhance accuracy</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ten Brinke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Carney</surname></persName>
		</author>
		<idno type="DOI">10.1037/pspi0000175</idno>
		<ptr target="https://doi.org/10.1037/pspi0000175" />
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">560</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">The Negative Consequences of Informing Voters about Deepfakes: Evidence from Two Survey Experiments</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ternovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Aronow</surname></persName>
		</author>
		<idno type="DOI">10.54501/jots.v1i2</idno>
		<ptr target="https://doi.org/10.54501/jots.v1i2" />
	</analytic>
	<monogr>
		<title level="j">Journal of Online Trust and Safety</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Investigating an alternate form of the cognitive reflection test</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Oppenheimer</surname></persName>
		</author>
		<idno type="DOI">10.1017/S1930297500007622</idno>
		<ptr target="https://doi.org/10.1017/S1930297500007622" />
	</analytic>
	<monogr>
		<title level="j">Judgment and Decision Making</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="99" to="113" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Deepfakes and beyond: A Survey of face manipulation and fake detection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tolosana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vera-Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fierrez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ortega-Garcia</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.inffus.2020.06.014</idno>
		<ptr target="https://doi.org/10.1016/j.inffus.2020.06.014" />
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="131" to="148" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">An Overview of Deepfake: The Sword of Damocles in AI</title>
		<author>
			<persName><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVIDL51233.2020.00-88</idno>
		<ptr target="https://doi.org/10.1109/CVIDL51233.2020.00-88" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision, Image and Deep Learning (CVIDL)</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="265" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Assessing miserly information processing: An expansion of the Cognitive Reflection Test</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Toplak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Stanovich</surname></persName>
		</author>
		<idno type="DOI">10.1080/13546783.2013.844729</idno>
		<ptr target="https://doi.org/10.1080/13546783.2013.844729" />
	</analytic>
	<monogr>
		<title level="j">Thinking &amp; Reasoning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="147" to="168" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Do People Watch what they Do Not Trust</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tsfati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cappella</surname></persName>
		</author>
		<idno type="DOI">10.1177/0093650203253371</idno>
		<ptr target="https://doi.org/10.1177/0093650203253371" />
	</analytic>
	<monogr>
		<title level="j">Communication Research</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="504" to="529" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">On the realness of people who do not exist: The social processing of artificial faces</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tucciarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vehar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chandaria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tsakiris</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.isci.2022.105441</idno>
		<ptr target="https://doi.org/10.1016/j.isci.2022.105441" />
	</analytic>
	<monogr>
		<title level="j">iScience</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">105441</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Towards Untrusted Social Video Verification to Combat Deepfakes via Face Geometry Consistency</title>
		<author>
			<persName><forename type="first">E</forename><surname>Tursman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kamara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tompkin</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPRW50498.2020.00335</idno>
		<ptr target="https://doi.org/10.1109/CVPRW50498.2020.00335" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="2784" to="2793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Deepfakes and Disinformation: Exploring the Impact of Synthetic Political Video on Deception, Uncertainty, and Trust in News</title>
		<author>
			<persName><forename type="first">C</forename><surname>Vaccari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chadwick</surname></persName>
		</author>
		<idno type="DOI">10.1177/2056305120903408</idno>
		<ptr target="https://doi.org/10.1177/2056305120903408" />
	</analytic>
	<monogr>
		<title level="j">Social Media + Society</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2056305120903408</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Media Forensics and DeepFakes: An Overview</title>
		<author>
			<persName><forename type="first">L</forename><surname>Verdoliva</surname></persName>
		</author>
		<idno type="DOI">10.1109/JSTSP.2020.3002101</idno>
		<ptr target="https://doi.org/10.1109/JSTSP.2020.3002101" />
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="910" to="932" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>IEEE Journal of Selected Topics in Signal Processing</note>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Need for cognition and external information search effort</title>
		<author>
			<persName><forename type="first">B</forename><surname>Verplanken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Hazenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Palenéwen</surname></persName>
		</author>
		<idno type="DOI">10.1016/0092-6566(92)90049-A</idno>
		<ptr target="https://doi.org/10.1016/0092-6566(92)90049-A" />
	</analytic>
	<monogr>
		<title level="j">Journal of Research in Personality</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="128" to="136" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">News literacy, social media behaviors, and skepticism toward information on social media</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Vraga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tully</surname></persName>
		</author>
		<idno type="DOI">10.1080/1369118X.2019.1637445</idno>
		<ptr target="https://doi.org/10.1080/1369118X.2019.1637445" />
	</analytic>
	<monogr>
		<title level="j">Information, Communication &amp; Society</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="150" to="166" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">CNN-generated images are surprisingly easy to spot</title>
		<author>
			<persName><forename type="first">S.-Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>For now</note>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Development and validation of brief measures of positive and negative affect: The PANAS scales</title>
		<author>
			<persName><forename type="first">D</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tellegen</surname></persName>
		</author>
		<idno type="DOI">10.1037//0022-3514.54.6.1063</idno>
		<ptr target="https://doi.org/10.1037//0022-3514.54.6.1063" />
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1063" to="1070" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">The Emergence of Deepfake Technology: A Review</title>
		<author>
			<persName><forename type="first">M</forename><surname>Westerlund</surname></persName>
		</author>
		<idno type="DOI">10.22215/timreview/1282</idno>
		<ptr target="https://doi.org/10.22215/timreview/1282" />
	</analytic>
	<monogr>
		<title level="j">Technology Innovation Management Review</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="40" to="53" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1811.00661</idno>
		<ptr target="https://doi.org/10.48550/ARXIV.1811.00661" />
		<title level="m">Exposing Deep Fakes Using Inconsistent Head Poses</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">A Survey on Deepfake Video Detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1049/bme2.12031</idno>
		<ptr target="https://doi.org/10.1049/bme2.12031" />
	</analytic>
	<monogr>
		<title level="j">IET Biometrics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="607" to="624" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Deepfake generation and detection, a survey</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11042-021-11733-y</idno>
		<ptr target="https://doi.org/10.1007/s11042-021-11733-y" />
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="6259" to="6276" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
