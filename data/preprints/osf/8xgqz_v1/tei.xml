<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Human languages trade off complexity against efficiency</title>
				<funder ref="#_B5J3Nkm">
					<orgName type="full">German Research Foundation (DFG)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Alexander</forename><surname>Koplenig</surname></persName>
							<email>koplenig@ids-mannheim.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Lexical Studies</orgName>
								<orgName type="institution">Leibniz Institute for the German Language (IDS)</orgName>
								<address>
									<settlement>Mannheim</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sascha</forename><surname>Wolfer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Lexical Studies</orgName>
								<orgName type="institution">Leibniz Institute for the German Language (IDS)</orgName>
								<address>
									<settlement>Mannheim</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jan</forename><forename type="middle">Oliver</forename><surname>RÃ¼diger</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Lexical Studies</orgName>
								<orgName type="institution">Leibniz Institute for the German Language (IDS)</orgName>
								<address>
									<settlement>Mannheim</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peter</forename><surname>Meyer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Lexical Studies</orgName>
								<orgName type="institution">Leibniz Institute for the German Language (IDS)</orgName>
								<address>
									<settlement>Mannheim</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Human languages trade off complexity against efficiency</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">146DC1B3A77C4ED968CDC7FED976A8F9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-10-22T06:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Quantitative linguistics</term>
					<term>computational linguistics</term>
					<term>linguistic typology</term>
					<term>language model</term>
					<term>language complexity</term>
					<term>information theory</term>
					<term>entropy</term>
					<term>machine learning</term>
					<term>spatial filtering</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>From a cross-linguistic perspective, language models are interesting because they can be used as idealised language learners that learn to produce and process language by being trained on a corpus of linguistic input. In this paper, we train different language models, from simple statistical models to advanced neural networks, on a database of 41 multilingual text collections comprising a wide variety of text types, which together include nearly 3 billion words across more than 6,500 documents in over 2,000 languages. We use the trained models to estimate entropy rates, a complexity measure derived from information theory. To compare entropy rates across both models and languages, we develop a quantitative approach that combines machine learning with semiparametric spatial filtering methods to account for both language-and document-specific characteristics, as well as phylogenetic and geographical language relationships. We first establish that entropy rate distributions are highly consistent across different language models, suggesting that the choice of model may have minimal impact on cross-linguistic investigations. On the basis of a much broader range of language models than in previous studies, we confirm results showing systematic differences in entropy rates, i.e. text complexity, across languages. These results challenge the long-held notion that all languages are equally complex. We then show that higher entropy rate tends to co-occur with shorter text length, and argue that this inverse relationship between complexity and length implies a compensatory mechanism whereby increased complexity is offset by increased efficiency. Finally, we introduce a multi-model multilevel inference approach to show that this complexity-efficiency trade-off is partly influenced by the social environment in which languages are used: languages spoken by larger communities tend to have higher entropy rates while using fewer symbols to encode messages.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Research objective</head><p>A model that assigns probabilities to sequences of linguistic symbols is called a language model (LM) <ref type="bibr" target="#b0">[1]</ref>. While originally only trained on (vast amounts of) textual data to predict upcoming linguistic material <ref type="bibr" target="#b1">[2]</ref>, modern LMs demonstrate impressive and at times surprising capabilities in a wide range of scientific applications beyond linguistic tasks, such as predicting protein structures <ref type="bibr" target="#b2">[3]</ref>, forecasting time series <ref type="bibr" target="#b3">[4]</ref>, accelerating drug and material discovery <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>, analysing genomic and epi-genomic data <ref type="bibr" target="#b6">[7]</ref>, and enhancing climate modelling <ref type="bibr" target="#b7">[8]</ref>.</p><p>In the fields of linguistics and natural language processing (NLP), LMs excel in traditional tasks, as evidenced by their ability to perform zero-shot learning, where they effectively generalise to new tasks without specific training, as shown by <ref type="bibr" target="#b8">[9]</ref>. This highlights their potential to acquire human-like grammatical language through statistical learning, without relying on a built-in grammar <ref type="bibr" target="#b9">[10]</ref>. On this basis, a vibrant research field has emerged, where LMs are being used as computational working models <ref type="bibr" target="#b10">[11]</ref> or models of languages <ref type="bibr" target="#b11">[12]</ref> to study different aspects of language processing and comprehension <ref type="bibr" target="#b1">[2]</ref>.</p><p>While we do not claim that LMs truly understand language <ref type="bibr" target="#b10">[11]</ref> or attribute meaning in a similar way as humans do <ref type="bibr" target="#b12">[13]</ref>, we believe that LMs are interesting from a cross-linguistic perspective, because they can be used as idealised language learners that learn to produce and process language by being trained on a corpus of linguistic input <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14]</ref>. A central goal of linguistics is to understand the diverse ways in which human language can be organised. By training an LM on linguistic material in different languages, researchers can investigate how these models learn and generalise linguistic structures and rules across different languages and language families.</p><p>In this paper, we use this framework to enhance our understanding of human language as a complex system. To this end, we custom-trained seven different LMs on a database of more than 6,500 different documents, covering over 2,000 languages.</p><p>The main contributions of this paper are fourfold:</p><p>-We develop a novel quantitative framework that integrates machine learning with semiparametric spatial filtering methods. This framework allows for the simultaneous consideration of language-and document-specific features, as well as phylogenetic and geographic relationships among languages. Additionally, we introduce a multimodel multilevel inference approach designed to test whether cross-linguistic variation is statistically associated with sociodemographic factors, while accounting for phylogenetic and spatial autocorrelation.</p><p>-We measure language complexity using information-theoretic entropy rates and extend previous work by applying this method to a broader range of language models (LMs)from simple statistical models to machine learning models, among them advanced deep learning models including transformers. Additionally, we expand the analysis to include sub-word but supra-character encoding levels, offering a more detailed and layered perspective on language complexity. We demonstrate that, despite their pronounced architectural differences, the investigated LMs produce remarkably consistent rankings of language complexity across all considered symbolic levels. This challenges the idea that all languages are equally complex.</p><p>-We present, discuss, and evaluate evidence for a previously undocumented trade-off between complexity and efficiency: higher entropy rates tend to co-occur with shorter text lengths. From an information-theoretic perspective, message length quantifies efficiencythe shorter the message, the higher the efficiency <ref type="bibr" target="#b14">[15]</ref>. We argue that this inverse relationship between complexity and length implies a compensatory mechanism, whereby increased complexity is offset by greater efficiency.</p><p>-We show that this trade-off is influenced by the social environments in which languages are used, with larger communities tending to use more complex but more efficient languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Background and related work</head><p>Parallel corpora are valuable in a cross-linguistic context because they enable systematic comparisons of language processing across different languages <ref type="bibr" target="#b15">[16]</ref>. By providing translations of the same text in multiple languages, parallel corpora allow researchers to study how LMs handle similar content across varying grammatical structures and lexicons. These datasets also facilitate the testing and understanding of linguistic laws, i.e., statistical patterns shared across human languages <ref type="bibr" target="#b16">[17]</ref>, or the examination of whether languages adapt to the geographical or sociodemographic environments in which they are learned and used <ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref>. Yet another idea dating back to the work of Greenberg <ref type="bibr" target="#b22">[23]</ref> that was revived with the development of large parallel corpora <ref type="bibr" target="#b23">[24]</ref> is to use parallel texts to classify and compare languages <ref type="bibr" target="#b15">[16]</ref>. Examples of such cross-linguistic studies are <ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref>. However, the majority of the aforementioned studies are based on very peculiar text types, especially translations of the Bible and there are several important challenges that the use of the Bible as a parallel text source pose <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>.</p><p>To address this limitation, we leveraged available corpora and multilingual text collections <ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref> and compiled a database of parallel texts comprising a large variety of different text types, e.g. religious texts, legalese texts, subtitles for various movies and talks, and machine translations. In addition, we added comparable corpora, i.e., texts that are not parallel but come from comparable sources and are therefore similar in content, again comprising very different text types/genres, e.g. newspaper texts, web crawls, Wikipedia articles, translation tables for system messages in the Ubuntu operating system, or translated example sentences from a free collaborative online database. Furthermore, we added information from the CrÃºbadÃ¡n project <ref type="bibr" target="#b35">[36]</ref> that aims at creating text corpora for a large number of (especially under-resourced) languages. In total, the compiled database contains 41 different multilingual corpora, comprising nearly 3 billion words or nearly 9 billion Unicode characters across more than 6,500 documents and covering over 2,000 languages. These languages are spoken as a native language by more than 90% of the world's population and represent almost half of all languages with a standardised written representation.</p><p>In a recent paper <ref type="bibr" target="#b36">[37]</ref>, we presented the first results based on this database. In this study, our primary focus was a cross-linguistic examination of language complexity, a topic that has garnered significant attention in linguistics and related fields over the past two decadesfor an overview, see <ref type="bibr" target="#b37">[38]</ref>. In our study, we quantitatively evaluated the so called equi-complexity hypothesis that suggests that all human languages, despite their diverse and varied nature, have the same level of overall complexity <ref type="bibr" target="#b38">[39]</ref>. To overcome the difficulty of measuring overall language complexity <ref type="bibr" target="#b39">[40]</ref>, we leveraged information theory, an area of mathematics that links probability and communication <ref type="bibr" target="#b40">[41]</ref> and provides notions of complexity that are both objective and theory-neutral <ref type="bibr" target="#b41">[42]</ref>. To this end, we trained a simple statistical LM on each of our documents and statistically analysed the training process to infer the average persymbol information content or entropy rate of each document, which can be interpreted as a measure of complexity <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref>: the harder it is, on average, to predict upcoming text, the higher the entropy rate, the greater is the complexity of the text as a whole <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b44">[45]</ref><ref type="bibr" target="#b45">[46]</ref><ref type="bibr" target="#b46">[47]</ref>. We argued the entropy rate can thus also be used to compare the complexity of different languages. We then statistically compared complexity rankings across different corpora by calculating correlation coefficients between the entropy rates across all possible pairs of multilingual corpora. For example, we correlated the entropy rate rankings derived from a corpus of movie subtitles in various languages with those from a similarly diverse corpus of Wikipedia sentences. This approach, applied comprehensively to all pairs among our 41 different multilingual corpora, makes it possible to assess the consistency of complexity rankings across various types of linguistic data. From an information-theoretic point of view, we showed that our results constitute evidence against the equi-complexity hypothesis: a language with high/low entropy rate in one corpus also tends to be more/less complex in another corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Research Question and Scope</head><p>As higher complexity in language results in more demanding processing efforts, encompassing both language production and comprehension, our study <ref type="bibr" target="#b36">[37]</ref> naturally leads to the question: Why is there a trend towards increased complexity in certain languages? In the present study we pursue this issue by providing evidence suggesting that languages with high entropy rates, which indicate greater complexity, require fewer symbols to encode messages. This points to a compensatory mechanism: higher complexity is balanced by increased efficiency, in terms of shorter message lengths.</p><p>The main purpose of this study is to present, discuss and evaluate evidence for such a complexity-efficiency trade-off, while also striving to enhance the following empirical and methodological aspects of our prior work.</p><p>First, in our previous paper we trained a rather simple statistical LM. However, as Baroni <ref type="bibr" target="#b47">[48]</ref> pointed out, different LMs have uniquely structured internal architectures, and thus cannot be viewed as "blank slates". Instead, they should be regarded as algorithmic linguistic theories "encoding non-trivial structural priors facilitating language acquisition and processing" <ref type="bibr" target="#b47">[48]</ref>. These architectural differences affect how LMs acquire and process language. For instance, ngram models primarily rely on local context and assume fixed-length dependencies. To estimate the probability of the next symbol, n-gram models build a conditional probability distribution based on a limited context window that only takes into account the preceding few words (e.g. n = 2, 3, 4, 5) <ref type="bibr" target="#b1">[2]</ref>. This restriction on context can lead to a loss in the model's ability to capture long-range dependencies that are essential for grammatical complexity in human languages. More advanced models, such as transformers and other neural network architectures, are equipped with mechanisms like self-attention that allow them to capture complex, long-range dependencies and richer contextual information from the input data <ref type="bibr" target="#b1">[2]</ref>. These architectural differences enable different models to capture linguistic complexity at various levels, from basic statistical patterns to more nuanced syntactic and semantic relationships.</p><p>To extend our previous work, we thus train various types of LMs on our data, ranging from simple statistical n-gram models to state-of-the-art transformer models. This approach allows us to compare how models with varying levels of sophistication interpret and represent language complexity, providing a more comprehensive understanding of the phenomenon across models. Surprisingly, we show that, from a cross-linguistic perspective, the type of LM has relatively little impact on the obtained results.</p><p>Secondly, we improve and extend our prior work methodologically by developing a machine learning method that fully accounts for the relatedness of languages: when comparing languages, statistical challenges arise due to the fact that closely related languages often exhibit more similarities among themselves in various aspects than they do with more distantly related languages. Additionally, languages originating from the same regions often tend to be influenced by common factors, further complicating the analysis <ref type="bibr" target="#b48">[49]</ref><ref type="bibr" target="#b49">[50]</ref><ref type="bibr" target="#b50">[51]</ref>. While we have included language family, macro-area and country as factors to account for the genealogical and geographic relatedness of languages in our prior paper, this approach ignores variation within language families and geographical units as pointed out in several recent studies <ref type="bibr" target="#b48">[49]</ref><ref type="bibr" target="#b49">[50]</ref><ref type="bibr" target="#b50">[51]</ref><ref type="bibr" target="#b51">[52]</ref><ref type="bibr" target="#b52">[53]</ref>. To address this issue, we develop two quantitative approaches: (i) a semiparametric machine learning estimation method capable of simultaneously controlling for document-and language-specific characteristics while directly modelling potential effects due to phylogenetic relatedness and geographic proximity; (ii) a multi-model multilevel inference approach designed to test whether cross-linguistic outcomes are statistically associated with sociodemographic factors, while accounting for phylogenetic and spatial autocorrelation via the inclusion of random effects and slopes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">Structure of the paper</head><p>The structure of this paper is as follows: the next section introduces the multilingual database and details the procedures for compiling the text data (Sect. 2.1). This is followed by a description of the sociodemographic and linguistic variables considered in this study (Sect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.2).</head><p>We then introduce the investigated LMs (Sect. 2.3) and describe how the textual data was pre-processed (Sect. 2.4). The methodology for estimating entropy is presented in Sect.</p><p>2.5. Sect. 2.6 is devoted to statistical methods. We first present a novel method for evaluating the similarity of entropy rates and length distributions across different corpora (Sect. 2.6.1), followed by a description of the multi-model inference approach used to examine the impact of the number language users on the entropy-length trade-off (Sect. 2.6.2).</p><p>In Sect. 3, we present our findings. The paper concludes with the "Discussion" section, where we evaluate potential limitations of our approach, examine the relevance of the complexityefficiency trade-off, and outline directions for future research (Sect. 4).</p><p>All data and code (Stata v18.0 and Python v3.6.8) needed to replicate our analyses are available at https://osf.io/xdwjc/. In addition, interactive results and visualisations are available online at https://www.owid.de/plus/tradeoffvis/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Materials and methods</head><p>Some material in this section is recycled from our prior publications <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b53">54]</ref>, in accordance with the guidelines provided by the Text Recycling Research Project <ref type="bibr" target="#b54">[55]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Database</head><p>In what follows, we give an overview regarding the database. Additional in-depth details regarding all corpora can be found in <ref type="bibr" target="#b36">[37]</ref>. In total, we analysed 41 different multilingual text collections. 40 text collections consist of actual full text data, while the remaining collection consists of word frequency information from the CrÃºbadÃ¡n project <ref type="bibr" target="#b35">[36]</ref>. Of the 40 full-text collections, 33 are fully parallel and 7 corpora contain comparable documents. The full-text corpora can be loosely categorised into the following text types: 5 religious text collections, 4 news/Wikipedia/Web crawls text collections, 5 text collections containing legalese texts, 22 multilingual subtitle corpora and 4 collections of other text types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Text types</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Religious texts</head><p>The two parallel text collections BibleNT and BibleOT are both part of the Parallel Bible Corpus (PBC) made available by Mayer and Cysouw <ref type="bibr" target="#b32">[33]</ref>   <ref type="bibr" target="#b2">3,</ref><ref type="bibr">259,</ref><ref type="bibr">354)</ref>. Each translation was pre-tokenised, Unicode normalised, and spaces were inserted between words, punctuation marks, and non-alphabetic symbols by the corpus compilers, who also manually checked and corrected the texts where necessary <ref type="bibr" target="#b32">[33]</ref>.</p><p>Additionally, great care was taken to account for language-specific peculiarities. For instance, in the Austronesian language Arifama-Miniafia, the right single quotation mark represents the glottal stop <ref type="bibr" target="#b32">[33]</ref>. Another example is tone marking, which in languages such as Chinantec, Mazatec, or NambikuÃ¡ra is represented by raised numbers indicating tones. For instance, in the Sochiapan Chinantec translation of the sentence "I am the God of Abraham," the text reads: "JnÃ¡Â¹Â³ bÃ­hÂ¹ laÂ³Â² DiÃ³Â³Â² JuoÂ¹Â³ HÃ¡Â²branÂ²Â¹" Automatically tokenising this sentence into words (see Sect. 2.4) would mistakenly split words into separate parts, e.g., "HÃ¡ bran." For further information, see Bentz et al. <ref type="bibr" target="#b26">[27]</ref>, Appendix A.</p><p>Another distinctive feature of the PBC, compared to the other multilingual text collections in corpus, is the usage of full vocalisation using tashkÄ«l diacritics, as highlighted by Gutierrez-Vasques et al. <ref type="bibr" target="#b55">[56]</ref>. In Arabic, short vowels are usually not writtenmost registers omit them, while some use diacritical marks to indicate their presence. For example, in Egyptian Arabic, the PBC encodes the word for "peace" as "salÄm" â«Ùï»¡(â¬ â«Ùï»¼â¬ â«,)ïº³â¬ marking the short vowels with diacritics. In other text collections, only the 'skeleton' letters "sl'm" â«)ïº³ï»¼ï»¡(â¬ are provided, leaving the short vowels implied. As discussed further in Sect. 2.4, this lack of vowel representation complicates computational and quantitative analyses. The quality and consistency of the PBC in encoding such features enhances the reliability of our crosslinguistic results. For further information regarding text pre-processing, see Sect. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Web crawls</head><p>The GlobalVoices comparable collection consist of contributions to the citizen media platform Global Voices. Raw text files of all articles were downloaded from http://casmacat.eu/corpus/global-voices.html (version: 2018Q4; accessed 4/30/20, ð ð· = 40; ð ð¿ = 39; ð¿ Ìð = 20,021; ð¿ Ìð¶ = 112,541). The other three collections were compiled based on plain text files from the Leipzig Corpora Collection (LCC) <ref type="bibr" target="#b34">[35]</ref> that presents corpora in a uniform format. Here we focus on three collections that we name as follows (i) LCCnews, i. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subtitles</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>299</head><p>The parallel subtitle collections consist of two types: subtitles of movies and subtitles of TED 300 talks. The 13 subtitle collections are based on the ParTy corpus <ref type="bibr" target="#b33">[34]</ref>. The Technology, 301 Entertainment, Design (TED) talk subtitles were downloaded from 302 https://amara.org/en/teams/ted/videos/ (accessed 4/30/20). Information regarding movie/talk 303 titles, number of translations/languages per corpus and median lengths are provided in Table <ref type="table" target="#tab_5">304</ref> 1. 305 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word list</head><p>To generate the word list data, we downloaded all available lists from the CrÃºbadÃ¡n project <ref type="bibr" target="#b35">[36]</ref> from http://crubadan.org/files/ (accessed 4/30/20). In total, we arrived at 2,216 word frequency lists for a total of ð ð¿ = 1,943 different languages (ð¿ Ìð = 101,079).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Overview of the database</head><p>Figure <ref type="figure" target="#fig_3">1</ref> displays a map highlighting the geographical distribution of languages for the compiled multilingual database. The figure reveals an imbalance at the language level within the database: over 100 languages have at least 10 documents, but approximately 75% of languages have fewer than four documents. This scarcity reflects the limited electronic availability of documents in languages spoken by smaller populations <ref type="bibr" target="#b35">[36]</ref>. This is exemplified by the contrast in median speaker numbers: while the median for all non-extinct languages documented by Ethnologue stands at 8,000 <ref type="bibr" target="#b56">[57]</ref>, the median for languages represented with at least one document in our database is significantly higher, at 30,000. In addition, the majority of our text collections contain a comparatively small number of individual documents, with a median of 40 documents per corpus. This limited size can be attributed to specific reasons in certain cases; for example, the EUconst collection is naturally restricted to translations of the European Constitution into the official languages of the European Union. In contrast, for other collections such as the subtitle corpora, translations into further languages were not available when we compiled the database. On the other side of the spectrum, we have 11 multilingual text collections that consist of more than 100 different documents. As described above, documents are rather short, e.g. 25% of the documents are below 14,575 characters or 3,181 words. However, 200 documents are longer than 1 million characters, 49 documents are longer than 10 million characters and the longest documents are several hundred million words and more than a billion characters long. documents. This imbalance reflects the limited electronic availability of documents in languages spoken by smaller populations <ref type="bibr" target="#b35">[36]</ref>.</p><p>In what follows we statistically compare the structure found in smaller corpora (i.e., those consisting of shorter documents and/or a limited number of available documents) with the structure found in larger corpora (i.e., those consisting of longer documents and/or data points for many languages). The idea is that if the results from both smaller and larger corpora align, this strengthens the claim that these results are not merely artefacts resulting from database bias. Additionally, we include control covariates, such as the number of corpora per language, to account for the unbalanced nature of our database, as described in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Sociodemographic and linguistic variables</head><p>Information on speaker population size, corpus, language family, language (identified by its ISO-639-3 code), macro-area, writing script, speaker population size, longitude and latitude are taken from <ref type="bibr" target="#b36">[37]</ref>. Writing script refers to the writing system in which the corresponding document is written in. In total, we have information for ð ð· = 6,513 different documents using 51 different writing scripts. The vast majority (ð ð· = 5,183 or 79.58%) of our documents are written in Latin script, followed by Cyrillic (ð ð· = 463 or 7.11%), Arabic (ð ð· = 156 or 2.40%) and Devanagari (ð ð· = 98 or 1.50%). Expanded Graded Intergenerational Disruption Scale (EGIDS) level information was initially sourced from <ref type="bibr" target="#b57">[58]</ref>, which is reported in Glottolog <ref type="bibr" target="#b58">[59]</ref> (v4.2.1). Country is defined by Ethnologue <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b60">61]</ref> as the primary country/country of origin of the language in question <ref type="bibr" target="#b56">[57]</ref>. To ensure completeness, we manually supplemented missing data from <ref type="bibr" target="#b57">[58]</ref> by cross-referencing with Glottolog and Ethnologue. The EGIDS level serves as a measure of a language's endangerment status <ref type="bibr" target="#b61">[62]</ref>.</p><p>We use the EGIDS level as a covariate to control for potential translation effects <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b63">64]</ref>, as languages with lower EGIDS levels are presumably more likely to be used as source languages, while languages with higher EGIDS levels are presumably more likely to be used as target languages. For example, an EGIDS level of 0 (labelled "International") pertains to the six official United Nations languages: Arabic, Chinese, English, French, Russian, and Spanish. On the other hand, languages with values of five and above pertain to languages that are not used in formal education, mass media or by the government, and they may consequently be more susceptible to (more) pronounced "translationese" influences <ref type="bibr" target="#b63">[64]</ref>. With a similar logic in mind and to account for the unbalancedness of our database (see Sect.</p><p>2.1.2), we also consider the number of corpora with a least one available document per language as an additional control variable in what follows.</p><p>Further information regarding the classification of languages into macro-family and subfamily are taken from <ref type="bibr" target="#b64">[65]</ref>. We manually added information for languages that was missing by using publicly available genealogical classifications (see the script 'prepare_language_info.do' available at https://osf.io/tkgph/ for details). Classifications in <ref type="bibr" target="#b64">[65]</ref> are given as comma-separated values. We define the first value as the macro-family and the second one as the sub-family, e.g. for the language "Ghotuo" the classification is "Niger-Congo, Atlantic-Congo, Volta-Congo, Benue-Congo, Edoid, North-Central, Ghotuo-Uneme-Yekhee", so the macro-family is "Niger-Congo" and the sub-family is "Atlantic-Congo".</p><p>Additionally, we use a phylogenetic similarity matrix also provided by <ref type="bibr" target="#b64">[65]</ref> that is based on word lists from the Automated Similarity Judgment Program (ASJP) <ref type="bibr" target="#b65">[66]</ref>. Information on the number of countries in which each language is spoken was sourced from Glottolog (v4.2.1).</p><p>We manually supplemented missing data by cross-referencing with Ethnologue <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b60">61]</ref>. The rationale behind considering this variable as a potential covariate is to account for the varying degrees of pluricentrism <ref type="bibr" target="#b66">[67]</ref>. For instance, languages such as Chinese or Spanish are spoken in several countries and may therefore have different codified standard forms.</p><p>For further information and a discussion of potential caveats and problems regarding the assignment of environmental variables to individual languages in order to reflect local grouping structure, see <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b68">69]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Language models</head><p>We use general-purpose data compression algorithms, taking advantage of the fact that language modelling and lossless compression are essentially equivalent <ref type="bibr" target="#b69">[70]</ref><ref type="bibr" target="#b70">[71]</ref><ref type="bibr" target="#b71">[72]</ref>. All data compression algorithms consist of a model and a coder <ref type="bibr" target="#b13">[14]</ref>. Our focus is on the class of (lossless) compressors where the algorithm uses training data to estimate a model, i.e., a conditional probability distribution, that can be used to generate predictions about upcoming symbols. To perform compression, the predicted probabilities are then used to encode symbols using a technique called arithmetic encoding <ref type="bibr" target="#b72">[73]</ref>. The seven LMs that we investigate are summarised in Table <ref type="table" target="#tab_1">2</ref>. In what follows, further details are given for each language model. Prediction by partial matching (PPM) <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b84">85]</ref> is a dynamic and adaptive variable-order n-gram LM. The algorithm assumes the Markov property: to predict the next symbol, the algorithm uses the last o immediately preceding symbols. For PPM2, we set o to 2, i.e., the last 2 symbols are used as context to generate predictions. For PPM6, we set o to 6. In both cases, the level of compression is set to maximum and the size of used memory is set to 2,000 megabytes. PAQ can be described as a weighted combination of predictions from a large number of models, where the individual models are combined using a gated linear network <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b80">81]</ref>. The network has a single layer with 552 input nodes and 3,080 input weights. The model has a total of ~1.7 million weights, but due to a sparse updating scheme which leads to faster compression and decompression, the effective number of parameters used in training is significantly lower.</p><p>Only 552â¢7 = 3,864 weights are updated for each bit of data. We use version PAQ8o and set the compression level to maximum, requiring 1,712 megabytes of memory.</p><p>NNCP <ref type="bibr" target="#b82">[83]</ref> is a lossless data compressor that is based on the Transformer XL model defined in <ref type="bibr" target="#b85">[86]</ref>. Modifications to the original Transformer XL model and algorithmic details are provided in <ref type="bibr" target="#b86">[87,</ref><ref type="bibr" target="#b87">88]</ref>. As for LSTM, the Adam optimiser is used and we use the "encode only" mode. For TRFsmall (version 3.1), we use the default options with four layers and a model dimension of 256, resulting in a total number of ~2.24 million parameters. For TRFmed (version 3.2), we set the number of layers to 12 and the model dimension to 512, resulting in a total number of ~19.1 million parameters. For TRFbig (version 3.2), we use the available "enwik9" profile that sets the number of layers to 20 and the model dimension to 1,024, resulting in a total number of ~279 million parameters.</p><p>In addition, NNCP offers compression based on a Long Short-Term Memory deep neural network (LSTM) <ref type="bibr" target="#b81">[82]</ref>. We use four layers of LSTM cells. The network is trained using truncated-like backpropagation <ref type="bibr" target="#b86">[87,</ref><ref type="bibr" target="#b88">89]</ref> and Adam optimisation is used to update network weights <ref type="bibr" target="#b89">[90]</ref>. We do not use a text pre-processor or tokeniser and we use the faster "encode only" mode (the output cannot be decompressed, but the compression itself is still lossless).</p><p>The total number of parameters is ~3.93 million parameters.</p><p>In addition, when discussing the relevance of the complexity-efficiency trade-off (Sect. 0), we use OpenAI's GPT-2 model <ref type="bibr" target="#b90">[91]</ref> with ~1.5 billion parameters, as implemented in the Hugging Face library <ref type="bibr" target="#b91">[92]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Text pre-processing and information encoding units</head><p>Each document is tokenised and Unicode normalised where necessary. All uppercase characters are lowered based on the closest language-specific International Organization for Standardization (ISO) code. Unless otherwise specified in <ref type="bibr" target="#b36">[37]</ref>, the word-break algorithm of the International Components for Unicode library [93; Annex #29] was used to detect word boundaries (in texts without spaces or marks between words, a dictionary lookup method is used by the algorithm <ref type="bibr" target="#b93">[94]</ref>). More details regarding each individual text collection can be found in <ref type="bibr" target="#b36">[37]</ref>.</p><p>Following <ref type="bibr" target="#b26">[27]</ref>, we represent a text Îº as a random variable that is created by drawing (with replacement) from a set of symbol types ð± = {ð  1 , ð  2 , ð  3 , â¦ , ð  ð }, where V is the number of symbol types, i.e., ð = |ð±|. Correspondingly, a symbol token is any reoccurrence of a symbol type <ref type="bibr" target="#b26">[27]</ref>. In what follows, we estimate the relevant information-theoretic quantities for the following information encoding units/symbol types: (i) (Unicode) characters, (ii) words and (iii) sub-word units. For (iii), we apply byte pair encoding (BPE) <ref type="bibr" target="#b94">[95]</ref><ref type="bibr" target="#b95">[96]</ref><ref type="bibr" target="#b96">[97]</ref> to split words into one or several units and then train the different LMs over the resulting sequences of sub-word units. We follow <ref type="bibr" target="#b94">[95]</ref> and set the number of BPE merges to 0.4â¢V.</p><p>Note that we neither remove spaces nor punctuation at the character level, nor before applying BPE, since punctuation can provide reliable information regarding stylistic features and differences between authors <ref type="bibr" target="#b97">[98]</ref>. Correspondingly, on the word level, each punctuation mark is treated as a separate token, except for the dash, which ensures that dash-separated words (e.g., 'presidency-in-office') are treated as a single word type. After tokenisation into words/sub-word units, each word/BPE type is replaced by one unique Unicode symbol. The different compression algorithms are then used to compress the resulting symbol sequence.</p><p>On the BPE level, we also compress the mapping of sub-word units to 4-byte Unicode symbols.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations &amp; caveats</head><p>It is important to acknowledge several caveats when interpreting cross-linguistic results based on the quantitative analysis of written language text at both the character and word levels <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b94">95,</ref><ref type="bibr" target="#b98">[99]</ref><ref type="bibr" target="#b99">[100]</ref><ref type="bibr" target="#b100">[101]</ref>. First, our analysis is based exclusively on corpora of written language, which begs the question whether it applies to human languages in general. In this context, it is important to note that "written language cannot be regarded as merely the transference of spoken language to another medium", as John Lyons put it in his classical introduction to theoretical linguistics <ref type="bibr" target="#b101">[102]</ref>. Indeed, there is a long-standing strand of research that recognises written language as a distinct linguistic object worthy of independent study, backed up by a large body of evidence against a mere derivative nature of written language, including evolutionary and biological aspects <ref type="bibr" target="#b102">[103]</ref><ref type="bibr" target="#b103">[104]</ref><ref type="bibr" target="#b104">[105]</ref>. Obviously, the texts in the PBC (see Sect.</p><p>2.1.1) are one prime example of texts that are not parasitic on prior oral language production. From this stance, our results clearly apply to written languages, seen as more or less 'autonomous' communication systems.</p><p>While we do not have analysed genuine oral language data due to the lack of sufficiently large and phonemically annotated corpora, it is a valid question whether our results remain valid if we treat the texts in our corpora as representing spoken language, which basically means that we interpret grapheme sequences as standing in for phoneme sequences. A notable challenge arises from the differences in the mapping between phonemes and graphemes across languages <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b98">99]</ref>. For instance, languages with deep orthographies like English have inconsistent phoneme-grapheme correspondences (e.g., "ough" in "thought" vs. "through" vs.</p><p>"dough" rendering /ÉË/, /uË/, and /ÉÊ/ phonemes, respectively), while languages with shallow orthographies like Spanish have more systematic correspondences (e.g., "a" as in "casa" always corresponds to /a/) <ref type="bibr" target="#b95">[96]</ref>. Another example is the difference in the representation of the phoneme /tÊ/ between Czech and German: Czech uses a single character ("Ä"), while German uses a four-character sequence ("tsch") to represent the same phoneme <ref type="bibr" target="#b63">[64]</ref>. As mentioned in Sect. 2.1.1, certain characters like short vowels are sometimes omitted or represented by diacritics, depending on the orthographic conventions of a given language.</p><p>A significant example for a well-known mismatch between a graphemic and an 'underlying' phonemic representation is how lexical tone is represented in tone languages. In some languages, such as Thai and Lao, tone is indicated using a combination of tone marks (diacritics) and consonant characters, while in Hmong, tone is marked with specific characters, like a letter at the end of words. Yet in other tone languages, like Mandarin Chinese, lexical tone is typically not encoded in the written form. This variation mirrors the challenges we previously discussed in the PBC, where tone languages like Chinantec or Mazatec use raised numbers to represent pitch, adding complexity to tokenisation and analysis (see Sect. 2.1.1). In languages where tone is omitted in writing, complexity estimates for the phonemic representation may be understated, while languages that explicitly mark tone could appear more complex. These differences in tonal representation across writing systems must be considered when interpreting cross-linguistic complexity estimates, especially between tone and non-tone languages.</p><p>Independent from the distinction between spoken and written language, there are several factors that add further complexity to cross-linguistic word-level analyses. First, differences in writing systems are highly relevant for cross-linguistic comparisons of written text. For example, written Mandarin Chinese uses a logographic system where characters typically represent words or morphemes, while languages like English or Russian use alphabetic systems, where symbols usually represent phonemes.</p><p>Secondly, while we use an orthography-based algorithmic definition of "word", which is standard in computational linguistics, it is important to note that there is no universally accepted definition of "word-hood" across languages <ref type="bibr" target="#b99">[100,</ref><ref type="bibr" target="#b100">101]</ref>, although see Haspelmath <ref type="bibr" target="#b105">[106]</ref>. However, Geertzen et al. <ref type="bibr" target="#b106">[107]</ref> demonstrate that the computational approach to "wordhood" we employ may be sufficiently well-suited within an information-theoretic and compression-based quantitative framework for capturing linguistic regularities in crosslinguistic analyses. Moreover, the substantial effort and typologically informed curation of the PBC adds further reliability to our cross-linguistic analyses (see Sect. 2.1.1). Therefore, if the patterns observed in the other multilingual text collections we investigate align with those from the PBC, this consistency would bolster confidence in the robustness and validity of our quantitative results.</p><p>Thirdly, languages show considerable variation with respect to the morphological complexity of words. Words in highly synthetic languages may, for example, contain long chains of derivational affixes, such as in Turkish tan-Ä±Å-tÄ±r-Ä±l-a-ma-dÄ±k-lar-Ä±n-dan-dÄ±r 'it is because they cannot be introduced to each other' <ref type="bibr" target="#b107">[108]</ref>. Verbs in particular may exhibit complex inflectional paradigms featuring incorporated object nouns and multiple tense, mood, and agreement markers, e.g. in Cayuga t-Ä-hÄn-atat-hÇ«na't-a-yÄ Ì:thw-ahs 'they will plant potatoes for each other' <ref type="bibr" target="#b108">[109]</ref>. Importantly, the status of such long morpheme sequences as single words is, in general, not controversial. In contrast, analytical languages like modern English express similar grammatical elements with multiple words, showing little or no inflection. This means that the number of grammatically possible words differs by several orders of magnitude between languages. This has a significant impact on word-level quantitative analyses, where any two different words are treated as unanalysed, unrelated entities. In addition, the number of words required to express the same propositional content varies significantly across languages.</p><p>To further address the aforementioned challenges, we compute our quantitative estimates at multiple levels of linguistic structure: characters, words, and the supra-character, sub-word level using BPE <ref type="bibr" target="#b94">[95,</ref><ref type="bibr" target="#b95">96]</ref>. BPE is a sub-word segmentation technique that iteratively merges the most frequent pairs of characters or character sequences, creating sub-word units that can capture many meaningful linguistic patterns. This method is crucial in modern language modelling, as it efficiently handles morphological variation and rare words, enhancing model performance across diverse languages. Moreover, BPE's ability to extract language-specific sub-word patterns from raw text makes it particularly valuable in cross-linguistic studies, as it reveals structural differences and encodes features that align with those described in traditional linguistic typology, as recently discussed in-depth by Gutierrez-Vasques et al. <ref type="bibr" target="#b55">[56]</ref>.</p><p>By comparing language-specific information-theoretic estimates across symbolic levels, languages, and corpora, we can assess the consistency of results. If these estimates, derived from different symbolic levels and corpora with varying qualities for cross-linguistic studies, point in the same direction, this would strengthen the validity of our quantitative findings.</p><p>Additionally, as described in Sect. 2.6, we statistically account for potential confounding factors by including the type of writing script as a covariate in our analyses. In the multimodel multilevel analyses (Sect. 2.6.2), we also include random intercepts for macro-family, sub-family andcruciallythe language itself. These controls are expected to help mitigate some of the issues outlined above. Moreover, in Sect. 4, we discuss the results of several additional analyses presented in the Supporting Information, which further explore how robust our results are with respect to the potential sources of influence described above.</p><p>Nevertheless, we wish to emphasise that, given the scope of this studyencompassing several hundred languages and a wide variety of writing systems, each with its own distinct and often idiosyncratic conventions for representing language in written formit is difficult, if not impossible, to completely rule out the possibility that these cross-linguistic variations may systematically influence the validity of our quantitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Entropy estimation</head><p>In order to quantify the amount of information contained in Îº, we can represent Îº as a distribution of symbol frequencies by counting how often each symbol j appears in Îº and call the resulting frequency ð ð . The Gibbs-Shannon unigram entropy H of this distribution can be computed as <ref type="bibr" target="#b40">[41]</ref>:</p><formula xml:id="formula_0">ð»(ð) = -â ð(ð  ð ) ð ð=1 â¢ log ð(ð  ð )<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">ð(ð  ð ) = ð ð â ð ð ð ð=1</formula><p>is the maximum likelihood estimator of the probability of ð  ð in Îº</p><formula xml:id="formula_2">consisting of â ð ð ð ð=1</formula><p>tokens. In what follows, all logs are to the base two, so the quantities are expressed in bits. H(Îº) can be interpreted as the average number of (yes/no) guesses that are needed to correctly predict the type of a symbol token that is randomly sampled from Îº. The entropy rate or per-symbol entropy of a stochastic process can be formally defined as <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b40">41]</ref>:</p><formula xml:id="formula_3">â(ð) = lim ðââ 1 ð ð» ð (ð) = lim ðââ 1 ð ð»(ð¡ 1 ð )<label>(2)</label></formula><p>where ð¡ 1 ð = ð¡ 1 , ð¡ 2 , â¦ , ð¡ ð represents a block of consecutive tokens of length N and ð» ð (ð)</p><p>denotes the so-called block entropy of block size N <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b109">110]</ref>.</p><p>Following <ref type="bibr" target="#b42">[43]</ref>, we define ð¹ ð as the prediction complexity of ð¡ ð given ð¡ 1 , ð¡ 2 , â¦ , ð¡ ð-1 as follows:</p><formula xml:id="formula_4">ð¹ ð â¡ ð»(ð¡ ð |ð¡ 1 ð-1 )<label>(3)</label></formula><p>ð¹ ð quantifies the average uncertainty of the Nth symbol, given all preceding tokens ð¡ 1 ð-1 .</p><p>Assuming a stationary ergodic stochastic process <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b43">44]</ref>, ð¹ ð reaches the entropy rate h as N tends to infinity <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b42">43]</ref>:</p><formula xml:id="formula_5">â(ð) = lim ðââ ð¹ ð<label>(4)</label></formula><p>In analogy to H(Îº), the entropy rate â(ð) can be informally understood as the average number of guesses that are needed to guess the next symbol of a sequence and thus incorporating the notion that prediction and understanding are intimately related <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b109">110,</ref><ref type="bibr" target="#b110">111]</ref>. Information can then be defined as any kind of knowledge that, when in your possession, allows you to make predictions with greater accuracy than mere chance <ref type="bibr" target="#b111">[112,</ref><ref type="bibr" target="#b112">113]</ref>. Thus, h encompasses complexity from various linguistic sub-domains, since any form of linguistic (e.g. grammatical, phonological, lexical, pragmatic) or non-linguistic (e.g. world) knowledge will help a reader or listener to predict upcoming linguistic material more accurately and will therefore reduce h <ref type="bibr" target="#b9">[10]</ref>. This implies that when using LMs trained on written text to draw conclusions about languages, there is yet another important caveat that must be emphasised: not all information is encoded in written language. Key aspects such as extra-linguistic information, including prosodyi.e., the supra-segmental features of speech such as pitch, loudness, and tempoare not fully captured. While Wolf et al. <ref type="bibr" target="#b113">[114]</ref> show that much of the prosodic information can be inferred from the words and surrounding context, they also demonstrate that prosodic features cannot be entirely predicted from text alone. This suggests that prosody carries information that extends beyond the written word, contributing additional layers of knowledge.</p><p>Since the probability distribution for any natural language is unknown <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b109">110]</ref>, we use the data compression algorithms described above to estimate â(ð). Per LM, the entropy rate estimate is computed (roughly speaking) as the number of bits per symbol in the compressed text:</p><formula xml:id="formula_6">â Ìð¿ð (ð) = ð¾ ð¿ð (ð) ð¿(ð)<label>(5)</label></formula><p>where ð¾ ð¿ð (ð) = ð ð¿ð (ð) -ð ð¿ð (ð ð¡ðððð ). Here, ð ð¿ð (ð) denotes the number of bits that are needed by the LM to compress ð, ð ð¡ðððð represents the first half of ð, ð¿(ð) represents the length of the second half of ð in words on the level of words or, both on the character and the BPE level, in Unicode characters. Note that on the BPE levels we also compress the mapping of unique symbols to 4-byte Unicode symbols mentioned above and add the resulting compressed lengths to ð ð¿ð (ð) and ð ð¿ð (ð ð¡ðððð ). Further note that â Ìð¿ð is directly related to the quantity perplexity that is often used in NLP to measure the quality of a language model, where perplexity is defined as 2 â Ìð¿ð <ref type="bibr" target="#b41">[42]</ref>. We use this relationship to also choose the LM that achieves the lowest perplexity on the test data:</p><formula xml:id="formula_7">â Ìððð ð¡ (ð) = min ð¿ðââ â Ìð¿ð (ð)<label>(6)</label></formula><p>where â denotes the set of different LMs, i.e., â ={PPM2, PPM6, PAQ, LSTM, TRFsmall, TRFmed, TRFbig}. In a similar vein, we choose ð¾ Ìððð ð¡ (ð).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Statistical analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.1">Comparing entropy/length distributions across LMs and corpora</head><p>We now take Îº to be a corpus that consists of individual texts ð ð , where i denotes 1,â¦, I different languages. For brevity, we omit the superscript and the hat in what follows.</p><p>However, we compute the correlation coefficients described below for both the best LM (â Ìððð ð¡ and ð¾ Ìððð ð¡ ) and for each individual LM (â Ìð¿ð and ð¾ Ìð¿ð ).</p><p>The entropy estimate for ð ð is denoted as â Ï (ð ð ), where Ï denotes one of three information encoding units (words, characters, BPE), likewise for ð¾ Ï (ð ð ) and ð¿ Ï (ð ð ). â Ï (ð ð ) and ð¾ Ï (ð)</p><p>are computed on all three levels, while ð¿ Ï (ð) is computed on either the word or the character level. Note that for languages with more than one available translation/document in a corpus, all quantities are averaged.</p><p>To evaluate the (dis-)similarity of entropy/length distributions across corpora and test for a potential trade-off between entropy and length, we first compute the pairwise Pearson To control for potential sources of influence, we fit linear models of the form:</p><formula xml:id="formula_8">ð² = ðð + ð (<label>7</label></formula><formula xml:id="formula_9">)</formula><p>where y is the n Ã 1 vector of observed values, either ð£ Ï â² (ð) or ð£ Ï â²â² (ð), n denotes the number of languages that are available in both ð and ð, X is the n Ã p design matrix of p covariates including a n Ã 1 vector of ones for the intercept, ð is the corresponding p Ã 1 vector of coefficients and ð is the n Ã 1 vector of residuals. We assume that ð ð are identically distributed with ð¸(ð ð ) = 0 with variance ð 2 . Importantly, we want to rule out potential autocorrelation among the residuals, i.e., we wish to test the following null hypothesis in what follows:</p><formula xml:id="formula_10">ð» 0 : ð¸[ðð T ] = ð 2 ð. (<label>8</label></formula><formula xml:id="formula_11">)</formula><p>where I is the n Ã n identity matrix and Î¤ denotes the matrix transpose.</p><p>As potential control variables, we consider the EGIDS level, the (logged) speaker population size, the (logged) number of corpora and the (logged) number of countries in which the language is spoken. We also include a set of indicator variables for the levels (categories) of writing script. To avoid overfitting, scripts that were unique to a single language were grouped into a common category. For HCr, we additionally control for the number of words and the number of documents (both logged) on which the word frequency list is based, and a binary variable indicating whether the word frequency list is truncated to account for differences in the way different CrÃºbadÃ¡n word lists were generated. Further information can be found in <ref type="bibr" target="#b36">[37]</ref>. To select the relevant control variables from the candidate set, we use the lasso machine learning technique <ref type="bibr" target="#b114">[115]</ref>. To choose the optimal value for the penalty parameter for each lasso, we use a heteroskedastic plugin estimator <ref type="bibr" target="#b115">[116]</ref>. Languages with missing information on any of the control variables are excluded in each case. Let ð Ì denote a n Ã ð Ì matrix of ð Ì covariates selected by the lasso. We then regress y on ð Ì and compute residuals denoted as ð Ì. To test H0 from above (eq. 8), we compute the modified version of Moran's I <ref type="bibr" target="#b116">[117]</ref> suggested by Kelejian and Prucha <ref type="bibr" target="#b117">[118]</ref>, written as:</p><formula xml:id="formula_12">ð¼ = ð(ð ÌTðð Ì) [(ð ÌTð Ì)âtr{(ð T + ð)ð}] -1<label>(9)</label></formula><p>where W denotes an n Ã n weighting matrix and tr represents the trace operator. For W, we consider two inverse distance matrices: (i) to test for spatial autocorrelation, we construct an inverse distance matrix WG based on longitude and latitude information, (ii) to test for phylogenetic autocorrelation, we construct an inverse distance matrix WP based on a phylogenetic similarity matrix provided by <ref type="bibr" target="#b64">[65]</ref>. In both cases, matrix elements are equal to the reciprocal of distance that are then normalised using spectral normalisation. We test for autocorrelation with (i) WG as input, (ii) WP as input and (iii), as explained below, both WG and WP as input. For brevity, we drop the subscript in what follows and describe our algorithmic approach for input matrix W. Since I 2 ~ Î§ 2 (1) <ref type="bibr" target="#b117">[118]</ref>, we test H0 via a standard Î§ 2test with one degree of freedom. If p &lt; 0.05, we extend our linear regression model (eq. 7) by a semiparametric filter <ref type="bibr" target="#b118">[119]</ref><ref type="bibr" target="#b119">[120]</ref><ref type="bibr" target="#b120">[121]</ref> as:</p><formula xml:id="formula_13">ð² = ðð + ðð + ð<label>(10)</label></formula><p>where, in addition to the above, F is a n Ã q matrix of q eigenvectors and ð is a n Ã 1 vector of parameters. F is computed based on a transformed version of W defined as <ref type="bibr" target="#b118">[119]</ref>:</p><formula xml:id="formula_14">ð â¡ (ð - ðð Î¤ ð ) ð (ð - ðð Î¤ ð )<label>(11)</label></formula><p>where ð represents a n Ã 1 column vector of ones. The eigensystem decomposition of M generates n eigenvalues and n corresponding eigenvectors. The eigenvalues are then sorted in descending order, denoted as ð = (Î» 1 , Î» 2 , Î» 3 , â¦ Î» ð ), so that the largest eigenvalue receives the subscript 1, the second largest eigenvalue receives the subscript 2 and so on. The corresponding set of eigenvectors can then be denoted as ð = (ð 1 , ð 2 , ð 3 , â¦ ð ð ). We include ð 1 into F and let the lasso select a subset of control variables from X. We then compute ð Ì based on a regression of y on ð Ì and F. After that, we perform the Î§ We then compute the Pearson correlation between those residuals and proceed as described above.</p><p>The correlation coefficients per condition (none, geographical, phylogenetic and both) are denoted as Ïnone, Ïgeo, Ïphylo and Ïboth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.2">Multi-model multilevel inference</head><p>To evaluate if the trade-off is moderated by the social environment in which languages are being used, we run separate multilevel effects models (MLEMs) with (i) h or K as the outcome on all three levels (words/characters/BPE) for all eight LMs (best, PPM2, PPM6, PAQ, LSTM, TRFsmall, TRFmed, TRFbig) and (ii) L as the outcome for words/characters.</p><p>For N = 3,705 individual documents, we fit MLEMs of the form <ref type="bibr" target="#b121">[122]</ref>:</p><formula xml:id="formula_15">ð² = ðð + ðð® + ð<label>(12)</label></formula><p>where, in addition to the above, Z is a matrix of random predictors and u is a vector of random effects that are assumed to follow a normal distribution, with mean 0 and variancecovariance matrix G. The residual errors Ïµ are assumed to follow a normal distribution, with mean 0 and variance matrix ð 2 ð; ð® â¥ ð. To enhance convergence, the outcome is standardised per corpus, i.e., the corpus-specific mean was subtracted from each observed value and the result was divided by the corpus-specific standard deviation, but we also provide results for log-transformed outcomes (see https://osf.io/93csg/) that can be visualised in our interactive online application (https://www.owid.de/plus/tradeoffvis/). We consider a fixed effect for the estimated speaker population size (logged) as a proxy for population structure <ref type="bibr" target="#b122">[123]</ref>. The following control variables are included: (i) fixed effects: corpus type (parallel/comparable), binary indicators for the first four EGIDS levels and the (logged) number of countries; (ii) random intercepts for the following groups: writing script, corpus, macro-area, macro-family, sub-family and language. We cross corpus, macro-area, macro-family and writing script and explicitly nest language within sub-family within macro-family; (iii) random slopes for population size, i.e., we allow the effect of population size to vary across the different groups.</p><p>We adopt a multi-model inference approach <ref type="bibr" target="#b123">[124]</ref> by sub-setting each full model, i.e., we generate a set of models with all possible control variable subsets, which are then fitted to the data. We fit sub-models per outcome, type and LM. All models were fitted with gradientbased maximization (maximal number of 20 iterations) and via maximum likelihood (ML).</p><p>Per outcome and per type, we then compute a frequentist model averaging (FMA) estimator over all R candidate models <ref type="bibr" target="#b124">[125,</ref><ref type="bibr" target="#b123">124,</ref><ref type="bibr" target="#b125">126]</ref>:</p><formula xml:id="formula_16">ð½ Ìð¥ = â ð ð ð ð=1 ð½ ð¥,ð<label>(13)</label></formula><p>where ð½ ð¥,ð denotes the estimated fixed effect of variable x for model j and ð ð is a weight computed as:</p><formula xml:id="formula_17">ð ð = ð (- 1 2 ð¥ ð ) ðº<label>(14)</label></formula><p>where ðº = â ð (-</p><formula xml:id="formula_18">1 2 ð¥ ð ) ð ð=1</formula><p>represents the sum of weights for all R models. To compute ð¥ ð , we use Akaike's information criterion (AIC) <ref type="bibr" target="#b126">[127]</ref>, where lower values indicate a better model, ð¥ ð = AIC ð -AIC ððð where AIC ð denotes the AIC value computed for model j and AIC ððð represents the minimum AIC value over all R models. Note that in models where x does not appear, ð½ ð¥,ð â¡ 0. On this basis, we compute an FMA estimator of the standard error (SE) as <ref type="bibr" target="#b123">[124]</ref>:</p><formula xml:id="formula_19">SE(ð½ Ìð¥) = â ð ð ð ð=1 âSE(ð½ ð¥,ð ) 2 + (ð½ ð¥,ð -ð½ Ìð¥) 2<label>(15)</label></formula><p>where SE(ð½ ð¥,ð ) denotes the estimated standard error of ð½ ð¥,ð for model j. In models where x does not appear, we set SE(ð½ ð¥,ð ) â¡ 0. To assess statistical significance, we compute a corresponding two-tailed p-value as ð = 2 â (1 -Î¦ (| ð½ Ìð¥</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SE(ð½ Ìð¥)</head><p>|)) where Î¦() denotes the cumulative standard normal distribution. In a similar vein, we compute a 95% confidence interval (95%-CI) as ð½ Ìð¥ Â± Î¦ -1 (0.975) â SE(ð½ Ìð¥) where Î¦ -1 () denotes the inverse cumulative standard normal distribution.</p><p>Note that the Akaike weights ð ð can be "interpreted as approximate probabilities of each model being the actual best model, given the data" <ref type="bibr" target="#b123">[124]</ref>. Thus, we can use the ð ð to estimate the relative importance of variable x, computed as <ref type="bibr" target="#b123">[124]</ref>:</p><formula xml:id="formula_20">ð ð¥ = â ð ð ð ð¥,ð ð ð=1<label>(16)</label></formula><p>where ð ð¥,ð is a binary indicator that is equal to 1 if x is explicitly in model j and 0 otherwise <ref type="bibr" target="#b123">[124]</ref>. The larger ð ð¥ , the more important x. To put the value of ð ð¥ into perspective, we show in Supporting Information S2 that its theoretical minimum is ~0.27. 2.5 for details) for most of the documents. For longer documents, the larger LMs (LSTM and the three Transformer LMs) achieve similar or lower entropy rates. Correspondingly, Table <ref type="table" target="#tab_4">3</ref> shows that PAQ has the lowest h in more than 90% of the documents across all three symbolic levels. To determine if entropy rate distributions are systematically affected by the choice of LM, we used the entropy estimates for the 40 full text corpora at each symbolic level to compute pairwise correlations Ïnone for each pair of LMs. The results presented in this section demonstrate that although there are differences in the performance of various LMs depending on document length (Figure <ref type="figure" target="#fig_4">2a-c</ref>), the resulting entropy rate distributions are remarkably consistent across LMs (Figure <ref type="figure" target="#fig_6">3</ref>). This suggests that, from a cross-linguistic perspective, the choice of LM for investigating different languages may have a minimal impact. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Comparing entropy/length distributions across LMs and corpora</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Comparing language models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Comparing languages</head><p>We proceed by comparing entropy rate distributions across corpora to determine whether a language that tends to be more complex in one corpus also tends to be more complex in another corpus. Given that the results from the previous section clearly indicate stability across different LMs, we will focus, for each corpus Îº, on the estimates of its best model, i.e., â Ìððð ð¡ (ð) (see Sect. 2.5 for details). Interactive visualisations for each LM, i.e., â Ìð¿ð (ð). are available at https://www.owid.de/plus/tradeoffvis/. As outlined above, we evaluate the similarity of â Ìððð ð¡ -distributions by computing Ïnone, Ïgeo, Ïphylo and Ïboth across corpora and across symbolic levels. Per correlation type, we compute ð ð = 7,254 individual correlations. Figure <ref type="figure" target="#fig_7">4a</ref> demonstrates that entropy rate distributions are very similar across corpora and symbolic levels as indicated by a strong positive correlation between corpus pairs. The results remain stable when we control for language-and document-specific characteristics, as well as the genealogical and geographic relatedness of languages (see Sect. 2.6.1 for details). Interactive LM-specific visualisations, available at https://www.owid.de/plus/tradeoffvis/ show that highly comparable patterns are obtained when estimating entropy rates for individual LMs.</p><p>Entropy rates are estimated as the ratio of the number of bits needed to compress the test data, K, to the length of the test data, i.e., L (cf. eq. 5). To further understand the above results, we repeated the analyses for both variables that are part of this ratio. Figure <ref type="figure" target="#fig_7">4b</ref> reveals that while the results are more pronounced for h, the distributions for L are also very comparable across corpora and symbolic levels (ð ð = 3,160). However, Figure <ref type="figure" target="#fig_7">4c</ref> shows that the results are much weaker for distributions of ð¾ Ìððð ð¡ (ð ð = 7,140). Since K is the product of h and L, this suggests a potential trade-off between h and L. To investigate this possibility, we compute Ïnone, Ïgeo, Ïphylo and Ïboth between â Ìððð ð¡ and L across corpora on all three symbolic levels. Table <ref type="table" target="#tab_5">4</ref> demonstrates that on all three symbolic levels and for all four correlation types, there is a pronounced negative statistical association between entropy rate and length distribution. Again, our interactive visualisation tool shows highly comparable patterns for individual LMs. These results indicate the existence of a tradeoff between both variables. To establish if the trade-off between entropy and length holds across symbolic levels, we compute Ïnone, Ïgeo, Ïphylo and Ïboth between â Ìððð ð¡ and L for all corpus pairs across all three symbolic levels. For each correlation type, ð ð = 20,100 individual correlations were calculated to generate a correlation matrix, which was then subjected to principal component analysis. Figure <ref type="figure" target="#fig_8">5</ref> presents scatterplots of the first two factors that explain most of the variance in the matrix. For both unadjusted and adjusted partial correlations, more than a third of the variance is attributed to the trade-off between entropy and length in each case.</p><p>Interactive LM-specific visualisations that are available at https://www.owid.de/plus/tradeoffvis/ illustrate that highly comparable patterns are observed when estimating entropy rates for different LMs.</p><p>Given that, from an information theoretic point of view, message length quantifies efficiency the shorter the message the higher the efficiency <ref type="bibr" target="#b14">[15]</ref> we arrive at our main empirical result: human languages trade off complexity against efficiency. More explicitly, a higher average amount of choice/uncertainty per produced/received symbol is compensated by a shorter average message length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Summary</head><p>In the last two sub-sections, we demonstrated that (i) entropy rate distributions are highly consistent across different LMs, suggesting that the choice of LM might have minimal impact on cross-linguistic investigations of the kind we presented here. We also showed that (ii) there is a pronounced trade-off between entropy rate and document length across different corpora, which implies that languages balance efficiency and complexity. This finding highlights a potentially fundamental principle in linguistic structure, where higher uncertainty per symbol is offset by shorter message lengths.</p><p>To bring these results together, we now compute fully adjusted partial correlations, Ïboth for all possible pairwise combinations of the 41 corpora, the three variables (â Ìð¿ð , HCr, L), the three symbolic levels (words, characters, BPE), and the seven investigated LMs (PPM2, PPM6, PAQ, LSTM, TRFsmall, TRFmed, TRFbig). In total, ð ð = 423,660 individual correlation coefficients were computed. Figure <ref type="figure" target="#fig_9">6</ref> shows that the findings point in the same direction as previously observed, confirming the consistency of entropy rate distributions and their trade-off with document length.</p><p>To emphasise this point, we extracted the first 80 factors from the factor analysis, which together account for ~90% of the variance in the correlation matrix. For each factor, we conducted separate linear regressions with the factor as the outcome and a binary indicator for the type of variable (1 = L vs. 0 = â Ìð¿ð or HCr) as the predictor. For each factor, we then extracted the amount of explained variance (R 2 ) as a measure of model fit. Among all factors, R 2 is highest for the first factor, with R 2 = 76.14%. We then repeated the analyses using indicator variables for the investigated LMs as predictors. Again, R 2 is highest for the first factor, but with a much smaller value of R 2 = 6.44%). Figure <ref type="figure" target="#fig_9">6</ref> demonstrates that the first factor distinguishes between length and entropic variables but not between LMs. We further visually inspected all remaining factors, none of which separated the LMs, reinforcing the robustness of our findings across different models.</p><p>These results underscore that the negative statistical association between entropy and length in human languages is consistent across various LMs and corpora, suggesting that the tradeoff between complexity and efficiency may reflect a fundamental property of human language. We compute adjusted partial correlations, Ïboth (see Sect. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-model multilevel inference</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Random effects models</head><p>To evaluate if the trade-off between complexity and efficiency is influenced by the social environment in which languages are used, we adopt the multi-model inference approach outlined in Sect. 2.6.2. Vertical lines represent the FMA-estimate, ð½ Ìð¥, of population size, here denoted as Î²log_pop, on â Ìððð ð¡ , L or ð¾ Ìððð ð¡ .</p><p>Horizontal lines show corresponding 95%-CIs (cf. Sect. 2.6.2 for details). Lines are coloured in black if the 95%-CI crosses zero (vertical dashed grey line), whereas blue and pink indicate significant negative and positive effects. Interactive LM-specific visualisations are available at https://www.owid.de/plus/tradeoffvis/.</p><p>By including several fixed and random effects, we control for both (i) language-and document-specific characteristics and (ii) spatial and phylogenetic autocorrelation. For each outcome and symbolic level, we fit R = 8,192 candidate models. Figure <ref type="figure" target="#fig_11">7a</ref> visualises the estimated relative importance, ð ð¥ , for each variable. With the exception of macro-family for ð¾ Ìððð ð¡ , all considered random effects have high relative importance for all three outcomes and across all three levels. Conversely, there is only weak evidence for the importance of most fixed effects, with the clear exception of speaker population size, which is maximally importantrelative to the other variablesfor both â Ìððð ð¡ and L, but not for ð¾ Ìððð ð¡ .</p><p>To investigate if there is also evidence for a trade-off between entropy and length similar to the results presented in the previous section, Figure <ref type="figure" target="#fig_11">7b</ref> plots the FMA-estimated effect, ð½ Ìð¥, of speaker population size on each outcome per symbolic level. There is a positive and significant effect of population size on â Ìððð ð¡ across all three symbolic levels and a negative significant effect on L for both characters and BPE. For ð¾ Ìððð ð¡ , there is no significant evidence of an effect of population size on any of the three symbolic levels. Table <ref type="table" target="#tab_6">5</ref> lists the corresponding estimates for all investigated fixed effects, showing that the only consistent evidence of a noteworthy effect is for speaker population size on either entropy or length.</p><p>These results substantiate the evidence of a trade-off between entropy and length and indicate that languages with more speakers tend to have higher entropic values, i.e., are more complex, but also tend to produce shorter messages, i.e., are more efficient.  to overfitting, adding unnecessary complexity to the model and especially reducing the power to detect true effects: as noted by <ref type="bibr" target="#b128">[129]</ref>, a too-complex MLEM, by including excessive random slopes, may inflate Type I error rates and reduce the ability to identify significant predictors due to overfitting. This underscores the importance of balancing model complexity with the need to capture meaningful variability.</p><p>We thus opted for a two-stage estimation process. We first include random slopes for macroarea, macro-family, and sub-family only in our multi-model multilevel approach (R =17,920).</p><p>As a second step, we then additionally include random slopes for writing script and corpus (R = <ref type="bibr" target="#b34">35,</ref><ref type="bibr">200)</ref>. Figure <ref type="figure" target="#fig_13">8</ref> visualises the results.</p><p>With respect to the relative importance of the fixed and the random effects, both Figure <ref type="figure" target="#fig_13">8a</ref> and Figure <ref type="figure" target="#fig_13">8b</ref> largely point in the same direction as the random-effects-only approach (see Figure <ref type="figure" target="#fig_11">7a</ref>). A noteworthy exception in both cases is that speaker population size is not only maximally important in predicting both â Ìððð ð¡ and L, but also very to maximally important in predicting ð¾ Ìððð ð¡ . Regarding the relative importance of the variables for which we include random slopes in both scenarios, there is significant agreement for the two slopes included for phylogenetic non-independence (macro-and sub-family): neither seems to play an important role in predicting either L or ð¾ Ìððð ð¡ . For â Ìððð ð¡ , random interactions between population size and either macro-family or sub-family is very important. Interestingly, the variable included to account for geographic proximity as a random slope (macro-area) only plays an important role in predicting either L or ð¾ Ìððð ð¡ in the first scenario. It seems that, to a large extent, this influence might be absorbed by the inclusion of random slopes for writing script and corpus in the second scenario, especially for L on the character level and for ð¾ Ìððð ð¡ on both the word and the character level. The inclusion numbers of estimated fixed effects and random effects (including random slopes) parameters, respectively. However, AIC is not the only potential choice as a criterion. For example, we can choose a variant of the Bayesian Information Criterion (BIC) <ref type="bibr" target="#b129">[130]</ref><ref type="bibr" target="#b130">[131]</ref><ref type="bibr" target="#b131">[132]</ref> that imposes a more substantial penalty on the inclusion of additional parameters than AIC, computed as -2â Ìð + 2(log(ð -ð ð ð ) ð ð ð ). If we use this criterion to compute results for the second scenario, all obtained FMA-estimated effects of speaker population size, i.e., positive effects for h and negative effects for L, reach significance at p &lt; 0.05 on all three symbolic levels. We invite interested readers to further explore this using our interactive visualization tool available at https://www.owid.de/plus/tradeoffvis/, where we offer results for a total of four different information criteria.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>As written above, Baroni argues that LMs should be seen as distinct algorithmic linguistic theories rather than "blank slates," as they inherently encode structural biases that shape their linguistic capabilities. Each LM thus represents a "general theory defining a space of possible grammars." <ref type="bibr" target="#b47">[48]</ref>. Put differently, an LM can be seen as a model of an idealised language learner <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b132">133,</ref><ref type="bibr" target="#b133">134]</ref>. Hence, we can think of an LM that is trained on language-specific data "as a grammar, that is, a computation system that, given an input, can predict whether the sequence is acceptable to an idealized speaker of the language" <ref type="bibr" target="#b47">[48]</ref>. In this paper, we investigated very different types of LMs, belonging to different classes: statistical, machine learning, and deep learning models. Each class exemplifies unique learning capabilities and limitations:</p><p>PPM can be seen as an idealised learner focused on identifying and refining local word sequence patterns. By memorising partial matches with the last few symbols (e.g., 2 for PPM2, 6 for PPM6), PPM constructs a probabilistic grammar of language that adjusts predictions based on immediate context. This model is particularly effective at learning the structure of short-term dependencies and frequent n-grams within sentences based on very little input, making it adept at capturing local grammatical rules but limited in handling longterm dependencies.</p><p>PAQ functions as an idealised learner that integrates insights from multiple models to form a coherent representation of grammar. Each model within PAQ contributes specialised 'knowledge', and a gated linear network moderates these contributions to balance and refine various perspectives. This process allows PAQ to learn from multiple strings simultaneously, adapting to a wide range of linguistic patterns and refining predictions through consensus.</p><p>However, the complexity of integrating multiple models may limit its ability to focus on highly specific patterns or rare linguistic structures.</p><p>LSTM networks represent an idealised learner adept at preserving and utilizing temporal sequences. By using memory cells and gates, LSTMs maintain continuity and context over longer sequences, enabling them to learn long-term dependencies and sequential information.</p><p>This capability allows LSTMs to represent the progression of language over time, capturing narrative coherence and integrating both long-term and short-term adjustments. However, LSTMs may struggle with complex hierarchical structures due to their sequential processing nature and consequently require extensive training time.</p><p>Lastly, the Transformer-XL model <ref type="bibr" target="#b85">[86]</ref>, which the NNCP compressor used by us is based on <ref type="bibr" target="#b82">[83]</ref>, can be seen as an idealised learner that is proficient at mapping global context through its self-attention mechanism. This mechanism enables each symbol to dynamically relate to others, constructing a comprehensive web of interactions between symbols. Transformers are highly capable of learning complex dependencies and contextual relationships within entire sentences or documents. Their ability to process all preceding tokens simultaneously allows them to represent language in a broad and interconnected manner, making them particularly adept at generating coherent and contextually relevant text. However, this ability also comes at a cost, as Transformer models need to be trained on huge amounts of data to achieve this level of performance (see, e.g., Figure <ref type="figure" target="#fig_4">2a-c</ref>).</p><p>Our first main result (Sect. 3.1.1) indicates that the choice of the LM has very little impact on the obtained results. Given the far-reaching architectural differences between the investigated LMs, we think this is a surprising result. For instance, a PPM2 model, by design, lacks the memory to store long-term dependencies, yet Figure <ref type="figure" target="#fig_4">2d</ref>-f shows that the results are highly comparable across LMs. This trend holds even for our largest corpus, UNPC (ð¿ Ìð = 341,723,872, see Sect. 2.1.3), which contains information for six languages. For example, the median value of Ïboth between â Ìððð2 and â Ìððð¹ððð , the former estimate being based on an LM with a context window of exactly two symbols and the latter based on a transformer model with ~19.1 million parameters (see Table <ref type="table" target="#tab_1">2</ref>), across symbolic levels (ð ð = 9) is ð Ìboth = 0.90.</p><p>Similarly, take HCr, which is based on an even simpler LM, i.e., a 1-gram LM that does not consider any relationships between words but only their frequency of occurrence. Yet the median of Ïboth between HCr and â Ìð¿ð for the seven considered LMs across levels and corpora (ð ð = 840) is ð Ìboth = 0.42. This consistency across LMs is an important observation for several reasons. Firstly, larger LMs are notably more expensive to train, requiring substantially more computational resources as compared to smaller models like PAQ, as outlined in Table <ref type="table" target="#tab_1">2</ref>. Additionally, larger models need a lot of training data to achieve optimal performance (see Figure <ref type="figure" target="#fig_4">2</ref>). This cost-effectiveness makes smaller LMs particularly attractive for cross-linguistic studies, especially when computational resources are scarce. Furthermore, as written above, the available electronic data for many languages, particularly those spoken by smaller populations, is very limited <ref type="bibr" target="#b35">[36]</ref>. Our results indicate that training smaller LMs in such endeavours seems to be a viable option.</p><p>It is important to point out that our demonstration of consistency across LMs occurs under a specific scenario: we examined whether the information-theoretic complexity of languages relative to each other remains consistent regardless of the LM used. In other words, if language A is deemed more complex than language B when analysed with one particular LM, this relationship persists even when a completely different type of LM is employed. Future cross-linguistic research should explore whether this agreement across LMs extends to other types of analyses and questions, particularly those that are not strictly based on informationtheoretic measures.</p><p>As our second main result (Sect. 3.1.2), we re-evaluated the hypothesis that all languages are equally complex. To address this, we developed a statistical approach that integrates machine learning with spatial filtering methods. This methodology, detailed in Sect. 2.6.1, was designed to control for language-and document-specific characteristics, as well as the phylogenetic and geographic relationships between languages. We used this approach to compare entropy estimates across corpora and showed that for different LMs, different types of symbols as information encoding units, and under control of potential sources of influence, a language with a high/low entropy rate in one corpus also tends to be more/less complex in another corpus. While entropy rate measures the difficulty in predicting text after the statistical structure of the input language has been learned <ref type="bibr" target="#b134">[135]</ref>, we can further explore the learning process itself by examining how difficult it is for LMs to learn these predictions. In our prior work <ref type="bibr" target="#b36">[37]</ref>, we showed that languages which are harder to predict often tend to be easier/faster to learn for PPM models. Building on this, recent findings <ref type="bibr" target="#b53">[54]</ref> provide evidence of a relationship between learning difficulty and speaker population size, suggestingcontrary to expectations derived from previous researchthat languages with larger speaker populations tend to be harder for LMs to learn. These results, along with the analyses presented here, offer information-theoretic evidence that challenges the equi-complexity hypothesis at the level of written language.</p><p>This result inevitably leads to the question: as higher complexity in language results in more demanding processing efforts, why should there be a trend towards increased complexity in certain languages? We provide a potential answer to this question as our third main result (Sect. 3.1.2): we showed that there is a trade-off between the distributions of estimated entropy rates and length across corpora and across LMs. Given that, from an informationtheoretic perspective, message length quantifies efficiency <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b135">136]</ref> we argue that this result suggests that higher complexity is compensated by higher efficiency.</p><p>In discussions about the obtained trade-off between L and the average amount of information per symbol, i.e., h, a recurring objection was that this result seems trivial. As some colleagues pointed out, if the same message is encoded in two languages, each symbol in the language with the shorter message length must transmit more information, almost by definition. In a similar vein, in a recent publication a large-scale quantitative information-theoretic analysis of parallel corpus data in almost 1,000 languages was presented to show that there are apparently strong associations between the way languages encode information into words and patterns of communication, e.g. the configuration of semantic information <ref type="bibr" target="#b136">[137]</ref>. This publication was criticised by <ref type="bibr" target="#b137">[138]</ref> who demonstrated that the results presented by <ref type="bibr" target="#b136">[137]</ref> are systematically biased by varying text lengths, which is a very well-known fact in quantitative linguistics as most, if not all, quantities in the context of word frequency distributions vary systematically with text length <ref type="bibr" target="#b138">[139]</ref><ref type="bibr" target="#b139">[140]</ref><ref type="bibr" target="#b140">[141]</ref>. The authors of <ref type="bibr" target="#b136">[137]</ref> responded that what they call "information density" and text length are "two sides of the same coin" and that the Gibbs-Shannon entropy and text length, conditional on the same content, measure the same underlying construct <ref type="bibr" target="#b141">[142]</ref>: "because the information content of the parallel translations is the same across comparison languages, we can infer that the more words present [sic] within a document covering the same material, the less information is encoded in each." Another recent response to <ref type="bibr" target="#b136">[137]</ref> made a similar argument: "if it takes a language more words to convey the 'same' message, then each word conveys less information." <ref type="bibr" target="#b142">[143]</ref>. Both arguments are incorrect.</p><p>First, let us note that a trade-off between h and L does not only occur for parallel corpora, but it is also observed (i) for comparable corpora (the median adjusted correlation, Ïboth, between â Ìððð ð¡ and L for the seven comparable corpora in our database (see Sect. (comparable) corpus is Ïboth = -0.79. Secondly, eq. 1 clearly demonstrates that the Gibbs-Shannon unigram entropy is not simply equivalent to text length, but rather a diversity index <ref type="bibr" target="#b44">[45]</ref> that measures the amount of "freedom in the combination of symbols" <ref type="bibr" target="#b143">[144]</ref>, as it is a function of the number of different symbols and how evenly those symbols are distributed.</p><p>Thirdly, the main problem with such arguments is that the information-theoretic concept of information is fully agnostic about the content of messages <ref type="bibr" target="#b14">[15]</ref>. As Shannon, the founding father of information theory, puts it <ref type="bibr" target="#b135">[136]</ref>:</p><p>"The fundamental problem of communication is that of reproducing at one point either exactly or approximately a message selected at another point. Frequently the messages have meaning; that is they refer to or are correlated according to some system with certain physical or conceptual entities. These semantic aspects of communication are irrelevant to the engineering problem. The significant aspect is that the actual message is one selected from a set of possible messages."</p><p>In fact, the use of "information" as a label (e.g., "amount of information" or "information content") has been a "continuous source of misunderstanding" since the very inception of information theory, as pointed out by Bar-Hillel <ref type="bibr" target="#b144">[145]</ref>, because "it is psychologically almost As a further illustration, take the following two sentences: "I watched TV." and "I watched television." From the point of view of propositional content, both sentences contain the same message. Following <ref type="bibr" target="#b141">[142,</ref><ref type="bibr" target="#b142">143]</ref>, we thus should expect that the self-information is the same in both cases. However, both ðª ð¤ððð 1ðððð and ðª ð¤ððð ðºðð-2 are lower for the first sentence (ðª ð¤ððð 1ðððð = 11.915, ðª ð¤ððð ðºðð-2 = 3.068) compared to the second one (ðª ð¤ððð 1ðððð = 12.397, ðª ð¤ððð ðºðð-2 = 3.210) again because "television" is much less frequent (ð ð¿ð¶ð¶ = 1,568) than "TV" (ð ð¿ð¶ð¶ = 4,269).</p><p>Finally, consider the sentence pair "They will attack at dawn." versus "They will attack at 5am." From the point of view of propositional content, the "total amount of information" of the second sentence should be higher because it specifies a precise time. Based on a unigram LM, the total amount of information is ðª ð¡ðð¡ðð 1ðððð = 53.389 bits for the first sentence and ðª ð¡ðð¡ðð 1ðððð = 55.884 bits for the second one. However, based on the arguably much better GPT-2 LM, we obtain the opposite result with ðª ð¡ðð¡ðð ðºðð-2 = 7.000 bits for the first sentence and ðª ð¡ðð¡ðð ðºðð-2 = 6.620 bits for the second one.</p><p>These illustrations demonstrate that both H and h reflect the statistical structure of "linguistic sequences independently of the specific information that [is] being encoded" <ref type="bibr" target="#b145">[146]</ref>. As such, it is important to point out that information theory cannot be used to measure something like the total amount of semantic information/propositional content of a sentence. Instead, information theory provides a framework for quantifying the efficiency of symbol sequences in transmitting data, focusing on the probabilistic structure and redundancy of the language rather than its semantic content.</p><p>To elaborate further, it should be noted that translation from one language into another is fundamentally different from what is meant by encoding in the information-theoretic sense. In early works on machine translation, such as Warren Weaver's famous memorandum <ref type="bibr" target="#b146">[147]</ref>,</p><p>translation was sometimes understood to be similar to, e.g., cryptographic decoding. But encoding schemes are basically just biunique mappings of an information source to symbol sequences that permit exact recovery of the original symbols, whereas there is no informationtheoretically well-defined sense in which one (of possibly very many) translation of a text into another human language conveys the "same content". The difference is similar to that between sign language and fingerspelling. A written English text can be translated into a sign language, such as American Sign Language (ASL). Since ASL is a full-blown natural language of its own (with no relation whatsoever to spoken English), there are usually many possible translations none of which allows unambiguous reconstruction of the source text from it. On the other hand, fingerspelling (which is a part of ASL) enables users to mechanically render e.g. English words through sequences of signs representing letters and can thus, in principle, be back-translated losslessly to the oral language original.</p><p>The biuniqueness of encoding schemes is the reason why text length and entropy rate are indeed trivially inversely related to each other when comparing different encodings of a source text, since different encodings of the same source can always be compressed to the exact same outcome and the length of that outcome is then used to estimate the entropy rate.</p><p>For translations between human languages no such argument is available. Indeed, as we will demonstrate in what follows, shorter translations may in principle come with a lower entropy rate instead of a higher one.</p><p>Before further interpreting the complexity-efficiency trade-off, we will discuss the results, presented in the Supporting Information, from several additional analyses we conducted to test the reliability and validity of this trade-off. First, practicing what we preached above, we rule out the possibility that the association between h and L is simply the result of a wellknown systematic text-length bias (Supporting Information S4). Secondly, we demonstrate that there is clear evidence for an entropy-length trade-off only between languages, but not within languages (Supporting Information S5). Thirdly, we show in Supporting Information S6 both theoretically and empirically that the trade-off is indeed not trivial, as one can define processes that increase both entropy and length at the same time. Fourthly, we control for cross-linguistic differences in the number of different words and characters that might stem from, amongst others, morphological typology, different writing systems, varying phonological constraints etc. (see Supporting Information S7). When using the number of different wordform types and the number of different characters (both log-transformed averages over all translations per language in the BibleNT collection) as additional control variables when assessing the (dis)similarity of entropy and length distributions across corpora.</p><p>Fifthly, to address the issue of varying levels of orthographic depth (see Sect. 2.4), we incorporated data from ref. <ref type="bibr" target="#b147">[148]</ref>, who uses a transformer LM to estimate the percentage of correct predictions in phoneme-to-grapheme and grapheme-to-phoneme "translation" tasks across 16 languages. These estimates, which serve as measures of phonemic transparency, provide a quantitative assessment of the ease with which written text can be converted to or from its spoken form, i.e., how directly characters map to phonemes. In Supporting Information S8, we used this information as an additional covariate to control for orthographic depth in our analyses. In both Supporting Information S7 and Supporting Information S8 our results remain robust: (i) languages with high/low entropy rates in one corpus also tend to be more/less complex in another corpus, (ii) an analogous pattern holds for cross-language length distributions, and (iii) there is a trade-off between the distributions of estimated entropy rates and text length across corpora. Sixthly, we show in Supporting Information S9 that the patterns observed in the other investigate multilingual text collection align with those from the PBC. As written above (see Sect. 2.4), we believe that this consistency bolsters confidence in the robustness and validity of our quantitative results, due to the substantial effort and typologically informed curation of the PBC (see Sect. 2.1.1).</p><p>Seventhly, our study focuses exclusively on written language. To explore a potential connection to spoken language, we utilised data from the VoxClamantis corpus <ref type="bibr" target="#b148">[149]</ref>, which Let us speculate that in large societies, institutionalised education potentially makes greater linguistic complexity possible by providing systematic and formalised language learning, which, in turn, can support the acquisition and use of more complex linguistic structures. In line with this, a recent large-scale study found a positive statistical correlation between grammatical complexity and speaker population size <ref type="bibr" target="#b52">[53]</ref>. At the same time, the importance of written communication in larger societies might create a natural pressure towards shorter messages, as it saves costs for producing, storing, and transmitting written texts (e.g., book paper, storage space, bandwidth). This dual influenceeducational systems enabling complexity and the practical need for efficiency in written communicationcould help explain why languages in larger communities might evolve to balance these pressures, resulting in shorter but more complex messages. Testing this hypothesis is an important avenue for future research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>and containing a total of 1,568 unique translations of the Bible. The BibleNT text collection consists of all 27 books that belong to the New Testament of the biblical canon. In total, ð ð· = 1,459 different documents, i.e., translations of the New Testament into another language, are available for ð ð¿ = 1,093 individual languages. The median length of individual documents is ð¿ Ìð = 227,391 words and ð¿ Ìð¶ = 1,190,294 characters. Correspondingly, the BibleOT text collection consists of all 39 books that belong to the Old Testament (ð ð· = 254; ð ð¿ = 147; ð¿ Ìð = 642,772; ð¿ Ìð¶ =</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2 . 4 .</head><label>24</label><figDesc>The two parallel text collections WatchtowerV1 and WatchtowerV2 are also part of the Parallel Bible Corpus, both containing translations of different introductory texts of the Jehovah's Witnesses' official web site<ref type="bibr" target="#b15">[16]</ref> (WatchtowerV1: ð ð· = 142; ð ð¿ = 140; ð¿ Ìð = 129,008; ð¿ Ìð¶ = 659,563; WatchtowerV2: ð ð· = 265; ð ð¿ = 260; ð¿ Ìð = 7,194; ð¿ Ìð¶ = 35,608). The Quran collection consists of parallel translations of the central text of the Islam downloaded from http://tanzil.net/trans/ (accessed 4/30/20, ð ð· = 43; ð ð¿ = 43; ð¿ Ìð = 182,950; ð¿ Ìð¶ = 860,590).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>e., text material of crawled newspapers available online (ð ð· = 112; ð ð¿ = 85; ð¿ Ìð = 196,899; ð¿ Ìð¶ = 1,119,551), (ii) LCCweb, i.e., text material crawled from randomly chosen web pages (ð ð· = 87; ð ð¿ = 85; ð¿ Ìð = 195,953; ð¿ Ìð¶ = 1,127,076), and (iii) LCCwiki, i.e., text material from Wikipedia dumps (ð ð· = 171; ð ð¿ = 171; ð¿ Ìð = 185,770; ð¿ Ìð¶ = 1,038,774). Each document in each corpus consists of 10,000 randomly shuffled sentences in the corresponding language. Legalese texts To compile the UDHR parallel collection, we downloaded parallel translations of the Universal Declaration of Human Rights from https://unicode.org/udhr/ (accessed 4/30/20, ð ð· = 452; ð ð¿ = 399; ð¿ Ìð = 1,978; ð¿ Ìð¶ = 10,822). The other four legalese parallel text collections are all obtained from the OPUS project [32]. The EUconst collection consists of different translations of the European Constitution (ð ð· = 21; ð ð¿ = 21; ð¿ Ìð = 92,607; ð¿ Ìð¶ = 620,502). Europarl is a corpus of documents extracted from the European Parliament web site (ð ð· = 21; ð ð¿ = 21; ð¿ Ìð = 5,362,935; ð¿ Ìð¶ = 31,959,314). The collection EUmed is compiled from PDF documents from the European Medicines Agency (ð ð· = 22; ð ð¿ = 22; ð¿ Ìð = 3,241,844; ð¿ Ìð¶ = 18,714,208). UNPC consists of manually translated documents in the six official languages of the United Nations (ð ð· = 6; ð ð¿ = 6; ð¿ Ìð = 341,723,872; ð¿ Ìð¶ = 879,903,168).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Global distribution of collected documents per language. Approximately 76% of languages have fewer than five documents. On the other side of this spectrum, over 160 languages have more than 10</figDesc><graphic coords="16,70.85,327.23,453.53,203.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparing language models. (ac) LM-specific entropy rates as a function of text length for different symbolic levels (words, characters, BPE). Each solid line represents a locally weighted scatterplot smoother (bandwidth = 0.3) for the entropy estimates of NÎº = 4,297 different documents that belong to the compiled multilingual database (see Sect. 2.1 for details, rug plots at the bottom of each graph illustrate the length distribution). Note that on the BPE level, ð Ìð³ð´ is plotted against L in characters. (df) Median unadjusted Pearson correlation, ð Ìð§ð¨ð§ð , across LMs. These values are calculated by first cross-correlating average entropy rates per language among LMs for each of the 40 full text corpora, followed by computing the median value for each LM pair. Interactive visualisations are available at https://www.owid.de/plus/tradeoffvis/.</figDesc><graphic coords="34,70.85,281.85,425.15,255.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure</head><label></label><figDesc>Figure 2d-f presents the resulting pairwise relationships as correlation matrices. Each cell</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Distribution of pairwise correlations across LMs for each symbolic level. For each of the 40 full text corpora and per symbolic level, we compute the median value of the pairwise correlation between the estimated entropy rate distributions for each LM pair. We compute both unadjusted correlations, i.e., Ïnone, and adjusted partial correlations, i.e., Ïgeo, Ïphylo and Ïboth (see Sect. 2.6.1 2.1 for details) across LM pairs. Per type of correlation coefficient and symbolic level, we compute ðµ ð = 840 individual correlation coefficients where each data point represents one LM pair.</figDesc><graphic coords="36,127.40,315.63,340.08,204.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Distribution of pairwise correlations across corpora and symbolic levels. For each corpus pair, we compute both unadjusted correlations, i.e., Ïnone, and adjusted partial correlations, i.e., Ïgeo, Ïphylo and Ïboth (see Sect. 2.6.1 for details). Correlations are computed both per symbolic level, where estimates are derived on the same symbolic level, and across symbolic levels, where, e.g., the entropy rate distribution calculated for words as information encoding units in one corpus is correlated with, e.g., the distribution calculated at either the character or the BPE level in another corpus. (a) Similarity of ð Ìðððð -distributions across corpora, including correlations between ð Ìðððð for the 40 full text corpora and HCr based on the CrÃºbadÃ¡n word lists (ðµ ð = 7,254). (b) Similarity of ð³-distributions across corpora (ðµ ð = 3,160). (c) Similarity of ð² Ìðððð -distributions across corpora (ðµ ð = 7,140). Interactive LM-specific visualisations are available at https://www.owid.de/plus/tradeoffvis/.</figDesc><graphic coords="37,155.90,141.69,283.45,340.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Comparing entropy and length across corpora. For each corpus pair and across symbolic levels, we compute both unadjusted correlations (Ïnone, a), and adjusted partial correlations (Ïphylo, b; Ïgeo, c; Ïboth, d), see Sect. 2.6.1 for details. For each correlation type, ðµ ð = 20,100 individual correlations are computed to generate a corresponding correlation matrix. Principal-component factoring reveals that for both unadjusted and adjusted correlations, more than ~50% of the variance in the matrix can be attributed to two factors: one main factor representing the strong negative correlation between length and entropy measures (accounting for 34.44% to 40.96% of the variance), and one factor distinguishing symbol types (accounting for 16.47% to 18.88% of the variance). Each marker label represents a numeric ID for one of the 41 investigated corpora (see Supporting Information S3). Interactive LM-specific visualisations are available at https://www.owid.de/plus/tradeoffvis/.</figDesc><graphic coords="39,113.15,70.85,368.50,368.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Evidence for a trade-off between complexity and efficiency across corpora and language models.</figDesc><graphic coords="42,70.85,70.85,453.60,453.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>2.6.1 for details), for each combination of the 41 corpora, the three variables (ð Ìð³ð´ , HCr, L), the three symbolic levels (words, characters, BPE), and the seven investigated LMs (PPM2, PPM6, PAQ, LSTM, TRFsmall, TRFmed, TRFbig), totalling ðµ ð = 423,660 individual correlations. The resulting correlation matrix is then analyzed with principal-component factoring. The scatterplot demonstrates that (i) different LMs are very similar and (ii) the most important factor, accounting for roughly a third of the variance in the matrix, represents a trade-off between complexity and efficiency: languages that tend to have a higher entropic value tend to need fewer symbols to encode messages. Each marker label represents a numeric ID for one of the 41 investigated corpora (see Supporting Information S3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Estimated variable importance and FMA-estimates by outcome and symbolic level ('word'words, 'char'characters, 'BPE'byte pair encoding). For each parameter combination (outcome/level), R = 8,192 candidate MLEMs that include fixed and random effects were run. (a) Estimated variable importance (ð Ìð¥) per variable. Higher values indicate greater importance (cf. Sect. 2.6.2 for details), ð Ìð¥-values range from 0 (white) to 1 (blue). (b) FMA-estimated effect of speaker population size on each outcome per symbolic level.</figDesc><graphic coords="43,161.90,233.44,270.90,406.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Estimated variable importance and FMA-estimates by outcome and symbolic level. (a and c) For each parameter combination, R = 17,920 candidate MLEMs that include fixed effects, random effects and random slopes for macro-area, macro-family and sub-family were run. (b and d) For each parameter combination, R = 35,200 candidate MLEMs that include fixed effects, random effects and random slopes for writing script, corpus, macro-area, macro-family and sub-family were run. (a and b) Estimated variable importance (ð Ìð¥) per variable. Higher values indicate greater importance (cf. Sect. 2.6.2 for details), ð Ìð¥-values range from 0 (white) to 1 (blue). (c and d) FMA-estimated effect of speaker population size on each outcome per symbolic level. Vertical lines represent the FMA-estimate, ð½ Ìð¥, of population size, here denoted as Î²log_pop, on â Ìððð ð¡ , L or ð¾ Ìððð ð¡ . Horizontal lines show corresponding 95%-CIs (cf. Sect. 2.6.2 for details). Lines are coloured in black if the 95%-CI crosses zero (vertical dashed grey line), whereas blue and pink indicate significant negative and positive effects. Analogous LM-specific interactive visualisations and numeric results are available at https://www.owid.de/plus/tradeoffvis/.</figDesc><graphic coords="47,70.85,70.85,453.53,340.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>2 . 1</head><label>21</label><figDesc>and Supporting Information S3) amounts to ð Ìboth = -0.47 for words, ð Ìboth = -0.59 for characters and ð Ìboth = -0.59 for BPE, NÏ = 49), and (ii) across corporain other words: if we know the entropy distributions in one multilingual text collection (parallel or not), we can predict the length in another corpus. For example, the adjusted pairwise correlation (N = 73) between â Ìððð ð¡ on the BPE level for the UDHR (parallel) corpus and L on the character level for the LCC news</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 : Overview of the subtitle text collections. 1st</head><label>1</label><figDesc></figDesc><table><row><cell>column: collection. 2nd column: collection id. 3rd</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Language models. The table lists each investigated LM along with its implementation techniques, source, and time required to train it on a document of median length. Training times are provided in seconds and categorised based on the hardware used: Central Processing Unit (CPU) and High-Performance Computing (HPC) cluster with Graphics Processing Unit (GPU) support. The first five LMs were run exclusively on a CPU, while the remaining two LMs were run on a GPU. For comparison, we also include the computation time required if these two models are run on a CPU. Further implementation details are given in Supporting</figDesc><table><row><cell>Information S1.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LM</cell><cell>Technique/Algorithm</cell><cell>Model specification</cell><cell>Source</cell><cell>Time</cell></row><row><cell>PPM2</cell><cell>N-gram modelling [74], prediction by partial matching [75], memory: 2000</cell><cell>order 2</cell><cell>[76,77]</cell><cell>0.1 (CPU)</cell></row><row><cell>PPM6</cell><cell>megabytes</cell><cell>order 6</cell><cell></cell><cell>0.1 (CPU)</cell></row><row><cell>PAQ</cell><cell>Context mixing [14,78], gated linear network [79]</cell><cell>weights ~1.7 million, parameters ~3,800</cell><cell>[80,81]</cell><cell>54.5 (CPU)</cell></row><row><cell>LSTM</cell><cell>Long short term memory [82]</cell><cell>million parameters ~3.93</cell><cell></cell><cell>98.5 (CPU)</cell></row><row><cell>TRFsmall</cell><cell></cell><cell>million parameters ~2.24</cell><cell></cell><cell>150.5 (CPU)</cell></row><row><cell>TRFmed</cell><cell>Transformer [84]</cell><cell>parameters ~19.1 million</cell><cell>[83]</cell><cell>1,252.5 (CPU) / 21.2 (GPU)</cell></row><row><cell></cell><cell></cell><cell>parameters ~279</cell><cell></cell><cell>37,759.8</cell></row><row><cell>TRFbig</cell><cell></cell><cell>million</cell><cell></cell><cell>(CPU) /</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>58.9 (GPU)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>correlation ð[ð£ Ï â² (ð), ð£ Ï â²â² (ð)] for all corpus pairs (ð, ð) where ð£ Ï â² denotes either h, L or K on the encoding level Ï. Likewise for ð£ Ï â²â² . In addition, we also consider H computed on the basis of the CrÃºbadÃ¡n word frequency information (eq. 1), denoted as HCr in what follows. Both ð£ Ï â² (ð) and ð£ Ï â²â² (ð) are logged. Pearson correlations range from -1 to 1, with higher absolute values indicating stronger associations. Positive values reflect positive associations, while negative values reflect negative associations between distributions. This allows us to assess both similarities and dissimilarities between entropy and length distributions across corpora.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Best LM per level. 1st column: LM. 2nd column: word level. 3rd column: character level. 4th column: BPE level. For each of NÎº = 4,297 different documents, each column lists, for a given symbolic level, the number of documents such that the LM in the given row is the best, i.e., ð Ìðððð = ð Ìð³ð´ . On all three levels, PAQ is the best LM in more than 90%.</figDesc><table><row><cell>LM</cell><cell>Words</cell><cell>Characters</cell><cell>BPE</cell></row><row><cell>PPM2</cell><cell>0 (0.00%)</cell><cell>0 (0.00%)</cell><cell>0 (0.00%)</cell></row><row><cell>PPM6</cell><cell>18 (0.42%)</cell><cell>261 (6.07%)</cell><cell>85 (1.98%)</cell></row><row><cell>PAQ</cell><cell cols="3">4,241 (98.70%) 3,960 (92.16%) 4,182 (97.32%)</cell></row><row><cell>LSTM</cell><cell>1 (0.02%)</cell><cell>3 (0.07%)</cell><cell>0 (0.00%)</cell></row><row><cell>TRFsmall</cell><cell>0 (0.00%)</cell><cell>0 (0.00%)</cell><cell>0 (0.00%)</cell></row><row><cell>TRFmed</cell><cell>3 (0.07%)</cell><cell>0 (0.00%)</cell><cell>1 (0.02%)</cell></row><row><cell>TRFbig</cell><cell>34 (0.79%)</cell><cell>73 (1.70%)</cell><cell>29 (0.67%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell>NÏ</cell><cell>1,638</cell><cell>1,600</cell><cell>1,600</cell></row><row><cell>ð Ìnone</cell><cell>-0.69</cell><cell>-0.69</cell><cell>-0.69</cell></row><row><cell>ð Ìphylo</cell><cell>-0.69</cell><cell>-0.62</cell><cell>-0.62</cell></row><row><cell>ð Ìgeo</cell><cell>-0.69</cell><cell>-0.62</cell><cell>-0.62</cell></row><row><cell>ð Ìboth</cell><cell>-0.69</cell><cell>-0.60</cell><cell>-0.61</cell></row></table><note><p>Association between ð Ìðððð and L across corpora. 1st row: number of individual correlation coefficients, NÏ. 2nd -4th column, type of correlation coefficient Ïnone, Ïphylo, Ïgeo and Ïboth. 2nd -3rd column: associations on each level (words, characters, BPE), listed quantities for Ï are median values per parameter combination. Median values for ð Ìðððð (words) include HCr based on the CrÃºbadÃ¡n word lists. Note that for ð Ìðððð on the BPE level, distributions are correlated with L in characters. ð Ìðððð (words) ð Ìðððð (chars) ð Ìðððð (BPE)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 : FMA-estimated fixed effects on each outcome per symbolic level.</head><label>5</label><figDesc>Each cell lists ð· Ìð, here denoted as Î² and the corresponding p-value (cf. Sect. 2.6.2 for details). 1st column: fixed effect. 2nd -4th column: results for ð Ìðððð . 5th + 6th column: results for L. 7th -9th column: results for ð² Ìðððð . Cell content is highlighted in bold if p &lt; .05.</figDesc><table><row><cell></cell><cell>ð Ìðððð</cell><cell></cell><cell>L</cell><cell></cell><cell></cell><cell>ð² Ìðððð</cell><cell></cell></row><row><cell>word</cell><cell>char</cell><cell>BPE</cell><cell>word</cell><cell>char</cell><cell>word</cell><cell>char</cell><cell>BPE</cell></row><row><cell>Population size</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>To train the TRFmed, TRFbig language models, we acknowledge support by the state of <rs type="person">Baden-WÃ¼rttemberg</rs> through bwHPC and the <rs type="funder">German Research Foundation (DFG)</rs> through grant <rs type="grantNumber">INST 35/1597-1 FUGG</rs>.</p></div>
			</div>
			<div type="funding">
<div><head>Funding</head><p>We received no specific funding for this work</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_B5J3Nkm">
					<idno type="grant-number">INST 35/1597-1 FUGG</idno>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Availability of data and materials</head><p>All data and code (Stata v18.0 and Python v3.6.8) needed to replicate our analyses are available at https://osf.io/xdwjc/. In addition, interactive results and visualisations are available online at https://www.owid.de/plus/tradeoffvis/.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Random effects and slope models</head><p>As argued in the introduction (Sect. 1), an approach that focuses exclusively on random effects ignores variation within language families and geographical units <ref type="bibr" target="#b48">[49]</ref><ref type="bibr" target="#b49">[50]</ref><ref type="bibr" target="#b50">[51]</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b127">128]</ref>. We thus proceed by including random slopes, i.e., we allow the effect of population size to vary across different groups, representing deviations from the overall mean linear effect of speaker population size. The methodological question in this context is, which random effects should include random slopes?</p><p>We do not include a random slope for language since population size does not vary within languages. Due to geographic proximity and phylogenetic non-independence, it makes sense to include random slopes for macro-area, macro-family, and sub-family as geographic and phylogenetic structures can be critical for understanding language diversity and evolution, which justifies including these random slopes to account for shared inheritance and environmental factors <ref type="bibr" target="#b48">[49]</ref>.</p><p>There are, however, no a priori reasons for whether or not random slopes for writing script and corpus also make sense since both are not necessarily tied to population size or linguistic features in a way that would suggest significant variability in the effect of population size across different scripts. Including random slopes for both without clear justification could lead of writing script as a random slope does not seem to be very important. However, including corpus seems to make a difference for all seven parameter combinations, except for â Ìððð ð¡ on the word level.</p><p>Regarding the FMA-estimated effect (ð½ Ìð¥) of speaker population size, Figure <ref type="figure">8c</ref> shows that in the first scenario, the results for each outcome and symbolic level are qualitatively identical to the random-effects-only approach (see Figure <ref type="figure">7b</ref>): (i) a significant positive effect of population size on â Ìððð ð¡ , (ii) a significant negative effect on L, and (iii) no significant evidence for any effect on ð¾ Ìððð ð¡ . This again suggests a trade-off between entropy and length.</p><p>In the second scenario, while there remains stable evidence for an entropy-length trade-off for words as symbols, at the character level, neither the positive effect on â Ìððð ð¡ nor the negative effect on L reaches significance at the 5% level (p = 0.064 for â Ìððð ð¡ and p = 0.086 for L).</p><p>Unexpectedly, for ð¾ Ìððð ð¡ , there is a significant positive effect of population size across all three levels. Future work could determine whether this indicates a true effect or if this result arises from an overly complex model structure that is unable to detect true effects accurately.</p><p>To foster such endeavours, we provide a dataset on our OSF repository (https://osf.io/93csg/) that contains all information needed to replicate our findings and conduct follow-up investigations. This dataset includes estimates for both (i) â Ìððð ð¡ and ð¾ ððð ð¡ and (ii) LMspecific estimates, â Ìð¿ð and ð¾ Ìð¿ð for the seven investigated LMs across all three symbolic levels and two different outcome transformations (standardised vs. logged, see Sect. 2.6.2 for details), totalling information for R = 3,520,000 individual models.</p><p>Further note that as mentioned above (see Sect. 2.6.2), we chose AIC ð as the criterion to weigh models. AIC is computed by balancing goodness-of-fit with model complexity, i.e., -2â Ìð + 2(ð ð ð + ð ð ð ), where â Ìð denotes the maximized log-likelihood of model j and ð ð ð and ð ð ð are the impossible not to make the shift from the one sense of information, [â¦], i.e., information = signal sequence, to the other sense, information = what is expressed by the signal sequence".</p><p>In information theory, messages are only treated as signal sequences but not as "contentbearing entities" <ref type="bibr" target="#b144">[145]</ref>. Bar-Hillel shows that only under very special circumstances, which do not typically apply to human language, can we infer the amount of information conveyed merely by the length of a message. There is no logical connection between the concept of semantic information (i.e., what is "expressed" by a transmitted signal sequence) and the rarity or improbability of the signal, which is measured by h. Put differently, information in the information-theoretic sense has nothing to do with meaning, but only measures the amount of uncertainty that is removed when receiving a message or the average amount of information learned about upcoming symbols when observing a symbol of the message.</p><p>To illustrate this, consider the sentence "Rain occurs most frequently in spring." Using LCC word frequency information (cf. Sect. 0), the predictive uncertainty or self-information, ðª, of the sentence, calculated based on a unigram LM (see eq. 1), is ðª ð¤ððð 1ðððð = 12.531 bits per symbol (bps). Now compare this with the sentence "Liquid precipitation occurs most frequently in spring." If <ref type="bibr" target="#b141">[142,</ref><ref type="bibr" target="#b142">143]</ref> were correct, we should expect that this sentence has lower per-symbol information content, as it contains the same message but uses 7 instead of 6 words. However, the self-information for this sentence is ðª ð¤ððð 1ðððð = 13.646 bps and therefore higher. This is due to the fact that "rain" is much more frequent in our example data (ð ð¿ð¶ð¶ = 1,594) than "liquid" (ð ð¿ð¶ð¶ = 324) or "precipitation" (ð ð¿ð¶ð¶ = 96). Since a unigram LM does not consider any contextual information, it makes sense to additionally use a large LM to compute self-information. Using a GPT-2 model with ~1.5 billion parameters, the self-information of the first sentence amounts to ðª ð¤ððð ðºðð-2 = 0.952 bps. Again, the self-information of the second sentence is higher with ðª ð¤ððð ðºðð-2 = 0.965 bps.</p><p>is based on audio recordings of the New Testament of the Bible. In Supporting Information S10, we outline how phoneme sequences were prepared and used to train PAQ in order to compute an entropy estimate at the phonemic level, denoted as â Ìð. For ð ð¿ = 26 languages from 8 different language families, we have estimates at both the phonemic level from the VoxClamantis corpus and the written level from the BibleNT corpus. Using these estimates, we calculated the Pearson correlation between the logarithms of â Ìð and â Ìððð ð¡ (ð). At the word level, the correlation is Ïwords = 0.32, while at the character and sub-word levels, the correlations are stronger, with Ïcharacters = 0.58 and ÏBPE = 0.54, respectively. The results also suggest an inverse relationship between â Ìð and the logged text length ð¿(ð), with a more pronounced effect at the character level (Ïcharacters = -0.52) compared to the word level (Ïcharacters = -0.26). While these findings are preliminary, due to the limited sample size, the varying quality and quantity of Bible readings per language, and other caveats <ref type="bibr" target="#b148">[149]</ref>, they suggest that the complexity-efficiency trade-off observed in written language may also extend to spoken language. We believe this indicates the potential for further exploration of spoken language as a valuable direction for future research.</p><p>With these results in mind, we conclude this paper by discussing potential reasons for a tradeoff between complexity and message length. This discussion is based on our fourth main result: using a multi-model multilevel approach detailed in Sect. 2.6.2, we presented findings in Sect. 3.2 indicating that the trade-off is influenced by the social environment in which languages are learned and used. Specifically, languages with more speakers tend to be more complex. At first glance, this result contrasts with previous research suggesting that languages spoken in larger communities tend to be less complex <ref type="bibr" target="#b149">[150]</ref><ref type="bibr" target="#b150">[151]</ref><ref type="bibr" target="#b151">[152]</ref><ref type="bibr" target="#b152">[153]</ref><ref type="bibr" target="#b153">[154]</ref><ref type="bibr" target="#b154">[155]</ref><ref type="bibr" target="#b155">[156]</ref>, as larger communities are assumed to favour simple and predictable language structures. At the same time, our results</p><p>indicate that languages with more speakers tend to produce shorter messages, i.e., are more efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Competing interests</head><p>We have no competing interests to declare. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Speech and Language processing: an introduction to natural language processing, computational Linguistics, and speech recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Martin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Pearson Education</publisher>
			<pubPlace>Upper Saddle River</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Modern language models refute Chomsky&apos;s approach to language</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Piantadosi</surname></persName>
		</author>
		<idno>doi:lingbuzz/007180</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Accurate structure prediction of biomolecular interactions with AlphaFold 3</title>
		<author>
			<persName><forename type="first">J</forename><surname>Abramson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dunger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41586-024-07487-w</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>cited 3 Jun 2024</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">N</forename><surname>Gruver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Finzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2310.07820</idno>
	</analytic>
	<monogr>
		<title level="s">Large Language Models Are Zero-Shot Time Series Forecasters. arXiv</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep learning enables rapid identification of potent DDR1 kinase inhibitors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zhavoronkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">A</forename><surname>Ivanenkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aliper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Veselov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Aladinskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Aladinskaya</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41587-019-0224-x</idno>
	</analytic>
	<monogr>
		<title level="j">Nat Biotechnol</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1038" to="1040" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Machine learning for molecular and materials science</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cartwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Isayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Walsh</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41586-018-0337-2</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">559</biblScope>
			<biblScope unit="page" from="547" to="555" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Predicting effects of noncoding variants with deep learningbased sequence model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">G</forename><surname>Troyanskaya</surname></persName>
		</author>
		<idno type="DOI">10.1038/nmeth.3547</idno>
	</analytic>
	<monogr>
		<title level="j">Nat Methods</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="931" to="934" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tackling Climate Change with Machine Learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rolnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Donti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Kaack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kochanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sankaran</surname></persName>
		</author>
		<idno type="DOI">10.1145/3485128</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Comput Surv</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="1" to="96" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Language Models are Few-Shot Learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/file/" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020. 1457c0d6</date>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
	<note>bfcb4967418bfb8ac142f64a-Paper.pdf</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ideal learning&apos; of natural language: Positive results about learning from positive evidence</title>
		<author>
			<persName><forename type="first">N</forename><surname>Chater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>VitÃ¡nyi</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jmp.2006.10.002</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Psychology</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="135" to="163" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Large Language Models Demonstrate the Potential of Statistical Learning in Language</title>
		<author>
			<persName><forename type="first">P</forename><surname>Contreras Kallens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Kristensen-Mclachlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Christiansen</surname></persName>
		</author>
		<idno type="DOI">10.1111/cogs.13256</idno>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page">13256</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Modelling</forename><surname>Grindrod</surname></persName>
		</author>
		<author>
			<persName><surname>Language</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2404.09579</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cognitive Network Science Reveals Bias in GPT-3, GPT-3.5 Turbo, and GPT-4 Mirroring Math Anxiety in High-School Students</title>
		<author>
			<persName><forename type="first">K</forename><surname>Abramski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Citraro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lombardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rossetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stella</surname></persName>
		</author>
		<idno type="DOI">10.3390/bdcc7030124</idno>
	</analytic>
	<monogr>
		<title level="j">BDCC</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">124</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Data Compression Explained</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mahoney</surname></persName>
		</author>
		<ptr target="http://mattmahoney.net/dc/dce.html" />
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Dell Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">How Efficiency Shapes Human Language</title>
		<author>
			<persName><forename type="first">E</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Futrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Piandadosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dautriche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mahowald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bergen</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2019.02.003</idno>
	</analytic>
	<monogr>
		<title level="j">TRENDS in Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="389" to="407" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Parallel texts: using translational equivalents in linguistic typology. Language Typology and Universals</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cysouw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>WÃ¤lchli</surname></persName>
		</author>
		<idno type="DOI">10.1524/stuf.2007.60.2.95</idno>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="95" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Zipf&apos;s law of abbreviation as a language universal</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bentz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ferrer-I-Cancho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Leiden Workshop on Capturing Phylogenetic Algorithms for Linguistics</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Bentz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>JÃ¤ger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Yanovich</surname></persName>
		</editor>
		<meeting>the Leiden Workshop on Capturing Phylogenetic Algorithms for Linguistics<address><addrLine>TÃ¼bingen</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
		<respStmt>
			<orgName>University of TÃ¼bingen</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The Low-complexity-belt: Evidence For Large-scale Language Contact In Human Prehistory?</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bentz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cuskley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mccrohon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>BarcelÃ³-Coblijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>FehÃ©r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Verhoef</surname></persName>
		</author>
		<ptr target="http://evolang.org/neworleans/papers/93.html" />
	</analytic>
	<monogr>
		<title level="m">The Evolution of Language: Proceedings of the 11th International Conference</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Language structure is influenced by the number of speakers but seemingly not by the proportion of non-native speakers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Koplenig</surname></persName>
		</author>
		<idno type="DOI">10.1098/rsos.181274</idno>
	</analytic>
	<monogr>
		<title level="j">Royal Society Open Science</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">181274</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Quantifying the efficiency of written language</title>
		<author>
			<persName><forename type="first">A</forename><surname>Koplenig</surname></persName>
		</author>
		<idno type="DOI">10.1515/lingvan-2019-0057</idno>
	</analytic>
	<monogr>
		<title level="j">Linguistics Vanguard</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">20190057</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Language structure is influenced by the proportion of non-native speakers: A reply to Koplenig (2019)</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kauhanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Einhaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Walkden</surname></persName>
		</author>
		<idno type="DOI">10.1093/jole/lzad005</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Language Evolution</title>
		<imprint>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Still no evidence for an effect of the proportion of non-native speakers on language complexity --A response to Kauhanen</title>
		<author>
			<persName><forename type="first">A</forename><surname>Koplenig</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2305.00217</idno>
	</analytic>
	<monogr>
		<title level="j">Einhaus &amp; Walkden</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note>cited 8 May 2023</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A Quantitative Approach to the Morphological Typology of Language</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Greenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of American Linguistics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="178" to="194" />
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A Comparison Between Morphological Complexity Measures: Typological Data vs. Language Corpora</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bentz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ruzsics</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Koplenig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Samardzic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Computational Linguistics for Linguistic Complexity (CL4LC)</title>
		<meeting>the Workshop on Computational Linguistics for Linguistic Complexity (CL4LC)<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Lexical typology through similarity semantics: Toward a semantic map of motion verbs</title>
		<author>
			<persName><forename type="first">B</forename><surname>WÃ¤lchli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cysouw</surname></persName>
		</author>
		<idno type="DOI">10.1515/ling-2012-0021</idno>
	</analytic>
	<monogr>
		<title level="j">Linguistics</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Word Order Typology through Multilingual Word Alignment</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ãstling</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P15-2034</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="205" to="211" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The Entropy of Words-Learnability and Expressivity across More than 1000 Languages</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bentz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Alikaniotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cysouw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ferrer-I-Cancho</surname></persName>
		</author>
		<idno type="DOI">10.3390/e19060275</idno>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">275</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The statistical trade-off between word order and word structure -Large-scale evidence for the principle of least effort</title>
		<author>
			<persName><forename type="first">A</forename><surname>Koplenig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wolfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>MÃ¼ller-Spitzer</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0173614</idno>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Smith</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">173614</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A surprisal-duration trade-off across and within the world&apos;s languages</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pimentel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Salesky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Teufel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cotterell</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.73</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Online and Punta</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing. Online and Punta<address><addrLine>Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="949" to="962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Some remarks on the use of Bible translations as parallel texts in linguistic research</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>De Vries</surname></persName>
		</author>
		<idno type="DOI">10.1524/stuf.2007.60.2.148</idno>
	</analytic>
	<monogr>
		<title level="j">Sprachtypologie und Universalienforschung</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="148" to="157" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Advantages and disadvantages of using parallel texts in typological investigations. Language Typology and Universals</title>
		<author>
			<persName><forename type="first">B</forename><surname>WÃ¤lchli</surname></persName>
		</author>
		<idno type="DOI">10.1524/stuf.2007.60.2.118</idno>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="118" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Parallel Data, Tools and Interfaces in OPUS</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC&apos;12 Proceedings</title>
		<meeting><address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<publisher>ELRA</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2214" to="2218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Creating a Massively Parallel Bible Corpus</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M ;</forename><surname>Cysouw</surname></persName>
		</author>
		<author>
			<persName><surname>Conference</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Choukri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Declerck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Loftsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Maegaard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mariani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)</title>
		<meeting>the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)<address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association (ELRA</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Verbs of letting in Germanic and Romance languages: A quantitative investigation based on a parallel corpus of film subtitles</title>
		<author>
			<persName><forename type="first">N</forename><surname>Levshina</surname></persName>
		</author>
		<idno type="DOI">10.1075/lic.16.1.04lev</idno>
	</analytic>
	<monogr>
		<title level="j">LiC</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="84" to="117" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Building Large Monolingual Dictionaries at the Leipzig Corpora Collection: From 100 to 200 Languages</title>
		<author>
			<persName><forename type="first">D</forename><surname>Goldhahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eckart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Quasthoff</surname></persName>
		</author>
		<ptr target="http://www.lrec-conf.org/proceedings/lrec2012/pdf/327_Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC&apos;12)</title>
		<meeting>the Eighth International Conference on Language Resources and Evaluation (LREC&apos;12)<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="759" to="765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The CrÃºbadÃ¡n Project: Corpus building for under-resourced languages</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Scannell</surname></persName>
		</author>
		<ptr target="http://cs.slu.edu/~scannell/pub/wac3.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Web as Corpus Workshop: Building and Exploring Web Corpora</title>
		<meeting>the 3rd Web as Corpus Workshop: Building and Exploring Web Corpora</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="5" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A large quantitative analysis of written language challenges the idea that all languages are equally complex</title>
		<author>
			<persName><forename type="first">A</forename><surname>Koplenig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wolfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meyer</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41598-023-42327-3</idno>
	</analytic>
	<monogr>
		<title level="j">Sci Rep</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">15351</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Meaning and Measures: Interpreting and Evaluating Complexity Metrics</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ehret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blumenthal-DramÃ©</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bentz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Berdicevskis</surname></persName>
		</author>
		<idno type="DOI">10.3389/fcomm.2021.640510</idno>
	</analytic>
	<monogr>
		<title level="j">Front Commun</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">640510</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A linguistic axiom challenged</title>
		<author>
			<persName><forename type="first">G</forename><surname>Sampson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Language complexity as an evolving variable</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Sampson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Gil</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Trudgill</surname></persName>
		</editor>
		<meeting><address><addrLine>Oxford</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Measuring language complexity: challenges and opportunities</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ehret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Berdicevskis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bentz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blumenthal-DramÃ©</surname></persName>
		</author>
		<idno type="DOI">10.1515/lingvan-2022-0133</idno>
	</analytic>
	<monogr>
		<title level="j">Linguistics Vanguard</title>
		<imprint>
			<biblScope unit="page">0</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<title level="m">Elements of information theory</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Hoboken</surname></persName>
		</editor>
		<editor>
			<persName><surname>Wiley-Interscience</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Information Theory as a Bridge Between Language Function and Language Form</title>
		<author>
			<persName><forename type="first">R</forename><surname>Futrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hahn</surname></persName>
		</author>
		<idno type="DOI">10.3389/fcomm.2022.657725</idno>
	</analytic>
	<monogr>
		<title level="j">Front Commun</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">657725</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Entropy Rate Estimation for English via a Large Cognitive Experiment Using Mechanical Turk</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tanaka-Ishii</surname></persName>
		</author>
		<idno type="DOI">10.3390/e21121201</idno>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">1201</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">C</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pimentel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wiher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cotterell</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2202.00666</idno>
	</analytic>
	<monogr>
		<title level="j">Locally Typical Sampling</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Three approaches to the quantitative definition of information</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Kolmogorov</surname></persName>
		</author>
		<idno type="DOI">10.1080/00207166808803030</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Mathematics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="157" to="168" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">The Complexity and Entropy of Literary Styles</title>
		<author>
			<persName><forename type="first">I</forename><surname>Kontoyiannis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">97</biblScope>
		</imprint>
		<respStmt>
			<orgName>Department of Statistics, Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">NSF Technical Report</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Kolmogorov Complexity, Data Compression, and Inference</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-94-009-5113-6_2</idno>
	</analytic>
	<monogr>
		<title level="m">The Impact of Processing Techniques on Communications</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Skwirzynski</surname></persName>
		</editor>
		<meeting><address><addrLine>Dordrecht; Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1985">1985</date>
			<biblScope unit="page" from="23" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">On the proper role of linguistically-oriented deep net analysis in linguistic theorizing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Baroni</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2106.08694</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Solving Galton&apos;s problem: practical solutions for analysing language diversity and evolution</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bromham</surname></persName>
		</author>
		<idno type="DOI">10.31234/osf.io/c8v9</idno>
	</analytic>
	<monogr>
		<title level="j">PsyArXiv</title>
		<imprint>
			<date type="published" when="2022-05">2022 May</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Neighbours and relatives: accounting for spatial distribution when testing causal hypotheses in cultural evolution</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bromham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Yaxley</surname></persName>
		</author>
		<idno type="DOI">10.1017/ehs.2023.23</idno>
	</analytic>
	<monogr>
		<title level="j">Evolut Hum Sci</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Statistical bias control in typology</title>
		<author>
			<persName><forename type="first">GuzmÃ¡n</forename><surname>Naranjo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
		<idno type="DOI">10.1515/lingty-2021-0002</idno>
	</analytic>
	<monogr>
		<title level="j">Linguistic Typology</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="605" to="670" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Cross-national analyses require additional controls to account for the non-independence of nations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Claessens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kyritsis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">D</forename><surname>Atkinson</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41467-023-41486-1</idno>
	</analytic>
	<monogr>
		<title level="j">Nat Commun</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">5776</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Societies of strangers do not speak less complex languages</title>
		<author>
			<persName><forename type="first">O</forename><surname>Shcherbakova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Haynie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Passmore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Gray</surname></persName>
		</author>
		<idno type="DOI">10.1126/sciadv.adf7704</idno>
	</analytic>
	<monogr>
		<title level="j">Science Advances</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">7704</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Languages with more speakers tend to be harder to (machine-)learn</title>
		<author>
			<persName><forename type="first">A</forename><surname>Koplenig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wolfer</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41598-023-45373-z</idno>
	</analytic>
	<monogr>
		<title level="j">Sci Rep</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">18521</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Text Recycling Research Project. Understanding text recycling. A guide for researchers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Moskovitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pemberton</surname></persName>
		</author>
		<ptr target="https://textrecycling.org/files/2021/06/Understanding-Text-Recycling_A-Guide-for-Researchers-V.1.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Languages Through the Looking Glass of BPE Compression</title>
		<author>
			<persName><forename type="first">X</forename><surname>Gutierrez-Vasques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bentz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>SamardÅ¾iÄ</surname></persName>
		</author>
		<idno type="DOI">10.1162/coli_a_00489</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="943" to="1001" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Global Dataset Ethnologue: Languages of the World, Twentieth edition</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Simons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Fennig</surname></persName>
		</author>
		<ptr target="http://www.ethnologue.com" />
	</analytic>
	<monogr>
		<title level="j">SIL</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Global predictors of language endangerment and the future of linguistic diversity</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bromham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dinnage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>SkirgÃ¥rd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cardillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Meakins</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41559-021-01604-y</idno>
	</analytic>
	<monogr>
		<title level="j">Nat Ecol Evol</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="163" to="173" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">H</forename><surname>HammarstrÃ¶m</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Forkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Haspelmath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bank</surname></persName>
		</author>
		<idno type="DOI">10.5281/ZENODO.8131084</idno>
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Simons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Fennig</surname></persName>
		</author>
		<title level="m">Ethnologue: languages of Africa and Europe</title>
		<meeting><address><addrLine>Dallas, TX</addrLine></address></meeting>
		<imprint>
			<publisher>SIL</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Twentieth edition</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Ethnologue: Languages of the World</title>
		<author>
			<persName><forename type="first">Gary</forename><forename type="middle">F</forename><surname>Simons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Fennig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>SIL International</publisher>
			<pubPlace>Dallas, Texas</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Assessing Endangerment: Expanding Fishman&apos;s GIDS</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Simons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Revue Roumaine de Linguistique</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="103" to="120" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Corpus Linguistics and Translation Studies -Implications and Applications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Baker</surname></persName>
		</author>
		<idno type="DOI">10.1075/z.64.15bak</idno>
	</analytic>
	<monogr>
		<title level="m">Text and Technology</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Baker</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Francis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Tognini-Bonelli</surname></persName>
		</editor>
		<meeting><address><addrLine>Amsterdam</addrLine></address></meeting>
		<imprint>
			<publisher>John Benjamins Publishing Company</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page">233</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Are All Languages Equally Hard to Language-Model?</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Mielke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eisner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Roark</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-2085</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Short Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="536" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Global-scale phylogenetic linguistic inference from lexical resources</title>
		<author>
			<persName><forename type="first">G</forename><surname>JÃ¤ger</surname></persName>
		</author>
		<idno type="DOI">10.1038/sdata.2018.189</idno>
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">180189</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">CLDF dataset derived from Wichmann et al.&apos;s &quot;ASJP Database</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wichmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Holman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Forkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tresoldi</surname></persName>
		</author>
		<idno type="DOI">10.5281/ZENODO.3835942</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">A Sociolinguistic Typology for Describing National Multilingualism</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Î</forename><surname>Stewart</surname></persName>
		</author>
		<idno type="DOI">10.1515/9783110805376.531</idno>
	</analytic>
	<monogr>
		<title level="j">DE</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Fishman</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="531" to="545" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
	<note>Readings in the Sociology of Language</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">The evolution of language families is shaped by the environment beyond neutral drift</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bentz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dediu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Verkerk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>JÃ¤ger</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41562-018-0457-6</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="816" to="821" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Mixed effect models for genetic and areal dependencies in linguistic typology</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pontillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linguistic Typology</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Prediction and Entropy of Printed English. Bell System Technical</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
		<idno type="DOI">10.1002/j.1538-7305.1951.tb01366.x</idno>
	</analytic>
	<monogr>
		<title level="j">Journal</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="50" to="64" />
			<date type="published" when="1951">1951</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">On the intelligibility of the universe and the notions of simplicity, complexity and irreducibility</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Chaitin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:math/0210035</idno>
		<ptr target="http://arxiv.org/abs/math/0210035" />
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>cited 25 Mar 2021</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Language Modeling Is Compression</title>
		<author>
			<persName><forename type="first">G</forename><surname>DelÃ©tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ruoss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P-A</forename><surname>Duquenne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Catt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Genewein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mattern</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2309.10668</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Generalized Kraft Inequality and Arithmetic Coding</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Rissanen</surname></persName>
		</author>
		<idno type="DOI">10.1147/rd.203.0198</idno>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="198" to="203" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Martin</surname></persName>
		</author>
		<ptr target="https://web.stanford.edu/~jurafsky/slp3/" />
		<title level="m">Speech and Language Processing</title>
		<imprint/>
	</monogr>
	<note>rd ed. 2021. Available</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Data Compression Using Adaptive Coding and Partial String Matching</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cleary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Witten</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCOM.1984.1096090</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Communications</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="396" to="402" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">PPM: one step to practicality</title>
		<author>
			<persName><forename type="first">D</forename><surname>Shkarin</surname></persName>
		</author>
		<idno type="DOI">10.1109/DCC.2002.999958</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings DCC 2002 Data Compression Conference</title>
		<meeting>DCC 2002 Data Compression Conference<address><addrLine>Snowbird, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Comput. Soc</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="202" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">I</forename><surname>Pavlov</surname></persName>
		</author>
		<ptr target="https://7-zip.org/" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">A Machine Learning Perspective on Predictive Coding with PAQ8</title>
		<author>
			<persName><forename type="first">B</forename><surname>Knoll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><surname>De</surname></persName>
		</author>
		<idno type="DOI">10.1109/DCC.2012.44</idno>
	</analytic>
	<monogr>
		<title level="m">Data Compression Conference</title>
		<meeting><address><addrLine>Snowbird, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="377" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lattimore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bhoopchand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mattern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1910.01526</idno>
		<imprint>
			<date type="published" when="2019-03-27">2019. 27 Mar 2023</date>
		</imprint>
	</monogr>
	<note>Gated Linear Networks. cited</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName><surname>Paq8</surname></persName>
		</author>
		<ptr target="http://mattmahoney.net/dc/paq8l.zip" />
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Adaptive weighing of context models for lossless data compression</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mahoney</surname></persName>
		</author>
		<ptr target="http://hdl.handle.net/11141/154" />
	</analytic>
	<monogr>
		<title level="j">Florida Tech</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">NNCP v3.1: Lossless Data Compression with Transformer</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bellard</surname></persName>
		</author>
		<ptr target="https://bellard.org/nncp/" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Attention is All You Need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">The zero-frequency problem: estimating the probabilities of novel events in adaptive text compression</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Bell</surname></persName>
		</author>
		<idno type="DOI">10.1109/18.87000</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1085" to="1094" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context. arXiv:190102860 [cs, stat</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1901.02860" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>cited 22 Oct 2020</note>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Lossless Data Compression with Neural Networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bellard</surname></persName>
		</author>
		<ptr target="https://bellard.org/nncp/nncp.pdf" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">NNCP v2: Lossless Data Compression with Transformer</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bellard</surname></persName>
		</author>
		<ptr target="https://bellard.org/nncp/nncp_v2.1.pdf" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<idno type="DOI">10.1038/323533a0</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>cited 15 May 2023</note>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Language Models are Unsupervised Multitask Learners</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">HuggingFace&apos;s Transformers: State-of-the-art Natural Language Processing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1910.03771</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Unicode Text Segmentation</title>
		<author>
			<persName><forename type="first">Unicode</forename><surname>Consortium</surname></persName>
		</author>
		<ptr target="http://www.unicode.org/reports/tr29/#Word_Boundaries" />
	</analytic>
	<monogr>
		<title level="m">UnicodeÂ® Standard Annex #29</title>
		<imprint>
			<date type="published" when="2019-07-23">2019. 23 Jul 2019</date>
		</imprint>
	</monogr>
	<note>Internet</note>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Stata tip 129: Efficiently processing textual data with Stata&apos;s new Unicode features</title>
		<author>
			<persName><forename type="first">A</forename><surname>Koplenig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stata Journal</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="287" to="289" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">What Kind of Language Is Hard to Language-Model?</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Mielke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gorman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Roark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eisner</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1491</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4975" to="4989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Neural Machine Translation of Rare Words with Subword Units</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Martin</surname></persName>
		</author>
		<ptr target="https://web.stanford.edu/~jurafsky/slp3/" />
		<title level="m">Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition with Language Models. 3</title>
		<imprint/>
	</monogr>
	<note>rd ed. 2024</note>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Pull out all the stops: Textual analysis via punctuation sequences</title>
		<author>
			<persName><forename type="first">Anm</forename><surname>Darmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Howison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Porter</surname></persName>
		</author>
		<idno type="DOI">10.1017/S0956792520000157</idno>
	</analytic>
	<monogr>
		<title level="j">Eur J Appl Math</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1069" to="1105" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cysouw</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.1296780</idno>
		<title level="m">The Unicode Cookbook For Linguists: Managing Writing Systems Using Orthography Profiles</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Language Science Press</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">The indeterminacy of word segmentation and the nature of morphology and syntax</title>
		<author>
			<persName><forename type="first">M</forename><surname>Haspelmath</surname></persName>
		</author>
		<idno type="DOI">10.1515/flin.2011.002</idno>
	</analytic>
	<monogr>
		<title level="j">Folia Linguistica</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><surname>Grammatik</surname></persName>
		</author>
		<author>
			<persName><surname>WÃ¶rter</surname></persName>
		</author>
		<ptr target="http://www.degruyter.com/view/books/9783110262339/9783110262339.345/9783110262339.345.xml" />
		<title level="m">Sprachliches Wissen zwischen Lexikon und Grammatik</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Engelberg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Holler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Proost</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Boston</addrLine></address></meeting>
		<imprint>
			<publisher>DE GRUYTER</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">Introduction to Theoretical Linguistics. 1st ed</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lyons</surname></persName>
		</author>
		<idno type="DOI">10.1017/CBO9781139165570</idno>
		<imprint>
			<date type="published" when="1968">1968</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Written Language Is as Natural as Spoken language: A Biolinguistic Perspective</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Joshi</surname></persName>
		</author>
		<idno type="DOI">10.1080/02702710600846803</idno>
	</analytic>
	<monogr>
		<title level="j">Reading Psychology</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="263" to="311" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Graphemic Analysis and the Spoken Language Bias</title>
		<author>
			<persName><forename type="first">K</forename><surname>Berg</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2016.00388</idno>
	</analytic>
	<monogr>
		<title level="j">Front Psychol</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">The Relation Between Written and Spoken Language</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chafe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tannen</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev.an.16.100187.002123</idno>
	</analytic>
	<monogr>
		<title level="j">Annu Rev Anthropol</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="383" to="407" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<title level="m" type="main">Defining the word</title>
		<author>
			<persName><forename type="first">M</forename><surname>Haspelmath</surname></persName>
		</author>
		<idno type="DOI">10.1080/00437956.2023.2237272</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="283" to="297" />
		</imprint>
	</monogr>
	<note>WORD</note>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<title level="m" type="main">Informativeness of linguistic unit boundaries. Apollo -University of Cambridge Repository</title>
		<author>
			<persName><forename type="first">J</forename><surname>Geertzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Blevins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Milin</surname></persName>
		</author>
		<idno type="DOI">10.17863/cam.69</idno>
		<imprint>
			<date type="published" when="2016-07-23">2016. 23 Jul 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<title level="m" type="main">Language Typology and Syntactic Description. 2nd ed</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nichols</surname></persName>
		</author>
		<idno type="DOI">10.1017/CBO9780511618437.003</idno>
		<editor>Shopen T</editor>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="page" from="169" to="240" />
		</imprint>
	</monogr>
	<note>Inflectional morphology. 2nd ed</note>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Introduction: problems of polysynthesis</title>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H-J</forename><surname>Sasse</surname></persName>
		</author>
		<idno type="DOI">10.1524/9783050080956.1</idno>
	</analytic>
	<monogr>
		<title level="m">Problems of Polysynthesis</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H-J</forename><surname>Sasse</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
	<note>AKADEMIE VERLAG</note>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Entropy estimation of symbol sequences</title>
		<author>
			<persName><forename type="first">T</forename><surname>SchÃ¼rmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Grassberger</surname></persName>
		</author>
		<idno type="DOI">10.1063/1.166191</idno>
	</analytic>
	<monogr>
		<title level="j">Chaos: An Interdisciplinary Journal of Nonlinear Science</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">414</biblScope>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<monogr>
		<title level="m" type="main">Information theory, inference, and learning algorithms</title>
		<author>
			<persName><forename type="first">Djc</forename><surname>Mackay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge, UK; New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title level="m" type="main">Probability and information</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Yaglom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Im ;</forename><surname>Yaglom</surname></persName>
		</author>
		<author>
			<persName><surname>Dordrecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Holland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">:</forename><surname>Boston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Reidel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<pubPlace>U.S.A. by Kluwer Boston</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">What is information?</title>
		<author>
			<persName><forename type="first">C</forename><surname>Adami</surname></persName>
		</author>
		<idno type="DOI">10.1098/rsta.2015.0230</idno>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences</title>
		<imprint>
			<biblScope unit="volume">374</biblScope>
			<biblScope unit="page">20150230</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
		<title level="m" type="main">Quantifying the redundancy between prosody and text</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pimentel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fedorenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Warstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wilcox</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2311.17233</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Regression Shrinkage and Selection via the Lasso</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Post-Selection Inference for Generalized Linear Models With Many Controls</title>
		<author>
			<persName><forename type="first">A</forename><surname>Belloni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chernozhukov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<idno type="DOI">10.1080/07350015.2016.1166116</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Business &amp; Economic Statistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="606" to="619" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Notes on Continuous Stochastic Phenomena</title>
		<author>
			<persName><forename type="first">Pap</forename><surname>Moran</surname></persName>
		</author>
		<idno type="DOI">10.2307/2332142</idno>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="1950">1950</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">On the asymptotic distribution of the Moran I test statistic with applications</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Kelejian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">R</forename><surname>Prucha</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0304-4076(01)00064-1</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Econometrics</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="219" to="257" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">A Spatial Filtering Specification for the Autologistic Model</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Griffith</surname></persName>
		</author>
		<idno type="DOI">10.1068/a36247</idno>
	</analytic>
	<monogr>
		<title level="j">Environ Plan A</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1791" to="1811" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Semiparametric Filtering of Spatial Autocorrelation: The Eigenvector Approach</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tiefelsdorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Griffith</surname></persName>
		</author>
		<idno type="DOI">10.1068/a37378</idno>
	</analytic>
	<monogr>
		<title level="j">Environment and Planning A</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1193" to="1221" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Geography and Model Uncertainty</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Oberdabernig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Humer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Crespo</forename><surname>Cuaresma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Democracy</surname></persName>
		</author>
		<idno type="DOI">10.1111/sjpe.12140</idno>
	</analytic>
	<monogr>
		<title level="j">Scottish J Political Eco</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="154" to="185" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Mixed-effects modeling with crossed random effects for subjects and items</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Baayen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Bates</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jml.2007.12.005</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="390" to="412" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Social scale and structural complexity in human languages</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nettle</surname></persName>
		</author>
		<idno type="DOI">10.1098/rstb.2011.0216</idno>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society B: Biological Sciences</title>
		<imprint>
			<biblScope unit="volume">367</biblScope>
			<biblScope unit="page" from="1829" to="1836" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Burnham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<idno type="DOI">10.1007/b97636</idno>
		<title level="m">Model Selection and Multimodel Inference</title>
		<meeting><address><addrLine>New York, NY; New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Model Selection: An Integral Part of Inference</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Buckland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Burnham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Augustin</surname></persName>
		</author>
		<idno type="DOI">10.2307/2533961</idno>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page">603</biblScope>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Model Averaging and Its Use in Economics</title>
		<author>
			<persName><forename type="first">Mfj</forename><surname>Steel</surname></persName>
		</author>
		<idno type="DOI">10.1257/jel.20191385</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Economic Literature</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="644" to="719" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">A new look at the statistical model identification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Akaike</surname></persName>
		</author>
		<idno type="DOI">10.1109/TAC.1974.1100705</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="716" to="723" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">The non-independence of nations and why it matters</title>
		<author>
			<persName><forename type="first">S</forename><surname>Claessens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Atkinson</surname></persName>
		</author>
		<idno type="DOI">10.31234/osf.io/m6bsn</idno>
	</analytic>
	<monogr>
		<title level="j">PsyArXiv</title>
		<imprint>
			<date type="published" when="2022-04">2022 Apr</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Balancing Type I error and power in linear mixed models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Matuschek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kliegl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vasishth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Baayen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bates</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jml.2017.01.001</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="305" to="315" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Estimating the Dimension of a Model</title>
		<author>
			<persName><forename type="first">G</forename><surname>Schwarz</surname></persName>
		</author>
		<idno type="DOI">10.1214/aos/1176344136</idno>
	</analytic>
	<monogr>
		<title level="j">Ann Statist</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<monogr>
		<title level="m" type="main">Linear and Nonlinear Models for the Analysis of Repeated Measurements</title>
		<author>
			<persName><forename type="first">E</forename><surname>Vonesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Chinchilli</surname></persName>
		</author>
		<idno type="DOI">10.1201/9781482293272</idno>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>CRC Press</publisher>
		</imprint>
	</monogr>
	<note>0 ed.</note>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Selecting the Best Linear Mixed Model Under REML</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Gurka</surname></persName>
		</author>
		<idno type="DOI">10.1198/000313006X90396</idno>
	</analytic>
	<monogr>
		<title level="j">The American Statistician</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="19" to="26" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">One model for the learning of language</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Piantadosi</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.2021865119</idno>
	</analytic>
	<monogr>
		<title level="j">Proc Natl Acad Sci USA</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page">2021865119</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Language acquisition, data compression and generalization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Wolff</surname></persName>
		</author>
		<idno type="DOI">10.1016/0271-5309(82)90035-0</idno>
	</analytic>
	<monogr>
		<title level="j">Language &amp; Communication</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="57" to="89" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Entropy Rate Estimates for Natural Language-A New Extrapolation of Compressed Large-Scale Corpora</title>
		<author>
			<persName><forename type="first">R</forename><surname>Takahira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tanaka-Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Å</forename><surname>DÄbowski</surname></persName>
		</author>
		<idno type="DOI">10.3390/e18100364</idno>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">364</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">A Mathematical Theory of Communication. Bell System Technical</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
		<idno type="DOI">10.1002/j.1538-7305.1948.tb01338.x</idno>
	</analytic>
	<monogr>
		<title level="j">Journal</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="379" to="423" />
			<date type="published" when="1948">1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Human languages with greater information density have higher communication speed but lower conversation breadth</title>
		<author>
			<persName><forename type="first">P</forename><surname>Aceves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Evans</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41562-024-01815-w</idno>
	</analytic>
	<monogr>
		<title level="j">Nat Hum Behav</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>cited 20 Feb 2024</note>
</biblStruct>

<biblStruct xml:id="b137">
	<monogr>
		<title level="m" type="main">Corpus size strongly matters when analysing word frequency distributions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Koplenig</surname></persName>
		</author>
		<idno type="DOI">10.31219/osf.io/p5nhd</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Baayen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frequency</forename><surname>Word</surname></persName>
		</author>
		<author>
			<persName><surname>Distributions</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Kluwer Academic Publishers</publisher>
			<pubPlace>Dordrecht</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">How Variable May a Constant be? Measures of Lexical Richness in Perspective</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Tweedie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Baayen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and the Humanities</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="323" to="352" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Studying Lexical Dynamics and Language Change via Generalized Entropies: The Problem of Sample Size</title>
		<author>
			<persName><forename type="first">A</forename><surname>Koplenig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wolfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>MÃ¼ller-Spitzer</surname></persName>
		</author>
		<idno type="DOI">10.3390/e21050464</idno>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<monogr>
		<title level="m" type="main">Conditional Word Count and Huffman Code Size are Two Sides of the Same Coin: Response to Koplenig</title>
		<author>
			<persName><forename type="first">P</forename><surname>Aceves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Evans</surname></persName>
		</author>
		<idno type="DOI">10.31219/osf.io/4b7mc</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Information density as a predictor of communication dynamics</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lupyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Contreras</forename><surname>Kallens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2024.03.012</idno>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="489" to="491" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Estimating the Entropy of DNA Sequences</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">O</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Herzel</surname></persName>
		</author>
		<idno type="DOI">10.1006/jtbi.1997.0493</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Theoretical Biology</title>
		<imprint>
			<biblScope unit="volume">188</biblScope>
			<biblScope unit="page" from="369" to="377" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">An Examination of Information Theory</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bar-Hillel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophy of Science</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="86" to="105" />
			<date type="published" when="1955">1955</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Towards the quantification of the semantic information encoded in written language</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Montemurro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Zanette</surname></persName>
		</author>
		<idno type="DOI">10.1142/S0219525910002530</idno>
	</analytic>
	<monogr>
		<title level="j">Advances in Complex Systems</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="135" to="153" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Translation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Weaver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Translation of Languages</title>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">N</forename><surname>Locke</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Boothe</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1949">1949</date>
			<biblScope unit="page" from="15" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Estimating the Transparency of Orthographies with an Artificial Neural Network</title>
		<author>
			<persName><forename type="first">X</forename><surname>Marjou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Oteann ; Vylomova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Salesky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mielke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lapesa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>HammarstrÃ¶m</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.sigtyp-1.1</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Workshop on Computational Typology and Multilingual NLP</title>
		<meeting>the Third Workshop on Computational Typology and Multilingual NLP</meeting>
		<imprint>
			<publisher>Online: Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">A Corpus for Large-Scale Phonetic Typology</title>
		<author>
			<persName><forename type="first">E</forename><surname>Salesky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chodroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pimentel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wiesner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.415</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online: Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4526" to="4546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">The consequences of talking to strangers: Evolutionary corollaries of socio-cultural influences on linguistic form</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Grace</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.lingua.2005.05.005</idno>
	</analytic>
	<monogr>
		<title level="j">Lingua</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="543" to="578" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Why Are There Different Languages? The Role of Adaptation in Linguistic Diversity</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lupyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dale</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2016.07.005</idno>
	</analytic>
	<monogr>
		<title level="j">TRENDS in Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="649" to="660" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Language Structure Is Partly Determined by Social Structure</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lupyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dale</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0008559</idno>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<editor>
			<persName><forename type="first">D</forename><surname>O'rourke</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">8559</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Speaker Input Variability Does Not Explain Why Larger Populations Have Simpler Languages</title>
		<author>
			<persName><forename type="first">M</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0129463</idno>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Caldwell</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">129463</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Larger communities create more systematic languages</title>
		<author>
			<persName><forename type="first">L</forename><surname>Raviv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lev</forename><forename type="middle">-</forename><surname>Ari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<idno type="DOI">10.1098/rspb.2019.1262</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the Royal Society B: Biological Sciences</title>
		<imprint>
			<biblScope unit="volume">286</biblScope>
			<biblScope unit="page">20191262</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Natural Population Growth Can Cause Language Simplification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K ;</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ravignani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Barbieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Flaherty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jadoul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lattenkamp</surname></persName>
		</author>
		<idno type="DOI">10.17617/2.3190925</idno>
	</analytic>
	<monogr>
		<title level="m">The Evolution of Language: Proceedings of the 13th International Conference</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">What is simple is actually quite complex: A critical note on terminology in the domain of language and communication</title>
		<author>
			<persName><forename type="first">L</forename><surname>Raviv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Peckre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Boeckx</surname></persName>
		</author>
		<idno type="DOI">10.1037/com0000328</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Comparative Psychology</title>
		<imprint>
			<date type="published" when="2022-12-07">2022. 7 Dec 2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
