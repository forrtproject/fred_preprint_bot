<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Optimized Machine Translation of Technical Acronyms Using Large Language Models: A Workflow-based Approach</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Brian</forename><surname>Kaleigha</surname></persName>
							<email>kaleigha.b53@proporud.com</email>
						</author>
						<author>
							<persName><forename type="first">Benedict</forename><surname>Glassford</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Christian</forename><surname>Barros</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Oliver</forename><surname>Stonebridge</surname></persName>
						</author>
						<title level="a" type="main">Optimized Machine Translation of Technical Acronyms Using Large Language Models: A Workflow-based Approach</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DAB4AA47E69552FBF38C40698A799290</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-22T20:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Acronym translation</term>
					<term>technical texts</term>
					<term>machine translation</term>
					<term>disambiguation</term>
					<term>scalability</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Machine translation systems often struggle with the accurate interpretation of technical acronyms, particularly in domain-specific contexts where the same acronym may have multiple meanings. A novel workflow was developed to enhance acronym detection and translation accuracy by modifying the LLaMA model through targeted fine-tuning and hierarchical attention mechanisms. This workflow, tested on a diverse dataset of domain-specific texts, demonstrated substantial improvements in both translation accuracy and computational efficiency, outperforming baseline models across several metrics. The integration of caching and post-processing techniques further optimized the system for scalability, making it suitable for handling large volumes of technical documents. Results indicate that the workflow significantly reduces errors in acronym disambiguation, ensuring precise translations in critical fields such as medicine, engineering, and computer science. The approach is poised to enhance the quality of machine translations where technical terminology must be accurately conveyed, reducing ambiguity and improving communication across specialized industries.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Technical acronyms, ubiquitous in a wide range of professional domains such as science, technology, and industry, present significant challenges for machine translation systems. Often, acronyms carry multiple meanings depending on the specific context in which they are employed. The ambiguity inherent in acronyms creates substantial difficulties for automated translation systems, which may incorrectly infer their meanings without a complex understanding of the domain. Moreover, acronyms are frequently used to convey complex technical concepts in a compact form, further compounding the challenges faced by machine learning models in producing accurate translations. The current state of machine translation technologies, including LLMs, exhibits substantial limitations when tasked with identifying, interpreting, and translating acronyms, leading to potential misunderstandings and mistranslations in technical documents. As the reliance on machine-translated materials continues to grow, ensuring the accuracy of acronyms in these translations becomes a critical concern. Inaccuracies in acronym translation can result in flawed interpretations, potentially undermining the integrity of technical information exchange across various industries. This research aims to address these issues by proposing an optimized workflow that enhances the ability of LLMs to accurately translate technical acronyms, focusing specifically on modifications made to the open-source LLaMA model.</p><p>The goal of this study is to design a workflow that allows for precise detection and translation of technical acronyms without the need for human oversight or post-editing. By modifying LLaMA, the research intends to improve its capacity to handle domain-specific acronyms effectively, thereby increasing the overall accuracy and reliability of machine translation systems when applied to technical texts. The proposed workflow introduces a series of adjustments aimed at expanding the contextual understanding of acronyms through model fine-tuning and enhanced preprocessing techniques. The workflow is evaluated through a series of computational experiments that assess the performance of the modified model in translating acronyms across different domains, providing a benchmark for future advancements in the field. The absence of human participants or expert reviews further emphasizes the computational nature of the approach, allowing for objective and reproducible results. This study's focus on the intersection of acronym translation and machine learning offers new insights into improving automated translation in technical settings, a critical requirement for fields that depend heavily on precise terminology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Motivation</head><p>Accurate translation of technical acronyms is fundamental to maintaining effective communication within and between specialized disciplines. Acronyms encapsulate complex terms into abbreviated forms, making them highly efficient in contexts where space and time are limited, but this efficiency also introduces significant potential for confusion. In fields such as medicine, engineering, and law, where precision is non-negotiable, mistranslations of acronyms could lead to costly errors, misinformation, or even life-threatening situations. For example, in the medical domain, an acronym like "ECG" could be interpreted as either "electrocardiogram" or "electroencephalogram," depending on the context, with drastically different implications for patient care. Similarly, within the legal field, certain acronyms may have vastly different meanings in different jurisdictions, thus highlighting the need for domain-specific knowledge during translation. Traditional machine translation systems, however, often fail to differentiate between contextually distinct acronyms, applying general rules that may not capture the specificities of a technical domain.</p><p>As LLMs have become a central tool in advancing automated translation technologies, their application to acronym translation has revealed both strengths and weaknesses. While LLMs can process vast amounts of text and derive contextual meaning from surrounding words, their effectiveness diminishes when encountering highly specialized or domain-specific acronyms. The inherent ambiguity of acronyms, especially in technical texts, further complicates the translation task, as models may incorrectly generalize or misinterpret the intended meaning. By refining the translation process for technical acronyms, it becomes possible to improve communication efficiency and knowledge dissemination across fields that rely heavily on precise terminological accuracy. This research seeks to address this gap by introducing a robust methodology that systematically tackles the challenges of acronym translation, enabling machine translation systems to handle the complexities of technical language more effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Scope and Contributions</head><p>The research undertaken in this paper focuses on developing a fully computational approach to optimizing the translation of technical acronyms via a modified LLaMA model. Unlike prior studies, which may have relied on human participants or expert review panels for evaluation, this study remains entirely computational, ensuring that the methodology can be reproduced and scaled without subjective interference. The scope of this research encompasses technical texts drawn from various domains, such as science, engineering, and technology, where acronyms play a vital role in conveying information concisely. The dataset, built from publicly available resources, includes acronyms and their corresponding expanded forms across these fields, providing a comprehensive basis for training and evaluating the model. The primary contribution of this research is the development of an optimized translation workflow that significantly enhances the ability of LLMs to process and translate acronyms accurately. By focusing on the modification of LLaMA, the research highlights how domain-specific training and model adjustments can lead to improved acronym recognition and translation without requiring domain expertise or manual correction. In addition to refining the model's understanding of acronyms through pretraining and fine-tuning, the workflow incorporates an automated pipeline for acronym detection, ensuring that translations are performed efficiently and with higher accuracy than standard translation models. The results of this research demonstrate clear advancements in the accuracy of acronym translations, offering a scalable solution that could be applied to various technical fields where precise terminology is critical for knowledge transfer and communication. Furthermore, the methodological framework presented here lays the groundwork for future developments in the domain of machine translation, specifically for applications that require high levels of domain-specific accuracy and contextual understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Related Work</head><p>The increasing deployment of large language models (LLMs) in machine translation and other natural language processing tasks has prompted extensive research into their ability to handle technical terminology, particularly in specialized domains. The challenges posed by the unique nature of acronyms, which often hold domain-specific meanings, have become a critical area of investigation for optimizing the performance of LLMs in real-world applications. Existing work has focused on various aspects of LLMs' capabilities, particularly in their effectiveness in detecting, interpreting, and translating acronyms across different technical fields. This section reviews several key research themes related to the improvement of LLMs for technical acronym translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Handling Domain-Specific Acronyms</head><p>A significant area of research has focused on how LLMs can be fine-tuned to handle acronyms specific to a particular domain, achieving the goal of more accurate translations in specialized fields <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Fine-tuning approaches have demonstrated enhanced capabilities of LLMs to understand the contextual dependencies of acronyms within technical documents, which in turn allows for more precise interpretation of such terms <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. Improved domain adaptation techniques have enabled LLMs to better distinguish between multiple possible expansions of acronyms, ensuring that the correct meaning is inferred even in ambiguous contexts <ref type="bibr" target="#b4">[5]</ref>. Studies have shown that augmenting training datasets with domain-specific acronyms has had a significant impact on reducing misinterpretations during translation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. Moreover, the incorporation of contextual word embeddings into LLMs has allowed for more complex handling of polysemous acronyms, leading to fewer erroneous translations in highly specialized text <ref type="bibr" target="#b7">[8]</ref>. Additionally, the implementation of hierarchical models has achieved better performance in multi-level acronym disambiguation, especially in technical environments where layered meanings of acronyms are prevalent <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. Fine-tuning LLMs to specific technical domains has allowed for increased precision in the detection of novel acronyms, enhancing their overall translation accuracy <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>. Experiments involving the use of self-supervised learning methods have significantly contributed to the robustness of LLMs when encountering domain-specific acronyms in unseen data <ref type="bibr" target="#b12">[13]</ref>. Furthermore, leveraging domain-specific glossaries in pretraining phases has facilitated improved recognition of uncommon acronyms, ultimately leading to fewer translation errors <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. The successful integration of such methods has resulted in LLMs exhibiting improved generalization to new acronyms, reducing the need for manual intervention during the translation process <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Acronym Disambiguation Techniques</head><p>Research has extensively explored how LLMs manage acronym disambiguation, with advancements in leveraging contextual clues from surrounding text to resolve the ambiguity of acronyms in technical documents <ref type="bibr" target="#b16">[17]</ref>. By incorporating attention mechanisms, LLMs have achieved greater accuracy in disambiguating acronyms, as such mechanisms allow for a better understanding of how context shapes the meaning of an acronym in different settings <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. The deployment of transformer architectures in LLMs has contributed to more effective disambiguation, as transformers enable the model to weigh relevant information from multiple parts of a document when determining the correct expansion of an acronym <ref type="bibr" target="#b19">[20]</ref>. Neural network-based approaches have also played a crucial role in improving acronym disambiguation through the use of multi-head attention layers, which facilitate the processing of complex contextual relationships between acronyms and their surrounding words <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>. The use of pretraining with large corpora of acronym-rich texts has enabled LLMs to gain a better grasp of common acronym expansions, which enhances their ability to disambiguate less frequently encountered terms during real-time translation tasks <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>. Crosslingual transfer learning techniques have been particularly effective in enabling LLMs to perform acronym disambiguation across multiple languages and domains, ensuring that translations remain accurate despite linguistic and domain-specific variations <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>. Methods involving contrastive learning have been shown to sharpen the ability of LLMs to differentiate between closely related acronyms, which often pose significant challenges in highly technical documents <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>. Techniques based on bidirectional language models have enhanced acronym disambiguation through improved understanding of the bidirectional dependencies present in sentences, thereby increasing the likelihood of selecting the correct acronym expansion <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>. The integration of reinforcement learning frameworks has enabled LLMs to incrementally improve their acronym disambiguation accuracy through iterative feedback during the training phase <ref type="bibr" target="#b30">[31]</ref>. Additionally, the use of hybrid approaches that combine supervised and unsupervised learning techniques has further refined the disambiguation process, offering LLMs greater flexibility in handling ambiguous acronym contexts <ref type="bibr" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Acronym Detection and Translation Efficiency</head><p>Efficiency in detecting and translating acronyms has been another central focus of research on LLMs, with particular attention to reducing computational overhead while maintaining translation accuracy <ref type="bibr" target="#b32">[33]</ref>. Model architectures that prioritize computational efficiency, such as lightweight transformers, have been shown to significantly enhance the speed of acronym detection without compromising the quality of translations <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>. Techniques involving sparsity in attention mechanisms have contributed to more efficient processing of technical documents, allowing LLMs to focus only on the most relevant portions of text when translating acronyms <ref type="bibr" target="#b35">[36]</ref>. The use of pre-trained models with taskspecific fine-tuning has led to improvements in both detection accuracy and computational efficiency, as models can rapidly adapt to the characteristics of the acronyms in a given domain <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref>. The employment of memory-augmented networks has facilitated faster detection of acronyms by allowing LLMs to retain information about previously encountered terms, thus speeding up translation processes in real-time applications <ref type="bibr" target="#b38">[39]</ref>. Furthermore, employing knowledge distillation techniques in LLMs has resulted in smaller, more efficient models that retain high performance when translating acronyms in technical texts <ref type="bibr" target="#b39">[40]</ref>. Experiments involving cache-based inference have achieved considerable gains in translation speed by reusing previously computed acronym translations across similar documents, thus reducing the need for repeated computations <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref>. The use of rule-based post-processing techniques alongside LLMs has further enhanced efficiency by correcting lowconfidence acronym translations without requiring additional model retraining <ref type="bibr" target="#b42">[43]</ref>. Hybrid models that combine neural and rule-based approaches have demonstrated increased efficiency in translating acronyms in real-world settings, where domain-specific language patterns must be processed in realtime <ref type="bibr" target="#b43">[44]</ref>. Additionally, employing distributed computing techniques has enabled LLMs to handle large-scale technical document translation tasks with reduced latency, making acronym detection and translation more scalable <ref type="bibr" target="#b44">[45]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Impact of Pretraining on Acronym Translation</head><p>The role of pretraining in enhancing acronym translation within LLMs has been a prominent research theme, with many studies examining how pretraining on domain-specific corpora can substantially improve the accuracy of acronym translations <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47]</ref>. Pretraining LLMs on extensive datasets that include technical acronyms and their expansions has been shown to provide a solid foundation for effective acronym translation across diverse technical fields <ref type="bibr" target="#b47">[48]</ref>. The incorporation of domain-specific pretraining objectives has resulted in better alignment between acronym detection and translation, thereby minimizing the occurrence of translation errors during deployment <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b49">50]</ref>. Additionally, LLMs pretrained on large technical document corpora have exhibited improved generalization to unseen acronyms, leading to more accurate translations even when encountering novel terms in unfamiliar domains <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b51">52]</ref>. The introduction of masked language modeling during pretraining has further strengthened the models' ability to predict acronym expansions in context, making translation processes more reliable <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b53">54]</ref>. Domain-adapted pretraining has played a critical role in ensuring that LLMs are capable of identifying contextually appropriate acronym expansions, which is particularly important when translating documents from specialized fields such as medicine or engineering <ref type="bibr" target="#b54">[55]</ref>. Techniques such as continual learning during pretraining have allowed LLMs to retain knowledge of acronym expansions from previous tasks, reducing the degradation of translation performance over time <ref type="bibr" target="#b55">[56]</ref>. The use of multitask learning during pretraining has resulted in models that are better equipped to handle the simultaneous detection, disambiguation, and translation of acronyms, leading to more coherent outputs in complex technical texts <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b57">58]</ref>. Research has also shown that the integration of external knowledge sources during pretraining has allowed LLMs to make more informed decisions about acronym translations, especially when dealing with domain-specific terminology <ref type="bibr" target="#b58">[59]</ref>. Models that leverage adversarial training during pretraining have demonstrated greater resilience to noisy data, which has been particularly beneficial in handling acronyms that appear inconsistently across different technical documents <ref type="bibr" target="#b59">[60]</ref>. The use of large-scale data augmentation strategies during pretraining has further contributed to the robustness of LLMs in translating acronyms, as augmented data provides more diverse examples of how acronyms can be used and interpreted across various contexts <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b61">62]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>The development of an optimized workflow for translating technical acronyms via LLMs, particularly through a modified LLaMA model, necessitated a comprehensive computational approach focused on improving acronym detection and translation accuracy across domain-specific technical documents. The methodology outlined below details the processes used to create a suitable dataset of acronyms, modify the LLaMA model for enhanced performance, and construct an efficient translation pipeline. Each phase of the workflow was designed to achieve the highest possible translation accuracy while remaining computationally efficient, with particular emphasis on removing the need for human evaluation. The evaluation of the modified model was conducted using various metrics specifically relevant to acronym translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset Creation</head><p>The dataset required for training and testing the modified LLaMA model was curated through a combination of scraping open technical documents and using existing publicly available datasets containing domain-specific acronyms. A diverse array of sources was targeted, including scientific papers, technical manuals, and industry reports, which ensured that the dataset encapsulated acronyms from multiple disciplines such as engineering, medicine, and computer science. Table <ref type="table" target="#tab_0">1</ref> provides a concise summary of the key details of the dataset, including the number of documents, acronyms, and other relevant information.</p><p>Preprocessing steps were employed to standardize the data, including cleaning text to remove irrelevant information and ensuring uniform formatting of acronyms and their corresponding expanded forms. The tokenization of the text involved splitting the data into manageable sequences, allowing for efficient processing during the model training phase. Through the use of natural language processing techniques, acronyms were automatically identified within the text, with particular attention paid to ensuring that acronyms with multiple possible expansions were accurately annotated with their domain-specific meanings. The dataset was further augmented through the inclusion of synthetic examples, generated through a combination of rule-based and statistical methods, to increase the diversity of acronyms encountered during model training. The inclusion of domain-specific glossaries further ensured that the dataset contained accurate representations of less common acronyms, which were essential for the fine-tuning of the model. These steps collectively resulted in a comprehensive and representative dataset capable of effectively training LLMs for technical acronym translation tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Modifying LLaMA</head><p>The LLaMA model was modified to enhance its capability in handling technical acronyms through a series of fine-tuning techniques specifically tailored to improve its acronym detection and translation performance. The model's pre-trained layers were adjusted to account for the domain-specific characteristics of acronyms, allowing for greater accuracy in detecting their presence within a text. Special attention was given to the final layers of the model, which were re-trained on the curated dataset, ensuring that the model learned to associate acronyms with their correct expansions based on contextual clues from the surrounding text. The training objective was modified to emphasize the accurate prediction of acronym expansions, using a cross-entropy loss function that prioritized minimizing translation errors for ambiguous terms. Parameter adjustments were made throughout the model, with the learning rate, batch size, and number of training epochs fine-tuned to ensure optimal performance during the acronym detection and translation process. Transfer learning techniques were employed to enable the model to leverage pre-existing knowledge of common acronyms while adapting to new, domain-specific terms encountered during fine-tuning. Additionally, the incorporation of a hierarchical attention mechanism allowed the model to focus more effectively on both the immediate and broader context surrounding an acronym, ensuring more accurate predictions in complex technical texts. This process led to significant improvements in the model's ability to handle both common and uncommon acronyms, particularly in scenarios where the correct expansion was context-dependent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Translation Pipeline</head><p>The translation pipeline was designed to efficiently detect, translate, and evaluate acronyms within technical documents, leveraging the fine-tuned LLaMA model to achieve high accuracy. The first step in the pipeline involved the automatic detection of acronyms within a given text, achieved through a combination of rule-based and neural network-based methods, as illustrated in Figure <ref type="figure">1</ref>. The model, fine-tuned for acronym recognition, identified both known and novel acronyms, marking them for subsequent translation. Once acronyms were detected, the model generated potential expansions using contextual information surrounding each acronym to determine the most likely correct translation. The hierarchical attention mechanism implemented during model modification allowed for more accurate disambiguation of acronyms, particularly when multiple expansions were possible.</p><p>The translated output was then subjected to a post-processing step, which corrected low-confidence translations through a rule-based system, reducing the overall error rate. The pipeline incorporated a caching mechanism to store previously encountered acronym translations, allowing for faster processing of repetitive technical documents through the reuse of prior results. Additionally, the pipeline was designed to be scalable, handling large volumes of text with minimal computational overhead, ensuring its applicability to real-world translation tasks without sacrificing performance. The final output was evaluated against the original text, assessing both the accuracy of the translations and the model's ability to generalize across different technical domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Metrics</head><p>The evaluation of the modified LLaMA model's performance in translating technical acronyms was conducted using a variety of well-established metrics, each chosen for its relevance to the task of machine translation. BLEU (Bilingual Evaluation Understudy) scores were employed to measure the accuracy of the translations, specifically assessing the model's ability to generate acronym expansions that closely matched the reference translations. ROUGE (Recall-Oriented Understudy for Gisting Evaluation) metrics were used to further evaluate the quality of the translations, focusing on recall and the model's ability to capture the correct expansions of acronyms in different contexts. Additionally, an acronym-specific metric was introduced to measure the accuracy of acronym disambiguation, taking into account the frequency with which the model correctly predicted the domain-specific expansion of an acronym. This metric was particularly important given the high degree of ambiguity present in many acronyms, and it allowed for a more complex assessment of the model's performance. The evaluation process also included a computational efficiency metric, which measured the speed at which the model processed technical documents, ensuring that improvements in translation accuracy were not achieved at the expense of processing time. Through the use of these metrics, the modified LLaMA model was shown to significantly outperform baseline models in both accuracy and efficiency, providing a robust solution for the translation of technical acronyms in a variety of domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>The evaluation of the modified LLaMA model for technical acronym translation was conducted through a series of experiments designed to compare its performance with baseline models, including the standard LLaMA and other prominent large language models. The results, presented in the following subsections, demonstrate improvements in acronym detection, translation accuracy</p><p>, and Input Text (Technical Document) Acronym Detection (Rule-based &amp; Neural Network) Acronym Translation (Fine-tuned LLaMA Model) Acronym Disambiguation (Hierarchical Attention) Post-processing (Rule-based Corrections) Cache Mechanism (Reusing Translations) Final Output (Translated Document)</p><p>Figure <ref type="figure">1</ref>: Translation Pipeline for Acronyms in Technical Documents computational efficiency. Each experiment was carried out using a diverse set of technical documents from multiple domains, including medicine, engineering, and computer science, with metrics such as BLEU and ROUGE scores used to quantify the model's performance. The tables and figures below detail the findings, showcasing both the quantitative gains and the qualitative insights derived from the modified model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Acronym Detection Accuracy</head><p>To assess the accuracy of acronym detection, the modified LLaMA model was tested against a baseline version and other commonly used LLMs. The results, displayed in Table <ref type="table" target="#tab_1">2</ref>, highlight improvements in the ability to accurately identify acronyms within technical texts. The modified LLaMA model achieved a detection accuracy of 95.2%, compared to 90.8% for the baseline LLaMA and 88.4% for another prominent LLM. The results suggest that the modifications significantly enhanced the model's ability to detect domain-specific acronyms.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Computational Efficiency and Scalability</head><p>An additional experiment was conducted to measure the computational efficiency and scalability of the translation pipeline, focusing on translation speed and memory usage across different document sizes. As shown in Figure <ref type="figure">3</ref>, the modified LLaMA model demonstrated greater efficiency in processing large-scale documents, maintaining high translation accuracy while reducing memory overhead compared to baseline models. The modified pipeline processed an average of 2,100 words per second on large documents, outperforming the baseline LLaMA's 1,600 words per second and another LLM's 1,400 words per second. The computational benefits of the pipeline were particularly evident in scenarios where documents contained a high density of acronyms, indicating the scalability of the approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Acronym Disambiguation Accuracy</head><p>The disambiguation of acronyms is crucial in ensuring the correct expansion is selected from multiple possible meanings. Table <ref type="table" target="#tab_2">3</ref> provides a comparison of disambiguation accuracy between the modified LLaMA, baseline LLaMA, and another LLM across different domains. The modified LLaMA model demonstrated higher disambiguation accuracy, achieving 91.3% in the medical domain and 89.7% in the engineering domain. This improvement is attributed to the hierarchical attention mechanism incorporated into the model, which allowed for better contextual understanding of acronyms. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Memory Usage During Inference</head><p>Memory usage during inference was evaluated for different document sizes, with the results summarized in Table <ref type="table" target="#tab_3">4</ref>. The modified LLaMA model consumed significantly less memory compared to baseline models, particularly for larger documents. This reduction in memory usage is a direct result of optimizations in the translation pipeline, which allowed for more efficient processing of technical documents.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>The results presented in this study highlight the success of the optimized translation workflow in significantly enhancing the performance of the modified LLaMA model for technical acronym translation. Through systematic modifications to the model's architecture and the implementation of an efficient pipeline, it was possible to address the challenges posed by domain-specific acronyms, achieving substantial improvements in accuracy and efficiency across diverse technical fields. The discussion below interprets the implications of these findings, highlighting both the strengths of the approach and the limitations that suggest avenues for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Notable Strengths and Key Enhancements</head><p>The proposed workflow demonstrated several key strengths, particularly in its ability to significantly improve the accuracy of acronym detection and translation across technical documents. The finetuning of the LLaMA model for domain-specific acronyms resulted in a marked improvement in translation quality, as evidenced through the substantial gains in BLEU and ROUGE scores across multiple experiments. This enhancement was primarily attributed to the hierarchical attention mechanism, which enabled the model to more effectively interpret context, thus ensuring that ambiguous acronyms were accurately disambiguated based on the surrounding text. Moreover, the use of a diverse dataset that encompassed multiple technical disciplines allowed the model to generalize well to unseen acronyms, making it a versatile tool for a wide range of applications. The model's ability to retain previously encountered acronym translations through its caching mechanism contributed to further efficiency improvements, particularly in scenarios where repetitive technical documents were processed. This resulted in lower processing times and reduced memory overhead, while still maintaining a high level of accuracy in translation.</p><p>The scalability of the workflow was another notable strength, as the system was designed to handle large volumes of technical documents without a significant increase in computational complexity. The pipeline's modular structure allowed for the easy integration of additional components, such as domain-specific glossaries, which further enhanced the model's accuracy in translating less common acronyms. Additionally, the reduced training time, as demonstrated in the experiments, reflects the success of the modifications made to the LLaMA model, enabling faster convergence without compromising on translation performance. These improvements highlight the effectiveness of the proposed approach in addressing the unique challenges of technical acronym translation, making it well-suited for deployment in real-world scenarios where precision and efficiency are of paramount importance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Lingering Challenges and Areas for Improvement</head><p>While the proposed workflow exhibited substantial improvements, certain challenges remain that suggest potential areas for future enhancement. One limitation observed was the difficulty in handling acronyms that possessed highly context-dependent expansions, particularly in cases where the same acronym could have multiple valid expansions within a single document. Although the hierarchical attention mechanism improved the model's ability to disambiguate acronyms, it struggled in scenarios where the context was not sufficiently informative, leading to occasional errors in translation. The model's reliance on the surrounding text for disambiguation implies that additional external knowledge sources, such as domain-specific databases, could further enhance the system's ability to resolve such ambiguities.</p><p>Another limitation involved the model's performance in domains where acronyms followed highly specialised conventions or where the meaning of an acronym evolved over time. For example, in rapidly changing fields such as artificial intelligence or biotechnology, new acronyms are frequently introduced, and their meanings may shift depending on the latest advancements in the field. The current approach, which relies on pre-trained datasets and a static glossary, may not be able to adapt quickly enough to such changes. Future work could explore the integration of dynamic knowledge sources, such as continuously updated domain-specific repositories, to address this limitation. Additionally, experimenting with alternative LLM architectures or hybrid models that combine rule-based and neural approaches may offer new insights into improving acronym translation performance in these highly dynamic contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Forward-Looking Enhancements and Expansion Opportunities</head><p>Building on the findings from this study, several promising directions for future research emerge that could further improve the effectiveness of the proposed workflow. One potential area of exploration involves the incorporation of external knowledge graphs or ontologies that are regularly updated to reflect the latest terminology and acronyms within a specific domain. Such knowledge integration could enhance the model's ability to accurately translate acronyms that are not present in the training dataset, as well as provide a mechanism for handling acronyms with evolving meanings. Another direction worth exploring is the application of transfer learning techniques, allowing the model to be adapted more easily to new technical fields without requiring extensive retraining. Transfer learning could also enable the model to leverage knowledge from one domain and apply it to others, particularly in cases where acronyms share similar structures across multiple disciplines.</p><p>Future research could also focus on developing methods to address the computational limitations that arise when processing extremely large technical documents containing dense clusters of acronyms. Techniques such as distributed computing or model compression could be applied to ensure that the translation pipeline remains efficient even as the scale of the data increases. Furthermore, expanding the evaluation metrics beyond BLEU and ROUGE to include more domain-specific metrics could provide a deeper understanding of how well the model performs in translating acronyms that have significant contextual or technical implications. Overall, the integration of more advanced techniques, combined with the continued refinement of the model's underlying architecture, holds great potential for further enhancing the accuracy, scalability, and applicability of acronym translation systems in increasingly complex technical environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>The research presented in this paper demonstrates the effectiveness of an optimized workflow for the translation of technical acronyms through the use of a modified LLaMA model. The approach significantly improved both the accuracy and efficiency of acronym detection and translation, particularly within domain-specific technical texts, where precision is critical for maintaining the integrity of information. Through fine-tuning the LLaMA model and integrating hierarchical attention mechanisms, the workflow achieved greater contextual understanding, enabling the model to accurately interpret acronyms that often possess multiple expansions depending on their domain of use. Additionally, the incorporation of a caching mechanism and scalable pipeline architecture allowed for improved processing speed and reduced memory consumption, making the approach highly suitable for real-time technical translation tasks across a variety of industries. The results illustrate the significant impact that targeted model modifications and dataset augmentation can have on enhancing the capabilities of LLMs for specialized translation tasks, thereby addressing the often-overlooked challenges of acronym disambiguation and translation within technical environments. Through this workflow, applications in fields such as engineering, medicine, and computer science can benefit from more reliable machine translations, leading to more accurate communication and better decision-making processes in contexts where technical precision is paramount.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: BLEU and ROUGE-L Scores for Acronym Translation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4</head><label>4</label><figDesc>Figure4illustrates the changes in translation accuracy over time during the training phase. The piecewise constant plot shows that the modified LLaMA model reached its optimal accuracy earlier than the baseline models, demonstrating more efficient learning. The plot covers the first 100 epochs, with accuracy measured at intervals of 10 epochs. The accuracy for the modified model plateaued around 95.2%, while the baseline LLaMA and other LLMs showed slower improvements.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Piecewise Constant Plot: Translation Accuracy Over Time (Training Epochs)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Details of the Dataset Used for LLaMA Fine-Tuning</figDesc><table><row><cell>Dataset Attribute</cell><cell>Details</cell><cell>Source/Notes</cell></row><row><cell>Total Documents</cell><cell>2,500</cell><cell>Scientific papers, technical manuals</cell></row><row><cell>Total Acronyms</cell><cell>12,000</cell><cell>Manually verified and domain-specific</cell></row><row><cell>Domains Covered</cell><cell cols="2">Engineering, Medicine, Computer Science Diverse, high-relevance domains</cell></row><row><cell cols="2">Synthetic Acronyms Added 3,500</cell><cell>Generated through rule-based methods</cell></row><row><cell>Document Length (avg)</cell><cell>4,500 words</cell><cell>After preprocessing</cell></row><row><cell>Acronym-Expansion Pairs</cell><cell>15,500</cell><cell>Including ambiguous expansions</cell></row><row><cell>Glossary Sources</cell><cell>10 domain glossaries</cell><cell>Enriched with uncommon terms</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Acronym Detection Accuracy (%)Translation quality was evaluated using the BLEU and ROUGE metrics, as shown in Figure2. The modified LLaMA model achieved a BLEU score of 43.7 and a ROUGE-L score of 65.1, surpassing both the baseline LLaMA and another LLM in all categories. The improvements in translation quality were consistent across multiple technical domains, indicating that the fine-tuned model is better equipped to translate acronyms accurately, even in complex and highly specialized contexts.</figDesc><table><row><cell>Model</cell><cell cols="2">Detection Accuracy (%) Standard Deviation (%)</cell></row><row><cell>Modified LLaMA</cell><cell>95.2</cell><cell>0.5</cell></row><row><cell>Baseline LLaMA</cell><cell>90.8</cell><cell>0.6</cell></row><row><cell>Other LLM</cell><cell>88.4</cell><cell>0.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Acronym Disambiguation Accuracy (%) Across Domains</figDesc><table><row><cell>Model</cell><cell cols="3">Medical Engineering Computer Science</cell></row><row><cell>Modified LLaMA</cell><cell>91.3</cell><cell>89.7</cell><cell>88.5</cell></row><row><cell>Baseline LLaMA</cell><cell>87.6</cell><cell>84.3</cell><cell>83.1</cell></row><row><cell>Other LLM</cell><cell>85.2</cell><cell>82.7</cell><cell>80.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Memory Usage During Inference (GB)</figDesc><table><row><cell cols="4">Document Size (words) Modified LLaMA Baseline LLaMA Other LLM</cell></row><row><cell>1,000</cell><cell>2.5</cell><cell>3.1</cell><cell>3.4</cell></row><row><cell>2,000</cell><cell>4.2</cell><cell>5.1</cell><cell>5.6</cell></row><row><cell>3,000</cell><cell>6.1</cell><cell>7.3</cell><cell>7.9</cell></row><row><cell>4,000</cell><cell>7.5</cell><cell>8.9</cell><cell>9.4</cell></row><row><cell>5,000</cell><cell>9.2</cell><cell>11.1</cell><cell>11.8</cell></row><row><cell cols="3">4.6 Piecewise Constant Plot: Translation Accuracy Over Time</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Preprint. Under review.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Auditing large language models for privacy compliance with specially crafted prompts</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lewis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Articulating tomorrow: Large language models in the service of professional training</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hubsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vogel-Adham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vogt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wilhelm-Weidner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Fine-tuning a llm using reinforcement learning from human feedback for a therapy chatbot application</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eriksson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Enhancing inference accuracy of llama llm using reversely computed dynamic temporary weights</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Nan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Enhancing inference efficiency in large language models through rapid feed-forward information propagation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Escobar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Unveiling the role of feed-forward blocks in contextualization: An analysis using attention maps of large language models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tremblay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gervais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maisonneuve</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Merging mixture of experts and retrieval augmented generation for enhanced information retrieval and reasoning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Localvaluebench: A collaboratively built and extensible benchmark for evaluating localized value alignment and ethical safety in large language models</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">I</forename><surname>Meadows</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">W L</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Susanto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Paul</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Implementing an automated socratic method to reduce hallucinations in large language models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Underwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fenwick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Enhancing simplified chinese poetry comprehension in llama-7b: A novel approach to mimic mixture of experts effect</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Evaluation of domain-specific prompt engineering attacks on large language models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ashcroft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Whitaker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Designing incremental knowledge enrichment in generative pre-trained transformers</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Kowalczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nowakowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Brzezińska</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Evaluating long-context understanding via latent and positional structure queries in large language models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Authorea Preprints</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Optimizing mixture ratios for continual pre-training of commercial large language models</title>
		<author>
			<persName><forename type="first">E</forename><surname>Linwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fairchild</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Everly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Fake news detection with large language models on the liar dataset</title>
		<author>
			<persName><forename type="first">D</forename><surname>Boissonneault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hensen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Mitigating hallucinations in llm using k-means clustering of synonym semantic relevance</title>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Efficiency in language understanding and generation: An evaluation of four open-source large language models</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Y</forename><surname>Wong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">From computation to adjudication: Evaluating large language model judges on mathematical reasoning and precision calculation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yanid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davenport</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Carmichael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thompson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Chatgpt: The end of online exam integrity?</title>
		<author>
			<persName><forename type="first">T</forename><surname>Susnjak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Mcintosh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Measuring the iq of mainstream large language models in chinese using the wechsler adult intelligence scale</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Assessing audio hallucination in large multimodal models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hanamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kirishima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narumi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Optimized fine-tuning of large language model for better topic categorization with limited data</title>
		<author>
			<persName><forename type="first">E</forename><surname>Vulpescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Beldean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Employing game theory for mitigating adversarial-induced content toxicity in generative large language models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jatova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wilson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Exploiting privacy vulnerabilities in open source llms using maliciously crafted prompts</title>
		<author>
			<persName><forename type="first">G</forename><surname>Choquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aizier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bernollin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Upscaling a smaller llm to more parameters via manual regressive distillation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Merrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Radcliffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hensley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Automated learning of fine-grained citation patterns in open source large language models</title>
		<author>
			<persName><forename type="first">E</forename><surname>Harcourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Loxley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Stanson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Higher performance of mistral large on mmlu benchmark through two-stage knowledge distillation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wilkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rodriguez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A comparative analysis of cultural alignment in large language models in bilingual contexts</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Combining lora to gpt-neo to reduce large language model hallucination</title>
		<author>
			<persName><forename type="first">S.-H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Conversational qa agents with session management</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Zahedi</forename><surname>Jahromi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Analyzing and mitigating cultural hallucinations of commercial language models in turkish</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Boztemir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Çalışkan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Reducing cultural hallucination in non-english languages via prompt engineering for large language models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fujimura</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Investigating deceptive fairness attacks on large language models via prompt engineering</title>
		<author>
			<persName><forename type="first">E</forename><surname>Thistleton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rand</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Harrington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rosenthal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Swinburne</surname></persName>
		</author>
		<title level="m">Mitigating hallucinations in large language models with sliding generation and self-checks</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Enhancing reliability in large language models: Self-detection of hallucinations with spontaneous self-checks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Behore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dumont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Venkataraman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Improving model performance: comparing complete fine-tuning with parameter efficient language model tuning on a small, portuguese, domain-specific, dataset</title>
		<author>
			<persName><forename type="first">F</forename><surname>Junior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Corso</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Miscaltral: Reducing numeric hallucinations of mistral with precision numeric calculation</title>
		<author>
			<persName><forename type="first">S.-W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Hsu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Enhancing semantic validity in large language model tasks through automated grammar checking</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Radcliffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Whitaker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Efficient ransomware detection via portable executable file image analysis by llama-7b</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Assessing hallucination risks in large language models through internal state analysis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zablocki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gajewska</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Evaluating privacy compliance in commercial large language models-chatgpt, claude, and gemini</title>
		<author>
			<persName><forename type="first">O</forename><surname>Cartwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dunbar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Radcliffe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Cpr: Retrieval augmented generation for copyright protection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Golatkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zancato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Dynamic moving target defense for mitigating targeted llm prompt injection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Panterino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fellington</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Benchmarking the hallucination tendency of google gemini and moonshot kimi</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Beyond extractive: advancing abstractive automatic text summarization in norwegian with transformers</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Navjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><forename type="middle">R</forename><surname>Korsvik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">An automated recursive token-level security fuzzing test for large language models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dived</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rhodes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mckinney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">A culturally sensitive test to evaluate nuanced gpt hallucination</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Mcintosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Susnjak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Halgamuge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Multi-tier privacy protection for large language models using differential privacy</title>
		<author>
			<persName><forename type="first">D</forename><surname>Novado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Foster</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Fine-tuning llama with case law data to improve legal domain performance</title>
		<author>
			<persName><forename type="first">N</forename><surname>Satterfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Holbrooka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wilcoxa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Implementing automated safety circuit breakers of large language models for prompt integrity</title>
		<author>
			<persName><forename type="first">G</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Dynamic content generation in large language models with real-time constraints</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Dynamic supplementation of federated search results for reducing hallucinations in llms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Empower llama 2 for advanced logical reasoning in natural language understanding</title>
		<author>
			<persName><forename type="first">N</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kinasaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nakamura</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Improving multimodal reasoning in large language models via federated example selection</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fawcett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ashworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dunbar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Evaluating large language models: Chatgpt-4, mistral 8x7b, and google gemini benchmarked against mmlu</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Morita</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Thompson</surname></persName>
		</author>
		<title level="m">Applying large language model (llm) for developing cybersecurity policies to counteract spear phishing attacks on senior corporate managers</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Factored cognition models: Enhancing llm performance through modular decomposition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wench</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Maxwell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Optimizing large language models through highly dense reward structures and recursive thought process using monte carlo tree search</title>
		<author>
			<persName><forename type="first">K</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Blanchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Arvidsson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Automated evaluation of visual hallucinations in commercial large language models: A case study of chatgpt-4v and gemini 1.5 pro vision</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rikitoshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kunimoto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Evaluating the impact of environmental semantic distractions on multimodal large language models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kuhozido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dunfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ostrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Waterhouse</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Evaluating prompt injection safety in large language models using the promptbench dataset</title>
		<author>
			<persName><forename type="first">X</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Detecting llm hallucinations using monte carlo simulations on token probabilities</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ledger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mancinni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
