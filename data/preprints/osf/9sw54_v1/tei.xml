<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mixed ASR System for Amazigh and Arabic Under-Resourced Dialects in Maghreb Region</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mohamed</forename><surname>Hamidi</surname></persName>
							<email>mohamed.hamidi.5@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">LISAC -FSDM</orgName>
								<orgName type="institution">USMBA University</orgName>
								<address>
									<settlement>Fes</settlement>
									<country key="MA">Morocco</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hassan</forename><surname>Satori</surname></persName>
							<email>hsatori@yahoo.com</email>
							<affiliation key="aff0">
								<orgName type="department">LISAC -FSDM</orgName>
								<orgName type="institution">USMBA University</orgName>
								<address>
									<settlement>Fes</settlement>
									<country key="MA">Morocco</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ouissam</forename><surname>Zealouk</surname></persName>
							<email>ouissam.zealouk@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">LISAC -FSDM</orgName>
								<orgName type="institution">USMBA University</orgName>
								<address>
									<settlement>Fes</settlement>
									<country key="MA">Morocco</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Khaled</forename><surname>Lounnas</surname></persName>
							<email>klounnas@usthb.dz</email>
							<affiliation key="aff1">
								<orgName type="department">Computational linguistics Dept-CRSTDLA</orgName>
								<address>
									<settlement>Algiers</settlement>
									<country key="DZ">ALGERIA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">LCPTS-USTHB University</orgName>
								<address>
									<settlement>Algiers</settlement>
									<country key="DZ">ALGERIA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mourad</forename><surname>Abbas</surname></persName>
							<email>m.abbas@crstdla.dz</email>
							<affiliation key="aff1">
								<orgName type="department">Computational linguistics Dept-CRSTDLA</orgName>
								<address>
									<settlement>Algiers</settlement>
									<country key="DZ">ALGERIA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mohamed</forename><surname>Lichouri</surname></persName>
							<email>m.lichouri@crstdla.dz</email>
							<affiliation key="aff1">
								<orgName type="department">Computational linguistics Dept-CRSTDLA</orgName>
								<address>
									<settlement>Algiers</settlement>
									<country key="DZ">ALGERIA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hocine</forename><surname>Teffahi</surname></persName>
							<email>hteffahi@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="institution">LCPTS-USTHB University</orgName>
								<address>
									<settlement>Algiers</settlement>
									<country key="DZ">ALGERIA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Mixed ASR System for Amazigh and Arabic Under-Resourced Dialects in Maghreb Region</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C54C94800D0C10714BF5E9A7DCDBE55D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-10-22T06:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Speech recognition</term>
					<term>Mixed dialects</term>
					<term>HMMs</term>
					<term>MFCCs</term>
					<term>GMMs</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic Speech Recognition (ASR) technology plays an essential role in human-machine interaction. In this paper, we describe our conducted speech experiments that are realized to develop and adapt a mixed automatic speech recognition system based on spoken digits for Amazigh and Arabic dialects that are considered as under-resourced dialects in the Maghreb region. Our used database includes speech samples collected from 24 Moroccans and Algerian speakers including both males and females. The designed system is implemented based on the combination of hidden Markov models and Gaussian mixture models, as well as the Mel frequency spectral coefficients (MFCCs) feature extraction method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic Speech Recognition (ASR) is an interdisciplinary subfield of computational linguistics that creates systems that enable the recognition and translation of spoken words into text by machines. ASR is a multidisciplinary technique; it includes knowledge and research in computer science, linguistics and engineering fields.</p><p>The last few years have seen tremendous advances in language and dialect recognition that are starting to gain the attention of speech technology research, especially speech recognition for regional dialect. A wide range of work on recognition of spoken digits and alphabets is being performed in the field of ASR and many algorithms have been developed using a variety of technologies. In general, ASR researchers have targeted alphabets and spoken numbers for different languages. Most of the research has been done in English, Japanese and Mandarin <ref type="bibr" target="#b0">[1]</ref>, but very little research can be found in Amazigh and Arabic dialects that are Under-Resourced dialects in Maghreb region. In this paper, we aim to create a mixed ASR system that allows recognizing the ten fist spoken digits for Amazigh and Arabic dialects used in Morocco and Algeria that are considered as under-resourced dialects in the Maghreb region. In our realization, we use the static approach with hidden Markov Models combined to Gaussian mixture models.</p><p>This paper is organized as follows, an introduction in Section 1. Section 2 presents the related works. Section 3 presents the used materials and methods. Section 4 shows the system preparation. Experimental Results are given in Section 5. Finally, the conclusion in Section 6. Hamidi et al, <ref type="bibr" target="#b2">[3]</ref> Study of the performance of the Amazigh interactive digital voice recognition system in a noisy train environment HMM-GMM 72,43% with 3 dB and 0% with 39 dB Barkani et al., <ref type="bibr" target="#b3">[4]</ref> Development of an ASR Amazigh control system with Raspberry Pi board HMM-GMM 90.43%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related works</head><p>Zealouk et al., <ref type="bibr" target="#b4">[5]</ref> Detecting the differences between the voice of normal and pathological speakers based on the ASR system.</p><p>HMM-GMM 84.00% for normal speakers 27.50% for pathological speakers</p><p>Hamidi et al, <ref type="bibr" target="#b5">[6]</ref> Amazigh speech recognition system via interactive voice response (IVR) HMM-GMM 89.64%</p><p>Addarrazi et al, <ref type="bibr" target="#b6">[7]</ref> Recognizing the lip movement for lip-reading system DCT-HMM 84.99%</p><p>Telmem et al., <ref type="bibr" target="#b7">[8]</ref> Build an Amazigh speech recognition system HMMs and CNN 92%</p><p>Addarrazi et al, <ref type="bibr" target="#b8">[9]</ref> Audio visual speech recognition system Viola-Jones approach 99% for face detection. 96.6% for mouth detection.</p><p>Satori et al., <ref type="bibr" target="#b9">[10]</ref> Create an ASR system that allows detecting the smoker speakers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HMM-GMM</head><p>The recognition rate for smokers is 50% lower than for non-smokers.</p><p>Satori et al., <ref type="bibr" target="#b10">[11]</ref> Create an Amazigh speech recognition system based on digits and alphabets HMM-GMM 92.89 %</p><p>The majority of ASR's previous work in both Amazigh and Arabic has focused on the official languages known as Amazigh Language (AL) and Modern Standard Arabic (MSA). However, we find that these languages are not the language of Daily communication in some countries, such as Morocco and Algeria, where other types called dialects are used, which are a less researched area. Tables 1 summarizes some ASR previous work for Amazigh language. Tables 2 presents some ASR previous work for Arabic language. Alshayeji et al, <ref type="bibr" target="#b14">[15]</ref> Studying the effect of diacritics on Arabic ASR systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DNN from 4.68% to 42%</head><p>Alsharhan et al, <ref type="bibr" target="#b15">[16]</ref> Enhancement of Arabic ASR system based on the automatic generation of accurate audio text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HMM 71%</head><p>Frihia et al, <ref type="bibr" target="#b16">[17]</ref> Building a large vocabulary continuous speech recognition system SVM-HMM 0.05% Khelifa, et al, <ref type="bibr" target="#b17">[18]</ref> Building a teaching and learning system based on Arabic ASR technology HMM-GMM 97%</p><p>Wahyuni, <ref type="bibr" target="#b18">[19]</ref> Recognize spoken Arabic letters ANN 92.42%</p><p>Satori et al, <ref type="bibr" target="#b19">[20]</ref> Build an Arabic Automated Speech Recognition System HMM 96,67%</p><p>In <ref type="bibr" target="#b20">[21]</ref> authors have presented a novel method that combines the automatic speech recognition and language identification systems. Their work aims to develop a speech system that allows identifying the used dialect and recognizing the spoken digits based on modern standard Arabic and Amazigh Moroccan language. Their system was based on the SVM and HMM methods and their findings present that the proposed system performs 33% better than an ordinary speech recognition system.</p><p>Researchers in <ref type="bibr" target="#b21">[22]</ref> have described an automatic speech recognition system based on Darija Moroccan Dialect. The implemented system is able to recognize the ten first Darija digits collected from 20 speakers (males and females). In their work, the HMM-GMM combination system was used with MFCC feature extraction method and their best-obtained rate is 96.27 %.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Materials and methods</head><p>In this section, we describe our used Automatic speech recognition system, extraction technique and modeling algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Automatic Speech Recognition</head><p>Speech recognition <ref type="bibr" target="#b22">[23]</ref> is the process of decoding speech signals picked up by a microphone and converting them into words. These words can be exploited as commands, data input, or application control. Recently, this technology-based applications are often found in many fields like military, commercial, industrial, healthcare, telephony, etc.Fig. <ref type="figure" target="#fig_0">1</ref> presents the structure of ASR system. Recently, for Moroccan Amazigh ASR systems were targeted by our lab researchers <ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref> ; </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hidden Markov model</head><p>The hidden Markov model (HMM) <ref type="bibr" target="#b28">[29]</ref> is a statistical modeling method, his structure includes a finite ensemble of states and each one is associated with a probability distribution where the transition probabilities govern the transitions among the states. Fig. <ref type="figure">2</ref> presents a case of 3 states of the Hidden Markov Model.</p><p>HMMs have efficient learning algorithms that allow consistent handling of insertion and deletion penalties. They can also handle variable-length entries. However, HMMs</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Features Extraction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acoustic model</head><p>Language model</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dictionary</head><p>Words have a large number of unstructured parameters and produce a number of sub-optimal modeling assumptions that limit their effectiveness Fig. <ref type="figure">2</ref>. Three states HMM architecture</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Mel Frequency Cepstral Coefficients (MFCC)</head><p>The extraction of Mixed Frequency Cepstrum Coefficients (MFCC) <ref type="bibr" target="#b29">[30]</ref> comprises a frame-by-frame analysis of an input speech where the speech signal is segmented into a sequence of frames. Each frame allows a Fast Fourier Transform (FFT) to generate certain parameters, which are then subjected to a Mel perception scale and decorrelation. The result is a sequence of feature vectors describing a useful logarithmically compressed amplitude and simplified frequency information. Fig. <ref type="figure" target="#fig_1">3</ref> shows the process of the Mel frequency ceptral coefficient technique. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Mixed System Preparation</head><p>Our proposed mixed speech system is implemented on Ubuntu 14.04 LTS operating system and the used hardware is laptop with Intel Core i5 CPU of 2.4 GHz speed and RAM 4G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Corpus Preparation</head><p>In the field of automatic Amazigh speech recognition, voice databases are rare. All the works published in this area have been tested on databases that were created in our lab as part of this work. Our vocal corpora are recorded using a microphone using the WaveSurfer recording tool in .wav format. Our database includes four corpuses consist of ten separate spoken digits collected from 24 Moroccan and Algerian speakers (male and female) aged 18 to 50 years. The recording sample rate is 16 kHz, with a resolution of 16 bits. During the recording sessions, speakers were asked to pronounce the digits sequentially. Each speaker's recordings have been saved in a ".wav" file. During the recording session, each file was replayed to ensure that all words were included in the recorded signal. Wrongly spoken recordings were ignored and only correct recordings are kept. Table <ref type="table" target="#tab_2">3</ref> presents the audio database information. Table <ref type="table" target="#tab_3">4</ref> presents the used digits with four dialects. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Acoustic Models Preparations</head><p>Our acoustic models include the representations of ten first spoken digits created in the training phase by using audio data. To design our acoustic model, we gathered a set of input data and processed it using the SphinxTrain tool as shown in Figure <ref type="figure" target="#fig_2">4</ref>. The following list shows the input data and the files used.</p><p> A set of audio data (specific data for training).  Configuration files that exclude the relationship between training and testing of text and audio files  Language model gives a representation of the probability of occurrence for each used digit.  The dictionary determines the pronunciation of the used digits </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Pronunciation Dictionary</head><p>The dictionary performs the role of the intermediary between the Acoustic Model and Language Model. In our work, we have designed a mixed dictionary that includes the different used dialects. In this work, we have created two pronunciation dictionaries; the first one is com_dictionary (See Table <ref type="table" target="#tab_3">4</ref>) which includes 40 digits and their pronunciations where we have combined the dictionaries of four dialects that are Amazigh dialect, Darija dialect, Kabyle dialect and Algerian dialect. The second proposed dictionary is mix_dictionary (See Table <ref type="table" target="#tab_5">5</ref>) contains 32 words where we have designed a mixed and inclusive dictionary that includes digits with two or more variants where the alternate transcriptions marked with parenthesis like (1) stand for second pronunciation. For example, the word YEN appears with two variants YEN and YEN (1) whose phonetic descriptions are respectively YEN and Y A N. So, the same method we will apply with the digits which have the same or close pronunciation such as THEMANYA digit in Algerian dialect and TMANIA digit in Kabyle dialect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Database_Name</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training audio data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Configuration files</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dictionary</head><p>Acoustic Model </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>In order to create a mixed speech recognition system, that includes the ten first digits of Moroccan Amazigh, Algerian Kabyle, Moroccan Darija and Algerian Darija dialects, we have conducted several experiments with 3 and 5 HMM states and different Gaussian Mixture Models (4, 8, 16, 32 and 64). The recognition rates were observed and recorded for each experiment. Figure <ref type="figure" target="#fig_3">5</ref> presents the recognition rates of combined-system, which is based on com_dictionary, and mixed-system that is based on the mix_dictionary. The differences between two systems are 29.1, 28.6, 30.6, 19.6 and 13.6% are found for using 4, 8,16, 32 and 64 GMMs, respectively. It is found that 4 Gaussian mixture distributions obtained the best recognition rate of 78,8 %.</p><p>For mixed system, the best recognition rate is 78,8 % was found with 4 GMMs and the lower rate is 41,9% found with 64 GMMs. For combined system, the heigher rate is 49,7 % was observed with 4 GMMs and the lower rate is 28,3% found with 64 GMMs.</p><p>Figure <ref type="figure">6</ref> illustrates the recognition rates of mixed system with 5 HMMs, the obtained results are 75.8, 70.8, 67.8, 39.2 and 20.6 % found with using 4, 8,16, 32 and 64 GMMs, respectively. The best obtained recognition rate is 75.8 % found with 4 GMMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 6. The recognition rates of mixed-system based on 5 HMMs and different GMMs values</head><p>We observe the drop of recognition rates with the augmentation of GMMs value. Also we have observed that the substitution words increase with drop of recognition rates and the augmentation of GMMs values.  As a comparison, our results are lower than of those <ref type="bibr" target="#b10">[11]</ref> because in their implementation, they use only one language. On the other hand, our results are higher than of those <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we have presented a mixed speech recognition system that includes four dialects, which are Moroccan Amazigh, Algerian Kabyle, Moroccan Darija and Algerian Darija dialects. We have designed a mixed system based on the combination of hidden Markov models and Gaussian mixture models. We have proposed an effective approach to improve the accuracy of a mixed system based on the pronunciation dictionary. Our best obtained result is 78,8 % found with 3 HMMs and 4 GMMs and Our technique made a difference of around 29.1% from an ordinary method.</p><p>In the future, we will try to improve our obtained results by using large data and more ASR parameters. In addition, we will try to develop a mixed speech recognition system based on several dialects and languages.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. ASR system Architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The MFCC architecture</figDesc><graphic coords="6,166.55,147.35,273.75,172.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Acoustic model preparation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The recognition rates of combined-system and mixed-system systems based on 3 HMMs and several GMMs values</figDesc><graphic coords="10,139.45,286.70,316.55,146.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7</head><label>7</label><figDesc>shows the evolution of substitution words with different GMMs values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. The evolution of substitution words based on 3 and 5 HMMs and different GMMs values.</figDesc><graphic coords="11,124.80,447.00,345.85,196.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="9,124.80,183.50,345.60,199.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="9,124.80,420.10,345.60,201.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Some research on Amazigh speech recognition systems</figDesc><table><row><cell>Authors</cell><cell>Description</cell><cell>Method</cell><cell>Results</cell></row><row><cell>Zealouk et al., [2]</cell><cell>Evaluation of Amazigh digits (0-9) under car and grinder noisy environments</cell><cell>HMM-GMM</cell><cell>72.92% with 5 dB and 0.17% with 35 dB</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Some research on Arabic speech recognition systems</figDesc><table><row><cell>Authors</cell><cell>Description</cell><cell>Method</cell><cell>Results</cell></row><row><cell>Meftah et al, [12]</cell><cell>Emotional Speech Recognition by using Arabic</cell><cell>MLP-SVM</cell><cell>54.07% and 84.14%</cell></row><row><cell></cell><cell>language</cell><cell></cell><cell></cell></row><row><cell>Eljawad et al, [13]</cell><cell>Speech Recognition of Arabic language based on Fuzzy Logic and Neural Network</cell><cell>Fuzzy logic and neural network</cell><cell>77.1% and 94.5%</cell></row><row><cell>Elharati et al, [14]</cell><cell>MFCC and HMMs based Arabic ASR system</cell><cell>MFCC-HMMs</cell><cell>92.92%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Database description</figDesc><table><row><cell>Corpus</cell><cell>Parameter</cell><cell>Value</cell></row><row><cell>Am_digits</cell><cell>Description Speakers</cell><cell>Moroccan Amazigh dialect 6 (four for training and two for testing)</cell></row><row><cell>Dar_digits</cell><cell>Description Speakers</cell><cell>Moroccan Darija dialect 6 (four for training and two for testing)</cell></row><row><cell>Al_digits</cell><cell>Description Speakers</cell><cell>Algerian Darija dialect 6 (four for training and two for testing)</cell></row><row><cell>Kb_digits</cell><cell>Description Speakers</cell><cell>Algerian Kabyle dialect 6 (four for training and two for testing)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>The used Ten first digits</figDesc><table><row><cell>Number</cell><cell>Amazigh Dialect</cell><cell>Darija Dialect</cell><cell>Kabyle Dialect</cell><cell>Algerian Dialect</cell></row><row><cell>0</cell><cell>ILEM</cell><cell>SIFR</cell><cell>OULECH</cell><cell>SIFER</cell></row><row><cell>1</cell><cell>YEN</cell><cell>WAHD</cell><cell>YEWAN</cell><cell>WAHED</cell></row><row><cell>2</cell><cell>SIN</cell><cell>JOJ</cell><cell>SIN</cell><cell>ZOUJ</cell></row><row><cell>3</cell><cell>KRAD</cell><cell>TLATA</cell><cell>THYATHA</cell><cell>TLATHA</cell></row><row><cell>4</cell><cell>KUZ</cell><cell>RABAA</cell><cell>REBAA</cell><cell>RBAA</cell></row><row><cell>5</cell><cell>SEMUS</cell><cell>KHAMSA</cell><cell>KHEMSA</cell><cell>KHMSA</cell></row><row><cell>6</cell><cell>SEDISS</cell><cell>STTA</cell><cell>SETSA</cell><cell>STTA</cell></row><row><cell>7</cell><cell>SA</cell><cell>SBAA</cell><cell>SEBAA</cell><cell>SEBAA</cell></row><row><cell>8</cell><cell>TAM</cell><cell>THMANYA</cell><cell>THMANIA</cell><cell>THMANYA</cell></row><row><cell>9</cell><cell>TZA</cell><cell>TSAAOD</cell><cell>TESSAA</cell><cell>TESAA</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>The combination of the separated used dictionaries</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>The proposed mixed dictionary</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Noise Effect on Arabic Alphadigits in Automatic Speech Recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Alotaibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mamun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghulam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IPCV</title>
		<imprint>
			<date type="published" when="2009-07">2009, July</date>
			<biblScope unit="page" from="679" to="682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Noise effect on Amazigh digits in speech recognition system</title>
		<author>
			<persName><forename type="first">O</forename><surname>Zealouk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Satori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Laaidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hamidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Satori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Speech Technology</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Amazigh digits through interactive speech recognition system in noisy environment</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hamidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Satori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Zealouk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Satori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Speech Technology</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="101" to="109" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Amazigh Speech Recognition Embedded System</title>
		<author>
			<persName><forename type="first">F</forename><surname>Barkani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Satori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hamidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Zealouk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Laaidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 1st International Conference on Innovative Research in Applied Science, Engineering and Technology (IRASET)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-04">2020, April</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pathological Detection Using HMM Speech Recognition-Based Amazigh Digits</title>
		<author>
			<persName><forename type="first">O</forename><surname>Zealouk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Satori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hamidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Satori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Embedded Systems and Artificial Intelligence</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="281" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Interactive Voice Application-Based Amazigh Speech Recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hamidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Satori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Zealouk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Satori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Embedded Systems and Artificial Intelligence</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="271" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Lip Movement Modeling Based on DCT and HMM for Visual Speech Recognition System</title>
		<author>
			<persName><forename type="first">I</forename><surname>Addarrazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Satori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Satori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Embedded Systems and Artificial Intelligence</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="399" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Comparative Study of HMMs and CNN Acoustic Model in Amazigh Recognition System</title>
		<author>
			<persName><forename type="first">M</forename><surname>Telmem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ghanou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Embedded Systems and Artificial Intelligence</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="533" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Amazigh audiovisual speech recognition system design</title>
		<author>
			<persName><forename type="first">I</forename><surname>Addarrazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Satori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Satori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 Intelligent Systems and Computer Vision (ISCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-04">2017, April</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Voice comparison between smokers and non-smokers using HMM speech recognition system</title>
		<author>
			<persName><forename type="first">H</forename><surname>Satori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Zealouk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Satori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Elhaoussi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Speech Technology</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="771" to="777" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Investigation Amazigh speech recognition using CMU tools</title>
		<author>
			<persName><forename type="first">H</forename><surname>Satori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Elhaoussi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Speech Technology</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="235" to="243" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Emotional Speech Recognition Using Rhythm Metrics and a New Arabic Corpus</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Meftah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qamhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Alotaibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Selouani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 16th IEEE International Colloquium on Signal Processing &amp; Its Applications (CSPA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-02">2020, February</date>
			<biblScope unit="page" from="57" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Arabic Voice Recognition Using Fuzzy Logic and Neural Network</title>
		<author>
			<persName><forename type="first">L</forename><surname>Eljawad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Aljamaeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alsmadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Almarashdeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Abouelmagd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Alsmadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename><surname>Alazzam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Eljawad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Aljamaeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Alsmadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Al-Marashdeh</forename></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Abouelmagd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Alsmadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Haddad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Alkhasawneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alzughoul</forename><surname>Ra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alazzam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="651" to="662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Arabic Speech Recognition System Based on MFCC and HMMs</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Elharati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alshaari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">Z</forename><surname>Këpuska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and Communications</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">03</biblScope>
			<biblScope unit="page">28</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Diacritics effect on arabic speech recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Alshayeji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sultan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arabian Journal for Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="9043" to="9056" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improved Arabic speech recognition system through the automatic generation of fine-grained phonetic transcriptions</title>
		<author>
			<persName><forename type="first">E</forename><surname>Alsharhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramsay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="343" to="353" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">HMM/SVM segmentation and labelling of Arabic speech for speech recognition applications</title>
		<author>
			<persName><forename type="first">H</forename><surname>Frihia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Speech Technology</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="563" to="573" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Constructing accurate and robust HMM/GMM models for an Arabic speech recognition system</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Khelifa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Elhadj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Abdellah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Belkasmi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Speech Technology</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="937" to="949" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Investigation Arabic Speech Recognition Using CMU Sphinx System</title>
		<author>
			<persName><forename type="first">H</forename><surname>Satori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hiyassat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Haiti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chenfour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Arab Journal of Information Technology (IAJIT)</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Arabic speech recognition using MFCC feature extraction and ANN classification</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Wahyuni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 2nd International conferences on Information Technology, Information Systems and Electrical Engineering (ICITISEE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-11">2017, November</date>
			<biblScope unit="page" from="22" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">CLIASR: A Combined Automatic Speech Recognition and Language Identification System</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lounnas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Satori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Teffahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abbas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lichouri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 1st International Conference on Innovative Research in Applied Science, Engineering and Technology (IRASET)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-04">2020. April</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Moroccan Dialect Speech Recognition System Based on CMU SphinxTools</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ezzine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Satori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hamidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Satori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 International Conference on Intelligent Systems and Computer Vision (ISCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06">2020. June</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Spoken language processing: A guide to theory, algorithm, and system development</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Hon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Foreword</forename><surname>By-Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Prentice hall PTR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Speech Coding Effect on Amazigh Alphabet Speech Recognition Performance</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hamidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Satori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Zealouk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Satori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Advanced Research in Dynamical and Control Systems</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1392" to="1400" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Vocal parameters analysis of smoker using Amazigh language</title>
		<author>
			<persName><forename type="first">O</forename><surname>Zealouk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Satori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hamidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Laaidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Satori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Speech Technology</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="85" to="91" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Interactive Voice Response Server Voice Network Administration Using Hidden Markov Model Speech Recognition System</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hamidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Satori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Zealouk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Satori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Laaidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 Second World Conference on Smart Trends in Systems, Security and Sustainability</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-10">2018, October</date>
			<biblScope unit="page" from="16" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Amazigh Digits Speech Recognition System Under Noise Car Environment</title>
		<author>
			<persName><forename type="first">O</forename><surname>Zealouk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hamidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Satori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Satori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Embedded Systems and Artificial Intelligence</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="421" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Building a first Amazigh database for automatic audiovisual speech recognition system</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ilham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khalid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Conference on Smart Digital Environment</title>
		<meeting>the 2nd International Conference on Smart Digital Environment</meeting>
		<imprint>
			<date type="published" when="2018-10">2018, October</date>
			<biblScope unit="page" from="94" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A tutorial on hidden Markov models and selected applications in speech recognition</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="257" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Human computer interaction using isolated-words speech recognition technology</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A A</forename><surname>Shariah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Ainon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zainuddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">O</forename><surname>Khalifa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2007 International Conference on Intelligent and Advanced Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007-11">2007, November</date>
			<biblScope unit="page" from="1173" to="1178" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
