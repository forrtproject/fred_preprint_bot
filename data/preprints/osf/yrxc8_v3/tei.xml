<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Physiological Encoding of Social Evaluation: Modeling Eye Movements, Pupillary, and Neuronal Responses to Faces</title>
				<funder ref="#_futcsTd">
					<orgName type="full">U.S. National Institutes of Health</orgName>
				</funder>
				<funder>
					<orgName type="full">National University of Singapore Department of Psychology</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Anqi</forename><surname>Mao</surname></persName>
							<email>mao.anqi@u.nus.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Runnan</forename><surname>Cao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">Washington University in St. Louis</orgName>
								<address>
									<country>U.S</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sai</forename><surname>Sun</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Frontier Research Institute for Interdisciplinary Sciences</orgName>
								<orgName type="institution">Tohoku University</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuo</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">Washington University in St. Louis</orgName>
								<address>
									<country>U.S</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dongwon</forename><surname>Oh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<addrLine>9 Arts Link AS4-02-07</addrLine>
									<postCode>117570</postCode>
									<settlement>Singapore</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<addrLine>9 Arts Link AS4-02-17</addrLine>
									<postCode>117570</postCode>
									<settlement>Singapore</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Physiological Encoding of Social Evaluation: Modeling Eye Movements, Pupillary, and Neuronal Responses to Faces</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3ABF5C993B4BE6C9B21D17DFA67ABA2F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-22T14:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>person perception</term>
					<term>face processing</term>
					<term>social cognition</term>
					<term>eye tracking</term>
					<term>single neuron recording</term>
					<term>computational modeling Physiological Encoding of Social Evaluation: Modeling Eye Movements</term>
					<term>Pupillary</term>
					<term>and Neuronal</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Face-based social evaluations are ubiquitous and consequential. These evaluations are organized along two fundamental dimensions: valence (positive versus negative intentions) and power (high versus low dominance). However, how these dimensions manifest in spontaneous physiological responses remains unclear. Across three studies (96 participants and 506 single neurons), we used data-driven modeling to identify dimension-specific physiological signatures. Positive evaluations along the valence dimension were encoded by pupillary constriction and decreased neural activity in both amygdala and hippocampus. Amygdalar suppression was feature-specific, emerging only during mouth fixations, whereas the hippocampus exhibited feature-invariant suppression across all fixations. Power evaluations were encoded by pupil dilation, prolonged eye-region fixations, and increased hippocampal activity. Furthermore, social anxiety selectively heightened the sensitivity of pupillary responses to valence but did not affect power encoding. This selective modulation confirms that these responses are driven by high-level social computations rather than reflexive reactions to low-level visual features. Our findings demonstrate how the fundamental dimensions of social evaluation are embodied across multiple physiological systems within moments of viewing a face.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Significance Statement</head><p>Everyday social decisions hinge on split-second impressions of unfamiliar faces. When we see a face, we instantly judge their intention (positive ↔ negative valence) and social influence (high ↔ low power). Here, across three complementary studies, we show that the same valence and power evaluation processes are reflected in physiological responses to faces across modalities: overt attention, autonomic arousal, and subcortical neuronal firing. This cross-system convergence bridges cognitive models of person perception with the cellular mechanisms that implement them, revealing how attention, arousal, and deep-brain coding jointly support rapid social evaluation.</p><p>Word Count: 91 these momentary expressions as indicators of stable personality traits (e.g., inferring trustworthiness from a smile) <ref type="bibr" target="#b47">(Zebrowitz, 2005</ref><ref type="bibr" target="#b48">(Zebrowitz, , 2017))</ref>. Even emotionally "neutral" faces that structurally resemble these emotional expressions can trigger similar impressions <ref type="bibr" target="#b20">(Jaeger &amp; Jones, 2022;</ref><ref type="bibr" target="#b36">Said et al., 2009)</ref>. However, whether social evaluations of emotionally neutral faces produce similar physiological responses across modality remains unclear.</p><p>Perceiver characteristics may further shape how social evaluation is encoded in eye movements and pupillary responses. Social anxiety, in particular, influences how individuals interpret facial cues during interactions, making them more sensitive to negative information <ref type="bibr" target="#b30">(Morrison &amp; Heimberg, 2013)</ref>. For example, socially anxious individuals evaluate threatening faces (e.g., angry faces) more negatively and ambiguous smiles as less trustworthy <ref type="bibr" target="#b15">(Gutiérrez-García &amp; Calvo, 2016;</ref><ref type="bibr" target="#b29">Mogg et al., 2004)</ref>. This heightened sensitivity further alters their eye movement patterns and pupillary responses, producing longer fixations on eyes and greater pupil dilation in response to negative faces <ref type="bibr" target="#b12">(Delchau et al., 2022;</ref><ref type="bibr" target="#b25">Kret et al., 2013)</ref>. Crucially, such modulation by perceiver traits provides validation that these eye movement and pupillary signatures index high-level social evaluation rather than low-level visual reflexes.</p><p>Neurons in the human amygdala and hippocampus encode both emotional properties of facial expressions and social evaluations. For example, amygdala activation positively correlates with emotion intensity (S. <ref type="bibr" target="#b45">Wang et al., 2017)</ref>, and distinguishes between positive and negative affect <ref type="bibr" target="#b23">(Kim et al., 2017)</ref>. Similarly, larger hippocampal volumes are associated with better facial expression perception <ref type="bibr" target="#b39">(Szymkowicz et al., 2016)</ref>. Beyond emotion processing, amygdalar and hippocampal neurons encode social evaluation processes <ref type="bibr" target="#b7">(Cao, Lin, et al., 2022)</ref>. Specifically, the amygdala is essential for evaluating trustworthiness, a valence-driven trait <ref type="bibr" target="#b0">(Adolphs, 2008)</ref>, while its activation level is independent of dominance evaluations, a proxy for power evaluations <ref type="bibr" target="#b40">(Todorov, 2012)</ref>.</p><p>Across three studies, we integrate eye tracking, pupillometry, and intracranial recordings to examine how valence and power evaluations, and the specific trait evaluations summarized by these dimensions, manifest in attention, physiological arousal, and neural activity. Preliminary Study tested whether eye movements and pupillary responses encode spontaneous social evaluations during passive face viewing. Study 1 probed the meaning of this response patternhigh-level social computations versus low-level visual confounds-by (1) controlling luminance across faces and (2) examining whether social anxiety could enhance physiological sensitivity to socially salient faces. Study 2 aligned amygdala and hippocampus activity with fixations and saccades to test how social evaluations are represented in neural responses, distinguishing between feature-specific and feature-invariant encoding. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stimuli and Materials</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stimuli Generation</head><p>All studies used emotionally neutral faces randomly generated with the FaceGen Software Development Kit (<ref type="url" target="https://facegen.com/sdk.htm">https://facegen.com/sdk.htm</ref>). In FaceGen, each face is represented as a point within a 100-dimensional space with each dimension capturing an orthogonal variation in facial shape or skin texture (50 dimensions each). Altering a point along any dimension changes the corresponding holistic aspect of the face shape or texture. These facial stimuli vary on the 50 shape and 50 texture dimensions. Each face can be represented as a vector fi = [Si(1), Si(2), …, Si(50), Ti(1), Ti(2), …, Ti(50)] T , where i denotes the number of faces, and Si(j) denotes the j th shape dimension of i th face, and Ti(j) denotes the j th texture dimension of i th face. The 300 faces can be represented by <ref type="bibr">⋯ 𝑇 300(50)</ref> ].</p><formula xml:id="formula_0">a face matrix F = [f1, f2, …, f300] or F = [ 𝑆 1(1) ⋯ 𝑆 300(1) ⋮ ⋱ ⋮ 𝑇 1(50)</formula><p>In Preliminary Study and Study 2, we used a stimulus set of 300 random emotionally neutral Caucasian faces <ref type="bibr" target="#b32">(Oosterhof &amp; Todorov, 2008)</ref>. In Study 1, we used a stimulus set of 300 random emotionally neutral East Asian faces <ref type="bibr" target="#b31">(Oh et al., 2018)</ref> to match participants' East-Asianmajority environment (Singapore). This matching procedure helps reduce potential biases in eye movements <ref type="bibr" target="#b19">(Hills &amp; Pake, 2013)</ref> and in evaluations stemming from viewing other-race faces <ref type="bibr" target="#b9">(Charbonneau et al., 2020)</ref>. To control for the influence of facial texture on observers' pupillary responses, all facial stimuli in Study 1 were created to vary only along shape dimensions while sharing the same facial texture. We further minimized low-level visual confounds by equalizing the average luminance across all faces using <ref type="bibr">Python (3.13.6)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Social Anxiety Questionnaire</head><p>To investigate how individual differences in social anxiety modulate the strength in which social evaluations are encoded in physiological responses, in Study 1, we assessed participants' social anxiety levels. We used the Liebowitz Social Anxiety Scale (LSAS-SR; <ref type="bibr" target="#b26">Liebowitz, 1987)</ref>, a 24-item scale measuring anxiety (0 = none; 3 = severe) and social-avoidance frequency (0 = never; 3 = usually) experienced in various social interaction (e.g., "Meeting strangers", "Going to a party") (see Table <ref type="table">S3</ref> for the full scale). The scale demonstrates good internal consistency <ref type="bibr" target="#b2">(Baker et al., 2002;</ref><ref type="bibr" target="#b17">Heimberg et al., 1999)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Apparatus</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Settings</head><p>All experiments were administered using MATLAB Psychtoolbox 3 <ref type="bibr" target="#b5">(Brainard, 1997;</ref><ref type="bibr" target="#b24">Kleiner et al., 2007)</ref>. In Preliminary Study and Study 1, participants performed tasks in a lightcontrolled, sound-attenuated laboratory room, with their eye movements tracked. In Preliminary Study, participants were seated ~60 cm from a 20-inch Lenovo CRT monitor (1024 × 768 pixels), while in Study 1, they were seated ~80 cm from a 24-inch VIEWPixx/EEG LCD monitor (1920 × 1080 pixels, 120 Hz refresh rate). The VIEWPixx/EEG monitor provided high spatial uniformity and reliable temporal luminance response <ref type="bibr" target="#b14">(Ghodrati et al., 2015)</ref>, minimizing unintended brightness variations that could confound pupillary responses. In Study 2, patients performed the experiment while undergoing clinical epilepsy monitoring, with simultaneous intracranial recordings and eye tracking (Figure <ref type="figure" target="#fig_0">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Eye Tracking</head><p>Eye movements and pupil size were recorded using noninvasive infrared eye trackers.</p><p>Preliminary Study used the EyeLink 1000 (Tower Mount), and Study 1 used the EyeLink 1000 Plus (Desktop Mount) (SR Research) with one eye tracked at 1,000 Hz. For Preliminary Study and Study 1, participants' heads were stabilized with a chin rest for precise measurement. In Study 2, which involved neurosurgical patients, we used the EyeLink 1000 in Remote Mode, with one eye tracked at 500 Hz. EyeLink 1000 Remote Mote allows for head-free tracking, making it ideal for patients who need to be integrated with other equipment.</p><p>A standard 9-point calibration was implemented before the task. Saccades were detected with a motion threshold of 0.1° visual angle, velocity of at least 30°/s, acceleration of 8,000°/s², and duration of at least 4 msec. Fixations were identified during intervals without saccadic movement, with the start and end marked by the offset of the previous saccade and the onset of the subsequent saccade, respectively.</p><p>Pupil size during each fixation was recorded in arbitrary units (100-10,000) with 1-unit precision and 0.2%-diameter noise. To convert arbitrary units to millimeters (mm), a calibration process using a printed artificial eye (8-mm diameter) was implemented. The eye tracker recorded the artificial eye to establish a reference scale, converting arbitrary units to its actual size. This calibrated scale was applied to all subsequent measurements, ensuring accurate unit conversion.</p><p>Areas of Interest (AOIs) for fixations were determined using the Voronoi tessellation method, recognized for its objectivity and robustness to noise <ref type="bibr" target="#b18">(Hessels et al., 2016)</ref>. Cell centers were defined over key facial regions (eyes, nose, mouth) and were connected by lines.</p><p>Perpendicular bisectors were created for each line, with the AOIs defined by the intersections of these bisectors. An oval mask was applied over the face to determine the analysis boundary, excluding fixations outside the facial contours from further analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single Neuron Recording</head><p>Single neuron recordings were conducted using implanted depth electrodes in the amygdala and hippocampus of patients with pharmacologically intractable epilepsy. The specific target locations within these regions were confirmed using post-implantation computed tomography (CT) scans and structural magnetic resonance imaging (MRI) scans. In accordance with previous procedures <ref type="bibr">(Rutishauser, Mamelak, et al., 2006)</ref>, recordings were made from eight 40-mm microwires that were integrated into a clinical electrode. Efforts were made to avoid passing the electrode through a sulcus and its attendant sulcal blood vessels, which led to slight variations in the electrode placement; however, it remained well within the body of the target area. The microwires extended medially from the end of the depth electrode at an estimated spread of approximately 20-30 deg. It is likely that the amygdala electrodes sampled neurons in the midmedial portion of the amygdala, predominantly within the basomedial nucleus, or potentially the deepest part of the basolateral nucleus.</p><p>Bipolar wide-band settings ranging from 0.1 to 9,000 Hz were employed, with one microwire serving as a reference. Signals were sampled at 32 kHz and stored continuously for offline analysis using a Neuralynx system. The raw signal was filtered with a zero-phase lag 300-3,000 Hz bandpass filter and spikes were sorted using a semiautomatic template matching algorithm as described previously <ref type="bibr">(Rutishauser, Schuman, et al., 2006)</ref>. Each unit was carefully isolated, and the quality of both the recording and the spike sorting were quantitatively assessed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure</head><p>Prior to the experiment, all participants provided informed consent. In Preliminary Study, participants passively viewed faces without any specific task. They completed 900 trials, divided into nine blocks of 100. The first three blocks each contained a distinct set of 100 faces from the pool of 300 stimuli; these three blocks were then repeated twice, with randomized trial order within each repetition. In Study 1 and Study 2, participants were randomly assigned to either a dominance or a trustworthiness judgment task. They were required to rate a face from 1 (not at all) to 4 (very trustworthy/dominant) by pressing the corresponding number keys. In Study 1, participants viewed 300 unique faces, organized into three blocks of 100 trials, with block sequence counterbalanced across participants. In Study 2, patients completed between 2 and 9 sessions depending on signal stability and medical condition, with each session containing 300 trials (three blocks of 100). Each session featured unique faces drawn from the stimulus set.</p><p>For each trial across studies, a fixation cross was presented for 1 to 2 sec, followed a face stimulus (1.5 sec in Preliminary Study and Study 2; 2 sec in Study 1). In the passive-viewing task, the next trial began automatically after stimulus presentation. In the rating tasks, the next trial began after participants made their response or automatically after 2 sec if no response was entered. Simultaneous Single Neuron Recording and Eye Tracking (Study 2) Note: Participants performed the experiment while undergoing simultaneous single neuron recording and eye tracking. Top Right: fixation locations (green dots) on a facial stimulus and a single neuron raster plot from an example trial. The plot is marked with the periods of stimulus presentation (blue line) and the corresponding fixation periods (green line). Bottom Right: Electrode localization within the hippocampus and amygdala in the left hemisphere in a sample participant. The sagittal slices show the positions in the left hemisphere. R=right.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistical Analyses</head><p>Eye movement measures were extracted using MATLAB (R2024b), while model building and subsequent model comparisons were performed in R Studio (version 4.2.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Eye-Tracking Measures</head><p>For each trial, 53 eye-tracking measures related to fixations, saccades, and pupil size were calculated for the period between facial stimulus onset and offset. For each participant, we averaged measures across repeated presentations of the same face to minimize measurement error. Fixation Measures. Fixation measures included fixation count and durations. For each trial, total fixation number, mean duration, and total duration were calculated between stimulus onset and offset. These measures were then further segregated by AOIs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pupil</head><p>Saccade Measures. Saccade metrics included mean duration, amplitude, velocity, and total number of saccades per trial. Saccades were also analyzed based on their onset and offset locations, with the number, average duration, amplitude, and velocity of saccades starting or ending in each AOI calculated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neuronal Measures</head><p>Neuronal firing data were synchronized with fixation and saccade points. Neuronal activity during each fixation or saccade was quantified by averaging the firing rate within a 400 ms window centered around the onset of each fixation or saccade (±200ms). Consistent with preceding studies, we only considered fixations and saccades that occurred between the onset and offset of the facial stimulus. For each trial, the mean firing rates (spikes/sec) were calculated based on fixationaligned or saccade-aligned neuronal activities. We also calculated averaged firing rates by grouping neuronal activities based on fixation AOIs and the starting or ending AOIs of saccades, yielding measures of mean neuronal firing rates during fixations on the eyes, nose, and mouth, or during saccades starting or ending at these regions.</p><p>For each recorded neuron, neuronal measures were calculated for each trial. These measures were then averaged across trials presenting the same face, resulting in a single value for each measure per face. Following previous procedures <ref type="bibr">(Cao, Todorov, et al., 2022)</ref>, only neurons with an overall firing rate (FR) of at least 0.15 Hz during fixations throughout the entire task were included. After applying these criteria, 615 neurons remained (320 in the amygdala, 295 in the hippocampus; see Table <ref type="table">S4</ref> for details) after screening 858 total neurons. To ensure that the selected neurons encode high-level, abstract social information evaluated from faces (or specific facial features), rather than merely responding preferentially to the physical properties of certain facial features (e.g., eye-preferred neurons in amygdala, <ref type="bibr" target="#b6">Cao et al., 2021)</ref>, paired-sample t-tests were conducted on each neuron to compare firing rates during fixations on the eyes versus the mouth. We identified 506 neurons (244 in the hippocampus, 262 in the amygdala) that showed no significant differences in their responses between fixations on these facial features (all ps ≥ .051).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Building</head><p>We used data-driven modeling to quantify how physiological measures vary with changes in facial appearance. This approach, validated in social perception research <ref type="bibr" target="#b32">(Oosterhof &amp; Todorov, 2008;</ref><ref type="bibr" target="#b42">Todorov &amp; Oh, 2021)</ref>, identifies facial features that systematically drive response variations.</p><p>First, the measure of interest, collected for each randomly generated face, was represented as a vector m = [m1, m2, …, m300]. The measure m then multiplied the face space F, resulting in a model vector 𝛥 . To facilitate model comparisons, Δ was standardized using its Frobenius norm. The standardized model 𝛥 ̂ represents the changes in facial appearance by 1 SD along the measurement dimension. With this learned model 𝛥 ̂, we could scale its weights along the learned dimension to synthesize new facial stimuli. Conceptually, this approach creates 'model faces' for each physiological measure, representing the specific facial features that most strongly drive changes in that response (e.g., faces that maximally increases/decreases pupil size). This data-driven approach allows for the discovery of subtle, complex patterns in facial appearance that may not be apparent to observers, providing a powerful tool for objectively understanding the perceptual basis of social evaluations without prior assumptions.</p><p>For each physiological measure, we first built individual models for each participant and then derived the final model by averaging these individual models. Unlike previous studies that averaged raw measures across participants (e.g., <ref type="bibr" target="#b32">Oosterhof &amp; Todorov, 2008)</ref>, we first created individual models to account for baseline individual differences in these physiological measures, then averaged these standardized models to produce the final group model. Baseline metrics, such as pupil diameter, fixation stability, eye movement patterns, and neuronal responses, vary considerably across individuals <ref type="bibr" target="#b4">(Bargary et al., 2017;</ref><ref type="bibr" target="#b11">de Haas et al., 2019;</ref><ref type="bibr" target="#b43">Unsworth et al., 2019;</ref><ref type="bibr" target="#b46">Waschke et al., 2025)</ref>. These individual differences can introduce noise that reduces the ability to detect meaningful relationships between physiological measures and facial appearances.</p><p>Only shape dimensions were included in the analysis to avoid potential confounding effects of texture variations. This was especially important for Preliminary Study and Study 2, where facial texture was not held constant. Texture can independently influence both social evaluations and pupillary responses through luminance changes. For example, darker skin, can influence both social evaluations (e.g., increased dominance, <ref type="bibr" target="#b32">Oosterhof &amp; Todorov, 2008)</ref>, and the luminance of the screen. The changes in luminance can trigger involuntary pupil dilation or constriction as a natural adjustment to light levels. Such pupillary responses could be misinterpreted as cognitive or emotional responses to the face, confounding the assessment of how higher-level face processing (e.g., social evaluations) affects pupillary responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Consistency</head><p>To facilitate meaningful representation of the final averaged model, it is important to ensure that individual models are similar across participants. Two-way random effects intraclass correlation coefficients (ICC2, two-way random effects) were calculated for each eye movement measure. Higher ICC values indicate that changes in eye movement responses are consistently observed across participants when facial appearances vary in the same way. To ensure that the selected models were generalizable across different conditions and populations, we included individual eye-tracking measure models from all participants across all three studies for consistency analysis. This approach also helped to compensate for the smaller sample sizes in Preliminary Study and Study 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Comparisons</head><p>The selected eye-tracking measure models were then compared to previously validated social trait models <ref type="bibr" target="#b32">(Oosterhof &amp; Todorov, 2008;</ref><ref type="bibr" target="#b41">Todorov et al., 2013;</ref><ref type="bibr" target="#b42">Todorov &amp; Oh, 2021)</ref>. Each social trait model captures the facial information people use to make specific trait evaluations. In the present work, we included models for nine traits: attractiveness, competence, trustworthiness, dominance, meanness, frightfulness, extroversion, threat, and likability. In addition to the nine social trait models, models of the valence and power dimensions of the social trait space were included. Valence and power dimensions were derived from principal component analysis on representative trait ratings on face images, capturing the largest shared variance among them <ref type="bibr" target="#b32">(Oosterhof &amp; Todorov, 2008)</ref>. Valence reflects a continuum from negative to positive evaluations, while power reflects the perceived level of social influence. These two-dimensional have also been extensively validated <ref type="bibr" target="#b22">(Jones et al., 2021;</ref><ref type="bibr" target="#b42">Todorov &amp; Oh, 2021)</ref>. Only the shape dimensions of the social trait models were included in the analysis.</p><p>To assess the similarity between the eye-tracking measure models and social trait models, Pearson correlations were calculated for each pair. Permutation tests with 1,000 iterations were conducted to further validate the robustness of these similarity measures. For each iteration, eyetracking measures (e.g., mean fixation duration, mean pupil size) were randomly shuffled across trials within each participant, preserving subject-level structure while disrupting the mapping between facial identity and physiological responses. By comparing the observed Pearson coefficients (based on the actual data) with those obtained from randomly shuffled data (null distribution), we empirically determined statistical significance. This additional procedure provided a stringent evaluation of the robustness of our findings.</p><p>If an eye-tracking measure model (e.g., fixation duration) correlates with a social trait model (e.g., trustworthiness), it suggests that the facial information driving that trait judgment also contributes to the variation in the eye-tracking measure. Therefore, a positive correlation implies that faces evaluated as higher on the trait tend to elicit larger values of the eye-tracking measure, whereas a negative correlation implies that the facial changes increasing one response tend to decrease the other. In either case, changes in the eye-tracking measure reflect the corresponding face-based social evaluation. By contrast, the absence of such a relationship indicates that the two models vary along largely distinct dimensions in face space, meaning that the eye-tracking measure does not encode the particular social information evaluated from faces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Social Anxiety</head><p>We first built models for each eye-tracking measure and then compared them to prevalidated social trait models, as in Preliminary Study. We then conducted correlation analyses between participants' social anxiety scores and the strength of the association between eyetracking measure models and social trait models (quantified by Fisher's Z-transformed correlation coefficients). This analysis tested whether the encoding of social information in eye movements and pupillary responses is modulated by individual differences in social anxiety-a modulation that would only occur if these measures encoded abstract social information. Specifically, if social anxiety increases the sensitivity of eye movements and pupillary responses to socially threatening faces, this would confirm that these physiological measures encode high-level, abstract social meaning, rather than merely low-level visual features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Eye Movements and Pupillary Responses</head><p>We first examined how face-based social evaluations are encoded in eye movements and pupillary responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Eye-Tracking Measure Model Building</head><p>We built individual participant models quantifying how facial appearance variations influenced eye movements and pupillary responses. Next, two-way random effects ICC were calculated for each eye-tracking measure to assess if individual models are similar across participants. Twenty eye-tracking measures whose models exhibited the highest cross-participant consistency across all three studies were selected for further analysis (ICCs &gt; .11; see Table <ref type="table">S5</ref> for details). These ICC values, while modest, represent meaningful consistency given substantial individual differences in baseline oculomotor behavior <ref type="bibr" target="#b4">(Bargary et al., 2017;</ref><ref type="bibr" target="#b43">Unsworth et al., 2019)</ref>. For each selected eye-tracking measure, the final model was derived by averaging the individual models across participants (Figures <ref type="figure" target="#fig_2">2b</ref>, <ref type="figure" target="#fig_3">3b</ref>, and <ref type="figure" target="#fig_4">4b</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Correlation with Social Trait Models</head><p>To investigate how pupillary responses and eye movements systematically vary with social traits evaluated from faces, we correlated each eye-tracking measure model with prevalidated social trait models <ref type="bibr" target="#b32">(Oosterhof &amp; Todorov, 2008)</ref> using Pearson correlation.</p><p>Preliminary Study. The results revealed that social evaluations are systematically encoded in certain eye-tracking measures (Figure <ref type="figure" target="#fig_2">2a</ref>; Table <ref type="table">S6</ref>).</p><p>Pupillary Responses. Pupil size increased across multiple measures, including the mean pupil size, last-fixation pupil size, and mean pupil size during mouth fixations, when participants viewed faces associated with negative/neutral-valence, high power evaluations. Mean pupil size increased for faces associated with negative-valence, high-power traits: dominant (r = .51), frightening (r = .49), mean (r = .51), and threatening (r = .49) (all ps &lt; .001). The same pattern was found for pupil size during the last fixation; larger pupils were observed for dominant (r = .64), frightening (r = .57), mean (r = .59), and threatening faces (r = .61) (all ps &lt; .001). Likewise, mean pupil size, when fixating on the mouth, increased for dominant (r = .38, p = .007), frightening (r = .46, p = .001), mean (r = .43, p = .002), and threatening faces (r = .42, p = .002).</p><p>Conversely, decreases in these measures were observed when participants viewed faces associated with positive-valence, low/neutral-power evaluations. Mean pupil size was smaller for faces evaluated as more attractive (r = -.34, p = .016), likable (r = -.33, p = .018), and trustworthy (r = -.33, p = .021). Similar decreases were seen for pupil size during the last fixation-smaller pupils for attractive (r = -.33, p = .018), likable (r = -.31, p = .028), and trustworthy faces (r = -.38, p = .007). Likewise, mean pupil size when fixating on the mouth was smaller for attractive (r = -.40, p = .005), likable (r = -.41, p = .003), and trustworthy faces (r = -.35, p = .013). Pupil size models showed no correlation with competence model (a positive-valence, high-power trait), |r|s ≤ .19, all ps ≥ .196.</p><p>Pupillary responses encoded both the valence and the power dimensions (Figure <ref type="figure" target="#fig_2">2c</ref>). Mean pupil size decreased for faces evaluated more positively on the valence dimension (r = -.46, p = .001) and increased when viewing faces evaluated as more powerful (r = .43, p = .002). The same pattern emerged for pupil size during the last fixation (valence: r = -.53, p &lt; .001; power: r = .56, p &lt; .001), and partially for mean pupil size during fixations on the mouth (valence: r = -.48, p &lt; .001, power: r = .27, p = .058).</p><p>Eye Movements. Social evaluations were also reflected in average fixation duration on the eyes. For faces associated with negative-valence, high-power evaluations, fixations on their eye regions were longer. Specifically, longer fixations on the eye regions were observed when viewing faces evaluated as dominant (r = .35, p = .013), frightening (r = .28, p = .045), and mean (r = .29, p = .042). No significant relationships were found between model of average fixation duration on the eyes and any models of positive, high-power traits (e.g., competence) or positive-valence, low/neutral-power traits (e.g., trustworthiness), |r|s ≤ .21, all ps ≥ .152.</p><p>In addition, the model of fixation duration on the eyes showed a significant positive relationship with the power dimension model (r = .37, p = .008), but not with the valence dimension model (r = -.13, p = .382) (Figure <ref type="figure" target="#fig_2">2c</ref>), suggesting that fixation durations on the eyes predominantly encode the power dimension.</p><p>Study 1. We replicated the key findings from Preliminary Study: pupil dilation for negative-valence and high-power faces, constriction for positive-valence and low-power faces, and prolonged eye fixations for high-power faces, with correlation magnitudes differing by ~.1 of original values (Figure <ref type="figure" target="#fig_3">3a</ref>; Table <ref type="table">S7</ref>).</p><p>Pupillary Responses. Mean pupil size increased for dominant (r = .39, p = .005), frightening (r = .38, p = .007), mean (r = .42, p = .003), and threatening (r = .39, p = .005) facestraits associated with negative/neutral-valence and high-power evaluations. This pattern replicated for last-fixation pupil size, with dilation observed for dominant (r = .35, p = .012), frightening (r = .34, p = .015), mean (r = .38, p = .006), and threatening (r = .32, p = .023) faces. Likewise, mean pupil size during mouth fixations, increased for dominant (r = .41, p = .003), mean (r = .42, p = .002), threatening (r = .42, p = .003), and frightening faces (r = .32, p = .024).</p><p>Conversely, decreases in these measures were observed when participants viewed faces associated with positive-valence, low/neutral-power evaluations. Mean pupil size decreased for faces evaluated as more attractive (r = -.41, p = .003), likable (r = -.31, p = .027), and trustworthy (r = -.37, p = .007). Similar decreases were seen for pupil size during the last fixation-smaller pupils for attractive (r = -.35, p = .014), likable (r = -.29, p = .039), and trustworthy faces (r = -.30, p = .034). Likewise, mean pupil size when fixating on the mouth was smaller for attractive (r = -.39, p = .006), likable (r = -.33, p = .021), and trustworthy faces (r = -.44, p = .002). Pupil size models showed no correlation with competence (a positive, high-power trait), all |r|s ≤ .20, all ps ≥ .172.</p><p>Pupillary responses encoded both the valence and the power dimensions (Figure <ref type="figure" target="#fig_3">3c</ref>). Mean pupil size decreased when viewing faces evaluated more positively on the valence dimension (r = -.42, p = .003) and increased when viewing faces evaluated as more powerful (r = .28, p = .050).</p><p>The same pattern emerged for mean pupil size during fixations on the mouth (valence: r = -.44, p = .001; power: r = .29, p = .042), and partially for pupil size during the last fixation (valence: r = -.35, p = .014: r = .27, p = .061).</p><p>Eye Movements. Social evaluations were also reflected in average fixation duration on the eyes. For faces associated with negative-valence and high-power evaluations, fixations on their eyes were longer. Specifically, prolonged fixations on the eyes were observed for faces evaluated as dominant (r = .40, p = .004), frightening (r = .31, p = .029), mean (r = .31, p = .028), and threatening (r = .32, p = .024). No significant relationships were found between the model of mean fixation duration on eyes and any models of positive-valence, high-power traits (e.g., competence) or positive-valence, low/neutral-power traits (e.g., trustworthiness), |r|s ≤ .17, all ps ≥ 230.</p><p>In addition, the model of mean fixation duration on eyes showed a significant positive relationship with the power dimension model (r = .36, p = .011), but not with the valence dimension model (r = -.26, p = .065), suggesting that fixation durations on the eyes predominantly encode the power dimension (Figure <ref type="figure" target="#fig_3">3c</ref>).</p><p>Study 2. We found patterns of results that were largely consistent with those observed in Preliminary Study and Study 1. Both pupil size and fixation duration on the eyes increased in response to faces evaluated as dominant, frightening, and threatening (negative-valence, highpower evaluations), while fixation duration on the eyes decreased for faces evaluated as trustworthy (positive evaluations) (see Figure <ref type="figure" target="#fig_5">S1</ref> and Table <ref type="table">S8</ref> for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Social Anxiety</head><p>In Study 1, we further examined how the encoding of social aneity through physiological responses and eye movements were modulated by individual differences in social anxiety, We found that social anxiety levels correlated with the strength of valence encoding in pupillary responses, r = -.31, p = .011, indicating that more anxious individuals showed stronger pupillary differentiation between positive and negative faces (i.e., pupil size during mouth fixations and 'threatening', r = .27, p = .027, 'frightening', r = .29, p = .018, and 'dominance', r = .25, p = .042;</p><p>Pupil size during the last fixation and 'threatening', r = .28, p = .021). Conversely, socially anxious individuals' pupillary response model (i.e., pupil size of fixations on the mouth) showed stronger negative correlations with models of positive social traits (i.e., likable: r = -.26, p = .036; trustworthy: r = -.29, p = .015). Interestingly, the correlation between the pupil response model and the 'extraversion' model changed from positive to negative in low versus high social anxiety individuals (r = -.29, p = .015). Social anxiety levels did not influence the strength of associations between the model of mean fixation duration on eyes and social trait models (all ps ≥ .219). These findings suggest that pupillary responses of socially anxious individuals are more sensitive to valence evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neuronal Activity in Amygdala and Hippocampus</head><p>In Study 2, we examined how social evaluations from faces are encoded in neuronal activity of the amygdala and hippocampus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neuronal Measure Model Building</head><p>To quantify how variations in facial appearance lead to changes in neuronal measures, we built models based on neuronal data <ref type="bibr" target="#b32">(Oosterhof &amp; Todorov, 2008;</ref><ref type="bibr" target="#b42">Todorov &amp; Oh, 2021</ref>) (Figure <ref type="figure" target="#fig_4">4b</ref>). Individual models for all neuronal measures were constructed per face for each neuron (a total of 320 amygdala neurons and 295 hippocampal neurons across participants). Final models were derived by averaging individual models. As in the preceding studies, only facial shape dimensions were included in the model-building process. All neuronal models exhibit good cross-participant consistency (ICCs &gt; .37) and were included for further analysis (see Table <ref type="table">S9</ref> for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Correlation with Social Trait Models</head><p>As in preceding studies, we correlated each neuronal measure model with social trait models <ref type="bibr" target="#b32">(Oosterhof &amp; Todorov, 2008)</ref> to investigate how social evaluations were encoded in neuronal responses to faces. The amygdala and hippocampus revealed complementary encoding strategies for the valence and power dimensions of social evaluation (Figure <ref type="figure" target="#fig_4">4</ref>; Table <ref type="table" target="#tab_0">S10-11).</ref> Amygdala. Amygdalar neurons demonstrated feature-specific valence encoding, with strongest effects during mouth fixations. When participants viewed the mouth region, amygdalar firing decreased for faces evaluated positively, including competent (r = -.33, p = .021), extraverted (r = -.45, p = .001), likable (r = -.45, p &lt; .001), and trustworthy faces (r = -.36, p = .011). This pattern was specific to valence: the mouth-fixation neuronal model correlated negatively with the valence dimension (r = -.37, p = .009) but showed no relationship with power (r = -.04, p = .772). Some feature-invariant encoding emerged, with decreased firing for likable (r = -.35, p = .012) and extraverted faces (r = -.32, p = .026) across all fixations, though these effects were weaker than the mouth-specific responses.</p><p>Hippocampus. Hippocampal neurons demonstrated encoding of both valence and power dimensions. During fixations (when extracting facial information), hippocampal firing rates increased for faces evaluated as dominant (r = .29, p = .041), frightening (r = .36, p = .010), mean (r = .32, p = .025), and threatening (r = .31, p = .029), while decreasing for attractive (r = -.29, p = .040) and trustworthy faces (r = -.28, p = .050). The fixation-period neuronal model correlated significantly with valence model (r = -.33, p = .019) but not with power model (r = .22, p = .117), though individual trait correlations suggest power encoding.</p><p>During saccades (when planning visual exploration), hippocampal neurons encoded both dimensions. Firing rates during saccades increased for faces high in power or negative in valence: dominant (r = .34, p = .017), frightening (r = .29, p = .043), and mean (r = .42, p = .002), and decreased for positive-valence, low-power faces: trustworthy (r = -.28, p = .049). The saccadeperiod neuronal model correlated with both valence (r = -.28, p = .047) and power models (r = .35, p = .014). Similar encoding patterns were observed in hippocampal firing rates saccades originated from the eye region of the face. Pupillary and Eye-Movement Encoding of Social Evaluation (Preliminary Study) Note: (a) Statistically significant correlations between social trait models and eye-tracking measure models in Pearson's r. Eye-tracking measure models that showed consistent relationships with social trait models across all three studies (Preliminary Study, Study 1, and Study 2) are presented here. (b) Variations in facial appearances that produced changes in mean pupil size and mean fixation duration on the eyes (± 20 SD), compared to the average face. (c) Projection of the sample faces onto the valence and power dimensions of the social trait judgment space. Pupillary and Eye-Movement Encoding of Social Evaluation (Study 1) Note: (a) Statistically significant correlations between social traits models and eye-tracking measure models in Pearson's r. Eye-tracking measure models that showed consistent relationships with social trait models across all three studies (Preliminary Study, Study 1, and Study 2) are presented here. (b) Variations in facial appearances that produced changes in mean pupil size and mean fixation duration on the eyes (± 20 SD), compared to an average face. (c) Projection of the sample faces onto the valence and power dimensions of the social trait judgment space For brevity, neuronal models that showed significant correlations with social trait models are presented here. (b) Variations in facial appearances that produced changes in the average firing rate of hippocampal neurons during fixations and saccades and the average firing rate of amygdalar neurons during fixations on the mouth (± 40 SD), compared to the average face. (c) Projection of the sample faces onto the valence and power dimensions of social trait judgment space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>General Discussion</head><p>The current study reveals how evaluations of valence and power-fundamental dimensions of face-based social evaluation-manifest across multiple physiological systems. Integrating eye tracking, pupillometry, and intracranial recordings, we identified dimension-specific signatures that emerge within moments of viewing a face. Rather than triggering uniform physiological responses, faces elicit dimension-specific patterns across attention, arousal, and neural activity (Table <ref type="table" target="#tab_0">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Eye-Movement and Pupillary Encoding of Valence and Power</head><p>Valence encoding (positive ↔ negative intentions) may reflect a threat-detecting defensive system. Negative faces triggered pupil dilation, indicating heightened autonomic arousal, whereas positive faces induced pupil constriction, reflecting reduced arousal. This pupillary modulation may serve as a rapid assessment mechanism, with arousal patterns preparing appropriate behavioral responses to perceived social intentions. Furthermore, the sensitivity of valence encoding through pupillary responses increases with social anxiety, reflecting hypervigilance to perceived threats or negative evaluations <ref type="bibr" target="#b25">(Kret et al., 2013;</ref><ref type="bibr" target="#b33">Price et al., 2013)</ref> and a pronounced relief response to positive social cues. This patterns support that pupillary responses to faces are not merely driven by low-level visual properties such as luminance difference, or reflexive mechanisms such as pupil mimicry <ref type="bibr" target="#b13">(Frisanco et al., 2021)</ref>, but rather reflect higher-level social evaluation processing.</p><p>Power encoding (high ↔ low dominance) may reflect mechanisms of attentional prioritization. High-power faces elicited pupil dilation, indicating increased autonomic arousal, and longer fixation durations on eyes, reflecting greater allocation of visual attention on diagnostic region. The eyes may provide critical information about social dominance and intentions <ref type="bibr" target="#b10">(Cheng et al., 2023;</ref><ref type="bibr" target="#b21">Jarick &amp; Kingstone, 2015)</ref>, warranting increased attention for high-power individuals.</p><p>The visual prioritization of socially diagnostic cues may be further amplified by heightened autonomic arousal. This mutual reinforcement promotes vigilance toward individuals with greater social influence who may significantly impact the observer's outcomes-acting as threats, allies, or leaders-and thereby shape adaptive social behaviors. Social anxiety did not modulate power encoding, indicating the two dimensions summarizing social evaluations rely on partially, if not fully, independent processing streams.</p><p>These results extend previous findings on emotional facial expressions <ref type="bibr" target="#b25">(Kret et al., 2013;</ref><ref type="bibr" target="#b37">Schurgin et al., 2014;</ref><ref type="bibr" target="#b44">C.-A. Wang et al., 2018)</ref> to emotionally neutral faces, showing that social evaluations based on facial appearance-beyond just transient emotional cues-can modulate physiological arousal and attention focus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neural Encoding of Valence and Power</head><p>Our intracranial recordings revealed that the amygdala and hippocampus implement distinct but complementary strategies, providing mechanistic insight into how the human cognitive system simultaneously extracts and acts upon social information. Amygdalar neurons showed specialized valence encoding with feature-specific responses.</p><p>Neurons selectively reduced firing to positive faces when participants fixated on the mouth region, but not when viewing the same faces while fixating on other features. This mouth-specific valence encoding aligns with evidence that the amygdala processes emotional valence from diagnostic facial regions <ref type="bibr" target="#b23">(Kim et al., 2017)</ref>, particularly mouth configurations that signal approach-avoidance intentions (e.g., upturned lip corners in a smile vs. tightened lips in negative expressions). The absence of power encoding confirms previous findings that this amygdala primarily processes threat-relevance rather than social hierarchy (e.g., dominance, <ref type="bibr" target="#b40">Todorov, 2012)</ref>. Together, these findings highlight a functional specialization of amygdala as an early warning system, detecting others' intentions-positive or negative-from the most diagnostic facial features.</p><p>Hippocampal neurons encode social evaluations relevant to both information extraction and the guidance of eye-movement planning. During fixations (when gathering visual information), hippocampal neurons primarily encoded valence, increasing firing for negative faces regardless of which feature was fixated. This feature-invariant encoding suggests the hippocampus integrates threat-relevant information across the entire face, rather than relying on specific diagnostic features like the amygdala. There were indications that the hippocampus was sensitive to power-related evaluations during fixations, (e.g., heightened responses to dominant faces). Such patterns suggest that while its primary role is to integrate threat-relevant information, the hippocampus may also incorporate cues related to social influence.</p><p>During saccades (when planning where to look next), the hippocampus encoded both valence and power dimensions: firing rates increased for faces that were negative and powerful, and decreased for faces that were positive and submissive. Building on hippocampus's established function in planning and guiding eye movements <ref type="bibr" target="#b28">(Liu et al., 2017)</ref>, this dual encoding shows that eye-movement planning is adaptively shaped by comprehensive social evaluation-integrating both threat potential (negative valence) and social influence (high power)-with increased firing for negative and powerful faces likely reflect more cautious, strategic scanning <ref type="bibr" target="#b16">(Han et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-System Threat Encoding</head><p>A convergent pattern emerged for faces combining negative valence with high power (and by extension, positive-valence × low-power; 2nd/4th quadrants of the evaluation space; Figures <ref type="figure" target="#fig_2">2c</ref>, <ref type="figure" target="#fig_3">3c</ref>, and <ref type="figure" target="#fig_4">4c</ref>): these faces simultaneously triggered autonomic arousal, increased visual attention, and enhanced hippocampal firing. This cross-system convergence suggests an adaptive threatassessment mechanism prioritizing potentially dangerous individuals. On the other hand, competence evaluation (positive/high power) shows no reliable pupillary association, an asymmetry indicating vigilance is tuned more to malevolence than to ability per se.</p><p>Complementing this pattern, amygdala responses track valence in a feature-specific manner (mouth-related suppression for positive faces), consistent with the idea that it functions as a gate that routes threat information to hippocampal circuits involved in exploration and planning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations and Future Direction</head><p>Our results provide robust evidence for spontaneous physiological encoding of social evaluations, though the distinction between process and content remains nuanced. Our findings suggest spontaneous encoding processes: physiological responses outside of participants' conscious awareness reliably reflect high-level social evaluation processes. However, participants' explicit judgments during some tasks make it difficult to determine whether the encoded social evaluation content is purely spontaneous. To address this concern, Preliminary study, which did not involve explicit judgments, produced results largely consistent with Studies 1 and 2, suggesting that the observed encoding patterns may reflect genuinely spontaneous evaluation processes.</p><p>Additionally, although synthesized faces allow precise control over facial parameters, they may not fully capture real-world complexity. While real and artificial faces elicit similar social evaluations <ref type="bibr" target="#b3">(Balas et al., 2018)</ref>, future research could use real faces and more naturalistic paradigms to further validate these insights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>Taken together with our pupillometry and eye-tracking findings, we provide a multi-level account of the physiological basis of social evaluation. These converging signatures show that social evaluation is not a disembodied cognitive calculation, but is instantiated in physiological processes, with pupils, eye movements, and medial temporal structures coordinating to parse social meaning.</p><p>Pupillary responses encoded both valence and power, whereas fixation durations on eyes selectively tracked power. These findings highlight how perceived social traits influence attentional focus and physiological arousal. Additionally, by linking single neuron recording with eye tracking, we show how amygdala and hippocampus form a functional architecture for processing social dimensions. The amygdala provided feature-specific valence detection, while the hippocampus processed both dimensions feature-invariantly and used these evaluations to guide moment-to-moment decisions about visual exploration. This division of labor ensures both rapid identification of potential threats and adaptive exploration based on the social significance of faces.</p><p>These findings bridge cognitive theories of impression formation and their embodied implementation, revealing how the fundamental dimensions organizing social cognition emerge spontaneously within moments of encountering a face and enable navigation of complex social environments.</p><p>Table S2 Participant Information in Study 2 ID Age Sex Race Epilepsy Diagnosis P6 33 Female Caucasian Left posterior neocortical extratemporal/parietal P7 28 Female Caucasian Right medial temporal P10 47 Female Caucasian Right medial temporal and neocortical temporal P11 33 Female Caucasian Right medial temporal and extratemporal P14 26 Male Caucasian Bilateral amygdala/hippocampus P15 37 Male Caucasian Left amygdala/hippocampus Table <ref type="table">S3</ref> The Liebowitz Social Anxiety Scale</p><p>Read each situation carefully and answer two questions about that situation.</p><p>• The first question asks how anxious or fearful you feel in the situation.</p><p>• The second question asks how often you avoid the situation.</p><p>• Please base your ratings on the way that the situations have affected you in the last week.</p><p>• If you come across a situation that you ordinarily do not experience, we ask that you imagine "what if you were faced with that situation," and then rate the degree to which you would fear this hypothetical situation and how often you would tend to avoid it. Fear: 0 = None; 1 = Middle; 2 = Moderate; 3 = Severe Avoidance: 0 = Never; 1 = Occasionally; 2 = Often; 3 = Usually Fear Avoidance 1. Using a telephone in public 2. Participating in a small group activity 3. Eating in public 4. Drinking with others 5. Talking to someone in authority 6. Acting, performing, or speaking in front of an audience 7. Going to a party 8. Working while being observed 9. Writing while being observed 10. Calling someone you don't know very well 11. Talking face to face with someone you don't know very well 12. Meeting strangers 13. Urinating in a public bathroom 14. Entering a room when others are already seated 15. Being the center of attention 16. Speaking up at a meeting 17. Taking a test of your ability, skill, or knowledge 18. Expressing disagreement or disapproval to someone you don't know very well 19. Looking someone who you don't know very well straight in the eyes 20. Giving a prepared oral talk to a group 21. Trying to make someone's acquaintance for the purpose of a romantic/sexual relationship 22. Returning goods to a store for a refund 23. Giving a party 24. Resisting a high pressure sales person Table S4 Distribution of Recorded Neurons in the Amygdala and Hippocampus in Study 2 Trustworthiness Task Dominance Task Number of amygdala neurons Number of hippocampus neurons Number of amygdala neurons Number of hippocampus neurons ID Total Left Right Total Left Right Total Left Right Total Left Right P6 6 6 0 20 20 0 9 9 0 27 27 0 P7 2 2 0 23 21 2 9 9 0 34 34 0 P10 22 0 22 0 0 0 3 0 3 0 0 0 P11 16 0 16 0 0 0 15 0 15 0 0 0 7 0 7 0 0 0 12 0 12 17 0 17 5 0 5 9 0 9 3 0 3 11 0 11 P14 22 18 4 7 5 2 22 18 4 7 5 2 22 19 3 8 6 2 25 21 4 8 6 2 19 16 3 32 31 1 21 18 3 40 39 1 11 9 2 6 4 2 10 8 2 6 4 2 P15 12 12 0 3 0 3 21 21 0 15 0 15 12 12 0 19 0 19 14 14 0 3 0 3 Sum 156 94 62 127 87 40 164 118 46 168 115 53 Note: Each row represents a separate recording session. Total: all neurons recorded from an area that had an overall firing rate greater than 0.15 Hz during fixations. Left: neurons in the left hemisphere. Right: neurons in the right hemisphere.</p><p>Table S5 Consistent Eye-Tracking Measure Models Measure ICC2 Mean Saccade Amplitude .61 Number of Fixations .57 Mean Amplitude of Saccades Ended at Eyes .54 Pupil Size of Last Fixation .54 Mean Pupil Size during Fixations on Mouth .47 Number of Saccades Started from Nose .46 Mean Pupil Size during Fixations .46 Total Fixation Duration .44 Mean Amplitude of Saccades Started from Eyes .42 Pupil Size of First Fixation .35 Mean Amplitude of Saccades Started from Nose .33 Number of Fixations on Nose .32 Total Fixation Duration on Nose .30 Mean Pupil Size during Fixations on Eyes .30 Number of Saccades Ended at Mouth .28 Mean Amplitude of Saccades Ended at Nose .22 Total Duration of Saccades Started from Eyes .22 Mean Amplitude of Saccades Ended at Mouth .22 Mean Duration of Fixations on Eyes .16 Mean Pupil Size during Fixations on Nose .12 Note: ICC2 represents the two-way random effects Intraclass Correlation Coefficient.</p><p>Table S6 Correlation Coefficients and Significance Tests for Eye Movements and Social Traits Models of Preliminary Study Mean Duration of Fixations on Eyes Permutation p .989 .199 .054 .230 .115 .867 .106 .213 .698 .465 .029 * Note: * p &lt; .050, ** p &lt; .010, *** p &lt; .001. p .982 .152 .013 * .162 .045 * .833 .042 * .110 .654 .382 .008 ** r .00 .21 .35 .20 .28 .03 .29 .23 -.06 -.13 .37 Mean Pupil Size of Fixations on Mouth Permutation p .014 * .253 .043 * .169 .010 ** .006 ** .019 * .019 * .053 .009 ** .133 p .005 ** .196 .007 ** .096 .001 *** .003 ** .002 ** .002 ** .013 * &lt;.001 *** .058 r -.40 -.19 .38 -.24 .46 -.41 .43 .42 -.35 -.48 .27 Pupil Size of the Last Fixation Permutation p .037 * .765 &lt;.001 *** .411 .001 *** .049 * &lt;.001 *** &lt;.001 *** .010 ** .002 ** .002 ** p .018 * .755 &lt;.001 *** .328 &lt;.001 *** .028 * &lt;.001 *** &lt;.001 *** .007 ** &lt;.001 *** &lt;.001 *** r -.33 .05 .64 -.14 .57 -.31 .59 .61 -.38 -.53 .56 Mean Pupil Size Permutation p .042 * .994 .002 ** .446 .005 ** .032 * .002 ** .004 ** .042 * .006 ** .007 ** p .016 * .995 &lt;.001 *** .377 &lt;.001 *** .018 * &lt;.001 *** &lt;.001 *** .021 * .001 *** .002 ** r -.34 -.00 .51 -.13 .49 -.33 .51 .49 -.33 -.46 .43 Social Traits Attractiveness Competence Dominance Extraverted Frightening Likable Mean Threatening Trustworthy Valence Dimension Power Dimension Table S7 Correlation Coefficients and Significance Tests for Eye Movements and Social Traits Models of Study 1 Mean Duration of Fixations on Eyes Permutation p .800 .568 .021* .301 .082 .587 .084 .074 .305 .135 .041* Note: * p &lt; .050, ** p &lt; .010, *** p &lt; .001. p .771 .515 .004 ** .230 .029 * .524 .028 * .024 * .237 .065 .011 * r .04 .09 .40 .17 .31 .09 .31 .32 -.17 -.26 .36 Mean Pupil Size of Fixations on Mouth Permutation p .016 * .279 .026 * .371 .076 .043 * .021 * .023 * .006 ** .015 * .116 p .006 ** 228 .003 ** .278 .024 * .021 * .002 ** .003 ** .002 ** .001 *** .042 * r -.39 -.17 .41 -.16 .32 -.33 .42 .42 -44 -.44 .29 Pupil Size of the Last Fixation Permutation p .030 * .488 .059 .813 .058 * .066 .024 * .086 .057 .038 * .153 p .014 * .459 .012 * .769 .015 * .039 * .006 ** .023 * .034 * .014 * .061 r -.35 -.11 .35 -04 .34 -.29 .38 .32 -.30 -.35 .27 Mean Pupil Size Permutation p .018 * .228 .033 * .798 .034 * .052 .014 * .032 * .018 * .013 * .118 p .003 ** .172 .005 ** .766 .007 ** .027 * .003 ** .005 ** .007 ** .003 ** .050 * r -.41 -.20 .39 -.04 .38 -.31 .42 .39 -.37 -.42 .28 Social Traits Attractiveness Competence Dominance Extraverted Frightening Likable Mean Threatening Trustworthy Valence Dimension Power Dimension Table S8 Correlation Coefficients and Significance Tests for Eye Movements and Social Traits Models of Study 2 Mean Duration of Fixations on Eyes Permutation p .175 .527 .015 * .998 .039 * .129 .032 * .008 ** .079 .051 .030 * Note: * p &lt; .050, ** p &lt; .010, *** p &lt; .001. p .114 .462 .001 *** .995 .007 ** .084 .007 ** .002 ** .041 * .014 * .004 ** r -.23 .11 .44 .00 .38 -.25 .38 .44 -.29 -.34 .40 Mean Pupil Size of Fixations on Mouth Permutation p .108 .309 .188 .348 .056 .071 .052 .085 .049 * .041 * .275 p .455 .333 .007 ** .506 .010 ** .690 .009 ** .018 * .151 .085 .005 ** r -.11 .14 .37 .10 .36 -.06 .37 .33 -.21 -.25 .39 Size of the Last Fixation Permutation p .891 .928 .105 .856 .167 .649 .495 .185 .973 .505 .138 p .849 .407 .002 ** .826 .016 * .520 .078 .009 ** .458 .177 .002 ** r .03 .12 .43 -.03 .34 -.09 .25 .37 -.11 -.19 .42 Mean Pupil Size Permutation p .826 .783 .211 .941 .252 .971 .550 .302 .844 .453 .292 p .867 .655 .002 ** .894 .012 * .804 .061 .006 ** .255 .096 .004 ** r -.02 .06 .43 -.02 .35 .04 .27 .38 -.16 -.24 .40 Social Traits Attractiveness Competence Dominance Extraverted Frightening Likable Mean Threatening Trustworthy Valence Dimension Power Dimension Table S11 Correlation Coefficients and Significance Tests for Hippocampus Neurons and Social Traits Models of Study 2 Mean Firing Rate during Saccades Started from Eyes Permutation p .327 .550 .108 .034 * .109 .087 .024 * .066 .020 * .026 * .181 Note: * p &lt; .050, ** p &lt; .010, *** p &lt; .001. p .227 .505 .039 * .011 * .036 * .043 * .005 ** .018 * .006 ** .005 ** .097 r -.17 -.10 .29 -.36 .30 -.29 .39 .33 -.38 -.39 .24 Mean Firing Rate during Saccades Permutation p .906 .529 .069 .257 .104 .433 .012 * .117 .089 .103 .048 * p .886 .457 .017 * .176 .043 * .377 .002 ** . 051 .049 * .047 * .014 * r .02 .11 .34 -.19 .29 -.13 .42 .28 -.28 -.28 .35 Mean Firing Rate during Fixations on Eyes Permutation p .790 .704 .233 .307 .159 .228 .037 * .182 .026 * .103 .163 p .751 .662 .139 .215 .094 .152 .012 * .094 .007 ** .048 * .102 r -.05 .06 .21 -18 .24 -.21 .35 .24 -.37 -.28 .23 Mean Firing Rate during Fixations Permutation p .069 .478 .119 .814 .031* .083 .073 .094 .089 .046 * .210 p .040 * .444 .041 * .783 .010 ** .058 .025 * .029 * .050 * .019 * .117 r -.29 -.11 .29 -.04 .36 -.27 .32 .31 -.28 -.33 .22 Social Traits Attractiveness Competence Dominance Extraverted Frightening Likable Mean Threatening Trustworthy Valence Dimension Power Dimension Pupillary and Eye-Movement Encoding of Social Evaluation (Study 2) Note: (a) Statistically significant correlations between social trait models and eye-tracking measure models in Pearson's r. Eye-tracking measure models that showed consistent relationships with social trait models across all three studies (Preliminary Study, Study 1, and Study 2) are presented here. (b) Variations in facial appearances that produced changes in mean pupil size and mean fixation duration on the eyes (± 20 SD), compared to the average face. (c) Projection of the sample faces onto the valence and power dimensions of the social trait judgment space.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1</head><label>1</label><figDesc>Figure 1</figDesc><graphic coords="14,72.00,185.39,468.00,234.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><figDesc>Measures. Pupil size was recorded in mm². Pupil measures included mean pupil size of all fixations between stimulus onset and offset, and pupil sizes of first and last fixations. Average pupil size was calculated for fixations on eye, nose, and mouth regions based on AOIs. In Study 2, unlike in the controlled lab settings of Preliminary Study and Study 1, patients performed the task during neurosurgical monitoring, which introduced variability in baseline pupil size across trials due head position changes (which affect light entry) and physiological responses to the surgical environment. To account for these factors, in Study 2, pupil size during the first fixation of each trial was used as a baseline and subtracted from all subsequent measurements. Subsequently, the pupil measures in Study 2 (except for pupil size during the first fixation) reflected relative changes rather than the absolute size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2</head><label>2</label><figDesc>Figure 2</figDesc><graphic coords="28,85.93,127.19,440.15,422.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3</head><label>3</label><figDesc>Figure 3</figDesc><graphic coords="29,84.00,127.20,443.85,425.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4</head><label>4</label><figDesc>Figure 4</figDesc><graphic coords="30,72.00,127.20,469.55,583.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure S1 Pupillary</head><label>S1</label><figDesc>Figure S1</figDesc><graphic coords="54,73.93,127.19,464.15,431.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Convergent Evidence for Social Evaluation Encoding in Physiological Responses</figDesc><table><row><cell>Encoding Patterns</cell><cell>Measures</cell><cell>Study</cell><cell>Physiological Signature</cell><cell>Functional Significance</cell></row><row><cell></cell><cell>Pupil size</cell><cell>Preliminary Study, Study 1</cell><cell>Constricted for positivity (r = -.35* to -.53***)</cell><cell>Autonomic arousal tracking affective significance</cell></row><row><cell>Valence (positive ↔ negative</cell><cell>Amygdala (mouth fixations)</cell><cell>Study 2</cell><cell>Decreased firing for positivity (r = -.37**)</cell><cell>Feature-specific threat detection from diagnostic regions</cell></row><row><cell>intention)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Hippocampus (all fixations, saccades)</cell><cell>Study 2</cell><cell>Decreased firing for positivity (r = -.33** and -.39**)</cell><cell>Global integration of threat-relevant information</cell></row><row><cell></cell><cell>Pupil size</cell><cell>Preliminary Study, Study 1</cell><cell>Dilated for high power (r = 27 to .57***)</cell><cell>Heightened arousal for socially influential targets</cell></row><row><cell>Power (high ↔ low social dominance)</cell><cell>Fixation duration (eye region)</cell><cell>Preliminary Study, Study 1</cell><cell>Longer for high power (r = .37** and .36*)</cell><cell>Greater monitoring of information from the eyes of socially influential targets</cell></row><row><cell></cell><cell>Hippocampus (saccades)</cell><cell>Study 2</cell><cell>Increased firing for high power (r = .35**)</cell><cell>Adaptive visual exploration based on social hierarchy</cell></row><row><cell>Individual-difference modulation (tight ↔ loose physiology-evaluation coupling)</cell><cell>Social anxiety × Pupil-valence coupling</cell><cell>Study 1</cell><cell>Coupling strengthened in socially anxious individuals (r = -.31*)</cell><cell>Confirms high-level social evaluation, not reflexive response</cell></row></table><note><p>Note: * p &lt; .050, ** p &lt; .010, *** p &lt; .001. participants (27 males, 41 females; aged 18-26 years; see</p><p>Table S1 for details). All participants had normal or corrected-to-normal vision and no known neurological or psychiatric disorders. The research protocols were approved by the Department Ethics Review Committee of the National University of Singapore. In Study 2, six neurosurgical patients (2 males, 4 females; aged 26-47 years; all Caucasian) undergoing epilepsy monitoring participated in the study (see Table S2 for details). The sample size is consistent with typical intracranial recording studies. Statistical power was achieved at the cellular level, through analyzing 506 individual neurons across participants. The research protocols were approved by the Institutional Review Board of West Virginia University.</p></note></figure>
		</body>
		<back>

			<div type="funding">
<div><p>This research was supported by the <rs type="funder">U.S. National Institutes of Health</rs> (<rs type="grantNumber">R01MH129426</rs>, S.W.) and the <rs type="funder">National University of Singapore Department of Psychology</rs> (D.O.). The authors thank <rs type="person">Runquan Yu</rs> for assistance with eye tracking setup, and <rs type="person">Z. H. Sylvia Tan</rs> and <rs type="person">Shing Yu Yeo</rs> for help with data collection. All data and analysis scripts are publicly available on the Open Science Framework</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_futcsTd">
					<idno type="grant-number">R01MH129426</idno>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>: <ref type="url" target="https://osf.io/8szbp/?view_only=ff6b61b3f67e4e2f9b25ac48378e6105">https://osf.io/8szbp/?view_only=ff6b61b3f67e4e2f9b25ac48378e6105</ref>.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Materials</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fear, faces, and the human amygdala</title>
		<author>
			<persName><forename type="first">R</forename><surname>Adolphs</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.conb.2008.06.006</idno>
		<ptr target="https://doi.org/10.1016/j.conb.2008.06.006" />
	</analytic>
	<monogr>
		<title level="j">Current Opinion in Neurobiology</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="166" to="172" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Predicting Elections: Child&apos;s Play! Science</title>
		<author>
			<persName><forename type="first">J</forename><surname>Antonakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Dalgas</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.1167748</idno>
		<ptr target="https://doi.org/10.1126/science.1167748" />
		<imprint>
			<date type="published" when="2009">2009. 5918</date>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="page">1183</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The Liebowitz social anxiety scale as a self-report instrument: A preliminary psychometric analysis</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heinrichs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Hofmann</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0005-7967(01)00060-2</idno>
		<ptr target="https://doi.org/10.1016/S0005-7967(01)00060-2" />
	</analytic>
	<monogr>
		<title level="j">Behaviour Research and Therapy</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="701" to="715" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Measuring social variables in real and artificial faces</title>
		<author>
			<persName><forename type="first">B</forename><surname>Balas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tupa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pacella</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.chb.2018.07.013</idno>
		<ptr target="https://doi.org/10.1016/j.chb.2018.07.013" />
	</analytic>
	<monogr>
		<title level="j">Computers in Human Behavior</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="236" to="243" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Individual differences in human eye movements: An oculomotor signature?</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bargary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Bosten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Goodbourn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Lawrance-Owen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Hogg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Mollon</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.visres.2017.03.001</idno>
		<ptr target="https://doi.org/10.1016/j.visres.2017.03.001" />
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="page" from="157" to="169" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The Psychophysics Toolbox</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Brainard</surname></persName>
		</author>
		<idno type="DOI">10.1163/156856897X00357</idno>
		<ptr target="https://doi.org/10.1163/156856897X00357" />
	</analytic>
	<monogr>
		<title level="j">Spatial Vision</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="433" to="436" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Encoding of facial features by single neurons in the human amygdala and hippocampus</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Brandmeir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1038/s42003-021-02917-1</idno>
		<ptr target="https://doi.org/10.1038/s42003-021-02917-1" />
	</analytic>
	<monogr>
		<title level="j">Communications Biology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A neuronal social trait space for first impressions in the human amygdala and hippocampus</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Todorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Brandmeir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41380-022-01583-x</idno>
		<ptr target="https://doi.org/10.1038/s41380-022-01583-x" />
	</analytic>
	<monogr>
		<title level="j">Molecular Psychiatry</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3501" to="3509" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Task Modulation of Single-Neuron Activity in the Human Amygdala and Hippocampus</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Todorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Brandmeir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1523/ENEURO.0398-21.2021</idno>
		<ptr target="https://doi.org/10.1523/ENEURO.0398-21.2021" />
	</analytic>
	<monogr>
		<title level="j">eNeuro</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Implicit race attitudes modulate visual information extraction for trustworthiness judgments</title>
		<author>
			<persName><forename type="first">I</forename><surname>Charbonneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fiset</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0239305</idno>
		<ptr target="https://doi.org/10.1371/journal.pone.0239305" />
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">239305</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Eye gaze and visual attention as a window into leadership and followership: A review of empirical insights and future directions</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">H</forename><surname>Gerpott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Benson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Foulsham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A M</forename><surname>Lansu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Schülke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tsuchiya</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.leaqua.2022.101654</idno>
		<ptr target="https://doi.org/10.1016/j.leaqua.2022.101654" />
	</analytic>
	<monogr>
		<title level="j">The Leadership Quarterly</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">101654</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Individual differences in visual salience vary along semantic dimensions</title>
		<author>
			<persName><forename type="first">B</forename><surname>De Haas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Iakovidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Schwarzkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Gegenfurtner</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1820553116</idno>
		<ptr target="https://doi.org/10.1073/pnas.1820553116" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page" from="11687" to="11692" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The effect of social anxiety on top-down attentional orienting to emotional faces</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Delchau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Christensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">V</forename><surname>Lipp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Goodhew</surname></persName>
		</author>
		<idno type="DOI">10.1037/emo0000764</idno>
		<ptr target="https://doi.org/10.1037/emo0000764" />
	</analytic>
	<monogr>
		<title level="j">Emotion</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="572" to="585" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">All that meets the eye: The contribution of reward processing and pupil mimicry on pupillary reactions to facial trustworthiness</title>
		<author>
			<persName><forename type="first">A</forename><surname>Frisanco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Biella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brambilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Kret</surname></persName>
		</author>
		<idno type="DOI">10.1007/s12144-021-02486-w</idno>
		<ptr target="https://doi.org/10.1007/s12144-021-02486-w" />
	</analytic>
	<monogr>
		<title level="j">Current Psychology</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">14</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The (un)suitability of modern liquid crystal displays (LCDs) for vision research</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S C</forename><surname>Price</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2015.00303</idno>
		<ptr target="https://doi.org/10.3389/fpsyg.2015.00303" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Social anxiety and trustworthiness judgments of dynamic facial expressions of emotion</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gutiérrez-García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Calvo</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jbtep.2016.04.003</idno>
		<ptr target="https://doi.org/10.1016/j.jbtep.2016.04.003" />
	</analytic>
	<monogr>
		<title level="j">Journal of Behavior Therapy and Experimental Psychiatry</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="119" to="127" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Interrelationships among men&apos;s threat potential, facial dominance, and vocal dominance</title>
		<author>
			<persName><forename type="first">C</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kandrik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Holzleitner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Debruine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Jones</surname></persName>
		</author>
		<idno type="DOI">10.1177/1474704917697332</idno>
		<ptr target="https://doi.org/10.1177/1474704917697332" />
	</analytic>
	<monogr>
		<title level="j">Evolutionary Psychology</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1474704917697332</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Psychometric properties of the Liebowitz Social Anxiety Scale</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Heimberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Horner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Juster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Safren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Schneier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Liebowitz</surname></persName>
		</author>
		<idno type="DOI">10.1017/S0033291798007879</idno>
		<ptr target="https://doi.org/10.1017/S0033291798007879" />
	</analytic>
	<monogr>
		<title level="j">Psychological Medicine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="199" to="212" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The area-of-interest problem in eyetracking research: A noise-robust solution for face and sparse stimuli</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Hessels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kemner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Van Den Boomen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">T C</forename><surname>Hooge</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13428-015-0676-y</idno>
		<ptr target="https://doi.org/10.3758/s13428-015-0676-y" />
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1694" to="1712" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Eye-tracking the own-race bias in face recognition: Revealing the perceptual and socio-cognitive mechanisms</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Hills</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Pake</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cognition.2013.08.012</idno>
		<ptr target="https://doi.org/10.1016/j.cognition.2013.08.012" />
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="586" to="597" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Which Facial Features Are Central in Impression Formation?</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Jones</surname></persName>
		</author>
		<idno type="DOI">10.1177/19485506211034979</idno>
		<ptr target="https://doi.org/10.1177/19485506211034979" />
	</analytic>
	<monogr>
		<title level="j">Social Psychological and Personality Science</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="553" to="561" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The duality of gaze: Eyes extract and signal social information during sustained cooperative and competitive dyadic gaze</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jarick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kingstone</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2015.01423</idno>
		<ptr target="https://doi.org/10.3389/fpsyg.2015.01423" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">To which world regions does the valence-dominance model of social perception apply?</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Debruine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Flake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Liuzza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Antfolk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Arinze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">L G</forename><surname>Ndukaihe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">G</forename><surname>Bloxsom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Foroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Willis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Cubillas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Vadillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Turiegano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gilead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Simchon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Saribay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Owsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Coles</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41562-020-01007-2</idno>
		<ptr target="https://doi.org/10.1038/s41562-020-01007-2" />
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="159" to="169" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Human Amygdala Tracks a Feature-Based Valence Signal Embedded within the Facial Expression of Surprise</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Mattek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Whalen</surname></persName>
		</author>
		<idno type="DOI">10.1523/JNEUROSCI.1375-17.2017</idno>
		<ptr target="https://doi.org/10.1523/JNEUROSCI.1375-17.2017" />
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">39</biblScope>
			<biblScope unit="page" from="9510" to="9518" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Kleiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brainard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pelli</surname></persName>
		</author>
		<ptr target="https://pure.mpg.de/rest/items/item_1790332/component/file_3136265/content" />
		<title level="m">What&apos;s new in Psychtoolbox-3?</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Emotional signals from faces, bodies and scenes influence observers&apos; face expressions, fixations and pupil-size</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Kret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stekelenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>De Gelder</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnhum.2013.00810</idno>
		<ptr target="https://doi.org/10.3389/fnhum.2013.00810" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Human Neuroscience</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Social phobia</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Liebowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Modern Problems of Pharmacopsychiatry</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">141</biblScope>
			<biblScope unit="page">173</biblScope>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Four dimensions characterize attributions from faces using a representative set of English trait words</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Keles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Adolphs</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41467-021-25500-y</idno>
		<ptr target="https://doi.org/10.1038/s41467-021-25500-y" />
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5168</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Visual Sampling Predicts Hippocampal Activity</title>
		<author>
			<persName><forename type="first">Z.-X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Olsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Ryan</surname></persName>
		</author>
		<idno type="DOI">10.1523/JNEUROSCI.2610-16.2016</idno>
		<ptr target="https://doi.org/10.1523/JNEUROSCI.2610-16.2016" />
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="599" to="609" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Selective Attention to Angry Faces in Clinical Social Phobia</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mogg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Philippot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Bradley</surname></persName>
		</author>
		<idno type="DOI">10.1037/0021-843X.113.1.160</idno>
		<ptr target="https://doi.org/10.1037/0021-843X.113.1.160" />
	</analytic>
	<monogr>
		<title level="j">Journal of Abnormal Psychology</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="160" to="165" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Social Anxiety and Social Anxiety Disorder</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Morrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Heimberg</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev-clinpsy-050212-185631</idno>
		<ptr target="https://doi.org/10.1146/annurev-clinpsy-050212-185631" />
	</analytic>
	<monogr>
		<title level="j">Annual Review of Clinical Psychology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="274" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Todorov</surname></persName>
		</author>
		<ptr target="https://figshare.com/articles/dataset/300_Random_Asian_Faces/7361270" />
		<title level="m">300 random Asian faces</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The functional basis of face evaluation</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">N</forename><surname>Oosterhof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Todorov</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.0805664105</idno>
		<ptr target="https://doi.org/10.1073/pnas.0805664105" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">32</biblScope>
			<biblScope unit="page" from="11087" to="11092" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sustained neural alterations in anxious youth performing an attentional bias task: A pupilometry study</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Siegle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Silk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ladouceur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mcfarland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Ryan</surname></persName>
		</author>
		<idno type="DOI">10.1002/da.21966</idno>
		<ptr target="https://doi.org/10.1002/da.21966" />
	</analytic>
	<monogr>
		<title level="j">Depression and Anxiety</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="30" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Single-trial learning of novel stimuli by individual neurons of the human hippocampus-amygdala complex</title>
		<author>
			<persName><forename type="first">U</forename><surname>Rutishauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Mamelak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Schuman</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuron.2006.02.015</idno>
		<ptr target="https://doi.org/10.1016/j.neuron.2006.02.015" />
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="805" to="813" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Online detection and sorting of extracellularly recorded action potentials in human medial temporal lobe recordings, in vivo</title>
		<author>
			<persName><forename type="first">U</forename><surname>Rutishauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Schuman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Mamelak</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jneumeth.2005.12.033</idno>
		<ptr target="https://doi.org/10.1016/j.jneumeth.2005.12.033" />
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience Methods</title>
		<imprint>
			<biblScope unit="volume">154</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="204" to="224" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Structural resemblance to emotional expressions predicts evaluation of emotionally neutral faces</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Said</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Todorov</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0014681</idno>
		<ptr target="https://doi.org/10.1037/a0014681" />
	</analytic>
	<monogr>
		<title level="j">Emotion</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="260" to="264" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Eye movements during emotion recognition in faces</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Schurgin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Iida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ohira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Franconeri</surname></persName>
		</author>
		<idno type="DOI">10.1167/14.13.14</idno>
		<ptr target="https://doi.org/10.1167/14.13.14" />
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Social inferences from faces: Ambient images generate a threedimensional model</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A M</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Oldmeadow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Towler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Michael Burt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Young</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cognition.2012.12.001</idno>
		<ptr target="https://doi.org/10.1016/j.cognition.2012.12.001" />
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="105" to="118" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Hippocampal Brain Volume Is Associated with Faster Facial Emotion Identification in Older Adults: Preliminary Results</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Szymkowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Persson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Ebner</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnagi.2016.00203</idno>
		<ptr target="https://doi.org/10.3389/fnagi.2016.00203" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Aging Neuroscience</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The role of the amygdala in face perception and evaluation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Todorov</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11031-011-9238-5</idno>
		<ptr target="https://doi.org/10.1007/s11031-011-9238-5" />
	</analytic>
	<monogr>
		<title level="j">Motivation and Emotion</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="16" to="26" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Validation of data-driven computational models of social perception of faces</title>
		<author>
			<persName><forename type="first">A</forename><surname>Todorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dotsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Porter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">N</forename><surname>Oosterhof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">B</forename><surname>Falvello</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0032335</idno>
		<ptr target="https://doi.org/10.1037/a0032335" />
	</analytic>
	<monogr>
		<title level="j">Emotion</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="724" to="738" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Chapter Four-The structure and perceptual basis of social judgments from faces</title>
		<author>
			<persName><forename type="first">A</forename><surname>Todorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Oh</surname></persName>
		</author>
		<idno type="DOI">10.1016/bs.aesp.2020.11.004</idno>
		<ptr target="https://doi.org/10.1016/bs.aesp.2020.11.004" />
	</analytic>
	<monogr>
		<title level="j">Advances in Experimental Social Psychology</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Gawronski</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="189" to="245" />
			<date type="published" when="2021">2021</date>
			<publisher>Academic Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Individual differences in baseline oculometrics: Examining variation in baseline pupil diameter, spontaneous eye blink rate, and fixation stability</title>
		<author>
			<persName><forename type="first">N</forename><surname>Unsworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Robison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13415-019-00709-z</idno>
		<ptr target="https://doi.org/10.3758/s13415-019-00709-z" />
	</analytic>
	<monogr>
		<title level="j">Cognitive, Affective, &amp; Behavioral Neuroscience</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1074" to="1093" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Arousal Effects on Pupil Size, Heart Rate, and Skin Conductance in an Emotional Face Task</title>
		<author>
			<persName><forename type="first">C.-A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Baird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Coutinho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Munoz</surname></persName>
		</author>
		<idno type="DOI">10.3389/fneur.2018.01029</idno>
		<ptr target="https://doi.org/10.3389/fneur.2018.01029" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neurology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The human amygdala parametrically encodes the intensity of specific facial emotions and their categorical ambiguity</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Tyszka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kovach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hurlemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">B</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Mamelak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Adolphs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Rutishauser</surname></persName>
		</author>
		<idno type="DOI">10.1038/ncomms14821</idno>
		<ptr target="https://doi.org/10.1038/ncomms14821" />
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">14821</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Single-neuron spiking variability in hippocampus dynamically tracks sensory content during memory formation in humans</title>
		<author>
			<persName><forename type="first">L</forename><surname>Waschke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Van Den Elzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Lindenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Rutishauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Garrett</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41467-024-55406-4</idno>
		<ptr target="https://doi.org/10.1038/s41467-024-55406-4" />
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">236</biblScope>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The origin of first impressions</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Zebrowitz</surname></persName>
		</author>
		<idno type="DOI">10.1556/jcep.2.2004.1-2</idno>
		<ptr target="https://doi.org/10.1556/jcep.2.2004.1-2" />
	</analytic>
	<monogr>
		<title level="j">Journal of Cultural and Evolutionary Psychology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="93" to="108" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">First impressions from faces</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Zebrowitz</surname></persName>
		</author>
		<idno type="DOI">10.1177/0963721416683996</idno>
		<ptr target="https://doi.org/10.1177/0963721416683996" />
	</analytic>
	<monogr>
		<title level="j">Current Directions in Psychological Science</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="237" to="242" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
