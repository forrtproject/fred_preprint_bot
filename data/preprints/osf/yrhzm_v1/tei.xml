<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GPT API Models Can Function as Highly Reliable Second Screeners of Titles and Abstracts in Systematic Reviews: A Proof of Concept and Common Guidelines</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mikkel</forename><forename type="middle">Helding</forename><surname>Vembye</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">VIVE -The Danish Center for Social Science Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Julian</forename><surname>Christensen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">VIVE -The Danish Center for Social Science Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anja</forename><forename type="middle">Bondebjerg</forename><surname>M√∏lgaard</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">VIVE -The Danish Center for Social Science Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Frederikke</forename><surname>Lykke</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Witth√∂ft</forename><surname>Schytt</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">VIVE -The Danish Center for Social Science Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Soeren</forename><surname>Frichs</surname></persName>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<postCode>8230</postCode>
									<settlement>Vej 36 G, Aabyhoej</settlement>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GPT API Models Can Function as Highly Reliable Second Screeners of Titles and Abstracts in Systematic Reviews: A Proof of Concept and Common Guidelines</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6C6ACCFA52E0700B2601A7E49C0766FD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-10-22T06:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>title and abstract screening</term>
					<term>OpenAI&apos;s GPT API models</term>
					<term>systematic review</term>
					<term>screening benchmarks</term>
					<term>Large Language Models (LLM)</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We would like to thank Jens Dietrichson, Trine Filges, Terri Pigott, Tiril Borge, Heather Melanie R. Ames, and Christopher James Rose for valuable comments and sharing of screening data. Also thanks to Sofie Elgaard Lisager Jensen and Johan Klejs for testing the AIscreenR software and for valuable inputs to the workflow.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>GPT API Models Can Function as Highly Reliable Second Screeners of Titles and Abstracts in Systematic Reviews: A Proof of Concept and Common Guidelines Systematic reviews are essential for informing policy, research, and practice. Hence, it is all-important that systematic reviews adhere to the highest scientific standards. Yet systematic reviews are time-consuming, potentially hindering a timely transfer of usable knowledge. Distinct from other types of reviews, systematic reviews are defined as the process of collecting, assessing, and synthesizing findings from (ideally all) relevant scientific studies using explicit and replicable research methods <ref type="bibr" target="#b33">(Gough et al., 2017;</ref><ref type="bibr" target="#b37">Hou &amp; Tipton, 2024)</ref>. A critical first step to ensure the quality of systematic reviews and meta-analyses herein involves detecting all eligible references related to the literature under review <ref type="bibr" target="#b50">(Polanin et al., 2019)</ref>. This entails searching all pertinent literature databases relevant to the given review, most often resulting in thousands of titles and abstracts to be screened for relevance. Manual screening hereof can be a time-consuming and tedious task. However, overlooking relevant studies at this stage can be consequential, leading to substantially biased results, if the missed studies are systematically different from the detected ones, threatening the internal validity of systematic reviews <ref type="bibr" target="#b6">(Brunton et al., 2017;</ref><ref type="bibr" target="#b35">Hedges, 1992;</ref><ref type="bibr" target="#b55">Rothstein et al., 2005;</ref><ref type="bibr" target="#b59">Shadish et al., 2002)</ref>. In effect, independent human double-screening is considered the 'gold standard' to hinder a biased selection of studies <ref type="bibr" target="#b34">(Guo et al., 2024;</ref><ref type="bibr" target="#b36">Higgins et al., 2019;</ref><ref type="bibr" target="#b62">Stoll et al., 2019;</ref><ref type="bibr" target="#b71">Wang et al., 2020)</ref>.</p><p>Independent human double-screening of all identified titles and abstracts is, however, a resource-demanding procedure, often requiring several months of skilled, full-time human labor <ref type="bibr" target="#b10">(Campos et al., 2024;</ref><ref type="bibr" target="#b37">Hou &amp; Tipton, 2024;</ref><ref type="bibr" target="#b60">Shemilt et al., 2016)</ref>. Consequently, many reviewers refrain from using duplicate screening methods (see <ref type="bibr" target="#b48">Pacheco et al., 2023</ref> for examples in medicine), for instance, due to low budgets or narrow time limits. Alternatively, reviewers narrow their searches to keep the number of records down to a manageable size, which again increases the risk of overlooking relevant studies <ref type="bibr" target="#b66">(Van De Schoot et al., 2021)</ref>. Over time these issues will only grow in size as the complexity of identifying all relevant studies increases with the rapid growth in the number of scientific publications <ref type="bibr" target="#b5">(Bornmann et al., 2021;</ref><ref type="bibr" target="#b46">O'Mara-Eves et al., 2015)</ref>. As such, it can be considered an economically inefficient and unsustainable use of human resources to rely solely on duplicate human screening of titles and abstracts in future systematic reviews <ref type="bibr" target="#b60">(Shemilt et al., 2016)</ref>.</p><p>An alternative to human double-screening is to use (semi-)automated screening tools based on text-mining or machine-learning algorithms to act either as a second screener, a coarsegrained classifier, or to sort citation records in a prioritized order <ref type="bibr" target="#b12">(Cohen et al., 2006;</ref><ref type="bibr" target="#b32">Gartlehner et al., 2019;</ref><ref type="bibr" target="#b46">O'Mara-Eves et al., 2015;</ref><ref type="bibr" target="#b66">Van De Schoot et al., 2021)</ref>. These tools can make substantial reductions in human screening workloads in systematic reviews <ref type="bibr" target="#b42">(K√∂nig et al., 2023;</ref><ref type="bibr" target="#b46">O'Mara-Eves et al., 2015;</ref><ref type="bibr" target="#b49">Perlman-Arrow et al., 2023)</ref>. However, most evaluations of traditional automated screening tools yield the general conclusion that these tools are not yet capable of replacing an independent human second screener without a significant risk of omitting a substantial number of eligible studies <ref type="bibr" target="#b7">(Burgard &amp; Bittermann, 2023;</ref><ref type="bibr" target="#b32">Gartlehner et al., 2019;</ref><ref type="bibr" target="#b43">Kugley et al., 2016;</ref><ref type="bibr" target="#b46">O'Mara-Eves et al., 2015;</ref><ref type="bibr" target="#b47">Olorisade et al., 2016;</ref><ref type="bibr" target="#b54">Rathbone et al., 2015)</ref>. By using the level of automation heuristic, developed by <ref type="bibr" target="#b45">O'Connor et al. (2019)</ref>, it can be said that current automated tools generally fail to function at the highest levels of automation where they make credible independent, deterministic screening decisions.</p><p>A potential solution to alleviate this issue and elevate automated screening tools to the highest levels of automation is to use large language models (LLM), such as the generative pre-trained transformer (GPT) models that have recently been introduced by OpenAI. Initial evaluations have generally yielded some promising results with regard to using OpenAI's GPT API (application programming interface) models for title and abstract (henceforth TAB) screening. To our knowledge, <ref type="bibr" target="#b63">Syriani et al. (2023)</ref> were the first team to evaluate these models for screening tasks, specifically comparing the GPT-3.5-turbo-0301 model to state-of-the-art machine learning algorithms in systematic reviews within software engineering. <ref type="foot" target="#foot_0">1</ref> They found the model's performance to be comparable with traditional classifier models and most often better.</p><p>In another study, <ref type="bibr" target="#b34">Guo et al. (2024)</ref> tested the use of OpenAI's GPT-4 API model for TAB screening of medical research literature. <ref type="foot" target="#foot_1">2</ref> Across six clinical reviews, when evaluating the model's inclusion and exclusion decisions against the final decisions of two independent human screeners, GPT-4 was found to have average recall (i.e., the proportion of relevant records being correctly classified as relevant) and specificity (i.e., the proportion of irrelevant records being correctly classified as irrelevant) values of .76 and .91, respectively. Based on these results, the authors concluded that GPT-4 was effective in excluding irrelevant studies but less reliable in identifying relevant ones, and therefore recommended using it as a support tool but not as a full replacement of the second human screener. Similar conclusions were reached by <ref type="bibr" target="#b31">Gargari et al. (2024)</ref> after testing the use of the gpt-3.5-turbo-0613 API model for TAB screening in a clinical systematic review. <ref type="foot" target="#foot_2">3</ref>On a related line of research, <ref type="bibr" target="#b0">Alshami et al. (2023)</ref>, <ref type="bibr" target="#b41">Khraisha et al. (2024)</ref>, and <ref type="bibr" target="#b38">Issaiy et al. (2024)</ref> explored the ChatGPT web browser interface for TAB screening, with mixed results, indicating performance similar to the API models but generally insufficient compared to human reviewers.</p><p>Although previous applications and evaluations of using OpenAIs GPT models for TAB screening represent a vital first step in validating these models as independent second screeners in systematic reviews, many questions remain unanswered. In this respect, it is still unclear how big error rates can be accepted when working with automated tools and how these tools' performances compare to typical human screening performances in high-quality systematic reviews. It is further unclear if screening performances can be improved by drawing on newly developed API features such as function calling and fine-tuning <ref type="bibr">(OpenAI, 2024c</ref><ref type="bibr">(OpenAI, , 2024b))</ref>. In addition, it is unclear if and how the GPT models can be implemented in systematic reviews in a standardized and reliable manner. In contrast to many well-established automated screening algorithms, no common workflow and guidelines exist for how to conduct GPT-based TAB screenings, including how to make reliable prompts. Also, no software has yet been developed to support and standardize the setup of GPT-based TAB screenings. This study therefore aims to 1) build a generic benchmark scheme to improve assessments about what constitutes acceptable screening behaviors of automated tools in high-quality systematic reviews, 2) test and validate the TAB screening performance of GPT API models using novel model features, 3) develop a heuristic workflow and guidelines for how and when to conduct TAB screening with GPT API models, and 4) present the R package AIscreenR <ref type="bibr" target="#b67">(Vembye, 2024)</ref>.</p><p>The remainder of the paper proceeds as follows. In the next section, we investigate how well a GPT model needs to perform to be accepted as an independent second screener of titles and abstracts in high-quality systematic reviews. To answer this question, we analyze human screening performances across 22 high-quality systematic reviews and use this investigation as the basis for developing a novel, empirically informed benchmark scheme for interpreting acceptable and unacceptable screening performances in high-quality systematic reviews. In the following section, we present three classification experiments to evaluate the screening performances of GPT API models relative to human screeners. This includes presentations of the prompt engineering and data underlying these experiments as well as the results of the experiments. In the subsequent section, we deduce tentative guidelines for when we consider it acceptable and unacceptable to use GPT API models as independent second screeners. In this section, we also elaborate on how we think reliable prompts can be developed in future reviews and present a standardized workflow for how to incorporate GPT screening in high-quality reviews. In the final sections, we recapitulate by reflecting on the limitations of our work, the prospect of using LLMs for TAB screening in high-quality systematic reviews, and what should concern future research as well as the implications of our results and recommendations.</p><p>What are acceptable human error rates in high-standard systematic reviews?</p><p>Before adopting automated tools, such as GPT API models, as independent TAB screeners for systematic reviews, we need to ensure that these tools are not inferior to human screeners, to avoid compromising the quality of systematic reviews <ref type="bibr" target="#b45">(O'Connor et al., 2019)</ref>. To allow for assessments of this, in this section, we develop a novel, empirically informed benchmark scheme for interpreting acceptable and unacceptable TAB screening performance in high-quality systematic reviews. We start by presenting the performance metrics that we use to develop our benchmarks. Next, we present and analyze data on human screening performances across 22 state-of-the-art systematic reviews that are used as the basis for our benchmark scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transparency and openness</head><p>All statistical data analyses were conducted using R 4.4.0 (R Core <ref type="bibr">Team, 2022)</ref> in RStudio <ref type="bibr" target="#b57">(RStudio Team, 2015)</ref>. For the main analyses behind the benchmark scheme, we used the metafor package, version 4.6.0 <ref type="bibr" target="#b68">(Viechtbauer, 2010)</ref>, including the sandwich estimators herein <ref type="bibr" target="#b51">(Pustejovsky, 2020)</ref>. RIS file data was handled by using the revtools package, version 0.4.1 <ref type="bibr" target="#b72">(Westgate, 2019)</ref>, and we used the ggplot2, version 3.5.1 <ref type="bibr" target="#b73">(Wickham, 2016)</ref> for visualization. Code and data for replicating the investigation behind the benchmark scheme are available at https://github.com/MikkelVembye/screen_benchmarks. All replication materials behind the experiments presented in later sections can be accessed at https://osf.io/apdfw/. This study has not been preregistered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation metrics</head><p>In the existing literature, a wide range of different metrics has been used to evaluate TAB screening performances in the context of systematic reviews. Our choice of metrics has been informed by the recommendations of <ref type="bibr" target="#b45">O'Connor et al. (2019)</ref> and <ref type="bibr" target="#b63">Syriani et al. (2023)</ref>. As such, the most central performance metrics in our analyses are the recall (sometimes referred to as the sensitivity) and specificity metrics since these are intuitive to understand and interpret and are not sensitive to imbalanced data, meaning that they are not sensitive to the data containing a large difference in the proportion of relevant and irrelevant references, which is commonly found in systematic reviews <ref type="bibr" target="#b6">(Brunton et al., 2017)</ref>. To be formal, the recall can be written as</p><formula xml:id="formula_0">ùëÖùëíùëêùëéùëôùëô = ùëáùëÉ ùëáùëÉ + ùêπùëÅ (1)</formula><p>where ùëáùëÉ (true positive) represents all the studies that are correctly included, and ùêπùëÅ (false negative) is the number of studies that are falsely excluded. By contrast, the specificity metric is given by</p><formula xml:id="formula_1">ùëÜùëùùëíùëêùëñùëìùëñùëêùëñùë°ùë¶ = ùëáùëÅ ùëáùëÅ + ùêπùëÉ (2)</formula><p>where ùëáùëÅ (true negative) represents all the studies that are correctly excluded, and ùêπùëÉ (false positive) is the number of studies falsely included.</p><p>For our purpose, we consider the recall measure to be the most important performance measure since missing relevant studies (i.e., having a low recall) is the main reason for automated tools to potentially introduce bias in systematic reviews <ref type="bibr" target="#b37">(Hou &amp; Tipton, 2024)</ref>. By contrast, a low specificity does not induce bias, it just means that reviewers have to re-examine the relevance of a larger share of the total pool of references. If reviewers can be sure that they find all relevant studies but have a specificity of, say, .5, this still implies that reviewers can confidently exclude 50% of the irrelevant records, which in most cases would be a significant reduction in the screening workload.</p><p>Therefore, we think tools should be accepted when high recalls can be reached, to a large extent independently of the accompanied specificity value. These scenarios are depicted in the Figures <ref type="figure" target="#fig_2">1A</ref> and<ref type="figure" target="#fig_2">1B</ref>. We will come back to this in the following sections.</p><p>In addition to the recall and specificity metrics, which concern the inclusion or exclusion performances individually, it may also be relevant to look at summary metrics that incorporate the overall performance across the inclusion and exclusion metrics. A common issue with such summary metrics is that they tend to be sensitive to imbalances in the data. For instance, assume that you have 10 relevant records per 1000 records. You could then reach a raw agreement of 99% simply by excluding all records (i.e., without identifying any relevant record). To overcome this issue, we used the balanced accuracy (ùëèùê¥ùëêùëê) metric, which accounts for imbalanced data. Thus, the balanced accuracy metric balances the accuracy of the performance across the recall and specificity metrics and is simply an average of those metrics, formally given by</p><formula xml:id="formula_2">ùëèùê¥ùëêùëê = ùëÖùëíùëêùëéùëôùëô + ùëÜùëùùëíùëêùëñùëìùëñùëêùëñùë°ùë¶ 2<label>(3)</label></formula><p>We could have calculated other relevant metrics as well, such as the normalized Matthew correlation coefficient <ref type="bibr" target="#b11">(Chicco &amp; Jurman, 2023)</ref>. However, we will mainly focus on the metrics presented in Equations 1-3 since these are most intuitive in their interpretation. Yet, we have shared all data behind all of our analyses, thereby allowing readers to add further estimations if necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Typical human screening performances in high-quality systematic reviews</head><p>In order to make valid comparisons between human and automated screening performances, we consider it important to have an impression of the human screening performance typically accepted in high-standard systematic reviews <ref type="bibr" target="#b45">(O'Connor et al., 2019)</ref>. We consider this comparison a reliable way to assess whether a given recall is good or bad. If, for example, humans on average tend to miss 20%-25% of all relevant studies during the title and abstract screening phase, as some previous research has suggested <ref type="bibr" target="#b8">(Buscemi et al., 2006;</ref><ref type="bibr" target="#b70">Waffenschmidt et al., 2019)</ref>,<ref type="foot" target="#foot_3">4</ref> then it might be misleading to infer that a GPT model with a recall of .75 cannot be used as an individual second screener. Hereto, we think it is important to acknowledge that individual human screening is not without significant errors and automated screening tools must be evaluated in light of this. Automated screening tools will probably always err to some degree, as will humans <ref type="bibr" target="#b70">(Waffenschmidt et al., 2019)</ref>, and the important factor here is to ensure that the difference between the error rates is acceptable. To assess this difference, the next section presents a tentative benchmark scheme for interpreting acceptable and unacceptable screening performances in high-standard systematic reviews.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data used for benchmarking</head><p>As the empirical basis of our benchmark scheme, we analyzed human screening performances in 22 high-standard systematic reviews that used independent duplicate human screening. Since all of the included Campbell Systematic Reviews drew on assistant (i.e., noncontent-expert) screeners, this could potentially downward bias the performance metrics for various reasons. For instance, assistant screeners have limited content expertise regarding the topic(s) under review, which might potentially lower their performance. In effect, their performances might not necessarily be on par with the typical screening performance of content expert screeners. Hence, we analyzed the Campbell review data separately for assistant/non-expert and researcher/expert screeners.</p><p>Moreover, differences in performance levels between the two types of screeners may not only reflect different levels of content expertise but could also be driven by authority imbalances between the often more senior content expert and the assistant screener, making the performances of the expert screeners look better than they actually were. We, therefore, added screening performance data from five systematic reviews conducted by NIPH. In these reviews, all TAB screenings were conducted by researchers with a high level of content expertise related to the given review. This should give a clearer picture of common expert/researcher performances in systematic reviews. The added NIPH data includes 13,825 title and abstract records that have been independently doublescreened by a total of 13 individual researchers. The five NIPH reviews were conducted from 2021 to 2024. When analyzing all of the above-presented data, we removed all training data to avoid artificially inflating human disagreements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistical analysis of the human screening performances</head><p>We estimated all the performance metrics via Equations (1) to (3). The ùëáùëÉ, ùëáùëÅ, ùêπùëÉ, and ùêπùëÅ conditions used in these equations were determined by comparing each single human screener decision with the final decision reached by a minimum of two human screeners within in the given review.</p><p>When working with proportion metrics, such as the ones presented in Equations ( <ref type="formula">1</ref>) to</p><p>(3), it is usually advantageous to transform these metrics into measures that have more appropriate statistical properties. This includes having a sampling distribution that more closely mirrors a normal distribution and a variance component that can more reliably be approximated <ref type="bibr" target="#b69">(Viechtbauer, 2022)</ref>.</p><p>Therefore, we used the arcsine transformation <ref type="bibr" target="#b56">(R√∂ver &amp; Friede, 2022;</ref><ref type="bibr" target="#b58">Schwarzer et al., 2019)</ref> to calculate sampling variance and confidence intervals for all metrics. <ref type="foot" target="#foot_5">6</ref> For the balanced accuracy metric, we calculated the sampling variance of the transformed measure by using the total number of records as the sample size.</p><p>To derive the overall average performances in terms of recall, specificity, and balanced accuracy metrics across the included studies, we fitted two versions of the so-called correlated-hierarchical effects (CHE) working models <ref type="bibr" target="#b52">(Pustejovsky &amp; Tipton, 2021)</ref>. For the investigation regarding differential performances between assistant and author screeners, we applied the subgroup correlated effects (SCE+) model, whereas we used the CHE-RVE model when analyzing the NIPH performance data. Both types of models account for the multi-level structure of the data with the screener performance measures being nested within studies. At the same time, the models account for the correlation between the within-study performance estimates. The sample correlation, ùúå, is often entirely or partially unknown and must be imputed. In all the used working models, we assumed ùúå = .7.</p><p>To guard against model misspecification both models have incorporated robust variance estimators. The main difference between the two models is that they draw on slightly different weighting schemes but the SCE model is generally recognized as the main working engine for deriving subgroup effects and conducting reliable contrast tests <ref type="bibr" target="#b52">(Pustejovsky &amp; Tipton, 2021)</ref>. For differential effects comparisons, we used the HTZ Wald test suggested by <ref type="bibr" target="#b65">Tipton and Pustejovsky (2015)</ref>.</p><p>For both models, we estimated two sources of heterogeneity: the variability of the true screener performances within (ùúî) and between studies (ùúè). This allowed us to investigate at what (if any) level the largest true difference between the human screener performances existed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Typical human screening performance results</head><p>All individual screening performances across the included reviews and their distribution around the overall performance means are illustrated in Figures <ref type="figure" target="#fig_3">2</ref> and<ref type="figure" target="#fig_4">3</ref>. In this section, we primarily comment on the results for the recall and specificity measures, since these are the main metrics we use to derive and develop the benchmark scheme. All results can be found in the background material.</p><p>Across the included Campbell Systematic Reviews, we found the overall average recall value for the assistant and author screeners to be .782, 95% CI[.747, .817] and .881, 95% CI[.823,</p><p>.931], respectively. Hereto, we found the two groups' average recall values to be statistically different from each other with F(1, 10.3) = 14.58, p = .003. We detected minor substantial variations between the performance measures within studies with ùúî = 0.026 and ùúî = 0.035 for the assistant and author screeners, respectively. We were unable to detect any true differences in performance levels between studies, indicating consistent average screening performances across the Campbell reviews, both for assistants and expert screeners. The overall average specificity for assistant screeners was .980, 95% CI[.966, .990], and for review authors .988, 95% CI[.980, .995]. We found no statistically significant difference between the two average estimates with F(1, 13.6) = 2.08, p = 0.172. We did only find very minor non-substantial variation within and between studies with ùúî = 0.004 as the maximum for author screeners. Moving on to the balanced accuracy metric, we found average performance levels of .874, 95% CI[.857, .890] among assistant screeners and .933, 95% CI[.899, .961] among author screeners. We found the difference between the group means to be statistically significant with F(1, 10.1) = 18.22, p = .002.</p><p>From these results, it appeared that compared to students, researcher (i.e., content expert) screeners are substantially better at detecting relevant studies. Yet, as previously noted, this difference may be driven by factors other than mere screening quality (e.g., authority relations with the potential to inflate the researchers' performance relative to that of the assistants). Interestingly, when analyzing the NIPH data, which was in all cases based on independent researcher-researcher screening comparisons, we found performance levels closer to those of the assistant screeners in the Campbell Systematic Reviews. The average recall value in the NIPH data was .839, 95% CI[.737,</p><p>.920]. Again, we only found minor true variation between the screener recall performances within studies with ùúî = 0.029 and ùúè = .0. The overall average specificity value of the NIPH researchers was .977, 95% CI[.955, .992], with only minor non-substantial true variation between screeners and between studies. The average balanced accuracy value of the NIPH researchers was .905, 95% CI[.859,</p><p>.943].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Benchmark scheme</head><p>Bearing on the empirical results presented in the previous section, we developed the screening benchmark scheme presented in Table <ref type="table">2</ref>. On this basis, we suggest-as a coarse-grained rule of thumbthat if an automated tool can reach a recall rate of at least .75 and specificity rates above .95, they can be said to resemble common second-screener performances in the context of high-quality systematic reviews. Consequently, and in contrast with previous evaluations <ref type="bibr" target="#b34">(Guo et al., 2024)</ref>, we would not necessarily interpret a recall value of .76 to be too low for a GPT API model to function as an independent second screener.</p><p>As can be seen from the benchmark scheme in Table <ref type="table">2</ref>, we do not necessarily conceive a specificity of 1 (i.e., 100%) to be ideal, since we would rather have our automated screenings be over-inclusive than over-exclusive. Thus, a low specificity value merely forces human screeners to double-check a larger number of potentially relevant references, which in turn lowers the risk of relevant studies being missed. As such, we think that a specificity value equal to or above .80 is acceptable as long as the recall value is equal to or above .75 (c.f. Figure <ref type="figure" target="#fig_2">1</ref>) as well. We, therefore, suggest that automated screening performances reaching recall of at least .75 and specificity above .80 should be accepted as independent screeners in high-quality systematic reviews. Also, we think that automated tools that yield high recalls may be used to reduce the total number of title and abstract records needed to be screened, even if the specificity value is below .80. This would especially be relevant when working with very large amounts of title and abstract records (see an example of this in <ref type="bibr" target="#b61">Shemilt et al., 2014)</ref>.</p><p>Finally, we believe that automated tools can also be useful under less restrictive conditions. We even believe that recall values between, say, .5-.75 should not disqualify automated screening tools from playing a role in TAB screening. Under such conditions, we would warn against using the tools as independent screeners but they could function as an extra assurance, working as a third screener that forces the human screeners to double-check close-to-relevant study records. This would enhance the screening quality, lowering the risk of humans overlooking any relevant records.</p><p>Yet, to be clear, we generally think that it will not be viable to use automated tools with performance levels (recall and specificity values) below .5. However, in Table <ref type="table">2</ref>, we use graduated shades of red, where the light red color for specificity values below .5 indicates that we cannot entirely reject that there might be cases where automated screenings could be useful even with specificity values below .5 (as long as the recall is high). For example, in extreme-sized reviews, even a 30% workload reduction might save multiple days of human labor. The number of title and abstract records does, however, need to be very high for this approach to be viable.</p><p>With the benchmark scheme presented in Table <ref type="table">2</ref>, we aim to make a more flexible tool partially for assessing the screening performance of automated tools in general and partially for assessing which screening tasks can be made under what performance conditions. This allows for more case-specific discussions regarding the adequacy of using GPT API models, or other automated tools, for TAB screening tasks in systematic reviews, avoiding trivial and binary 'for and against' discussions. Furthermore, we will use this benchmark scheme for interpreting our conducted classifier experiment that we present in the next section.</p><p>Classification experiments: How do GPT API models perform in light of the benchmark scheme?</p><p>In this section, we present the data and prompts used as well as the results for three large-scale classifier experiments that we have conducted to evaluate the screening performance of OpenAI's GPT API models. Differently from previous research, these classifier experiments aimed to test the performance of GPT API models 1) when applied in psychological reviews, 2) when using function calling (OpenAI, 2024c), that is using JSON functions ensuring structured responses, and 3) when using multi-prompt screening in complex review settings. We set up the experiment so that each of the three experiments represented different levels of complexity in terms of inclusion criteria and contained different challenges to overcome when conducting TAB screening using GPT API models. The main purpose of the experiments is not to show that GPT API models work in all instances. Instead, we aim to show that if configured adequately, these models can function as highly reliably independent second screeners across various types of systematic review questions. This also means that using GPT API models as a second screener is not always ideal for various reasons. We return to this issue in a later section. A side-effect of conducting these experiments was further to ensure the quality of the AIscreenR package <ref type="bibr" target="#b67">(Vembye, 2024)</ref> for automated TAB screenings using GPT API models. We considered this test to be an important step in ensuring a scalable screening approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data used for classifier experiments</head><p>In Experiment 1, we tested the performance of GPT API models in the context of a Campbell Systematic Review, conducted by <ref type="bibr" target="#b22">Filges et al. (2015)</ref>, on the effects of functional family therapy (FFT) on drug abuse reduction for young people in treatment for nonopioid drugs. By leveraging a previously published review, we were able to immediately evaluate the GPT API models' performances against the inclusion and exclusion decisions made by two human screeners during the original review. Moreover, the inclusion criteria of the review were rather simple and the FFT intervention is well-defined. This made it an ideal initial test case for proof of concept purposes. If the GPT API models could not achieve satisfactory performance in this context, they would be unlikely to do so in the context of more complex reviews. This experiment was based on a highly imbalanced dataset with only 69 of 4135 records being relevant, amounting to an approximate inclusion ratio of 17 relevant studies per 1000 records.</p><p>A potential weakness in Experiment 1 is that it is assumed that the OpenAI's GPT models have been trained on publicly available text data from the internet, available from 2021 and back, which could in theory have inflated the screening performances in Experiment 1 caused by the GPT models being trained on this particular open-access review and its protocol. If this was the case, it may reduce the generalizability of the experiment's results to other review cases where the models have not been trained on any relevant data. To address this issue, we conducted a second classifier experiment drawing on data from an unpublished/ongoing systematic review. Thus, in Experiment 2, we used screening data from a Campbell Systematic Review regarding the effects of the FRIENDS preventive programme on anxiety symptoms in children and adolescents conducted by <ref type="bibr">Filges, et al. (2023)</ref>. <ref type="foot" target="#foot_6">7</ref> The FRIENDS data in many aspects resembles the FFT data, with inclusion criteria being rather simple and the intervention being well-defined. Moreover, the data is highly imbalanced with only 64 of 2572 records being relevant, amounting to an approximate inclusion ratio of 25 relevant studies per 1000 records.</p><p>As noted, Experiments 1 and 2 can both be said to involve rather simple TAB screening tasks and without further investigation, it is unclear to what extent their results can be generalized to more complex review settings with more complex inclusion and exclusion criteria as is often the case in psychological reviews. A challenge in making GPT screening work in complex review settings is that it likewise requires reviewers to make a broader and more complex prompt. Hereto, <ref type="bibr" target="#b31">Gargari et al. (2024)</ref> suggested that long and broad prompts may not perform well in terms of finding relevant studies. To address this challenge, we conducted a third experiment. In this experiment, we introduced and tested the performance of multi-prompt screening (i.e., sometimes referred to as prompt chaining), that is making one concise prompt per inclusion/exclusion criteria in a review (instead of adding all inclusion and exclusion criteria to the same prompt), to test if this screening approach would yield better performance in a complex review setting.</p><p>To emulate a complex psychological review setting, we used screening data from an ongoing Campbell Systematic Review of the effects of testing frequencies on students' academic achievement <ref type="bibr" target="#b64">(Thomsen et al., 2022)</ref>, which has been a common review topic in psychology (c.f. <ref type="bibr" target="#b74">Yang et al., 2021)</ref>. Compared to the screenings in Experiments 1 and 2, we considered Experiment 3 a more difficult screening case, since the inclusion criteria of this review were more complex, including concepts that are not well-defined. As such, the intervention (student testing) is a type of learning strategy that is ubiquitous in education and psychology and is used in a variety of ways for many different purposes. <ref type="foot" target="#foot_7">8</ref> In effect, testing is not a uniform type of intervention, but a multi-facetted phenomenon encompassing a variety of approaches and a heterogeneous terminology (tests are not just called tests, but may also be referred to, e.g., as quizzes, progress-monitoring, curriculum-based measures, and retrieval practice). Assessing the eligibility of particular studies therefore requires a great deal of subject matter familiarity, and contrary to Experiments 1 and 2, we think that if GPT API models can achieve satisfactory performances in this context, they will likely be able to do so in most review contexts. In this experiment, the data consisted of 2000 irrelevant and 100 relevant records randomly sampled from the total pool of 5612 irrelevant and 627 relevant records, respectively. We did so to optimize our use of resources as this screening was carried out as a multi-prompt screening, with each title and abstract being evaluated against six individual prompts, each corresponding to a specific inclusion criterion.</p><p>Across all three experiments, we excluded study records that did not have an abstract.</p><p>This excluded 208, 150, and 41 study records in the FFT, FRIENDS, and testing frequency (henceforth TF) data, respectively. In the FRIENDS data, we further deleted 20 titles and abstracts containing a myriad of special symbols, causing the GPT response to return insufficient JSON data from the server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prompt engineering</head><p>For Experiments 1 and 2, we engineered prompts to include an introduction section describing the general aim of the review followed by the inclusion criteria of the review. To exemplify, Textbox 1 exhibits the prompt used for Experiment 2.</p><p>Next, when given study IDs, 9 titles, and abstracts, the AIscreenR automatically integrates this information in the prompt, using the text in Textbox 2.</p><p>By pasting the prompt together with each title and abstract, we aim to guard against model drifting/hallucinations. Different from prior evaluation studies, we did not add any instruction regarding how the model should respond to our request in the main prompt. Instead, we relied on 9 If not provided by the user, study IDs are automatically generated when using the AIscreenR.</p><p>function calling and built two JSON functions (one function call yielding simple trinary results and another yielding descriptive screening responses) with instructions on how the models should respond to our requests. 10 According to OpenAI, this should result in more reliable and standardized responses from the models (OpenAI, 2024c). The main JSON respond function we built included the instructions presented in Textbox 3.</p><p>For Experiment 3, involving six inclusion criteria, we tested two different prompt engineering strategies: multi-prompt/prompt-chaining and single-prompt screening. In the former approach, six short prompts were made, each containing one inclusion criterion only. In the latter, we added all inclusion criteria to a single prompt. This allowed us to test how GPT performances are impacted by different prompt strategies, and particularly whether multi-prompt screening can make GPT screenings viable in settings where the usefulness of GPT screenings has been questioned in previous research <ref type="bibr" target="#b31">(Gargari et al., 2024)</ref>.</p><p>All engineered prompts used for Experiment 3 were initiated by a short introduction to the review followed by a description of the given inclusion criterion. As with the prompts used in Experiments 1 and 2, the prompts used in Experiment 3 were all pasted together with the text present in Textbox 2 and drew the function call from Textbox 3. The specific prompts can be found in the Supplementary Material Textbox S1 and S2. Moreover, we elaborate further on multi-prompt screening below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance testing</head><p>Before initiating each classification experiment, we tested and refined our prompts on a subset of the title and abstract records. For the FFT review, we started by testing our prompt on a single relevant reference, and we refined the prompt until the GPT models consistently included this 10 The exact wording of each function can be found at bit.ly/3Vl0SRp study record. Then, we scaled up the test to include 150 irrelevant and 50 relevant records. When the test yielded satisfactory results, demonstrating an ability to reach a recall above .75 and a specificity above .9, we moved on to screening all records using the GPT API models to investigate if the models' test performances persisted when used on the full sample of records. For both the FRIENDS and TF reviews, we tested the prompts on 150 irrelevant and 50 relevant study records randomly sampled from the total pool of irrelevant and relevant records, respectively. Again, we initiated the full screening after finding the GPT API models to yield satisfactory screening performances. It should be noted that in our multi-prompt screening of title and abstract records for the TF review, we found that the best screening performances yielded by coding studies as relevant if they were included in at least five of the six prompts (instead of requiring them to be included by all six prompts). Therefore, we applied this threshold in the full screening. We allowed for this flexible inclusion threshold to further account for the uncertainty in the models' decision when limited information appears in an abstract.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation design</head><p>In all three classifier experiments, we evaluated the performance of the GPT API models by using Equations (1) to (3). In this respect, the ùëáùëÉ, ùëáùëÅ, ùêπùëÅ, and ùêπùëÉ conditions were determined by comparing the GPT decision with the final decision made by agreement between at least two independent human screeners. Human inclusion at this first level of screening did not necessarily imply that study records were relevant for the final review-merely that they were considered to be relevant for full-text screening. In Experiments 1 and 2, we used the gpt-3.5-turbo-0613 and gpt-4-0613 models, reached from the 'v1/chat/completions/' endpoint. Since the GPT-3.5 models are generally less consistent in their responses relative to GPT-4, we repeated the same screening request 10 times for each title and abstract when using this model, as also done by <ref type="bibr" target="#b63">Syriani (2023)</ref>. We did so to test the model's consistency across screenings and to assess how this impacted its final inclusion decisions. The final inclusion decision of GPT-3.5 was then based on the probability of inclusion across the repeated requests. In part, because the GPT-4 models are more consistent in their responses, and in part because of the higher costs 11 of using these models, we only conducted one screening per title and abstract when calling GPT-4. We will present the result for the GPT-3.5 model but our main focus is on the performance of GPT-4 since the GPT-3.5 model has been deprecated 12 and expired September 13, 2024.</p><p>For Experiment 3, which involved multi-prompt screening, we only drew on GPT-4, and the final inclusion decision was then based on the probability of inclusion across all used prompts.</p><p>For all experiments, we used invariant top_p and temperature values, using the default value of 1 for both hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results of the classification experiments</head><p>All results for the three classifier experiments are presented in Table <ref type="table" target="#tab_2">3</ref>. As can be seen from Table <ref type="table" target="#tab_2">3</ref>, the GPT-4 model yielded recall and specificity values of .899 and .933 in Experiment 1, which, held against the benchmark scheme presented in Table <ref type="table">2</ref>, can be considered to be on par with typical researcher screener performances in high-quality systematic reviews. The GPT-3.5</p><p>model was also able to reach human-like screening performances. Yet these results varied substantially depending on the chosen inclusion probability threshold, reflecting the model's higher level of inconsistency in screening decisions, especially when it comes to detecting relevant studies (cf. Table <ref type="table" target="#tab_2">3</ref>'s recall column). When using an inclusion probability threshold of .2 (meaning that a study was coded as relevant if the GPT-3.5 model included it in two or more of the ten screenings), the GPT-11 Note: After we conducted the experiment OpenAI has presented the GPT-4o-mini which is cheaper than the GPT-3.5 models that we used. All of OpenAI's new GPT API models are configured differently than the GPT-3.5 and GPT-4 model, we draw upon. Therefore, we have not evaluated these models yet since they need to be coded anew.</p><p>12 Deprecation "refer[s] to the process of retiring a model or endpoint. When we announce that a model or endpoint is being deprecated, it immediately becomes deprecated. All deprecated models and endpoints will also have a shut down date. At the time of the shut down, the model or endpoint will no longer be accessible." <ref type="bibr">(OpenAI, 2024a)</ref> 3.5 model yielded a recall of .81 and a specificity of .94. However, when using an inclusion probability threshold of .5, the performance became unacceptably low compared to human screening, with a recall of only .69. A full overview of the impact of the inclusion probability threshold on the performance metrics for Experiment 1 can be found in Supplementary Material Figure <ref type="figure" target="#fig_2">S1A</ref>.</p><p>When used on the FRIENDS data, the GPT-4 model yielded performances exceeding common human screening performances. Specifically, it yielded a recall of .98 (only missing one relevant study) and a specificity value of .97. When using an inclusion probability threshold of .7, the GPT-3.5 model also performed well, with a recall of .953 and specificity of .899. Yet, again, the screening performance of GPT-3.5 hinged on the chosen inclusion probability threshold. A full overview of the impact of the inclusion probability threshold on the performance metrics for Experiment 2 can be found in Supplementary Material Figure <ref type="figure" target="#fig_2">S1B</ref>.</p><p>Finally, when used on the TF data (Experiment 3), the GPT-4 model yielded a recall of .80 when including studies that were included by the model in at least 5 out of 6 prompts. This is on par with typical human screening performances (cf. the benchmark scheme in Table <ref type="table">2</ref>) and did exceed three out of six human recalls within this review (see <ref type="bibr" target="#b64">Thomsen et al. (2022)</ref> under column 1 in Figure <ref type="figure" target="#fig_3">2</ref>). Moreover, the model yielded an acceptable level of specificity of ~.84 when based on multiprompt screening. Relative to human performances, the model in this case can be said to be rather over-inclusive. As such, we do not necessarily consider this to be disadvantageous, since it reduces the risk of overlooking relevant studies, which might be even more important in complex review settings where exclusion decisions may more often be difficult to make at the first level of screening due to insufficient information in the abstracts.</p><p>When using a single prompt to screen the TF data, we found a human-like recall of .9.</p><p>Compared to human performances, the single-prompt screening was rather overinclusive with a specificity of .743. This performance resembled the results of the multi-prompt screening when using a threshold where title and abstract records were coded as relevant if included by the GPT model in at least four of the six prompts.</p><p>As can be seen in Table <ref type="table" target="#tab_2">3</ref>, when using an even more inclusive threshold, coding studies as relevant if included by the GPT model in at least three of the six, the GPT-based reached a recall of .95, but with a low specificity of .67, leaving a rather high number of title and abstract records to be double-checked by human screeners. Yet, if this approach was used it could still reliably reduce the total screening workload.</p><p>To summarise, we find that GPT API models can work as highly reliable and independent second screeners with recall performances on par with or better than common human screeners, even in highly complex screening settings. This finding contrasts previous evaluations <ref type="bibr" target="#b31">(Gargari et al., 2024;</ref><ref type="bibr" target="#b34">Guo et al., 2024)</ref> suggesting that the GPT API models mainly have high performances in terms of correctly excluding irrelevant records. This discrepancy might be explained by the fact that we used different prompting strategies. A part of our comparably high performance might also be caused by the fact we instructed the models to include title and abstract records with very little information (cf. Textbox 3). We note that based on our tests, the GPT-4 API model seems to be preferable relative to GPT-3.5 since the latter is rather sensitive to the chosen inclusion probability threshold. Based on this finding, we generally recommend not using the GPT-3.5 API models when GPT-4 API models are available. Moreover, in cases where researchers have to rely on GPT-3.5, different inclusion probability thresholds should be considered at the initial stage of the screening.</p><p>We found that in some applications, the specificity rate reached by the GPT-4 API model can be seen to be on the lower end compared with human screeners. Yet, we do not find this to be a major issue when having a high recall rate (cf. Figure <ref type="figure" target="#fig_2">1</ref>) since this can just be seen as an extra opportunity to double-check close-to-relevant studies, thus enhancing the chance of not overlooking any relevant study records.</p><p>Based on our results, we cannot firmly conclude that multi-prompt screening is significantly better than single-prompt screening in complex review settings. Yet, it is a more flexible approach that can reduce the over-inclusiveness of GPT models, while still yielding sufficient recalls on par with typical human second screeners. Although we cannot reject that single-prompt screening might be viable in complex review settings with many inclusion criteria, we think multi-prompt screening is more appropriate to use in complex review settings. When using multi-prompt screening, all titles and abstracts will be mapped on the exact reasons for exclusion, increasing the transparency of the review and making it easier to decode what factors made the GPT model work or not. Furthermore, as we discuss in the next section, the use of multi-prompt screening can have additional advances in the prompt development and testing phases, making it a useful tool adding to the screening toolbox.</p><p>Overall, we think there is a huge potential for GPT API models to be used for TAB screening tasks in high-quality systematic reviews-also as independent second screeners in complex reviews. Furthermore, we believe that the relevancy of using LLMs will only increase over time as the models improve. This demands a standardized setup to ensure a reliable use of LLMs in systematic reviews. In the next section, we therefore develop a tentative workflow and guidelines for how such screenings practically can be set up in a standardized manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tentative workflow and guidelines</head><p>Premised on our developed benchmark scheme, our experience, and the results of the three classifier experiments, we have developed the following tentative workflow and guidelines for when and how GPT API models can be used as independent second screeners of titles and abstracts.</p><p>All steps in this process are presented in Table <ref type="table">4</ref>.</p><p>Before initiating a full-scale TAB screening using GPT API models, we generally recommend thoroughly testing and validating the screening performance of the prompt(s) and GPT API model(s) until it is ensured that the screening performances pass certain thresholds within the given review context. The first step of the testing procedure involves locating approximately 10 relevant and 150 irrelevant titles and abstracts, respectively. Locating more than 10 relevant study records might be ideal to test if the prompt(s) and model(s) can detect various types of relevant records. That said, we experienced that using fewer than 10 relevant records could also unveil a proper recall performance of the prompts and models in more simple screening cases. Thus, we cannot set this step in stone. When locating irrelevant records, we suggest randomly sampling those from the total pool of records, thereby increasing the chances that the specificity test value can be generalized to the full sample of study records.</p><p>After having collected the testing dataset composed of the relevant and irrelevant study records, the next step concerns prompt engineering. A key part of developing well-performing prompts entails making them as concisely written as possible. The models do not need to be trained <ref type="foot" target="#foot_8">13</ref>and should therefore in general only be fed with a minimum of information.</p><p>When conducting complex reviews, prompt engineering gets more complicated. Specifically, we experienced that it can be rather difficult to decipher what exact text part(s) of a prompt makes it yield insufficient performances when adding multiple inclusion criteria to a single prompt.</p><p>To overcome this issue, we suggest that one can draw on the multi-prompt screening strategy, as evaluated in the previous section, where each inclusion criterion is prompted individually. All title and abstract records are then screened with all prompts. It should, thereby, be easier to evaluate what exact inclusion criterion questions and sentences yield (in)sufficient performances. If reviewers opt to use multi-prompt screening for the full screening, the inclusion probability threshold should be tested and decided at this point, meaning that it should be decided how many of the multiple prompts a title and abstract need to be included in to be considered relevant.</p><p>When engineering prompts, we suggest that these should be refined until reaching a recall of .75 and a specificity of at least .8 (cf. the benchmark scheme in Table <ref type="table">2</ref>). Lower specificity values may be accepted as long as the recall exceeds .75. However, if a specificity value of .8 cannot be reached, then the GPT API models should mainly be used to reduce the total number of study records needed to be screened by two independent human screeners. We suggest that if a recall of .75 cannot be reached, then the given GPT API model should not be used as an independent second screener. This can only be accepted if the given reviewer lacks financial resources. In this case, singlescreening is still less desirable than using a low-performing GPT API model as an extra 'pair of eyes'</p><p>to increase the chances of finding all relevant studies. However, the reviewer must be earnest about this shortcoming of the screening, and we do not think this should be accepted in high-quality reviews.</p><p>When the test threshold has been passed, and the reviewers have decided to leverage the GPT API model as a second screener, we suggest that the human reviewers screen all study records before initiating the automated screening. This prevents human reviewers from being impacted by GPT's decisions. An alternative to screening all records at once is to repeat steps 6 to 9 in Table <ref type="table">4</ref> with batches of 500-1000 study records. This would be an adequate way to steer the screening process and to continuously ensure that the given GPT API model performs as expected. Moreover, this reduces the risk of running large screenings that break for some technical reasons, which in turn hinders unnecessary money waste.</p><p>When all study records have been screened by both the human and automated screener, reviewers should investigate and solve disagreements. In this regard, it can be advantageous to rescreen all study records where humans and the automated screener disagreed to test the consistency of the automated screening decision but also to get detailed responses for GPT's decisions. For the latter purpose, we mainly recommend using the GPT-4 model since it provides substantially better descriptions of its screening behavior. If the specificity performance of the GPT screener is high (e.g. &gt; 99%), the reviewers can consider just letting all study records that have been included by either human or GPT enter the full-text screening stage.</p><p>When not to use GPT API models for TAB screening?</p><p>Although we think that GPT API models have the potential to revolutionize TAB screening in systematic reviews, we can envision at least two cases, beyond when the test performance thresholds are not met, where we find this screening approach to be inappropriate. That is, for example, when the complexity of the review question(s) and/or inclusion criteria is high and the number of references needed to be screened is low (e.g., less than 2000). In such a situation, it might take longer to engineer reliable prompts than it would take to instantly initiate duplicate human screening.</p><p>In general, we think that when having few records, it is better merely to let humans double-screen all records because it is more time-efficient relative to engineering well-performing prompts. That said, we experienced that we were able to quickly set up a reliable screening with the FRIENDS data (i.e., Experiment 2). Therefore, if the complexity of a review's inclusion criteria is low, it may be advantageous to conduct a rapid investigation of whether GPT API model screening is appropriate in the specific case, even if the number of records is not high. However, we do not think reviewers should spend too much time on this task in such cases. Table <ref type="table">5</ref> visualizes the conditions under which we consider it adequate vs. inadequate to use GPT API models for TAB screening tasks in systematic reviews.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>Although we have strived to make a comprehensive evaluation of the use of GPT API models for TAB screening tasks, our study has some important limitations. First of all, none of our analyses were pre-registered. However, to at least ensure openness and thereby make it possible to replicate our work, we have shared all data, codes, and material behind the analyses conducted in this study. It should be noted that-even if running the exact same codes as we used in our tests-the screening performances might not be identical to ours since, as illustrated in our analyses, the screening decisions of the GPT API models (like humans) are not 100% consistent across screenings. Yet, we still firmly believe that the overall patterns of our results can be replicated, and we gladly invite readers to test this hypothesis. In future applications, reviewers will be able to set a specific seed (currently a beta argument) to the request body ensuring the reproducibility of the given screening.</p><p>We did not use this functionality since it was not developed at the time when we ran our experiments but we consider it to be a helpful feature for future applications.</p><p>Another clear limitation is that the models we drew on represent black-box and closedsource algorithms. While we have demonstrated that current GPT API models-if configured adequately-are capable of doing TAB screening tasks, we are unable to say why the models work.</p><p>Model dependency is a major issue when working with GPT API models since we do not know how they are trained and/or will develop. This also means that the generalizability across different models and across time is unclear. Consequently, we cannot infer that the results of our experiments are generalizable to other GPT models, such as the GPT-4o and GPT-4-turbo models, and, more so, to other models such as the API models from Claude (ANTHROPIC, 2024) or Mistral AI (2024). From a scientific point of view, and to increase the transparency of GPT API screening, it is, therefore, important that future research revolves around investigating the performances of local, open-source, and downloadable models.</p><p>That said, we do think it is important to note that most human duplicate screenings also represent black box operations that are hardly replicable, and we believe that the GPT API models should be judged in light of this. This is not to say that we should not strive to make screenings replicable and reproducible since this would increase the transparency of high-quality reviews. Yet, we just do not think that the black box argument should be a major reason for abandoning GPT API models for TAB screening in high-quality reviews. Moreover, model sensitivity is the exact reason why we have developed the benchmark scheme so that if the GPT models eventually appear to underperform, this scheme serves as a means to ensure that biased screenings do not enter high-quality reviews.</p><p>On a similar, but technical line, it is rather demanding and time-consuming to keep up with new model developments and updates as well as how they are reached via the API. Model deprecation is a serious threat to the validity of our suggested approach. For now, the GPT-4-0613 model is stable but we expect that this model will eventually deprecate as with previous models. Already, the original function-calling arguments that we used have been deprecated (but can still be used) and moved to the tools argument in the request body. Therefore, future research must evaluate whether our results can be achieved with other GPT API models such as updated models as well as the GPT-4o and GPT-4-turbo, etc. Likewise, the GPT-3.5-turbo-0613 model that we drew upon has expired so one cannot replicate the screenings we made with this model. On this note, we again think, it is pivotal that future research investigates if downloadable GPT models can perform on par with OpenAI's GPT API models. This would secure a more stable applicability of using GPT models for TAB screening, supporting the functionality of this technology.</p><p>Even though the use of GPT API models as second screeners can be considered more efficient than using a human second screener, reviewers should be aware that it still can induce significant costs to one's project, especially when working with GPT-4 models and multi-prompt screenings. To exemplify, we spent approximately $220 making the 12,600 screening requests (2100 references x 6 prompts) for Experiment 3. Although prices have already dropped, 14 we recommend using the models carefully. In some applications, it might be advantageous to combine traditional classifier tools with GPT API models to reduce the total cost. In extreme-size reviews (i.e., &gt; 100,000 references), reviewers could consider combining priority screening/classifier modeling with the GPT API screening. The GPT API screener could, for example, then be used as an extra guardian, checking the performance of one's selected stopping rule <ref type="bibr" target="#b2">(Boetje &amp; van de Schoot, 2024;</ref><ref type="bibr" target="#b10">Campos et al., 2024;</ref><ref type="bibr" target="#b42">K√∂nig et al., 2023)</ref>, either by screening a subsample of, say, 1000 references on the wrong side of the set threshold or by randomly sampling 1000 references from the pool of studies considered to be irrelevant. Then all references on the right side of the threshold of the stopping rule could be screened by at least one human and the GPT screener together. In this respect, we do not think traditional automated tools and GPT API models should be considered competing tools. Instead, they should be used together to overcome each other's disadvantages.</p><p>The screening approach that we suggest is limited by its prompt dependency, meaning that this screening approach is in theory rather sensitive to the prompt(s) made by the user. This can potentially complicate the use of the GPT API screenings as it can be time-consuming to build wellperforming prompts. Reviewers must, therefore, always thoroughly consider whether the use of GPT API screening is resource-efficient in the given review case. A key purpose of our paper is, thus, also to guide reviewers on when GPT API screening might not be appropriate, which would be the case if prompt performances are not on par with human screening (cf. the benchmark scheme in Table <ref type="table">2</ref>), or if the time needed to reach satisfactory performance exceeds the time required for a human second screener to independently screen the titles and abstracts (cf. Table <ref type="table">5</ref>).</p><p>Although we have strived to build a user-friendly setup for GPT API screening in the AIscreenR package, a limitation is that our screening approach is function-based, meaning that reviewers need to have (or at least acquire) some minor R coding skills. In the future, the screening approach may be embedded in a shiny app or in existing screening tools (similar to what has been done with the data extraction GPT API tool in the EPPI-Reviewer (EPPI-Centre, 2024)), thereby making it easier to use without prior R coding skills. A tempting solution to accommodate userfriendliness in the short run is to copy our approach to the ChatGPT web browser interface. However, we have not been able to reach satisfactory screening performances by using the ChatGPT internet interface. Specifically, the GPT API models reached from the 'v1/chat/completions' endpoint worked significantly better relative to the GPT models embedded in the ChatGPT interface. Consequently, future research must be aware of these model deviations, avoiding the performance of different models being mixed up.</p><p>Finally, several caveats should be mentioned regarding the data underlying the benchmark scheme that we have developed for interpreting screener performances in high-quality reviews.</p><p>First of all, it is based on screener performances deduced from a convenience sample of systematic reviews, possibly restricting the generalizability of the estimated average screening performance measures. Even so, we believe that the screening performance measures provide key insights regarding what human screening standards are currently being accepted in high-standard reviews. Although our results indicate that the human screener performances seem to be comparable across distinct disciplines, future research may usefully investigate typical screener performances more systematically and across various research fields within psychology and the social sciences to make even more refined screening guidelines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>Independent human duplicate TAB screening in systematic reviews is time-consuming, requiring a substantial amount of human labor which decelerates the review process and thereby the dissemination of evidence for practice, research, and policy. In this paper, we evaluated the use of OpenAI's GPT API models to conduct title and abstract screening to reduce human labor in systematic reviews and found that GPT API models can function as highly reliable second screeners even in complex review settings, making it possible to substitute one human in the duplicate screening process and reallocate human resources, potentially speeding up the review process.</p><p>Our findings suggest that, when configured correctly, GPT API models can perform on par with or even surpass human screeners with regard to finding relevant studies. We found that the GPT-4 model outperforms the GPT-3.5-turbo model, and we therefore recommend primarily using the GPT-4 model for TAB screening. Moreover, we found that GPT API models can yield specificity values that are on par with humans, but in some applications appear to be slightly over-inclusive (i.e., they yield lower specificity values than typical human screeners). We do, however, not consider this a problem as long as the models obtain high recall values since low specificity values do not induce a bias-they just force human reviewers to double-check a higher number of records.</p><p>Based on our findings, we believe TAB screening with GPT API models can change the way duplicate title and abstract screening is conducted in high-quality systematic reviews, making it possible to replace human second screeners. However, this necessitates a standardized screening approach to make it scalable and acceptable in high-quality reviews. Therefore, we also developed a reproducible workflow and tentative guidelines for when such screenings can and cannot be accepted in high-quality reviews. To increase the user-friendliness of our suggested approach, we developed the AIscreenR R package <ref type="bibr" target="#b67">(Vembye, 2024)</ref>.</p><p>With this paper, we have strived to make a foundation on which evidence organizations  <ref type="bibr">2023)</ref>. These requirements have played a key part in this paper, and we have used them as the main pillars to build the suggested screening framework.</p><p>Concretely, we have aimed to accommodate requirement (a) by building our framework and codes so that they can be remodeled to work with other API models than OpenAI's. This means that our setup aims to be agnostic to the given provider of the given LLM and will be viable as long as reviewers have public access to LLM models. We aimed to support Campbell's requirement (b)</p><p>by developing the new benchmark scheme and by showing that GPT API screening can be appropriate in high-quality reviews, whereas the development of the AIscreenR package and the quality tests hereof were meant to accommodate Campbell's requirement (c). Moreover, we developed our workflow and guidelines to underpin requirements (d) and (e). Requirement (e) is as such not necessary in our case since we are working with pre-trained models. 15 Instead, the performance of the prompt(s) used for screening needs to be tested and compared against human performance measures before credible TAB screening can be initiated. Finally, to fulfill requirement (f), we built the AIscreenR package as open-source software, allowing others (e.g., the Evidence Synthesis Hackathon, Campbell</p><p>Collaboration, the EPPI-Reviewer team, etc.) to contribute to the development and ongoing support of the software.</p><p>Although this study is not without limitations, as mentioned in the previous section, we believe that the implications of this work are rather extensive beyond what we have presented and possibly can imagine. First, using well-functioning automated tools renders the possibility for reviewers not to make unnecessary restrictions on their search string to steer the number of study records, which, in turn, increases the likelihood of finding all or close to all relevant studies for the review in the given databases. Moreover, it makes it possible to screen literature for extreme-sized reviews <ref type="bibr" target="#b61">(Shemilt et al., 2014</ref><ref type="bibr" target="#b60">(Shemilt et al., , 2016</ref>) that would otherwise have been considered unmanageable and/or unremunerative for humans to initiate. Second, this approach can potentially elevate the quality of reviews conducted by single researchers restricted by resources such as limited budgets and/or time. Third,</p><p>we believe that a huge potential exists in combining traditional automated tools and GPT modeling.</p><p>For example, GPT API models could play a key role in validating a decided stopping rule <ref type="bibr" target="#b10">(Campos et al., 2024;</ref><ref type="bibr" target="#b42">K√∂nig et al., 2023)</ref> whereto it could be used to screen records close to the stopping rule on the wrong side, reducing the risk of relevant studies being overlooked. Combining traditional tools and GPT screening could furthermore reduce the cost of using GPT API models since it reduces the number of titles and abstracts needed to be screened by the GPT API models. Fourth, even if reviewers prefer to use duplicate human screening, we think that using a GPT API model as a third screener would be valuable since it can guard against missing relevant studies due to human screener drifting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>To recapitulate, we believe that using GPT API models can radically change duplicate TAB screening in high-quality reviews across all kinds of scientific disciplines. In fact, we envision that the GPT-4 models can perform even more adequately when used on more structured article abstracts as typically found in medicine. We think TAB screening is an ideal use case where artificial intelligence (AI) can meaningfully take on rigid human labor, and where no legal issues arise. Even more edifying, GPT API model screening can ensure a more rapid transfer of usable knowledge to research, practice, and policy, which ultimately underpins the core rationale for doing systematic reviews.      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>(</head><label></label><figDesc>such as Cochrane and Campbell Collaboration) and review journals (such as Psychological Bulletin) can assess and potentially accept the use of TAB screening with GPT API models. According to the Campbell Collaboration, the acceptance of using automation tools in their reviews "requires (a) functioning tech (b) proof that it is functioning appropriately (c) the tech embodied in usable products (d) agreed guidelines for appropriate use (e) training (f) ongoing support." (Campbell Collaboration,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>A:</head><label></label><figDesc>High recall, high specificity B: High recall, low specificity C: Low recall, high specificity D: Low recall, low specificity Note: The blue-colored circles indicate the proportion of relevant title and abstract records; the gray-colored circles represent the proportion of records included by the screener; the white circles represent the proportion of irrelevant records that are correctly excluded by the screener.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Recall and specificity performances</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Performance measures within Campbell Systematic Reviews across assistant vs. author screeners. Dashed lines indicate the average estimated via the SCE+ model.</figDesc><graphic coords="45,88.32,85.20,383.90,624.48" type="vector_box" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Researcher-researcher screening performance measures within NIPH Systematic Reviews.</figDesc><graphic coords="46,56.76,85.08,481.44,219.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Results of the three classifier experiments.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Note that they used the gpt-3.5-turbo-0301 model which has been deprecated and is not longer available at the server.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>It is uncertain what exact model the authors used. We expect it was the gpt-4-0613 API model.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p><ref type="bibr" target="#b63">Syriani et al. (2023)</ref>,<ref type="bibr" target="#b34">Guo et al. (2024), and</ref><ref type="bibr" target="#b31">Gargari et al. (2024)</ref> did all use Python to access the GPT API models. While the former two did not share replication materials,<ref type="bibr" target="#b31">Gargari et al. (2024)</ref> shared their codes, thereby allowing others to replicate their workflow (though, requiring rather advanced Python coding skills).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>In medicine, the number of missed studies may be even higher, especially when relying on student screeners(Ng et al.,   2014).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>In four of the Campbell Systematic Reviews the first level of title and abstract screening has been conducted but the final review has not yet been published. We, therefore, refer to the protocol of these four reviews in Table1.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>We did not use double arcsine transformation<ref type="bibr" target="#b19">(Doi &amp; Xu, 2021)</ref> due to the inadequate properties of the back transformation of this measure<ref type="bibr" target="#b56">(R√∂ver &amp; Friede, 2022;</ref><ref type="bibr" target="#b58">Schwarzer et al., 2019)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>We conducted this experiment on the 4 th of November 2023. This was before the corresponding protocol was published on the 15 th of December 2023.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>Testing can be used as a formative tool, e.g., to promote retention of academic content, adjust instructional strategies, and uncover student needs for remediation or more intensive support. In most school systems, testing is also used summatively for assigning grades, determining graduation or certification, and for school accountability assessment.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_8"><p>A new feature has been developed p√• OpenAI that allows user to fine-tune/train model to do specific task(OpenAI,  2024b). Therefore, future application might potentially involve model traning as well.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_9"><p>For now, it might e.g. be beneficial for researchers to investigate the performance of GPT-4o-mini, GPT-4o, or GPT-4-turbo since these models are significantly cheaper than the GPT-4 model we used. Moreover, OpenAI has developed batch APIs, allowing the user to lower the cost by 50% if they can wait up till 24 hours to get answers to its requests.</p></note>
		</body>
		<back>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>//osf.io/apdfw/ and GitHub https://github.com/MikkelVembye/screen_benchmarks. The AIscreenR R package presented in the article can be assessed at https://mikkelvembye.github.io/AIscreenR/. A preprint of this article is available at https://osf.io/preprints/osf/yrhzm.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>On par with common human screening performance</head><p>Note: Red areas indicate conditions under which the TAB screening performance is unacceptability low. Gray areas represent insufficient performance conditions but some applications with these performance measures might still be viable.</p><p>Green areas represent acceptable screening performances on par with or better than human screening. records without an abstract and human screen those references. 9 Investigate and solve disagreements between the human and automated screening decisions.</p><p>Note: For a detailed presentation of how, in practice, to conduct TAB screening using GPT API models, see the vignette accompanying the AIscreenR package <ref type="bibr" target="#b67">(Vembye, 2024)</ref>. programme which can be used as both prevention and treatment of child and youth anxiety. The study should focus exclusively on this topic and we are exclusively searching for studies with a treatment and a comparison group.</p><p>For each study, I would like you to assess: 1) Is the study about the FRIENDS preventive programme? 2) Is the study estimating an effect between a treatment and control/comparison group?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Short background information</head><p>Inclusion criteria</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Harnessing the power of ChatGPT for automating systematic review process: Methodology, case study, limitations, and future directions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Alshami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E E</forename><surname>Eltoukhy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zayed</surname></persName>
		</author>
		<idno type="DOI">10.3390/systems11070351</idno>
		<ptr target="https://doi.org/10.3390/systems11070351" />
	</analytic>
	<monogr>
		<title level="j">Systems</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">351</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Acceptability, values, and preferences of older people for chronic low back pain management; a qualitative evidence synthesis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ames</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Hestevik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Briggs</surname></persName>
		</author>
		<idno type="DOI">10.1186/s12877-023-04608-4ANTHROPIC</idno>
		<ptr target="https://claude.ai/new" />
	</analytic>
	<monogr>
		<title level="j">BMC Geriatrics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2024">2024. 2024</date>
			<pubPlace>Claude 2.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The SAFE procedure: a practical stopping heuristic for active learning-based screening in systematic reviews and meta-analyses</title>
		<author>
			<persName><forename type="first">J</forename><surname>Boetje</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van De Schoot</surname></persName>
		</author>
		<idno type="DOI">10.1186/s13643-024-02502-7</idno>
		<ptr target="https://doi.org/10.1186/s13643-024-02502-7" />
	</analytic>
	<monogr>
		<title level="j">Systematic Reviews</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">81</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deployment of personnel to military operations: impact on mental health and social functioning</title>
		<author>
			<persName><forename type="first">M</forename><surname>B√∏g</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Filges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>J√∏rgensen</surname></persName>
		</author>
		<idno type="DOI">10.4073/csr.2018.6</idno>
		<ptr target="https://doi.org/10.4073/csr.2018.6" />
	</analytic>
	<monogr>
		<title level="j">Campbell Systematic Reviews</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="127" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The effects of small class sizes on students&apos; academic achievement, socioemotional development and well-being in special education: A systematic review</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bondebjerg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">T</forename><surname>Dalgaard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Filges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C A</forename><surname>Viinholt</surname></persName>
		</author>
		<idno type="DOI">10.1002/cl2.1345</idno>
		<ptr target="https://doi.org/10.1002/cl2.1345" />
	</analytic>
	<monogr>
		<title level="j">Campbell Systematic Reviews</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">1345</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Growth rates of modern science: a latent piecewise growth curve approach to model publication numbers from established and new literature databases</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bornmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Haunschild</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mutz</surname></persName>
		</author>
		<idno type="DOI">10.1057/s41599-021-00903-w</idno>
		<ptr target="https://doi.org/10.1057/s41599-021-00903-w" />
	</analytic>
	<monogr>
		<title level="j">Humanities and Social Sciences Communications</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Finding relevant studies</title>
		<author>
			<persName><forename type="first">J</forename><surname>Brunton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stansfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Caird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">An introduction to systematic reviews</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Gough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Oliver</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">&amp;</forename><forename type="middle">J</forename><surname>Thomas</surname></persName>
		</editor>
		<imprint>
			<publisher>Sage</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="93" to="122" />
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Reducing literature screening workload with machine learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bittermann</surname></persName>
		</author>
		<idno type="DOI">10.1027/2151-2604/a000509</idno>
		<ptr target="https://doi.org/10.1027/2151-2604/a000509" />
	</analytic>
	<monogr>
		<title level="j">Zeitschrift F√ºr Psychologie</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Single data extraction generated more errors than double data extraction in systematic reviews</title>
		<author>
			<persName><forename type="first">N</forename><surname>Buscemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hartling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Vandermeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tjosvold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Klassen</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jclinepi.2005.11.010</idno>
		<ptr target="https://doi.org/10.1016/j.jclinepi.2005.11.010" />
	</analytic>
	<monogr>
		<title level="j">Journal of Clinical Epidemiology</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="697" to="703" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Stepping up evidence synthesis: faster, cheaper and more useful</title>
		<author>
			<orgName type="collaboration">Campbell Collaboration.</orgName>
		</author>
		<ptr target="https://www.campbellcollaboration.org/news-and-events/news/stepping-up-evidence-synthesis.html" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Screening smarter, not harder: A comparative analysis of machine learning screening algorithms and heuristic stopping criteria for systematic reviews in educational research</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>F√ºtterer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gfr√∂rer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Lavelle-Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murayama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>K√∂nig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zitzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scherer</surname></persName>
		</author>
		<idno>10648- 024-09862-5</idno>
		<ptr target="https://doi.org/10.1007/s" />
	</analytic>
	<monogr>
		<title level="j">Educational Psychology Review</title>
		<imprint>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The Matthews correlation coefficient (MCC) should replace the ROC AUC as the standard metric for assessing binary classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chicco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jurman</surname></persName>
		</author>
		<idno type="DOI">10.1186/s13040-023-00322-4</idno>
		<ptr target="https://doi.org/10.1186/s13040-023-00322-4" />
	</analytic>
	<monogr>
		<title level="j">BioData Mining</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reducing workload in systematic review preparation using automated citation classification</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-Y</forename><surname>Yen</surname></persName>
		</author>
		<idno type="DOI">10.1197/jamia.M1929</idno>
		<ptr target="https://doi.org/10.1197/jamia.M1929" />
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="206" to="219" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adult/child ratio and group size in early childhood education or care to promote the development of children aged 0-5 years: A systematic review</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">T</forename><surname>Dalgaard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bondebjerg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Klokker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C A</forename><surname>Viinholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dietrichson</surname></persName>
		</author>
		<idno type="DOI">10.1002/cl2.1239</idno>
		<ptr target="https://doi.org/10.1002/cl2.1239" />
	</analytic>
	<monogr>
		<title level="j">Campbell Systematic Reviews</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1239</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The effects of inclusion on academic achievement, socioemotional development and wellbeing of children with special educational needs</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">T</forename><surname>Dalgaard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bondebjerg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C A</forename><surname>Viinholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Filges</surname></persName>
		</author>
		<idno type="DOI">10.1002/cl2.1291</idno>
		<ptr target="https://doi.org/10.1002/cl2.1291" />
	</analytic>
	<monogr>
		<title level="j">Campbell Systematic Reviews</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1291</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Parenting interventions to support parent/child attachment and psychosocial adjustment in foster and adoptive parents and children: A systematic review</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">T</forename><surname>Dalgaard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Filges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C A</forename><surname>Viinholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pontoppidan</surname></persName>
		</author>
		<idno type="DOI">10.1002/cl2.1209</idno>
		<ptr target="https://doi.org/10.1002/cl2.1209" />
	</analytic>
	<monogr>
		<title level="j">Campbell Systematic Reviews</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1209</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">PROTOCOL: Group-based community interventions to support the social reintegration of marginalised adults with mental illness</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">T</forename><surname>Dalgaard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Flensborg Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bengtsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">F</forename><surname>Krassel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Vembye</surname></persName>
		</author>
		<idno type="DOI">10.1002/cl2.1254</idno>
		<ptr target="https://doi.org/10.1002/cl2.1254" />
	</analytic>
	<monogr>
		<title level="j">Campbell Systematic Reviews</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">1254</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Targeted school-based interventions for improving reading and mathematics for students with, or at risk of, academic difficulties in Grades 7-12: A systematic review</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dietrichson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Filges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Klokker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C A</forename><surname>Viinholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>B√∏g</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">H</forename><surname>Jensen</surname></persName>
		</author>
		<idno type="DOI">10.1002/cl2.1081</idno>
		<ptr target="https://doi.org/10.1002/cl2.1081" />
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Campbell Systematic Reviews</publisher>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">1081</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Targeted school-based interventions for improving reading and mathematics for students with or at risk of academic difficulties in Grades K-6: A systematic review</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dietrichson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Filges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Seerup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Klokker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C A</forename><surname>Viinholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>B√∏g</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Eiberg</surname></persName>
		</author>
		<idno type="DOI">10.1002/cl2.1152</idno>
		<ptr target="https://doi.org/10.1002/cl2.1152" />
	</analytic>
	<monogr>
		<title level="j">Campbell Systematic Reviews</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1152</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The Freeman-Tukey double arcsine transformation for the metaanalysis of proportions: Recent criticisms were seriously misleading</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Doi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1111/jebm.12445</idno>
		<ptr target="https://doi.org/10.1111/jebm.12445" />
	</analytic>
	<monogr>
		<title level="j">Journal of Evidence-Based Medicine</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="259" to="261" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Automated data extraction using GPT</title>
		<author>
			<persName><surname>Eppi-Centre</surname></persName>
		</author>
		<ptr target="https://eppi.ioe.ac.uk/cms/Default.aspx?tabid=3921" />
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Sutur av degenerative rotatorcuff-rupturer: en fullstendig metodevurdering [Rotator cuff repair for degenerative rotator cuff tears: a health technology assessment</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Evensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kleven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Dahm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">V</forename><surname>Hafstad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Holte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Robberstad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Risstad</surname></persName>
		</author>
		<ptr target="https://www.fhi.no/publ/2023/sutur-av-degenerative-rotatorcuff-rupturer/" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Functional Family Therapy (FFT) for young people in treatment for non-opioid drug use: A systematic review</title>
		<author>
			<persName><forename type="first">T</forename><surname>Filges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-M</forename><forename type="middle">K</forename><surname>J√∏rgensen</surname></persName>
		</author>
		<idno type="DOI">10.4073/csr.2015.14</idno>
		<ptr target="https://doi.org/10.4073/csr.2015.14" />
	</analytic>
	<monogr>
		<title level="j">Campbell Systematic Reviews</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="77" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Outreach programs to improve life circumstances and prevent further adverse developmental trajectories of at-risk youth in OECD countries: A systematic review</title>
		<author>
			<persName><forename type="first">T</forename><surname>Filges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">T</forename><surname>Dalgaard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C A</forename><surname>Viinholt</surname></persName>
		</author>
		<idno type="DOI">10.1002/cl2.1282</idno>
		<ptr target="https://doi.org/10.1002/cl2.1282" />
	</analytic>
	<monogr>
		<title level="j">Campbell Systematic Reviews</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1282</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Service learning for improving academic success in students in grade K to 12: A systematic review</title>
		<author>
			<persName><forename type="first">T</forename><surname>Filges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dietrichson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C A</forename><surname>Viinholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">T</forename><surname>Dalgaard</surname></persName>
		</author>
		<idno type="DOI">10.1002/cl2.1210</idno>
		<ptr target="https://doi.org/10.1002/cl2.1210" />
	</analytic>
	<monogr>
		<title level="j">Campbell Systematic Reviews</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1210</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The impact of detention on the health of asylum seekers: A systematic review</title>
		<author>
			<persName><forename type="first">T</forename><surname>Filges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Montgomery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kastrup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-M</forename><forename type="middle">K</forename><surname>J√∏rgensen</surname></persName>
		</author>
		<idno type="DOI">10.4073/csr.2015.13</idno>
		<ptr target="https://doi.org/10.4073/csr.2015.13" />
	</analytic>
	<monogr>
		<title level="j">Campbell Systematic Reviews</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="104" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Voluntary work for the physical and mental health of older volunteers: A systematic review</title>
		<author>
			<persName><forename type="first">T</forename><surname>Filges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Siren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fridberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C V</forename><surname>Nielsen</surname></persName>
		</author>
		<idno type="DOI">10.1002/cl2.1124</idno>
		<ptr target="https://doi.org/10.1002/cl2.1124" />
	</analytic>
	<monogr>
		<title level="j">Campbell Systematic Reviews</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1124</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">PROTOCOL: The FRIENDS preventive programme for reducing anxiety symptoms in children and adolescents: A systematic review</title>
		<author>
			<persName><forename type="first">T</forename><surname>Filges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Smedslund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eriksen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Birkefoss</surname></persName>
		</author>
		<idno type="DOI">10.1002/cl2.1374</idno>
		<ptr target="https://doi.org/10.1002/cl2.1374" />
	</analytic>
	<monogr>
		<title level="j">Campbell Systematic Reviews</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1374</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Small class sizes for improving student achievement in primary and secondary schools: A systematic review</title>
		<author>
			<persName><forename type="first">T</forename><surname>Filges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Sonne-Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Nielsen</surname></persName>
		</author>
		<idno type="DOI">10.4073/csr.2018.10</idno>
		<ptr target="https://doi.org/10.4073/csr.2018.10" />
	</analytic>
	<monogr>
		<title level="j">Campbell Systematic Reviews</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="107" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Effectiveness of continuing professional development training of welfare professionals on outcomes for children and young people: A systematic review</title>
		<author>
			<persName><forename type="first">T</forename><surname>Filges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Torgerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gascoine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dietrichson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Viinholt</surname></persName>
		</author>
		<idno type="DOI">10.1002/cl2.1060</idno>
		<ptr target="https://doi.org/10.1002/cl2.1060" />
	</analytic>
	<monogr>
		<title level="j">Campbell Systematic Reviews</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1060</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">PROTOCOL: Participation in organised sport to improve and prevent adverse developmental trajectories of at-risk youth: A systematic review</title>
		<author>
			<persName><forename type="first">T</forename><surname>Filges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Verner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ladekjaer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bengtsen</surname></persName>
		</author>
		<idno type="DOI">10.1002/cl2.1321</idno>
		<ptr target="https://doi.org/https://doi.org/10.1002/cl2.1321" />
	</analytic>
	<monogr>
		<title level="j">Campbell Systematic Reviews</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1321</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Enhancing title and abstract screening for systematic reviews with GPT-3.5 turbo</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">K</forename><surname>Gargari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahmoudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hajisafarali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Samiee</surname></persName>
		</author>
		<idno type="DOI">10.1136/bmjebm-2023-112678</idno>
		<ptr target="https://doi.org/10.1136/bmjebm-2023-112678" />
	</analytic>
	<monogr>
		<title level="j">BMJ Evidence-Based Medicine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="70" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Assessing the accuracy of machine-assisted abstract screening with DistillerAI: a user study</title>
		<author>
			<persName><forename type="first">G</forename><surname>Gartlehner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Affengruber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dobrescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kaminski-Hartenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Viswanathan</surname></persName>
		</author>
		<idno type="DOI">10.1186/s13643-019-1221-3</idno>
		<ptr target="https://doi.org/10.1186/s13643-019-1221-3" />
	</analytic>
	<monogr>
		<title level="j">Systematic Reviews</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">277</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Gough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thomas</surname></persName>
		</author>
		<title level="m">An introduction to systematic reviews</title>
		<imprint>
			<publisher>Sage</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Automated paper screening for clinical reviews using large language models: Data analysis study</title>
		<author>
			<persName><forename type="first">E</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Naugler</surname></persName>
		</author>
		<idno type="DOI">10.2196/48996</idno>
		<ptr target="https://doi.org/10.2196/48996" />
	</analytic>
	<monogr>
		<title level="j">J Med Internet Res</title>
		<imprint>
			<biblScope unit="page" from="26" to="e48996" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Modeling publication selection effects in meta-analysis</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Hedges</surname></persName>
		</author>
		<ptr target="http://www.jstor.org/stable/2246311" />
	</analytic>
	<monogr>
		<title level="j">Statistical Science</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="246" to="255" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P T</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Cumpston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Welch</surname></persName>
		</author>
		<idno type="DOI">10.1002/9781119536604</idno>
		<ptr target="https://doi.org/10.1002/9781119536604" />
		<title level="m">Cochrane handbook for systematic reviews of interventions</title>
		<meeting><address><addrLine>Online Library</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Enhancing recall in automated record screening: A resampling algorithm</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tipton</surname></persName>
		</author>
		<idno type="DOI">10.1002/jrsm.1690</idno>
		<ptr target="https://doi.org/10.1002/jrsm.1690" />
	</analytic>
	<monogr>
		<title level="j">Research Synthesis Methods</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Methodological insights into ChatGPT&apos;s screening performance in systematic reviews</title>
		<author>
			<persName><forename type="first">M</forename><surname>Issaiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ghanaati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kolahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shakiba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Jalali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zarei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kazemian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Avanaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Firouznia</surname></persName>
		</author>
		<idno type="DOI">10.1186/s12874-024-02203-8</idno>
		<ptr target="https://doi.org/10.1186/s12874-024-02203-8" />
	</analytic>
	<monogr>
		<title level="j">BMC Medical Research Methodology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">78</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Effekten av antipsykotika ved f√∏rstegangspsykose: en systematisk oversikt [The effect of antipsychotics on first episode psychosis</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S J</forename><surname>Jardim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Borge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Johansen</surname></persName>
		</author>
		<ptr target="https://fhi.no/publ/2021/effekten-av-antipsykotika-ved-forstegangspsykose/" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Samvaers-og bostedsordninger etter samlivsbrudd: betydninger for barn og unge: en systematisk oversikt [Custody and living arrangements after parents separate: implications for children and adolescents: a systematic review</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>N√∏kleby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Lang√∏ien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Borge</surname></persName>
		</author>
		<ptr target="https://www.fhi.no/publ/2022/samvars--og-bostedsordninger-etter-samlivsbrudd-betydninger-for-barn-og-ung/" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Can large language models replace humans in systematic reviews? Evaluating GPT-4&apos;s efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Khraisha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Put</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kappenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Warraitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hadfield</surname></persName>
		</author>
		<idno type="DOI">10.1002/jrsm.1715</idno>
		<ptr target="https://doi.org/10.1002/jrsm.1715" />
	</analytic>
	<monogr>
		<title level="j">Research Synthesis Methods</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">When to stop and what to expect-An evaluation of the performance of stopping rules in AI-assisted reviewing for psychological meta-analytical research</title>
		<author>
			<persName><forename type="first">L</forename><surname>K√∂nig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zitzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>F√ºtterer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hecht</surname></persName>
		</author>
		<idno type="DOI">10.31234/osf.io/ybu3w</idno>
		<ptr target="https://doi.org/10.31234/osf.io/ybu3w" />
	</analytic>
	<monogr>
		<title level="j">Open Science Framework</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Searching for studies: A guide to information retrieval for Campbell</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kugley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Mahood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-M</forename><forename type="middle">K</forename><surname>J√∏rgensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hammerstr√∏m</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sathe</surname></persName>
		</author>
		<idno type="DOI">10.4073/cmg.2016.1</idno>
		<ptr target="https://doi.org/10.4073/cmg.2016.1" />
	</analytic>
	<monogr>
		<title level="j">Campbell Systematic Reviews</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="73" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Psykologisk debriefing for helsepersonell involvert i u√∏nskede pasienthendelser: en systematisk oversikt [Psychological debriefing for healthcare professionals involved in adverse events: a systematic review</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Meneses Echavez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Borge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Nyg√•rd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-V</forename><surname>Gaustad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Huckvale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Clavisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gruen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename></persName>
		</author>
		<idno type="DOI">10.1186/2046-4053-3-121</idno>
		<ptr target="https://doi.org/10.1186/2046-4053-3-121" />
	</analytic>
	<monogr>
		<title level="j">Systematic Reviews</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2014">2022. 2024. 2014</date>
			<pubPlace>La Plateforme</pubPlace>
		</imprint>
	</monogr>
	<note>Title and Abstract Screening and Evaluation in Systematic Reviews (TASER): A pilot randomised controlled trial of title and abstract screening by medical students</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A question of trust: can we build an evidence base to gain trust in systematic review automation technologies?</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>O'connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tsafnat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Glasziou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hutton</surname></persName>
		</author>
		<idno type="DOI">10.1186/s13643-019-1062-0</idno>
		<ptr target="https://doi.org/10.1186/s13643-019-1062-0" />
	</analytic>
	<monogr>
		<title level="j">Systematic Reviews</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Using text mining for study identification in systematic reviews: a systematic review of current approaches</title>
		<author>
			<persName><forename type="first">A</forename><surname>O'mara-Eves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcnaught</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ananiadou</surname></persName>
		</author>
		<idno type="DOI">10.1186/2046-4053-4-5</idno>
		<ptr target="https://doi.org/10.1186/2046-4053-4-5" />
	</analytic>
	<monogr>
		<title level="j">Systematic Reviews</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A critical analysis of studies that address the use of text mining for citation screening in systematic reviews</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Olorisade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>De Quincey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brereton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Andras</surname></persName>
		</author>
		<ptr target="https://platform.openai.com/docs/guides/function-calling" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Evaluation and Assessment in Software Engineering</title>
		<meeting>the 20th International Conference on Evaluation and Assessment in Software Engineering</meeting>
		<imprint>
			<date type="published" when="2016">2016. 2024b</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
	<note>Fine-tuning. Function calling</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Many systematic reviews with a single author are indexed in PubMed</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Pacheco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Riera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M M</forename><surname>S√°</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G P</forename><surname>Bomfim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Da Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>De Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L C</forename><surname>Martimbianco</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jclinepi.2023.01.007</idno>
		<ptr target="https://doi.org/10.1016/j.jclinepi.2023.01.007" />
	</analytic>
	<monogr>
		<title level="j">Journal of Clinical Epidemiology</title>
		<imprint>
			<biblScope unit="volume">156</biblScope>
			<biblScope unit="page" from="124" to="126" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A real-world evaluation of the implementation of NLP technology in abstract screening of a systematic review</title>
		<author>
			<persName><forename type="first">S</forename><surname>Perlman-Arrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Loo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bobrovitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Arora</surname></persName>
		</author>
		<idno type="DOI">10.1002/jrsm.1636</idno>
		<ptr target="https://doi.org/10.1002/jrsm.1636" />
	</analytic>
	<monogr>
		<title level="j">Research Synthesis Methods</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="608" to="621" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Best practice guidelines for abstract screening large-evidence systematic reviews and meta-analyses</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Polanin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Pigott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Espelage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Grotpeter</surname></persName>
		</author>
		<idno type="DOI">10.1002/jrsm.1354</idno>
		<ptr target="https://doi.org/10.1002/jrsm.1354" />
	</analytic>
	<monogr>
		<title level="j">Research Synthesis Methods</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="330" to="342" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">clubSandwich: Cluster-robust (sandwich) variance estimators with smallsample corrections (0.5.5). cran.r-project</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Pustejovsky</surname></persName>
		</author>
		<ptr target="https://cran.r-project.org/web/packages/clubSandwich/index.html" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Meta-analysis with robust variance estimation: Expanding the range of working models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Pustejovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tipton</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11121-021-01246-3</idno>
		<ptr target="https://doi.org/10.1007/s11121-021-01246-3" />
	</analytic>
	<monogr>
		<title level="j">Prevention Science</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="425" to="438" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">R: A language and environment for statistical computing</title>
		<author>
			<orgName type="collaboration">R Core Team.</orgName>
		</author>
		<ptr target="https://www.r-project.org/" />
	</analytic>
	<monogr>
		<title level="m">R Foundation for Statistical Computing</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Faster title and abstract screening? Evaluating Abstrackr, a semi-automated online screening program for systematic reviewers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rathbone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Glasziou</surname></persName>
		</author>
		<idno type="DOI">10.1186/s13643-015-0067-6</idno>
		<ptr target="https://doi.org/10.1186/s13643-015-0067-6" />
	</analytic>
	<monogr>
		<title level="j">Systematic Reviews</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Publication bias in meta-analysis</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Rothstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Borenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Publication bias in meta-analysis: Prevention, assessment and adjustments</title>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Rothstein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Sutton</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Borenstein</surname></persName>
		</editor>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Double arcsine transform not appropriate for meta-analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>R√∂ver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Friede</surname></persName>
		</author>
		<idno type="DOI">10.1002/jrsm.1591</idno>
		<ptr target="https://doi.org/10.1002/jrsm.1591" />
	</analytic>
	<monogr>
		<title level="j">Research Synthesis Methods</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="645" to="648" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">RStudio: Integrated development for R</title>
		<author>
			<persName><forename type="first">Rstudio</forename><surname>Team</surname></persName>
		</author>
		<ptr target="https://www.rstudio.com/" />
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>RStudio, Inc</publisher>
			<pubPlace>Boston, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Seriously misleading results using inverse of Freeman-Tukey double arcsine transformation in meta-analysis of single proportions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Schwarzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chemaitelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Abu-Raddad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>R√ºcker</surname></persName>
		</author>
		<idno type="DOI">10.1002/jrsm.1348</idno>
		<ptr target="https://doi.org/10.1002/jrsm.1348" />
	</analytic>
	<monogr>
		<title level="j">Research Synthesis Methods</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="476" to="483" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Shadish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Campbell</surname></persName>
		</author>
		<title level="m">Experimental and Quasi-Experimental Designs for Generalized Causal Inference</title>
		<imprint>
			<publisher>Cengage Learning, Inc</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Use of cost-effectiveness analysis to compare the efficiency of study identification methods in systematic reviews</title>
		<author>
			<persName><forename type="first">I</forename><surname>Shemilt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thomas</surname></persName>
		</author>
		<idno type="DOI">10.1186/s13643-016-0315-4</idno>
		<ptr target="https://doi.org/10.1186/s13643-016-0315-4" />
	</analytic>
	<monogr>
		<title level="j">Systematic Reviews</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Pinpointing needles in giant haystacks: use of text mining to reduce impractical screening workload in extremely large scoping reviews</title>
		<author>
			<persName><forename type="first">I</forename><surname>Shemilt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Hollands</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Marteau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ogilvie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>O'mara-Eves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thomas</surname></persName>
		</author>
		<idno type="DOI">10.1002/jrsm.1093</idno>
		<ptr target="https://doi.org/10.1002/jrsm.1093" />
	</analytic>
	<monogr>
		<title level="j">Research Synthesis Methods</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="49" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">The value of a second reviewer for study selection in systematic reviews</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R T</forename><surname>Stoll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fowler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Suls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Colditz</surname></persName>
		</author>
		<idno type="DOI">10.1002/jrsm.1369</idno>
		<ptr target="https://doi.org/10.1002/jrsm.1369" />
	</analytic>
	<monogr>
		<title level="j">Research Synthesis Methods</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="539" to="545" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Assessing the ability of ChatGPT to screen articles for systematic reviews</title>
		<author>
			<persName><forename type="first">E</forename><surname>Syriani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kumar</surname></persName>
		</author>
		<idno>ArXiv:2307.06464</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">PROTOCOL: Testing frequency and student achievement: A systematic review</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Thomsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Seerup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dietrichson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bondebjerg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C A</forename><surname>Viinholt</surname></persName>
		</author>
		<idno type="DOI">10.1002/cl2.1212</idno>
		<ptr target="https://doi.org/10.1002/cl2.1212" />
	</analytic>
	<monogr>
		<title level="j">Campbell Systematic Reviews</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1212</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Small-sample adjustments for tests of moderators and model fit using robust variance estimation in meta-regression</title>
		<author>
			<persName><forename type="first">E</forename><surname>Tipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Pustejovsky</surname></persName>
		</author>
		<idno type="DOI">10.3102/1076998615606099</idno>
		<ptr target="https://doi.org/10.3102/1076998615606099" />
	</analytic>
	<monogr>
		<title level="j">Journal of Educational and Behavioral Statistics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="604" to="634" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">An open source machine learning framework for efficient and transparent systematic reviews</title>
		<author>
			<persName><forename type="first">R</forename><surname>Van De Schoot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>De Bruin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zahedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>De Boer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Weijdema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huijts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hoogerwerf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ferdinands</surname></persName>
		</author>
		<idno type="DOI">10.1038/s42256-020-00287-7</idno>
		<ptr target="https://doi.org/10.1038/s42256-020-00287-7" />
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="125" to="133" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">AIscreenR: AI screening tools for systematic reviews</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Vembye</surname></persName>
		</author>
		<ptr target="https://mikkelvembye.github.io/AIscreenR/" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>GitHub version 0.0.0.9999</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Conducting meta-analyses in R with the metafor package</title>
		<author>
			<persName><forename type="first">W</forename><surname>Viechtbauer</surname></persName>
		</author>
		<idno type="DOI">10.18637/jss.v036.i03</idno>
		<ptr target="https://doi.org/10.18637/jss.v036.i03" />
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="48" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">W</forename><surname>Viechtbauer</surname></persName>
		</author>
		<ptr target="https://www.metafor-project.org/doku.php/analyses:miller1978?s" />
		<imprint>
			<date type="published" when="1978">2022. 1978</date>
			<publisher>Miller</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Single screening versus conventional double screening for study selection in systematic reviews: a methodological systematic review</title>
		<author>
			<persName><forename type="first">S</forename><surname>Waffenschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Knelangen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sieben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>B√ºhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pieper</surname></persName>
		</author>
		<idno type="DOI">10.1186/s12874-019-0782-0</idno>
		<ptr target="https://doi.org/10.1186/s12874-019-0782-0" />
	</analytic>
	<monogr>
		<title level="j">BMC Medical Research Methodology</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">132</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Error rates of human reviewers during abstract screening in systematic reviews</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nayfeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tetzlaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>O'blenis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Murad</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0227742</idno>
		<ptr target="https://doi.org/10.1371/journal.pone.0227742" />
	</analytic>
	<monogr>
		<title level="j">PloS One</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">227742</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">revtools: An R package to support article screening for evidence synthesis</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Westgate</surname></persName>
		</author>
		<idno type="DOI">10.1002/jrsm.1374</idno>
		<ptr target="https://doi.org/10.1002/jrsm.1374" />
	</analytic>
	<monogr>
		<title level="j">Research Synthesis Methods</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="606" to="614" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">ggplot2: Elegant graphics for data analysis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wickham</surname></persName>
		</author>
		<ptr target="https://cran.r-project.org/web/packages/ggplot2/index.html" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Testing (quizzing) boosts classroom learning: A systematic and meta-analytic review</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Vadillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Shanks</surname></persName>
		</author>
		<idno type="DOI">10.1037/bul0000309</idno>
		<ptr target="https://doi.org/10.1037/bul0000309" />
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">147</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="399" to="435" />
			<date type="published" when="2021">2021</date>
			<publisher>American Psychological Association</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
