<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Studying attention to IPCC climate change maps with mobile eyetracking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Doga</forename><surname>Gulhan</surname></persName>
							<email>dogagulhan@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution" key="instit1">Royal Holloway</orgName>
								<orgName type="institution" key="instit2">University of London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Faculty of General Psychology and Education</orgName>
								<orgName type="institution">Ludwig Maximilian University of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bahador</forename><surname>Bahrami</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Faculty of General Psychology and Education</orgName>
								<orgName type="institution">Ludwig Maximilian University of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ophelia</forename><surname>Deroy</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Faculty of Philosophy</orgName>
								<orgName type="department" key="dep2">Philosophy of Science and the Study of Religion</orgName>
								<orgName type="institution">Ludwig Maximilian University of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Studying attention to IPCC climate change maps with mobile eyetracking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">96B2324E46D4A1A3B94CB0EC9EBB6DF1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-10-22T06:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Climate communication</term>
					<term>IPCC maps</term>
					<term>mobile eye-tracking</term>
					<term>visual attention</term>
					<term>social viewing</term>
					<term>aesthetic perception</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many visualisations used in the climate communication field aim to present the scientific models of climate change to the public. However, relatively little research has been conducted on how such data are visually processed, particularly from a behavioural science perspective.</p><p>This study examines trends in visual attention to climate change predictions in world maps using mobile eye-tracking while participants engage with the visualisations. Our primary aim is to assess engagement with the maps, as indicated by gaze metrics. Secondary analyses assess whether social context (as social viewing compared to solitary viewing) affects these trends, the relationship between projection types and visual attention, compare gaze metrics between scientific map and artwork viewing, and explore correlations between self-reported climate anxiety scores and attention patterns. We employed wearable, head-mounted eye-tracking to collect data in relatively naturalistic conditions, aiming to enhance ecological validity. In this research, participants engaged with ten world maps displaying near-and far-term climate projections across five data categories, adapted from the online interactive atlas provided by the International Panel on Climate Change (IPCC). To compare scientific information processing with aesthetic perception, participants also viewed two large-scale artworks. Responses to the Climate Change Anxiety Scale (CCAS) were also collected. Participants viewed the displays alone (single-viewing condition, N=35) or together with a partner (paired-viewing condition, N=12). Results revealed that the upper parts of the maps, particularly the continental Europe, received significant attention, suggesting a Euro-centric bias in viewing patterns. Spatial gaze patterns were similar between single and paired conditions, indicating that the visual attributes of the maps predominantly shaped attention locations. Although dwell times were comparable, the paired condition showed higher fixation counts, shorter average fixation durations, and longer scanpaths, suggesting a potentially dissociable viewing strategy and more exploratory viewing patterns influenced by social interaction. No substantial differences were observed in attention across projection timeframes or types, although individual variations were noted.</p><p>Artwork viewing exhibited notably shorter average fixation durations compared to climate map viewing, potentially reflecting different visual engagement styles. Despite positive linear correlations among the four CCAS subscales, there was no apparent correlation between CCAS scores and main gaze metrics, indicating a lack of a direct relationship between self-reported anxiety and gaze behaviour. In summary, visual attention to climate change visualisations appears to be mainly influenced by the inherent visual attributes of the maps, but the social context may subtly influence visual attention. Additionally, the comparison with aesthetic viewing highlights relatively distinct attentional patterns in scientific versus aesthetic engagements.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Introduction</head><p>Most of us have seen maps of the earth, often coloured in shades of red and dark orange, illustrating the predicted rise in temperature or rainfalls over the coming decades. But beyond the broad message that we probably already knew, what did we pay attention to? Maps are a major instrument for reporting and communicating climate change to journalists, politicians, and the wider public <ref type="bibr">[1]</ref>. They can convey a rich wealth of the spatial, temporal, and quantitative information agreed upon by the community in a simpler and more vivid manner <ref type="bibr">[2]</ref>. Maps make up more than 25% of the visualisations in the annual reports of the International Panel on Climate Change (IPCC). These often global representations are widely circulated and used for further decisions and communication campaigns. The IPCC has invested efforts into building better maps and visualisations <ref type="bibr">[3]</ref>, although suggestions from researchers for more solution-oriented framing <ref type="bibr">[4]</ref> remain valid criticism. Nevertheless, intuitions of experts on what constitutes good design in data visualisation do not always materialise, making it important to test data visualisations with the public empirically.</p><p>While map-viewing in general is being explored in various contexts through eyetracking, aiming to answer a wide range of research questions <ref type="bibr">[5]</ref><ref type="bibr">[6]</ref><ref type="bibr">[7]</ref><ref type="bibr">[8]</ref><ref type="bibr" target="#b9">[9]</ref><ref type="bibr">[10]</ref><ref type="bibr" target="#b11">[11]</ref><ref type="bibr" target="#b12">[12]</ref>, we know little about how people specifically look at world maps, particularly those visualising climate change. This gap can be surprising given how much the behavioural sciences have contributed to climate science communication by measuring public perceptions and attitudes toward the crisis, and how many recommendations for improving visualisations they have made over the years <ref type="bibr">[1,</ref><ref type="bibr">13,</ref><ref type="bibr">14]</ref>, along with improving textual contents <ref type="bibr">[15]</ref>. While a few studies have recently confirmed the effectiveness of climate communication with maps <ref type="bibr">[16]</ref>, other studies raise concerns about possible biases induced by the same visualisations, for instance through the misuse of colour <ref type="bibr">[17,</ref><ref type="bibr" target="#b18">18]</ref>. In parallel, research on artwork perception highlights the importance of viewing context (such as laboratory versus gallery or museum environments, spatial layouts, and the authenticity of artworks) in shaping engagement and judgment, with distinct patterns emerging between categories such as digital versus physical or genuine versus replica <ref type="bibr">[19]</ref><ref type="bibr">[20]</ref><ref type="bibr">[21]</ref><ref type="bibr">[22]</ref><ref type="bibr" target="#b23">[23]</ref><ref type="bibr" target="#b24">[24]</ref><ref type="bibr" target="#b25">[25]</ref><ref type="bibr" target="#b26">[26]</ref>. However, comparisons between aesthetic and scientific viewing contexts remain underexplored, offering a promising direction for further investigation.</p><p>To assess public responses and provide evidence-based recommendations, it is essential to move to flexible and varied data collection methodologies beyond traditional questionnaires and focus groups. To address this, we demonstrate the feasibility and relevance of collecting behavioural data from the public as they engage with climate communications and, briefly, with reproduction of artworks, in situ. Mobile eye-tracking technology provides a relatively objective and unobtrusive means to measure where viewers direct their attention when engaging with visual stimuli. It is adaptable for use on screens, mobile devices, and in virtual reality, and can be scaled up to widespread, in situ implementation.</p><p>Our study primarily aimed to deploy and validate mobile eye-tracking in a controlled lab setting to investigate how individuals direct their visual attention to climate projection maps.</p><p>Secondarily, the study also explored potential differences in visual engagement with scientific versus aesthetic stimuli, providing preliminary insights into how people process informationoriented maps compared to visually expressive artworks, while recognising that engagement in lab-based engagement may differ from that in museum environments. While eye-tracking captures gaze in an agnostic way, it is often assumed that distinct pre-attentive (commonly associated with bottom-up processes) and attentive (commonly associated with top-down processes) mechanisms exist. Arguably, our research primarily focuses on bottom-up responses, where participants' gaze is likely driven by factors such as the visual saliency of the maps, reflecting early, pre-attentive processing, but also influenced by top-down factors like prior knowledge on the issue. To approximate the viewing conditions that the public may encounter in museums or classrooms, we compared solitary viewing conditions to social conditions, where two individuals looked at the same item at the same time. Here, we tested climate change maps as projected data from the IPCC, and included two artworks, to examine differences between viewing patterns aimed at gathering information versus aesthetic appreciation. We aimed to evaluate the utility of mobile eye-tracking as a tool for collecting data on a large scale outside of lab settings, such as during exhibitions, public events, and in educational settings. Ultimately, understanding public behavioural responses, such as visual attention to climate change visualisations, can yield insights that enhance design strategies, making visualisations more explanatory and inclusive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Participants</head><p>This study initially recruited 50 participants through convenience sampling. Three recordings were excluded because the gaze mapping algorithm failed to normalise fixation data, and three additional participants did not provide questionnaire responses. The final sample included 47 participants for gaze analysis and 44 for questionnaire responses <ref type="bibr" target="#b33">(33</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Stimuli and materials</head><p>This study utilised two sets of stimuli (maps and artworks), divided into two sequentially conducted parts. The experiment was conducted in the available space of the Virtual Reality (VR) Lab at the Department of Psychology, Royal Holloway, University of London (although participants did not use VR). The primary stimuli for Part 1 consisted of ten world maps displayed on a 17-inch laptop monitor (Dell Alienware 2019), depicting global climate change projections, including near and far future scenarios for five key measures: mean temperature, sea surface temperature, sea level rise, anthropogenic CO2 emissions, and atmospheric particulate matter concentrations (PM2.5). These maps were generated using data visualisation tools <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b28">28]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Design</head><p>This study was primarily exploratory and descriptive, aiming to analyse participants' viewing patterns using mobile eye-tracking data while they engaged with ten world maps depicting the climate crisis and two artworks in a laboratory setting.</p><p>Data were collected under two viewing conditions: a primary individual viewing condition, where participants viewed the stimuli alone, and a secondary paired viewing condition, where pairs of participants viewed the stimuli together. Despite challenges in recruiting pairs, which resulted in unequal group sizes <ref type="bibr">(35 and 12</ref> in single-and paired-viewing conditions) the setup allowed for the collection and comparative analysis of gaze metrics across different viewing contexts.</p><p>The primary analyses focused on fixation-based metrics and their derivatives, which are often linked to attentional processes, such as overt attention and visual attention guidance.</p><p>Descriptive statistics such as averages, frequencies, and heatmap visualisations were used to present data rather than formal hypothesis testing, highlighting engagement patterns with the stimuli.</p><p>Additionally, the study served as a preliminary evaluation of the feasibility of the research procedures and the analysis pipeline in a laboratory setting and beyond, establishing groundwork for future in-situ experiments in public spaces. A significant goal was to compile a sizeable eye-tracking dataset, which, due to the inherent limitations of mobile eye-tracking systems, was expected to be noisier than data from stationary systems. Lastly, this open dataset and code were prepared for reuse in future research, allowing for expanded analyses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Procedure</head><p>The experiment was conducted during regular working hours. Participants began by receiving detailed written and oral instructions, and the experimenter addressed any questions.</p><p>They provided written informed consent before being equipped with the mobile eye tracker, with correction lens inserts provided when needed. Although the mobile eye tracker was calibration-free and self-correcting, calibration was visually checked using a standard five-point calibration panel, and offset corrections were applied if necessary.</p><p>For the first part of the experiment (map viewing), participants' calibration was confirmed before the recording started. Each participant was assigned a randomly generated three-digit ID and viewed maps displayed on a laptop in a semi-randomised order, spending at least 30 seconds per map. The near-term projection map of each type was always displayed before the corresponding long-term projection map, but the order of projection types was randomised across participants (see S4 Fig for the procedure diagram). Participants in the paired-viewing condition were encouraged to discuss the maps with their partner, while those in the single-viewing condition viewed them independently.</p><p>For the second part (artwork viewing), participants viewed two large-scale paintings mounted on the wall. They carried the companion device with them and were free to choose the viewing order and spend as much time as they wished on the artworks, moving freely around them.</p><p>After both sessions, participants filled out the 22-item climate change anxiety scale and optionally provided demographic information and comments. The experiment concluded with a debriefing by the experimenter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Data analysis</head><p>Data analysis progressed through several stages, from raw recordings to detailed analyses. Recordings from the companion device were uploaded to Pupil Cloud, a GDPRcompliant online platform for data processing and visualisation. Fixation detection used an extended I-VT (Identification by Velocity Threshold) algorithm. Data quality was inspected informally by visually checking raw gaze data overlaid with fixations on Pupil Cloud, alongside fixation duration rates relative to recording duration. No valid recordings were excluded. Some spatiotemporal random noise in gaze data, as reported by the manufacturer, was assumed not to affect the primarily descriptive analyses.</p><p>To map the XY-coordinates of raw gaze and fixation data, two streams of parallel processing were employed on the cloud. This dual approach was designed to take advantage of a newly available algorithm in beta version at the time of data collection, which was later released as a stable version. The first, marker mapper enrichment (MME), utilised fiducial markers (Apriltags) placed around the laptop monitor and paintings to define areas of interest (AOIs). Recordings were manually time-stamped for segmentation, and markers served as anchor points to normalise gaze data. The second, reference image mapper (RIM), employed video recordings and snapshots to create a structure-from-motion model for normalisation.</p><p>While both methods produced comparable data structures, RIM demonstrated higher accuracy and was used for all subsequent analyses (see S5 Fig for an overview). RIM preserved the height-to-width ratio of stimuli in pixel-based values, unlike MME, which distorted the aspect ratio during normalisation. Surface-normalised fixation data were used to create heatmaps visualising viewing patterns.</p><p>The enriched dataset, along with supplementary materials, was uploaded to Kaggle and Google Colab for further analysis using Python-based notebooks (e.g., pandas, matplotlib, seaborn), alongside offline software (e.g., jamovi). Affinity Designer and Affinity Photo were used to refine plot outputs. Data and analyses were also uploaded to the Open Science Framework (osf.io). Primary gaze metrics included total fixation duration (in milliseconds and percentage), fixation count, average fixation duration, and proxy saccadic scanpath length. As mobile eye tracking lacked constant participant-to-stimulus distance, scanpath length was calculated using pixel-based on-screen values as a proxy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>The results section generally reports descriptive values, specifically the mean (M) and standard error of the mean (±SEM), unless stated otherwise. The primary indicators of participant engagement with the maps and artworks were derived from fixation-based metrics, including total fixation duration (also referred to as dwell time), fixation count, average fixation duration, and proxy saccadic scanpath length. Instances of transient engagement, such as brief glances at fiducial markers, were excluded from the analysis due to their minimal duration. As previously described in the Data Analysis section, the pre-processed, enriched data were obtained using the reference image mapper (RIM) technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Descriptive statistics for maps</head><p>The main descriptive statistics for the maps reflect averages across all stimuli, without differentiation by map type. Supplementary statistics were broken down either by map projection timeframe or projection type. Sample sizes were 35 for the single viewing condition and 12 for the paired viewing condition, with each of the 10 stimuli viewed under both conditions. This resulted in a total of 350 observations for the single viewing and 120 for the paired viewing conditions. Initial visualisation using cumulative heatmaps revealed a strong upper-central tendency in viewing patterns, indicating that regions on and around continental Europe received significant attention across most cases. These heatmaps also illustrated comparable spatial patterns between single and paired viewing conditions. Notably, a considerable number of fixations were concentrated on the scales and on areas of the maps displaying the minima and maxima values of the corresponding scale. These areas often represent the most salient regions in terms of contrast and colour, suggesting that such bottom-up factors are major determinants of spatial attention location among participants. For an illustrative overview of these patterns,    (e) proxy scanpath length (pixels). Whilst total fixation durations were comparable between conditions, paired viewing showed higher fixation counts, shorter average fixation durations, and longer scanpaths. Each plot displays the range (excluding outliers), interquartile range, median, and mean (triangle overlay).</p><p>Despite the relatively low sample size and exploratory nature of the research, a nonparametric ANOVA (Kruskal-Wallis test) was used to analyse these metrics. While fixation duration showed no significant differences, fixation count, average fixation duration, and scanpath length displayed significant differences between single and paired viewing conditions</p><formula xml:id="formula_0">(see S2 Table for detailed breakdown of χ2, df, p, ε 2 values)</formula><p>Additionally, descriptive statistics were reported based on two categorisations: map projection timeframe (near and far future) and map projection type (main temperature, sea surface temperature, sea level rise, anthropogenic CO2 emissions, and fine particulate matter PM2.5). Metrics were similar between the two timeframes but showed some variation across the five types of projections. The overall variance was generally larger, especially for the maximas, in paired condition for all metrics except average fixation duration. These results are presented in S3 Table . To further illustrate minor trends and detailed data across the five metrics, results were divided by both viewing conditions and the ten stimuli. Lastly, given the inherent gaze-estimation accuracy limits of the mobile eye tracker (approximately reported as ≈4º by the whitepaper from the manufacturer), conducting a highly granular AOI-based analysis may lead to significant errors: particularly for a generic viewing condition, the monitor surface area might roughly translate to a surface of a 38º × 21.5º of visual angles. For instance, it is impractical to confidently display fixations on individual countries due to these accuracy limitations and the relatively low sample size, which could skew the gaze estimation errors beyond mere random noise in the data. Nevertheless, as a proof of concept, the stimulus was divided into two broad AOIs: the upper section representing the map and the lower section the scale. On average, participants spent four times as much time viewing the main map compared to the scale at the bottom. This 80-20% relative dwell time difference was interestingly consistent across both viewing conditions and remained relatively stable when broken down by individual maps (see S5 Table ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Descriptive statistics for paintings</head><p>Similar to the analysis of the maps, descriptive statistics for the two paintings were calculated, averaging across stimuli. Sample sizes were 35 for the single viewing condition and 12 for the paired viewing condition, with each of the two stimuli viewed under both conditions. This resulted in a total of 70 observations for the single viewing and 24 for the paired viewing conditions. Initial visualisation using cumulative heatmaps for the paintings indicated a pronounced central tendency consistent with the layout of the artworks. The first painting, being more figurative with numerous elements, exhibited diverse focal points such as faces and bodies, resulting in a more dispersed gaze pattern across both X and Y axes. In contrast, the second painting, which is more abstract and centrally composed, showed gaze dispersion primarily concentrated at the centre and one particular area on bottom-right (albeit gaze dispersions were not further plotted). These differences highlight how compositional elements influence visual attention as indexed by cumulative fixations. For a detailed view of these attentional distributions, refer to Fig 3 , showcasing the heatmaps of the paintings.  Despite the relatively low sample size and the exploratory nature of the research, a nonparametric ANOVA (Kruskal-Wallis test) was employed for statistical analysis. While average fixation duration showed no significant differences, total fixation duration, fixation count, and scanpath length exhibited significant differences across conditions (see S7 Although not subjected to statistical comparison, it may be useful to highlight observed trends between map and painting viewing. On average, participants spent about twice as much time viewing paintings compared to maps, as indexed by dwell time. This discrepancy may be attributed to several factors: Participants might have found artworks more engaging than climate crisis maps, or the smaller number of paintings (two) compared to maps (ten) could have allowed for longer individual viewing times per painting. Additionally, the maps might not have contained as much visual information or complexity, which might have required less time to view. Another notable difference was also observed in average fixation duration, which was highly shorter for painting viewing compared to map viewing. Longer average fixation durations sometimes suggest a higher cognitive load, while shorter fixations might indicate that the task of viewing paintings was less cognitively demanding, or that the information was easier to process. This difference could also be influenced by methodological factors: fixation detection algorithms might perform differently on screen-based stimuli versus in situ observation, with the latter possibly affected by participant mobility during painting viewing. Lastly, in both settings, fixation counts were higher in paired viewing conditions, indicating a consistent trend across this metric. Interestingly, while dwell times were comparable between single and paired map viewing conditions, they differed significantly in painting viewing, suggesting a minor preferential difference between the two types of stimuli. Overall, levels of climate change anxiety were relatively low across the sample. For the first two, often highly correlated subscales (cognitive-emotional impairment and functional impairment), participants typically reported low scores. More than half of the responses were "never," and approximately a quarter were "rarely." However, a small subset of participants exhibited mid or high scores on these measures, suggesting notable levels of climate-related anxiety for a minority of participants within the group. Furthermore, responses on the personal experience with climate change subscale, and more distinctly on the behavioural engagement subscale, were comparatively higher. While the distribution of responses on the personal experience subscale was relatively even across all five points of the Likert scale, the behavioural engagement subscale showed a negatively skewed distribution. This suggests that on average, participants either exhibited or aspired to positive behaviours towards addressing the climate crisis. This suggests that a combined CCAS score could be formed as a cohesive construct for further analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Climate change anxiety scale (CCAS) responses</head><p>To explore the relationship between this aggregated CCAS score (treated as ordinal data) and the four primary gaze metrics (treated as continuous data), we conducted correlation analyses. The results, however, indicated no significant correlations; all relationships were effectively flatlined across the metrics. This lack of significant findings implies that there is no immediate or obvious connection between main gaze metrics and self-reported climate anxiety, as detailed in S11 Table. Given these outcomes, we did not proceed further, such as dividing participants into low and high anxiety groups based on median, quartile, or range-based splits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion and Conclusions</head><p>The present study established the relevance of using mobile eye-tracking to examine how people look at climate change maps, and provided insights for scaling it up to more naturalistic settings, notably social ones. Despite individual differences, on average, the comparison between conditions during map viewing suggests that social interaction can subtly alter gaze patterns. Paired viewing was associated with higher fixation counts, shorter average fixation durations, and longer scanpaths, though dwell times remained comparable. These variations may indicate differences in visual and cognitive processing, such as varying cognitive efforts <ref type="bibr" target="#b30">[30]</ref>, or social motivation <ref type="bibr" target="#b31">[31]</ref>.</p><p>Attentional hotspots, indexed by fixation heatmaps, were broadly similar, indicating that the content of visual stimuli primarily captures attention, often displaying a Euro-centric bias.</p><p>This implies that the bottom-up, object-based factors such as image saliency may have a larger effect on viewing patterns. However, dissimilarities can be partially explained by how social contexts shape viewer engagement, thereby influencing their information processing strategies.</p><p>Additionally, gaze metrics differed between viewing maps and artworks. Scientific information processing and aesthetic perception showed some distinct viewing strategies, with map viewing generally associated on average with longer fixation durations. These preliminary findings align to some extent with prior work highlighted in the Introduction, suggesting that viewing context (including differences between laboratory and museum environments) and the authenticity of stimuli (such as reproductions versus originals) can influence gaze patterns and overall engagement with visual stimuli. Although not directly comparable (given that this study was conducted in a lab setting with reproductions of artworks), it is worth considering what constitutes the genuine context for climate change maps, which are encountered in a variety of settings.</p><p>Although there were significant positive linear cross-correlations among the subscales of the CCAS, the lack of a significant linear correlation between main gaze metrics and climate anxiety scores suggests that visual attention, as indexed by main gaze metrics, may not directly relate to self-reported trait anxiety levels. It is important to note that no additional measures were used to capture contextual emotional responses of participants to the content in this study.</p><p>The small size of the paired-viewing group, due to convenience sampling, and the inherent data noise in mobile eye-tracking need to be considered when assessing these pilot results. The imbalance between single-and paired-viewing conditions, as well as the limited diversity in gender and age, further limits the generalisability of the findings. These constraints reflect the challenges of recruiting larger, more balanced samples for mobile eye-tracking studies, which typically require specialised equipment and substantial resources. Additionally, participants might have treated the viewing differently within the experimental setting compared to a naturalistic environment, such as a museum or classroom. Nonetheless, the observed differences between isolated and social settings, and between maps and artworks, show that the method can successfully capture differences in viewing patterns, even under these conditions. This study provides a foundation for future research to consider the impact of various types of climate-related visual stimuli on a broader audience. Extending this research to more ecologically valid settings, as highlighted earlier, could help further clarify how real-world contexts shape engagement with scientific and aesthetic stimuli. Additionally, integrating momentary affective assessments and qualitative assessments in mixed-methods designs could also provide deeper insights into the cognitive and emotional dimensions of viewer engagement.</p><p>Our study highlights the potential of mobile eye-tracking for understanding how people engage with climate projections. Behavioural science has been hinted as a precious source of recommendations to enhance climate visualisations in four key areas: how to direct visual attention, reduce visual complexity, support inference-making, and integrate text with graphics <ref type="bibr">[1]</ref>, and this study clearly demonstrates how it can contribute to the first objective. This should not mean that the other aspects are equally essential and require further investigation. Visual complexity and text-graphic integration can also be tested with eye-tracking and highlights future uses for the methods. The reason not to pursue those here is that our approach aimed to mimic everyday encounters with climate content by allowing participants to simply view images naturally, without additional inference, understanding, or memory questions as in some other research <ref type="bibr" target="#b32">[32]</ref>.</p><p>The differences in visual engagement between single and paired viewing conditions, as well as between maps and artworks, underscore the potential for visual communication strategies better tailored to contexts. Research indicates that changing graph designs derived from IPCC reports can intentionally alter perceptions and even shift the credibility of the presented data <ref type="bibr" target="#b33">[33]</ref>. Strategies leveraging different media formats can enhance public understanding of climate science. For instance, the memorability of visualisations <ref type="bibr" target="#b34">[34]</ref> can be utilised to promote climate action, with eye-tracking and visual attention serving as useful tools to assess the effectiveness of different visualisations.</p><p>Although our study used static images, previous research has shown that interactive visualisations that are more personally relevant yield promising results in terms of perceived reality of climate change, attitude certainty, and concern <ref type="bibr" target="#b35">[35]</ref>. Tailoring of communication can also be done for different viewers. In the present case, we did not find evidence of differences between individuals with different trait climate anxiety. Other directions remain open. For instance, individuals with varying levels of optimism, when presented with climate change messages in text, show different allocations of visual attention and recall, underlining attentional bias and suggesting the need to redesign our communications <ref type="bibr" target="#b36">[36]</ref>. Preliminary evidence also suggests different viewing strategies and actions between political groups (liberals and conservatives), highlighting the ideological influence on visual attention <ref type="bibr" target="#b37">[37]</ref> and the need for tailored communication tools to address such attentional and perceptual biases <ref type="bibr" target="#b38">[38]</ref>.</p><p>Behavioural sciences can play a crucial role in identifying and overcoming psychological barriers to climate action <ref type="bibr">[39]</ref>. However, creating effective interventions poses significant challenges, as evidenced by large-scale cross-cultural studies <ref type="bibr">[40]</ref>. Therefore, from a methodological standpoint, employing behavioural data collection using mobile, screen-based, or XR eye-tracking can be seen as essential for pinpointing visual attention, and later on help for improving climate communication and interventions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Supplementary information</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Data and code availability</head><p>All anonymized data sets are publicly accessible for verification and reuse, in two directories corresponding to maps and paintings: osf.io/2jqcu and osf.io/5bh78</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Acknowledgements</head><p>The authors extend their gratitude to Prof Szonya Durant for providing access to the RHUL VR Lab (Virtual Reality Laboratory, Department of Psychology, Royal Holloway, University of London) as the data collection site; and to Dr Étienne Serbe-Kamp for the valuable comments on the original draft. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Authorship contribution statement</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Competing interests</head><p>The authors have declared that no competing interests exist.  This figure provides an infographic overview of the main stimuli, consisting of five pairs of world maps with a Robinson projection. These maps were generated using the online data visualisation tool from the Intergovernmental Panel on Climate Change (IPCC) and downloaded from their website (ipcc.ch and interactiveatlas.ipcc.ch). Each map features a heatmap overlay representing a specific type of climate projection, accompanied by a relevant scale and brief data description below it. The maps are organised into two columns representing different projection timeframes: the near future (2021-2040) and the far future (2081-2100), with corresponding icons beneath for illustrative purposes. The five rows depict different projection types: temperature change in °C, sea surface temperature change in °C, sea level rise in metres, anthropogenic CO2 emissions in kg/m 2 per year, and particulate matter (PM2.5) concentration changes in μg/m 3 , with icons on the right for illustration. In the experiment, these maps were displayed full-screen on a 17-inch laptop monitor. Each map presentation was preceded by a title card (not shown here) that displayed these icons and titles, informing participants about the content of the upcoming map. A countdown from 30 seconds and a brief instruction text afterwards (not illustrated here) were also present during each trial, indicating that participants could advance to the next map at any time by pressing the spacebar. Below the maps, naming conventions are listed, which are used in subsequent data visualisations like bar plots in this paper. Note that small Apriltags around each map were initially intended for the Marker Mapper (MM) preprocessing pipeline but were not used, as the Reference Image Mapper (RIM) preprocessing pipeline was used instead, which does not require Apriltags. (See Methods for details). Participants viewed the complete set of ten maps in a pairwise randomised order, starting with the near-future version followed by the far-future version of the same projection type. Each map was displayed for a minimum of 30 seconds, after which a small description appeared at the top, allowing participants to view the map as long as desired until proceeding to the next by pressing a spacebar key. Before any map presentation, a brief title screen (lasting approximately five seconds) introduced the content of the upcoming map with a couple of words and two icons (similar to those in Figure <ref type="figure" target="#fig_2">1</ref>). In paired viewing conditions, participants were encouraged to discuss the maps if desired, although this varied among pairs. Each map remained visible until the group chose to proceed, provided at least 30 seconds had elapsed. The sequence of map presentations was randomised in pairs, with NStimulus = 10 as five pairs of near-and far-future projections. Note that a similar procedure was employed for viewing the two paintings (not illustrated here for succinctness), with participants deciding the order and duration of viewing for the printed posters. superimposed on the scene camera view. The raw gaze data are then normalised against this model and the reference snapshot. Due to its relatively higher data accuracy, only data processed via RIM was used for analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">Funding statement</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Supplementary materials</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Right (c):</head><p>This panel shows the process of normalising raw gaze data against the model and reference image, where the red circle denotes the normalised location of the fixation with normalised XY coordinates relative to the pixels of the snapshot image. Note that the same preprocessing methods were applied to the supplementary painting stimuli (not shown here for brevity).   Note that the sample size for these analyses was NSample = 44.  found between the total CCAS score and any of the gaze metrics, indicating that there is no apparent relationship between these relatively raw gaze metrics and self-reported anxiety scores as measured by the CCAS. Note that NSample = 44.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S1</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>females, 8 males, 1 non-binary, 2 unspecified; MAge = 20.93 years, SDAge = 4.14 years, RAge = 18-37 years). Participants, primarily students and staff from Royal Holloway, University of London, were recruited via online platforms and campus flyers. All participants provided written informed consent. The research protocols were certified by the researcher in accordance with the self-certification guidelines provided by the Research Ethics Committee at Royal Holloway, University of London (approval ID: 3527-TFJT002, 2022-12-02). The study was conducted in compliance with the ethical standards outlined in the Declaration of Helsinki. The recruitment period spanned from 05/12/2022 to 05/03/2023. Corrective lens inserts were provided for participants requiring glasses where possible, but data from those with high prescriptions or other unusable recordings were excluded. Participants were assigned to either single-viewing (N = 35) or paired-viewing conditions (N = 12, six pairs). Unequal group sizes reflected the practical challenges of recruiting pairs, resulting in more participants in the single-viewing condition. Participants received £5 or course credit as compensation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>, provided by the Intergovernmental Panel on Climate Change (IPCC) and were accessible at interactive-atlas.ipcc.ch at the time of writing (see S1 Fig for an overview). The supplementary stimuli for Part 2 included two large-scale artworks printed on A0-sized posters and mounted on the laboratory wall (see S2 Fig for details). The Climate Change Anxiety Scale (CCAS), a 22-item questionnaire [29], was used to measure participants' responses to climate change on a 5-point Likert scale, covering four subcategories (see S1 File for the full scale). An exit-questionnaire was also administered to gather optional demographics data and participant feedback. Gaze data were recorded with the Pupil Invisible mobile eye-tracker using Pupil Invisible Companion App (version 1.4.21). The raw gaze data were pre-processed on the GDPR-compliant Pupil Cloud platform. The maps were presented using PsychoPy (version 2022.2.5), the CCAS and exit-questionnaire were presented using Google Forms (see S3 Fig for the experimental setup).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>refer to Fig 1 ,</head><label>1</label><figDesc>which displays the heatmaps of the maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig 1 .</head><label>1</label><figDesc>Fig 1. Fixation heatmaps for maps. Heatmaps illustrate fixation distributions across maps under single and paired-viewing conditions, using a green-to-red colour scale for shorter to longer fixation durations. Single-and paired-viewing conditions are shown in the first/third and second/fourth columns, respectively. Spatial common ground generally exhibits an upper-central tendency. Substantial overlap between conditions suggests that image-based saliency predominantly drives visual attention, over social context.</figDesc><graphic coords="12,99.00,77.25,396.84,337.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>aggregate metrics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig 2 .</head><label>2</label><figDesc>Fig 2. Box plots of gaze metrics for maps. Box plots illustrate five gaze metrics across all maps (single-viewing in light grey and paired-viewing in dark grey): (a) total fixation duration (s), (b) normalised fixation duration (%), (c) fixation count, (d) average fixation duration (ms), and</figDesc><graphic coords="13,85.00,296.20,425.03,219.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>S6 Fig displays bar plots overlaid with individual data points, and S4 Table provides a comprehensive descriptive summary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig 3 .</head><label>3</label><figDesc>Fig 3. Fixation heatmaps for paintings. Heatmaps show fixation distributions for two paintings, based on data from all participants in both single-viewing (a-b) and paired-viewing (cd) conditions.</figDesc><graphic coords="16,152.15,81.45,291.30,371.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig 4 .</head><label>4</label><figDesc>Fig 4. Box plots of gaze metrics for paintings. Box plots depicts five gaze metrics for two paintings (single-viewing in light grey and paired-viewing in dark grey): (a) total fixation duration (s), (b) normalised fixation duration (%), (c) fixation count, (d) average fixation duration (ms), and (e) proxy scanpath length (pixels). On average, single viewing showed slightly shorter dwell times, lower fixation counts, slightly longer average fixation durations, and significantly shorter scanpath lengths. Each plot shows the range (excluding outliers), interquartile range, median, and mean (triangle overlay).</figDesc><graphic coords="17,85.00,240.25,425.05,221.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Irrespective of the viewing condition, whether single or paired, the analysis of the climate change anxiety scale involved four subscales of the 22-item questionnaire, with responses gathered using a 5-point Likert scale. For a visual representation of the responses, refer to S8 Fig for the frequency plot of individual items, S9 Fig for the aggregated frequency plot across the four subscales, and S9Table for a statistical breakdown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>When investigating the relationships between the four subscales of the climate change anxiety scale (CCAS), the 5-point Likert scale responses were treated as ordinal data, assigning values from 0 (never) to 4 (almost always). Despite the frequency distributions varying across the subscales, their relationships were examined through cross-correlation, using Spearman's rho and Kendall's tau-b. These analyses showed positive linear correlations between the subscales, as illustrated in S10 Fig, and detailed in S10 Table.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Contributions of the authors are defined according to the CRediT (Contributor Roles Taxonomy). Doga Gulhan: Conceptualization, Methodology, Software, Formal analysis, Investigation, Data Curation, Writing -Original Draft, Writing -Review &amp; Editing, Visualisation. Bahador Bahrami: Writing -Review &amp; Editing, Supervision, Project administration, Funding acquisition. Ophelia Deroy: Writing -Review &amp; Editing, Funding acquisition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>We acknowledge support from Royal Holloway, University of London (RHUL) and Ludwig Maximilian University of Munich (LMU) for providing salary and stipend arrangements to the main author, Doga Gulhan. Partial funding for this project was provided by a grant (ID: TRT-0480) from the Templeton Religion Trust (templetonreligiontrust.org), titled "Aesthetic Cognitivism for Groups: Collective Ways of Looking and Sense-making", awarded to co-author Bahador Bahrami, and by a grant (ID: 10349) from the Volkswagen Stiftung Grant (volkswagenstiftung.de), titled "Ways of Seeing, Ways of Knowing", awarded to co-author Ophelia Deroy. Bahador Bahrami was supported by the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (819040 -acronym: rid-O). Ophelia Deroy was supported by the Volkswagen Stiftung Momentum Grant (Co-Sense) and the additional module (Ways of Seeing). The funders did not play any role in the study design, data collection and analysis, decision to publish, or preparation of the manuscript.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>3 1</head><label>3</label><figDesc>Studying attention to IPCC climate change maps with mobile eyetracking Doga Gulhan *,1,2 , Bahador Bahrami 2 , Ophelia Deroy Department of Psychology, Royal Holloway, University of London, UK 2 Faculty of General Psychology and Education, Ludwig Maximilian University of Munich, Germany 3 Faculty of Philosophy, Philosophy of Science and the Study of Religion, Ludwig Maximilian University of Munich, Germany *Corresponding author contact information: dogagulhan@gmail.com (DG) S1 Fig. Overview of main stimuli: ten climate crisis maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>PROJECTION TYPE S2 Fig. Overview of supplementary stimuli: two paintings. This figure showcases high-resolution digital images of two paintings printed on A0-sized posters, serving as a small supplementary stimulus set. These paintings were selected for their distinct content and downloaded from Wikimedia Commons (commons.wikimedia.org). Top-left (a): The Baptism of Christ by Piero della Francesca, completed circa 1448-50, actual size approximately 167 cm × 116 cm. Top-right (b): The Swan, No. 10, by Hilma af Klint, completed in 1915, actual size approximately 150 cm × 150 cm. Bottom-left (c) and Bottomright (d) feature photographs of the original artworks as seen at the National Gallery, London, and Tate Modern, London, respectively, photographed by DG. A Artwork #1 digital/printed C Artwork #1 in-situ B Artwork #2 digital/printed D Artwork #2 in-situ A B C S4 Fig. Procedure diagram for viewing ten maps.This diagram illustrates the time-course of viewing conditions for both individual and paired participants.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>data pre-processing using MME and RIM methods. This figure demonstrates the two data processing methods used within Pupil Cloud for both screen-based (maps) and in-situ (paintings) stimuli. Left (a): The Marker Mapper Enrichment (MME) method involved placing four markers around the laptop screen (highlighted in green). The boundaries defined by these markers (shown with blue and red borders) depict the surface within which raw gaze data are normalised. However, this method was later abandoned due to relatively low data accuracy. Middle (b): The Reference Image Mapper (RIM) method required separate scanning recordings lasting about one minute and snapshot images for each stimulus. It employs a structure-from-motion technique to create a digital model from the video frames, shown as white dots</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>of box plots of descriptive statistics for main gaze metrics by map. Supplementing the cumulative box plots, this figure details the breakdown of gaze metrics for ten individual maps. It follows the same format as the previous figure, presenting metrics for: (a) total fixation duration in milliseconds, (b) normalised fixation duration as a percentage (dwell time), (c) fixation count, (d) average fixation duration in milliseconds, and (e) total proxy scanpath length in pixels. Box plots are colour-coded in light and dark greys to represent single-(NStimulusSingle = 35) and paired-viewing (NStimulusPaired = 12) conditions, respectively. Each box plotshows the range (excluding outliers), IQR, median, and mean (marked with a small triangle). The categorical xaxis categorises the ten maps as different stimuli, while the y-axis denotes each metric. Scanpath distances are calculated similar to the previous figure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>of box plots of descriptive statistics for main gaze metrics by painting. This figure supplements the cumulative box plots by detailing the breakdown of gaze metrics for two individual paintings. It showcases the same five main gaze metrics: (a) total fixation duration in milliseconds (dwell time), (b) normalised fixation duration as a percentage, (c) fixation count, (d) average fixation duration in milliseconds, and (e) total proxy scanpath length in pixels. The box plots are colour-coded in light and dark greys to distinguish between single-viewing (NSampleSingle = 35, NStimulusSingle = 35) and paired-viewing (NSamplePaired = 12, NStimulusPaired = 12) conditions, as described in the legend at the top right. Each plot displays the range (excluding outliers), interquartile range (IQR), median, and mean (indicated by a small triangle overlay), providing a detailed view of the variability. Data points within each plot reflect the values of individual stimuli, hence these are the brokendown box plots corresponding to each painting. Note that the proxy scanpath distances, calculated for both viewing conditions, are the cumulative sums of the Euclidean distances between two consecutive fixations for each painting and participant. These distances are based on the arbitrary pixel values from the reference images used for each painting: 685 × 1000 pixels for The Baptism of Christ (Painting #1) and 983 × 1000 pixels for The Swan (Painting #2), which introduces some compatibility issues in unified analysis, and requires further normalisation. The categorical x-axis denotes the two different paintings as stimuli, while the y-axis denotes each gaze metric.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>plots of survey results, by four subcategories. This figure displays the cumulative frequency table, aggregating responses into four sub-categories to show overall trends and frequency distributions within the Climate Change Anxiety Scale: (a) Four sub-categories plotted along a frequency X-axis, and (b) the same four subcategories visualised as count-based data, reflecting the differing number of questions per sub-scale. The y-axis counts reflect the total number of individual responses, varying by the number of questions in each sub-category: 8 questions under cognitive-emotional impairment, 5 under functional impairment, 3 under personal experience of climate change, and 6 under behavioural engagement. With NSample = 44 due to three unrecorded participant responses, the total NCount for responses are: 353 for cognitive-emotional impairment, 220 for functional impairment, 225 for personal experience, and 264 for behavioural engagement. The 5-point Likert scale is colour-coded from black to white, as shown in the accompanying legends. A B S10 Fig. Correlation plots between four subscales of the CCAS.This figure presents the cross-correlation between the four subscales of the Climate Change Anxiety Scale (CCAS).Labels at the top and right (from Subscale 1 to Subscale 4) correspond to the four subscales of the CCAS. Along the diagonal axis from top-left to bottom-right, the graphs display the response distribution for each subscale, where 5-point Likert scale data for each question (treated as ordinal data) is converted to 0-4 scores. Below the diagonal, six correlation plots reveal all possible pairwise correlations among the subscales. Each plot includes individual data points, a linear correlation line, and confidence intervals for visual guidance. Above the diagonal, the corresponding areas display the Spearman's rho correlation coefficients for each pair of subscales.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head></head><label></label><figDesc>(CCAS) score (treating it again as ordinal data) and four gaze metrics (as continuous data). The total CCAS score was calculated for each participant based on the cumulative sum of their responses on the 5-point Likert scale, based on the high positive linear correlation between its four subscales. The gaze metrics reported here include total dwell time, total fixation count, average fixation duration, and average proxy scanpath length. Correlation coefficients (Spearman's rho and Kendall's tau-b) along with p-values and degrees of freedom (df) are presented to assess the statistical significance of these relationships. No significant correlation was</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="33,135.95,217.59,323.22,211.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="50,155.90,70.90,283.44,247.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table for a</head><label>for</label><figDesc>detailed breakdown of χ2, df, p, ε 2 . Similar trends were observed when the data were further disaggregated by the two paintings (see S7 Fig and S8 Table).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table. Gaze metrics for maps.</head><label></label><figDesc>This table presents descriptive statistics for five gaze metrics observed while viewing maps: total fixation duration (in seconds and as a percentage of total viewing time), fixation count, average fixation duration (in milliseconds), and total proxy scanpath length in pixels. The metrics are reported with the following statistics for each:</figDesc><table><row><cell></cell><cell>Descriptives</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Normalised</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Condition</cell><cell>Total fixation duration in s</cell><cell>fixation duration in</cell><cell>Fixation count</cell><cell>Average fixation duration in ms</cell><cell>Total scanpath length in px</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>percentage</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>N</cell><cell>single</cell><cell>350</cell><cell>350</cell><cell>350</cell><cell>350</cell><cell>350</cell></row><row><cell></cell><cell></cell><cell>paired</cell><cell>120</cell><cell>120</cell><cell>120</cell><cell>120</cell><cell>120</cell></row><row><cell>S2 Table.</cell><cell>Mean</cell><cell>single paired</cell><cell>29.7 32.92</cell><cell>10 10</cell><cell>55.12 70.7</cell><cell>586.43 504.45</cell><cell>19534.89 26288.61</cell></row><row><cell cols="2">Std. error mean</cell><cell>single</cell><cell>0.26</cell><cell>0.06</cell><cell>0.87</cell><cell>11.29</cell><cell>365.87</cell></row><row><cell></cell><cell></cell><cell>paired</cell><cell>0.93</cell><cell>0.21</cell><cell>2.47</cell><cell>15.29</cell><cell>1036.95</cell></row><row><cell></cell><cell>Median</cell><cell>single</cell><cell>29.02</cell><cell>9.95</cell><cell>54</cell><cell>546.19</cell><cell>19280.35</cell></row><row><cell></cell><cell></cell><cell>paired</cell><cell>29.45</cell><cell>9.68</cell><cell>65.5</cell><cell>479.26</cell><cell>25215.88</cell></row><row><cell cols="2">Standard deviation</cell><cell>single</cell><cell>4.81</cell><cell>1.2</cell><cell>16.2</cell><cell>211.18</cell><cell>6844.86</cell></row><row><cell></cell><cell></cell><cell>paired</cell><cell>10.24</cell><cell>2.29</cell><cell>27.09</cell><cell>167.5</cell><cell>11359.19</cell></row><row><cell></cell><cell>Minimum</cell><cell>single</cell><cell>16.45</cell><cell>6.01</cell><cell>21</cell><cell>276.2</cell><cell>5285.11</cell></row><row><cell></cell><cell></cell><cell>paired</cell><cell>18.19</cell><cell>5.78</cell><cell>30</cell><cell>221.36</cell><cell>7104.28</cell></row><row><cell></cell><cell>Maximum</cell><cell>single</cell><cell>55.73</cell><cell>17.75</cell><cell>125</cell><cell>1523</cell><cell>57201.18</cell></row><row><cell></cell><cell></cell><cell>paired</cell><cell>64.65</cell><cell>17.91</cell><cell>140</cell><cell>1048.9</cell><cell>53560.01</cell></row><row><cell cols="8">observation size (N), mean (M), standard deviation (SD), standard error of the mean (SEM), minimum (Min), and</cell></row><row><cell cols="8">maximum (Max) values. The data (NMapStimulus = 10) are segmented into single (NSingleSample = 35) and paired</cell></row><row><cell cols="8">(NPairedSample = 12) viewing conditions, allowing for a direct comparison of these metrics under different social</cell></row><row><cell cols="2">viewing contexts.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Nonparametric ANOVA (Kruskal-Wallis test) for maps, comparing single and paired viewing conditions.</head><label></label><figDesc>This table details the results of a nonparametric ANOVA (Kruskal-Wallis test) used to analyse the differences in gaze metrics between single and paired viewing conditions for maps. Despite the exploratory nature of the research and the relatively low sample sizes (N = 35 for single and N = 12 for paired), significant differences were observed in fixation count, average fixation duration, and scanpath length, while no significant differences were found in total fixation duration. The table provides a detailed breakdown of the test results for each metric, including the chi-squared (χ 2 ) values, degrees of freedom (df), p-values, and effect sizes (ε 2 ). Note, although there is not a consensus on effect size interpretation, ε 2 &lt; .08 may be assumed to be small, ε 2 &lt; .26 assumed to be medium, ε 2 &gt;= .26 assumed to be large.</figDesc><table><row><cell>Kruskal-Wallis</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>χ²</cell><cell>df</cell><cell>p</cell><cell>ε²</cell></row><row><cell>Total fixation duration in s</cell><cell>1.7</cell><cell>1</cell><cell>0.193</cell><cell>0</cell></row><row><cell>Normalised fixation duration in percentage</cell><cell>3.22</cell><cell>1</cell><cell>0.073</cell><cell>0.01</cell></row><row><cell>Fixation count</cell><cell>28.69</cell><cell>1</cell><cell>&lt; .001</cell><cell>0.06</cell></row><row><cell>Average fixation duration in ms</cell><cell>15.65</cell><cell>1</cell><cell>&lt; .001</cell><cell>0.03</cell></row><row><cell>Total scanpath length in px</cell><cell>38.27</cell><cell>1</cell><cell>&lt; .001</cell><cell>0.08</cell></row><row><cell>S3 Table.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Gaze metrics for maps, broken down by projection timeframe and type.</head><label></label><figDesc>The observation count for each type is based on twice the total sample size, resulting in NObservations = 94 for each type, as in each cell value.</figDesc><table><row><cell cols="2">Descriptives</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>A -Descriptives</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Normalised</cell></row><row><cell cols="8">Projection timeframe Stimulus Total fixation duration in s Condition Mean 01NearT single paired</cell><cell cols="3">Normalised fixation duration Total fixation duration in s fixation duration in percentage 29.01 9.77 in percentage 30.29 9.21</cell><cell>Fixation count Fixation count 58.6 74.58</cell><cell>Average fixation duration in ms Average fixation Total scanpath duration in ms length in px Total scanpath length in px 533.67 21988.47 451.1 28576.68</cell></row><row><cell>Mean</cell><cell cols="3">Near</cell><cell>02LongT</cell><cell cols="3">30.74 single paired</cell><cell cols="2">10.08 29.44 32.07</cell><cell>9.88 9.72</cell><cell>60.51 54.46 72.75</cell><cell>548.73 583.67 485.04</cell><cell>19699.17 28036.92</cell><cell>22296.16</cell></row><row><cell></cell><cell cols="2">Long</cell><cell cols="5">30.31 03NearSST single</cell><cell cols="2">9.92 28.94</cell><cell>9.75</cell><cell>57.69 54.23</cell><cell>582.27 578.82</cell><cell>19499.82</cell><cell>20222.32</cell></row><row><cell>Std. error mean</cell><cell cols="5">Near Long 04LongSST</cell><cell cols="2">0.44 paired 0.44 single paired</cell><cell cols="2">0.11 30.37 0.1 30.16 33.28</cell><cell>9.19 10.13 9.94</cell><cell>1.35 66.83 1.35 53.06 73</cell><cell>11.31 488.31 14.98 635.75 498.52</cell><cell>25174.1 18814.56 26956.82</cell><cell>561.84 570.83</cell></row><row><cell>Median</cell><cell cols="7">Near 05NearSLR 29.15 single</cell><cell cols="2">9.88 29.64</cell><cell>10</cell><cell>56 59.43</cell><cell>519.62 529.42</cell><cell>21483.81</cell><cell>21093.2</cell></row><row><cell>Standard deviation</cell><cell cols="5">Long Near 06LongSLR</cell><cell cols="2">29 paired 6.78 single paired</cell><cell cols="2">9.89 31.63 1.64 29.66 32.89</cell><cell>9.88 10 10.07</cell><cell>55 71.67 20.65 58.23 72</cell><cell>540.36 467.16 173.35 536.5 490.57</cell><cell>27790.66 20076.06 25646.38</cell><cell>19544.69 8612.78</cell></row><row><cell></cell><cell cols="5">Long 07NearCO2</cell><cell cols="2">6.76 single</cell><cell cols="2">1.46 32.25</cell><cell>10.88</cell><cell>20.66 54.34</cell><cell>229.64 642.27</cell><cell>19656.07</cell><cell>8750.73</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>paired</cell><cell cols="2">38.12</cell><cell>11.41</cell><cell>76.25</cell><cell>538.09</cell><cell>27327.28</cell></row><row><cell>B -Descriptives</cell><cell cols="5">08LongCO2</cell><cell></cell><cell>single paired</cell><cell cols="2">29.63 31.91</cell><cell>9.97 9.62</cell><cell>50.26 65.83</cell><cell>656.27 514.54</cell><cell>16064.97 22839.75</cell></row><row><cell cols="8">Projection type 09NearPM25 Total fixation duration in s single paired 10LongPM25 single paired Std. error mean 01NearT single</cell><cell cols="3">29.36 Normalised 9.85 fixation duration 36.47 11.13 28.95 9.75 32.16 9.84 in percentage 0.72 0.17</cell><cell>56.34 Fixation count 70.42 52.26 63.67 2.51</cell><cell>544.9 Average fixation 20278.84 duration in ms 549.87 27619.02 Total scanpath 623.06 17787.09 length in px 561.29 22918.46 30.23 1192.99</cell></row><row><cell>Mean</cell><cell>T SST</cell><cell></cell><cell></cell><cell>02LongT</cell><cell cols="3">29.73 paired 30.13 single paired</cell><cell>2.67 0.83 4.08</cell><cell>9.73 9.85</cell><cell>0.6 0.16 1</cell><cell>60.9 9.11 57.8 2.6 9.41</cell><cell>535.54 53.42 578.21 34.24 61.25</cell><cell>3393.83 1215.88 4035.72</cell><cell>22749.26 20921.01</cell></row><row><cell></cell><cell>SLR</cell><cell></cell><cell cols="5">30.32 03NearSST single</cell><cell>0.6</cell><cell>10</cell><cell>0.14</cell><cell>62.15 2.6</cell><cell>519.15 34.59</cell><cell>1080.85</cell><cell>22296.17</cell></row><row><cell></cell><cell cols="7">CO2 PM25 04LongSST 31.98 paired 30.47 single paired</cell><cell cols="2">10.45 2.25 9.97 0.87 3.64</cell><cell>0.28 0.19 0.65</cell><cell>57.09 7.45 57.55 2.91 9.77</cell><cell>617.88 40.56 576.73 44.05 51.89</cell><cell>2977.65 1189.82 4216.38</cell><cell>19704.69 20625.08</cell></row><row><cell>Std. error mean</cell><cell>T</cell><cell cols="4">05NearSLR</cell><cell cols="2">0.73 single</cell><cell>0.64</cell><cell>0.17</cell><cell>0.16</cell><cell>2.24 2.72</cell><cell>20.16 25.55</cell><cell>1072.97</cell><cell>969.22</cell></row><row><cell>S6</cell><cell>SST SLR</cell><cell cols="4">06LongSLR</cell><cell cols="2">0.67 paired 0.58 single paired</cell><cell>1.87 0.66 3.14</cell><cell>0.13 0.16</cell><cell>0.75 0.16 0.87</cell><cell>2.22 6 1.93 2.7 7.37</cell><cell>22.92 37.04 14.86 22.14 50.32</cell><cell>2382.96 1038.97 2677.83</cell><cell>924.52 759.54</cell></row><row><cell></cell><cell>CO2</cell><cell cols="4">07NearCO2</cell><cell></cell><cell>0.8 single</cell><cell>1.03</cell><cell>0.17</cell><cell>0.31</cell><cell>2.33 3.34</cell><cell>22.09 31.45</cell><cell>1429.01</cell><cell>963.33</cell></row><row><cell>Median</cell><cell cols="7">PM25 T 08LongCO2 28.41 0.69 paired single paired</cell><cell>3.94 0.66 2.93</cell><cell>0.16 9.8</cell><cell>0.67 0.12 0.49</cell><cell>1.91 9.58 58.5 2.54 7.46</cell><cell>22.91 49.92 487.4 43.24 44.31</cell><cell>3969.73 938.7 3107.72</cell><cell>852.72 22221.5</cell></row><row><cell></cell><cell cols="7">SST 09NearPM25 28.8 single</cell><cell>1.11</cell><cell>9.84</cell><cell>0.29</cell><cell>53 2.76</cell><cell>530.67 23.01</cell><cell>954.22</cell><cell>20033.92</cell></row><row><cell></cell><cell cols="7">SLR CO2 10LongPM25 single 28.94 paired 30.16 paired</cell><cell cols="2">9.84 10.14 2.65 0.81 1.66</cell><cell>0.63 0.21 0.21</cell><cell>58 6.73 54.5 2.53 6.56</cell><cell>497.18 42.15 590.5 51.74 56.18</cell><cell>3486.96 1190.17 2815.41</cell><cell>21146.47 17778.25</cell></row><row><cell>Median</cell><cell cols="3">PM25</cell><cell>01NearT</cell><cell cols="3">29.05 single</cell><cell cols="2">9.91 29.18</cell><cell>9.87</cell><cell>54.5 60</cell><cell>550.93 454.16</cell><cell>22816.85</cell><cell>19390.62</cell></row><row><cell>Standard deviation</cell><cell>T SST SLR</cell><cell></cell><cell cols="3">02LongT 03NearSST</cell><cell cols="2">7.08 paired 6.49 single 5.58 paired single</cell><cell cols="2">1.63 27.96 1.22 29.06 1.59 25.75 28.81</cell><cell>8.99 9.85 8.49 9.93</cell><cell>21.69 65.5 21.51 54 18.68 66.5 53</cell><cell>195.44 408.06 222.26 536.45 144.12 403.91 533.87</cell><cell>26378.1 19048.63 26891.85 20015.09</cell><cell>9396.89 8963.56 7363.99</cell></row><row><cell></cell><cell>CO2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">7.77 paired</cell><cell cols="2">1.66 28.63</cell><cell>9.54</cell><cell>22.63 53.5</cell><cell>214.19 499.67</cell><cell>24475.15</cell><cell>9339.81</cell></row><row><cell></cell><cell cols="5">PM25 04LongSST</cell><cell cols="2">6.65 single paired</cell><cell cols="2">1.56 29.28 27.66</cell><cell>9.97 9.94</cell><cell>18.5 50 67</cell><cell>222.12 580.53 438.09</cell><cell>18738.52 23553.26</cell><cell>8267.46</cell></row><row><cell></cell><cell></cell><cell cols="4">05NearSLR</cell><cell></cell><cell>single</cell><cell cols="2">29.04</cell><cell>9.85</cell><cell>57</cell><cell>522.46</cell><cell>20520.23</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>paired</cell><cell cols="2">30.87</cell><cell>9.16</cell><cell>74.5</cell><cell>458.61</cell><cell>26892.28</cell></row><row><cell></cell><cell></cell><cell cols="4">06LongSLR</cell><cell></cell><cell>single</cell><cell cols="2">28.54</cell><cell>9.96</cell><cell>56</cell><cell>516.82</cell><cell>19925.46</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>paired</cell><cell cols="2">27.45</cell><cell>9.32</cell><cell>71</cell><cell>441.25</cell><cell>25065.54</cell></row><row><cell></cell><cell></cell><cell cols="4">07NearCO2</cell><cell></cell><cell>single</cell><cell cols="2">31.28</cell><cell>10.35</cell><cell>50</cell><cell>604.08</cell><cell>18096.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>paired</cell><cell>35.7</cell><cell></cell><cell>11.44</cell><cell>59.5</cell><cell>499.68</cell><cell>23160.18</cell></row><row><cell></cell><cell cols="5">08LongCO2</cell><cell></cell><cell>single</cell><cell cols="2">29.27</cell><cell>10</cell><cell>49</cell><cell>618.23</cell><cell>16119.56</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>paired</cell><cell cols="2">29.89</cell><cell>9.61</cell><cell>61.5</cell><cell>480.47</cell><cell>22533.31</cell></row><row><cell></cell><cell cols="6">09NearPM25</cell><cell>single</cell><cell cols="2">28.56</cell><cell>9.82</cell><cell>53</cell><cell>547.81</cell><cell>18741.36</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>paired</cell><cell cols="2">34.78</cell><cell>10.52</cell><cell>70</cell><cell>538.77</cell><cell>24338.31</cell></row><row><cell></cell><cell cols="7">10LongPM25 single</cell><cell>28.3</cell><cell></cell><cell>9.77</cell><cell>53</cell><cell>564.06</cell><cell>17442.67</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>paired</cell><cell cols="2">30.67</cell><cell>9.84</cell><cell>66</cell><cell>544.2</cell><cell>24325.92</cell></row><row><cell cols="2">Standard deviation</cell><cell></cell><cell></cell><cell>01NearT</cell><cell></cell><cell></cell><cell>single</cell><cell>4.24</cell><cell></cell><cell>1</cell><cell>14.87</cell><cell>178.84</cell><cell>7057.84</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>paired</cell><cell>9.24</cell><cell></cell><cell>2.08</cell><cell>31.57</cell><cell>185.06</cell><cell>11756.56</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>02LongT</cell><cell></cell><cell></cell><cell>single</cell><cell>4.91</cell><cell></cell><cell>0.93</cell><cell>15.38</cell><cell>202.59</cell><cell>7193.23</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>paired</cell><cell cols="2">14.14</cell><cell>3.46</cell><cell>32.58</cell><cell>212.17</cell><cell>13980.13</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">03NearSST</cell><cell></cell><cell>single</cell><cell>3.55</cell><cell></cell><cell>0.81</cell><cell>15.38</cell><cell>204.64</cell><cell>6394.38</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>paired</cell><cell>7.78</cell><cell></cell><cell>0.98</cell><cell>25.8</cell><cell>140.5</cell><cell>10314.88</cell></row><row><cell></cell><cell></cell><cell cols="4">04LongSST</cell><cell></cell><cell>single</cell><cell>5.16</cell><cell></cell><cell>1.09</cell><cell>17.21</cell><cell>260.58</cell><cell>7039.09</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>paired</cell><cell cols="2">12.61</cell><cell>2.25</cell><cell>33.83</cell><cell>179.76</cell><cell>14605.98</cell></row><row><cell></cell><cell></cell><cell cols="4">05NearSLR</cell><cell></cell><cell>single</cell><cell>3.77</cell><cell></cell><cell>0.95</cell><cell>16.1</cell><cell>151.14</cell><cell>6347.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>paired</cell><cell>6.49</cell><cell></cell><cell>2.6</cell><cell>20.78</cell><cell>128.3</cell><cell>8254.83</cell></row><row><cell></cell><cell></cell><cell cols="4">06LongSLR</cell><cell></cell><cell>single</cell><cell>3.89</cell><cell></cell><cell>0.96</cell><cell>15.99</cell><cell>130.97</cell><cell>6146.61</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>paired</cell><cell cols="2">10.87</cell><cell>3</cell><cell>25.52</cell><cell>174.3</cell><cell>9276.27</cell></row><row><cell></cell><cell></cell><cell cols="4">07NearCO2</cell><cell></cell><cell>single</cell><cell>6.09</cell><cell></cell><cell>1.83</cell><cell>19.75</cell><cell>186.05</cell><cell>8454.14</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>paired</cell><cell cols="2">13.65</cell><cell>2.33</cell><cell>33.19</cell><cell>172.93</cell><cell>13751.54</cell></row><row><cell></cell><cell cols="5">08LongCO2</cell><cell></cell><cell>single</cell><cell>3.93</cell><cell></cell><cell>0.69</cell><cell>15.01</cell><cell>255.79</cell><cell>5553.42</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>paired</cell><cell cols="2">10.14</cell><cell>1.7</cell><cell>25.85</cell><cell>153.5</cell><cell>10765.45</cell></row><row><cell></cell><cell cols="6">09NearPM25</cell><cell>single</cell><cell>6.54</cell><cell></cell><cell>1.69</cell><cell>16.34</cell><cell>136.11</cell><cell>5645.24</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>paired</cell><cell>9.19</cell><cell></cell><cell>2.17</cell><cell>23.3</cell><cell>146.02</cell><cell>12079.2</cell></row><row><cell></cell><cell cols="7">10LongPM25 single</cell><cell>4.79</cell><cell></cell><cell>1.24</cell><cell>14.98</cell><cell>306.07</cell><cell>7041.12</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>paired</cell><cell>5.75</cell><cell></cell><cell>0.72</cell><cell>22.73</cell><cell>194.62</cell><cell>9752.87</cell></row><row><cell cols="12">Following the analysis presented in previous tables, this table further breaks down the same five gaze metrics by</cell></row><row><cell cols="12">both viewing conditions (single and paired) and individual map stimuli (10 in total). To conserve space, only means</cell></row></table><note><p>This table extends the analysis of cumulative gaze metrics by breaking down the data into either two projection timeframes or five projection types. It presents the same five metrics using mean (M), median, standard deviation (SD), and standard error of the mean (SEMs) without distinguishing between single or paired viewing conditions for brevity. Top (a) As projection timeframe breakdown, metrics are categorised into short-term (2021-2040) and long-term (2081-2100) projections. The observation count for each category is based on five times the total sample size (NSample = 47), resulting in NObservations = 235 for each timeframe, as in each cell value. Bottom (b) As projection type breakdown, metrics are detailed for five types of projections: main temperature (T), sea surface temperature (SST), sea level rise (SLR), anthropogenic CO2 emissions (CO2), and fine particulate matter PM2.5 (PM25). and SEMs are reported: For the single viewing condition, each cell data is derived from a sample size of NSingle = 35. For the paired viewing condition, each cell data is based on a sample size of NPaired = 12.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table. Gaze metrics for paintings.</head><label></label><figDesc>This table presents descriptive statistics for five gaze metrics observed while viewing paintings: total fixation duration (in seconds and as a percentage of total viewing time), fixation count, average fixation duration (in milliseconds), and total proxy scanpath length in pixels. The metrics are reported with the following statistics for each: observation size (N), mean (M), standard deviation (SD), standard error of the mean (SEM), minimum (Min), and maximum (Max) values. The data (NPaintingStimulus = 2) are segmented into single (NSingleSample = 35) and paired (NPairedSample = 12) viewing conditions, allowing for a direct comparison of these metrics under different social viewing contexts.</figDesc><table><row><cell>Descriptives</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Condition</cell><cell>Total fixation duration in s</cell><cell>Normalised fixation duration in percentage</cell><cell>Fixation count</cell><cell>Average fixation duration in ms</cell><cell>Scanpath length in px</cell></row><row><cell>N</cell><cell>single</cell><cell>70</cell><cell>70</cell><cell>70</cell><cell>70</cell><cell>70</cell></row><row><cell></cell><cell>paired</cell><cell>24</cell><cell>24</cell><cell>24</cell><cell>24</cell><cell>24</cell></row><row><cell>Mean</cell><cell>single</cell><cell>56.97</cell><cell>0.50</cell><cell>154.87</cell><cell>380.51</cell><cell>24196.75</cell></row><row><cell></cell><cell>paired</cell><cell>72.09</cell><cell>0.50</cell><cell>216.88</cell><cell>359.95</cell><cell>37468.70</cell></row><row><cell>Std. error mean</cell><cell>single</cell><cell>2.40</cell><cell>0.01</cell><cell>7.07</cell><cell>11.41</cell><cell>1205.88</cell></row><row><cell></cell><cell>paired</cell><cell>5.30</cell><cell>0.02</cell><cell>20.27</cell><cell>18.68</cell><cell>3332.50</cell></row><row><cell>Median S7 Table.</cell><cell>single</cell><cell>56.78</cell><cell>0.50</cell><cell>153.50</cell><cell>357.63</cell><cell>21697.80</cell></row><row><cell></cell><cell>paired</cell><cell>65.30</cell><cell>0.50</cell><cell>219.50</cell><cell>341.29</cell><cell>36599.86</cell></row><row><cell>Standard deviation</cell><cell>single</cell><cell>20.10</cell><cell>0.07</cell><cell>59.19</cell><cell>95.50</cell><cell>10089.12</cell></row><row><cell></cell><cell>paired</cell><cell>25.97</cell><cell>0.08</cell><cell>99.29</cell><cell>91.53</cell><cell>16325.86</cell></row><row><cell>Minimum</cell><cell>single</cell><cell>15.34</cell><cell>0.36</cell><cell>70</cell><cell>216.00</cell><cell>10718.39</cell></row><row><cell></cell><cell>paired</cell><cell>41.40</cell><cell>0.36</cell><cell>75</cell><cell>236.78</cell><cell>11845.43</cell></row><row><cell>Maximum</cell><cell>single</cell><cell>106.25</cell><cell>0.64</cell><cell>307</cell><cell>790.08</cell><cell>63731.17</cell></row><row><cell></cell><cell>paired</cell><cell>118.51</cell><cell>0.64</cell><cell>434</cell><cell>578.53</cell><cell>71705.66</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Nonparametric ANOVA (Kruskal-Wallis test) for paintings, comparing single and paired viewing conditions.</head><label></label><figDesc>This table details the results of a nonparametric ANOVA (Kruskal-Wallis test) used to analyse the differences in gaze metrics between single and paired viewing conditions for paintings. Despite the exploratory nature of the research and the relatively low sample sizes (N = 35 for single and N = 12 for paired), significant differences were observed only in total fixation duration, and proxy scanpath length, but not in fixation count or average fixation duration. The table provides a detailed breakdown of the test results for each metric, including the chi-squared (χ 2 ) values, degrees of freedom (df), p-values, and effect sizes (ε 2 ). Note, although there is not a consensus on effect size interpretation, ε 2 &lt; .08 may be assumed to be small, ε 2 &lt; .26 assumed to be medium, ε 2 &gt;= .26 assumed to be large.</figDesc><table><row><cell>Kruskal-Wallis</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>χ²</cell><cell>df</cell><cell>p</cell><cell>ε²</cell></row><row><cell>Total fixation duration in s</cell><cell>6.06</cell><cell>1</cell><cell>0.014</cell><cell>0.07</cell></row><row><cell>Normalised fixation duration in percentage</cell><cell>0.00</cell><cell>1</cell><cell>1.000</cell><cell>0.00</cell></row><row><cell>Fixation count</cell><cell>6.54</cell><cell>1</cell><cell>0.011</cell><cell>0.07</cell></row><row><cell>Average fixation duration in ms</cell><cell>1.37</cell><cell>1</cell><cell>0.242</cell><cell>0.01</cell></row><row><cell>Scanpath length in px</cell><cell>13.20</cell><cell>1</cell><cell>&lt; .001</cell><cell>0.14</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>S11 Table. Correlations between gaze metrics and total CCAS score.</head><label></label><figDesc>This table extends the analysis from the previous correlation table by examining the relationship between the total Climate Change Anxiety Scale</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>This table serves as a supplementary, proof-of-concept exploration of fixation count, fixation duration (in seconds),</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>and relative dwell time (in percentage) between two AOIs. The stimulus was divided into two parts as two AOIs:</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>the top part covering the world map, and the bottom part covering the scale and its accompanying brief</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3"><p>explanatory text. For stimuli displayed on a screen size of 1920 × 1080 pixels, the division line was set at the 767th</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_4"><p>horizontal pixel, marking the boundary between the map and scale areas. The table is organised into three sections:</p></note>
		</body>
		<back>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>S3 Fig. Data recording setup. Top-left (a): For gaze data collection, the mobile eye-tracker (Pupil Invisible) was attached to a companion recording device (OnePlus 8 Android phone) running the Pupil Invisible Companion App. Gaze data were collected at 200 Hz with a resolution of 192 × 192 pixels using two near-eye cameras with infrared illumination. Scene videos were recorded at 30 Hz with a resolution of 1088 × 1080 pixels, roughly corresponding to an 82° × 82° field of vision. Top-right (b): The laptop used (Dell Alienware 17 -2019) displayed the maps and collected responses for the climate scale task and exit questionnaire. It featured a screen resolution of 1920 × 1080 pixels and a diagonal size of 17.3 inches (~44 cm), set in a 16:9 aspect ratio (~38 × 21.5 cm display dimensions).</p><p>Although participants sat at a regular work desk, the distance from screen to participant varied and was not controlled; however, it is approximately noted that at an average distance of 57 cm, 1 cm on the screen roughly equates to 1 degree of visual angle. Bottom (c): Two artwork posters were displayed side by side in the lab, colour-printed on A0 papers (841 × 1189 mm) with a white border and surrounded by six April tags for positional referencing. For further technical details on April tags and their implementations in the Pupil Cloud, Pupil Cloud data streams, marker mapper and reference image mapper enrichments, see the following resources: https://april.eecs.umich.edu/software/apriltag https://docs.pupil-labs.com/core/software/pupil-capture/#markers https://docs.pupil-labs.com/invisible/basic-concepts/data-streams/ https://docs.pupil-labs.com/enrichments/marker-mapper/ https://docs.pupil-labs.com/enrichments/reference-image-mapper/</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>[39] Van Der Linden S, Weber EU. Editorial overview: Can behavioral science solve the climate crisis? Curr Opin Behav Sci 2021;42:iii-viii. https://doi.org/10.1016/j.cobeha.2021.09.001.</p><p>[40] Vlasceanu M, Doell KC, Bak-Coleman JB, Todorova B, Berkebile-Weinberg MM, Grayson SJ, et al.</p><p>Addressing climate change with behavioral science: A global intervention tournament in 63 countries. Sci Adv 2024;10:eadj5778. https://doi.org/10.1126/sciadv.adj5778.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Supporting information</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>This frequency table presents the aggregated responses from the Climate Change Anxiety Scale (refer to</head><p>Supplementary Material 1). The survey was conducted on a 5-point Likert scale across 22 items and presented here irrespective of single or paired viewing conditions. Responses are colour-coded from black to white, corresponding to the frequency from "never" to "almost always", as shown in the legend below the table. The scale includes four sub-scales, each associated with a set of questions: cognitive-emotional impairment (questions 1-8), functional impairment (questions 9-13), personal experience of climate change (questions 14-16), and behavioural engagement (questions <ref type="bibr">[17]</ref><ref type="bibr" target="#b18">[18]</ref><ref type="bibr">[19]</ref><ref type="bibr">[20]</ref><ref type="bibr">[21]</ref><ref type="bibr">[22]</ref>. These sub-scales are labelled on the left side of the frequency plot. Note that NSample = 44, as responses from three participants were not recorded.  Initially, responses on the 5-point Likert scale ranging from "never" to "almost always" were treated as ordinal data, with values from 0 to 4 assigned to each response. Subsequently, the sum of scores for each subscale was calculated. Correlations between the subscales were analysed using both Spearman's rho and Kendall </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cognitive and psychological science insights to improve climate change data visualization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Harold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Lorenzoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Shipley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Coventry</surname></persName>
		</author>
		<idno type="DOI">10.1038/nclimate3162</idno>
		<ptr target="https://doi.org/10.1038/nclimate3162" />
	</analytic>
	<monogr>
		<title level="j">Nat Clim Change</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1080" to="1089" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cartographic content analysis of compelling climate change communication</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Fish</surname></persName>
		</author>
		<idno type="DOI">10.1080/15230406.2020.1774421</idno>
		<ptr target="https://doi.org/10.1080/15230406.2020.1774421" />
	</analytic>
	<monogr>
		<title level="j">Cartogr Geogr Inf Sci</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="492" to="507" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Co-designing engaging and accessible data visualisations: a case study of the IPCC reports</title>
		<author>
			<persName><forename type="first">A</forename><surname>Morelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pidcock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Harold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gomis</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10584-021-03171-4</idno>
		<ptr target="https://doi.org/10.1007/s10584-021-03171-4" />
	</analytic>
	<monogr>
		<title level="j">Clim Change</title>
		<imprint>
			<biblScope unit="volume">168</biblScope>
			<biblScope unit="page">26</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The visual framing of climate change impacts and adaptation in the IPCC assessment reports</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wardekker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lorenz</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10584-019-02522-6</idno>
		<ptr target="https://doi.org/10.1007/s10584-019-02522-6" />
	</analytic>
	<monogr>
		<title level="j">Clim Change</title>
		<imprint>
			<biblScope unit="volume">156</biblScope>
			<biblScope unit="page" from="273" to="292" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Eye Movement Studies In Cartography And Related Fields</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Steinke</surname></persName>
		</author>
		<idno type="DOI">10.3138/J166-635U-7R56-X2L1</idno>
		<ptr target="https://doi.org/10.3138/J166-635U-7R56-X2L1" />
	</analytic>
	<monogr>
		<title level="j">Cartogr Int J Geogr Inf Geovisualization</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="40" to="73" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Using eye tracking to evaluate the usability of animated maps</title>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11430-013-4685-3</idno>
		<ptr target="https://doi.org/10.1007/s11430-013-4685-3" />
	</analytic>
	<monogr>
		<title level="j">Sci China Earth Sci</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="512" to="522" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Study of the attentive behavior of novice and expert map users using eye tracking</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ooms</surname></persName>
		</author>
		<author>
			<persName><forename type="first">De</forename><surname>Maeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename></persName>
		</author>
		<idno type="DOI">10.1080/15230406.2013.860255</idno>
		<ptr target="https://doi.org/10.1080/15230406.2013.860255" />
	</analytic>
	<monogr>
		<title level="j">Cartogr Geogr Inf Sci</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="37" to="54" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Where Am I? Investigating Map Matching During Self-Localization With Mobile Eye Tracking in an Urban Environment</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kiefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Giannopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raubal</surname></persName>
		</author>
		<idno type="DOI">10.1111/tgis.12067</idno>
		<ptr target="https://doi.org/10.1111/tgis.12067" />
	</analytic>
	<monogr>
		<title level="j">Trans GIS</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="660" to="686" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">User performance and reading strategies for metro maps: An eye tracking study</title>
		<author>
			<persName><forename type="first">R</forename><surname>Netzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ohlhausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kurzhals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Woods</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<idno type="DOI">10.1080/13875868.2016.1226839</idno>
		<ptr target="https://doi.org/10.1080/13875868.2016.1226839" />
	</analytic>
	<monogr>
		<title level="j">Spat Cogn Comput</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="39" to="64" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">FeaturEyeTrack: automatic matching of eye tracking data with map features on interactive maps</title>
		<author>
			<persName><forename type="first">F</forename><surname>Göbel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kiefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raubal</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10707-019-00344-3</idno>
		<ptr target="https://doi.org/10.1007/s10707-019-00344-3" />
	</analytic>
	<monogr>
		<title level="j">GeoInformatica</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="663" to="687" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Measuring the influence of map label density on perceived complexity: a user study using eye tracking</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Meng</surname></persName>
		</author>
		<idno type="DOI">10.1080/15230406.2018.1434016</idno>
		<ptr target="https://doi.org/10.1080/15230406.2018.1434016" />
	</analytic>
	<monogr>
		<title level="j">Cartogr Geogr Inf Sci</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="210" to="227" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploring the Cognitive Load of Expert and Novice Map Users Using EEG and Eye Tracking</title>
		<author>
			<persName><forename type="first">M</forename><surname>Keskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ooms</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">O</forename><surname>Dogru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">De</forename><surname>Maeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
		<idno type="DOI">10.3390/ijgi9070429</idno>
		<ptr target="https://doi.org/10.3390/ijgi9070429" />
	</analytic>
	<monogr>
		<title level="j">ISPRS Int J Geo-Inf</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">429</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cartograms Facilitate Communication of Climate Change Risks and Responsibilities</title>
		<author>
			<persName><forename type="first">P</forename><surname>Döll</surname></persName>
		</author>
		<idno type="DOI">10.1002/2017EF000677</idno>
		<ptr target="https://doi.org/10.1002/2017EF000677" />
	</analytic>
	<monogr>
		<title level="j">Earths Future</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1182" to="1195" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Towards more effective visualisations in climate services: good practices and recommendations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Terrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Calvo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Christel</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10584-022-03365-4</idno>
		<ptr target="https://doi.org/10.1007/s10584-022-03365-4" />
	</analytic>
	<monogr>
		<title level="j">Clim Change</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The interpretation of IPCC probabilistic statements around the world</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Budescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H-H</forename><surname>Por</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Broomell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Smithson</surname></persName>
		</author>
		<idno type="DOI">10.1038/nclimate2194</idno>
		<ptr target="https://doi.org/10.1038/nclimate2194" />
	</analytic>
	<monogr>
		<title level="j">Nat Clim Change</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="508" to="512" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">People can understand IPCC visuals and are not influenced by colors</title>
		<author>
			<persName><forename type="first">V</forename><surname>Battocletti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sotis</surname></persName>
		</author>
		<idno type="DOI">10.1088/1748-9326/acfb95</idno>
		<ptr target="https://doi.org/10.1088/1748-9326/acfb95" />
	</analytic>
	<monogr>
		<title level="j">Environ Res Lett</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">114036</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The Feeling of Red and Blue-A Constructive Critique of Color Mapping in Visual Climate Change Communication</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T ;</forename><surname>Nocke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Leal Filho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Manolas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Azul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">M</forename><surname>Azeiteiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mcghie</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-70066-3_19</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-70066-3_19" />
	</analytic>
	<monogr>
		<title level="m">Handb</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="289" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The misuse of colour in science communication</title>
		<author>
			<persName><forename type="first">F</forename><surname>Crameri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Shephard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Heron</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41467-020-19160-7</idno>
		<ptr target="https://doi.org/10.1038/s41467-020-19160-7" />
	</analytic>
	<monogr>
		<title level="j">Nat Commun</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">5444</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Original Paintings versus Slide and Computer Reproductions: A Comparison of Viewer Responses</title>
		<author>
			<persName><forename type="first">P</forename><surname>Locher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.2190/R1WN-TAF2-376D-EFUH</idno>
		<ptr target="https://doi.org/10.2190/R1WN-TAF2-376D-EFUH" />
	</analytic>
	<monogr>
		<title level="j">Empir Stud Arts</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="121" to="129" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The Experience of Art in Museums: An Attempt to Dissociate the Role of Physical Context and Genuineness</title>
		<author>
			<persName><forename type="first">D</forename><surname>Brieber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Leder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nadal</surname></persName>
		</author>
		<idno type="DOI">10.1177/0276237415570000</idno>
		<ptr target="https://doi.org/10.1177/0276237415570000" />
	</analytic>
	<monogr>
		<title level="j">Empir Stud Arts</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="95" to="105" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">In the white cube: Museum context enhances the valuation and memory of art</title>
		<author>
			<persName><forename type="first">D</forename><surname>Brieber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nadal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Leder</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.actpsy.2014.11.004</idno>
		<ptr target="https://doi.org/10.1016/j.actpsy.2014.11.004" />
	</analytic>
	<monogr>
		<title level="j">Acta Psychol (Amst)</title>
		<imprint>
			<biblScope unit="volume">154</biblScope>
			<biblScope unit="page" from="36" to="42" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Effects of Context and Genuineness in the Experience of Art</title>
		<author>
			<persName><forename type="first">S</forename><surname>Grüner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Specker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Leder</surname></persName>
		</author>
		<idno type="DOI">10.1177/0276237418822896</idno>
		<ptr target="https://doi.org/10.1177/0276237418822896" />
	</analytic>
	<monogr>
		<title level="j">Empir Stud Arts</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="138" to="152" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The display makes a difference: A mobile eye tracking study on the perception of art before and after a museum&apos;s rearrangement</title>
		<author>
			<persName><forename type="first">L</forename><surname>Reitstätter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Brinkmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Santini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Specker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bakondi</surname></persName>
		</author>
		<idno type="DOI">10.16910/jemr.13.2.6</idno>
		<ptr target="https://doi.org/10.16910/jemr.13.2.6" />
	</analytic>
	<monogr>
		<title level="j">J Eye Mov Res</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Similarity of gaze patterns across physical and virtual versions of an installation artwork</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gulhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Durant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Zanker</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41598-021-91904-x</idno>
		<ptr target="https://doi.org/10.1038/s41598-021-91904-x" />
	</analytic>
	<monogr>
		<title level="j">Sci Rep</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">18913</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">How are real artworks and reproductions judged? The role of anchoring in empirical investigations of the genuineness effect</title>
		<author>
			<persName><forename type="first">E</forename><surname>Specker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Arató</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Leder</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jesp.2023.104494</idno>
		<ptr target="https://doi.org/10.1016/j.jesp.2023.104494" />
	</analytic>
	<monogr>
		<title level="j">J Exp Soc Psychol</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page">104494</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The unconscious emotions that art evokes: Neuroscience research into the impact of a museum visit</title>
		<author>
			<persName><forename type="first">Mauritshuis</forename><surname>Museum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neurofactor</forename><surname>Neurensics</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>Final Report</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Iturbide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Gutiérrez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bedia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cimadevilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Díez-Sierra</surname></persName>
		</author>
		<idno type="DOI">10.5281/ZENODO.3691645</idno>
		<ptr target="https://doi.org/10.5281/ZENODO.3691645" />
		<title level="m">Repository supporting the implementation of FAIR principles in the IPCC-WGI Atlas 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Climate Change 2021 -The Physical Science Basis: Working Group I Contribution to the Sixth Assessment Report of the Intergovernmental Panel on Climate Change. 1st ed</title>
		<idno type="DOI">10.1017/9781009157896</idno>
		<ptr target="https://doi.org/10.1017/9781009157896" />
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
	<note>Intergovernmental Panel On Climate Change (Ipcc)</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Development and validation of a measure of climate change anxiety</title>
		<author>
			<persName><forename type="first">S</forename><surname>Clayton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Karazsia</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jenvp.2020.101434</idno>
		<ptr target="https://doi.org/10.1016/j.jenvp.2020.101434" />
	</analytic>
	<monogr>
		<title level="j">J Environ Psychol</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page">101434</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Eye activity as a measure of human mental effort in HCI</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Epps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1145/1943403.1943454</idno>
		<ptr target="https://doi.org/10.1145/1943403.1943454" />
	</analytic>
	<monogr>
		<title level="m">Proc. 16th Int. Conf. Intell. User Interfaces</title>
		<meeting>16th Int. Conf. Intell. User Interfaces<address><addrLine>Palo Alto CA USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="315" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Co-perceiving: Bringing the social into perception</title>
		<author>
			<persName><forename type="first">O</forename><surname>Deroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Longin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bahrami</surname></persName>
		</author>
		<idno type="DOI">10.1002/wcs.1681</idno>
		<ptr target="https://doi.org/10.1002/wcs.1681" />
	</analytic>
	<monogr>
		<title level="j">WIREs Cogn Sci</title>
		<imprint>
			<biblScope unit="page">1681</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A comparison of the performance on extrinsic and intrinsic cartographic visualizations through correctness, response time and cognitive processing</title>
		<author>
			<persName><forename type="first">Č</forename><surname>Šašinka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Stachoň</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Čeněk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Šašinková</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Popelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ugwitz</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0250164</idno>
		<ptr target="https://doi.org/10.1371/journal.pone.0250164" />
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">250164</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Seeing is believing: Climate change graph design and user judgments of credibility, usability, and risk</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Courtney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Mcneal</surname></persName>
		</author>
		<idno type="DOI">10.1130/GES02517.1</idno>
		<ptr target="https://doi.org/10.1130/GES02517.1" />
	</analytic>
	<monogr>
		<title level="j">Geosphere</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1508" to="1527" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">What Makes a Visualization Memorable?</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Borkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Bylinskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2013.234</idno>
		<ptr target="https://doi.org/10.1109/TVCG.2013.234" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Vis Comput Graph</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="2306" to="2315" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Communicating Local Climate Risks Online Through an Interactive Data Visualization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Herring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Melton</surname></persName>
		</author>
		<idno type="DOI">10.1080/17524032.2016.1176946</idno>
		<ptr target="https://doi.org/10.1080/17524032.2016.1176946" />
	</analytic>
	<monogr>
		<title level="j">Environ Commun</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="90" to="105" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Staying over-optimistic about the future: Uncovering attentional biases to climate change messages</title>
		<author>
			<persName><forename type="first">G</forename><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marselle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mcguire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Litchfield</surname></persName>
		</author>
		<idno type="DOI">10.1515/sem-2016-0074</idno>
		<ptr target="https://doi.org/10.1515/sem-2016-0074" />
	</analytic>
	<monogr>
		<title level="j">Semiotica</title>
		<imprint>
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page" from="21" to="64" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Motivated Attention in Climate Change Perception and Action</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2019.01541</idno>
		<ptr target="https://doi.org/10.3389/fpsyg.2019.01541" />
	</analytic>
	<monogr>
		<title level="j">Front Psychol</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">1541</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attentional and perceptual biases of climate change</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cobeha.2021.02.010</idno>
		<ptr target="https://doi.org/10.1016/j.cobeha.2021.02.010" />
	</analytic>
	<monogr>
		<title level="m">S1 File. The Climate Change Anxiety Scale (CCAS)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="22" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Development and validation of a measure of climate change anxiety</title>
		<author>
			<persName><forename type="first">S</forename><surname>Clayton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Karazsia</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jenvp.2020.101434</idno>
		<ptr target="https://doi.org/10.1016/j.jenvp.2020.101434" />
	</analytic>
	<monogr>
		<title level="j">Journal of Environmental Psychology</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page">101434</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">The Climate Change Anxiety Scale is a 22-item measure of the emotional response to climate change. The measure has four sub-scales including cognitive and emotional impairment, functional impairment, personal experience of climate change, and behavioral engagement. The scale has been validated in the U.S. ITEMS: Please rate how often the following statements are true of you. Cognitive-emotional impairment</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Thinking about climate change makes it difficult for me to concentrate</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Thinking about climate change makes it difficult for me to sleep</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m">I have nightmares about climate change</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">I find myself crying because of climate change</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">why can&apos;t I handle climate change better</title>
		<author>
			<persName><surname>Think</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m">I go away by myself and think about why I feel this way about climate change</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">I write down my thoughts about climate change and analyze them</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">why do I react to climate change this way?</title>
		<author>
			<persName><surname>Think</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Functional impairment</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m">I have problems balancing my concerns about sustainability with the needs of my family</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">My friends say I think about climate change too much. Personal experience of climate change</title>
		<imprint/>
	</monogr>
	<note>Subscale 3</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m">I have been directly affected by climate change</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">I know someone who has been directly affected by climate change</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">I have noticed a change in a place that is important to me due to climate change. Behavioral engagement</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">I wish I behaved more sustainably</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m">I turn off lights</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">I try to reduce my behaviors that contribute to climate change</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m">I feel guilty if I waste energy</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">I believe I can do something to help address the problem of climate change. Response Options: Never -1 Rarely -2 Sometimes -3</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Often -4 Almost</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
