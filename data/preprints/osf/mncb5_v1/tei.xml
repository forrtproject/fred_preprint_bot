<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Is Complexity an Illusion?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Michael</forename><forename type="middle">Timothy</forename><surname>Bennett</surname></persName>
							<email>michael.bennett@anu.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">The Australian National University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Is Complexity an Illusion?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">82F5BD7BA80803BC3C2504951D849ECF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-23T06:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>complexity</term>
					<term>weakness</term>
					<term>causality</term>
					<term>AGI</term>
					<term>information theory We build upon previous work [18, 16], in which:</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Simplicity is held by many to be the key to general intelligence. Simpler models tend to "generalise", identifying the cause or generator of data with greater sample efficiency. The implications of the correlation between simplicity and generalisation extend far beyond computer science, addressing questions of physics and even biology. Yet simplicity is a property of form, while generalisation is of function. In interactive settings, any correlation between the two depends on interpretation. In theory there could be no correlation and yet in practice, there is. Previous theoretical work showed generalisation to be a consequence of "weak" constraints implied by function, not form. Experiments demonstrated choosing weak constraints over simple forms yielded a 110 -500% improvement in generalisation rate. Here we show that all constraints can take equally simple forms, regardless of weakness. However if forms are spatially extended, then function is represented using a finite subset of forms. If function is represented using a finite subset of forms, then we can force a correlation between simplicity and generalisation by making weak constraints take simple forms. If function is determined by a goal directed process that favours versatility (e.g. natural selection), then efficiency demands weak constraints take simple forms. Complexity has no causal influence on generalisation, but appears to due to confounding.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Complexity is a quality of systems we find difficult to understand. Formal analogues include entropy <ref type="bibr" target="#b0">[1]</ref>, compression <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> and even fractal dimension <ref type="bibr" target="#b3">[4]</ref>. Physicist Leonard Susskind believes complexity may be the key to a unified theory of physics <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. Cyberneticist Francis Heylighen recently argued that goals are attractors of dynamical systems that self organise in complex reaction networks <ref type="bibr" target="#b6">[7]</ref>. If complex reaction networks do self organise as he argues <ref type="bibr" target="#b7">[8]</ref>, then it goes some way towards explaining the origins of goal directed behaviour, and thus life. Finally, many hold that complexity is the key to general intelligence <ref type="bibr" target="#b8">[9]</ref>. Language models like GPT-4 amount to compressed representations of human language <ref type="bibr" target="#b9">[10]</ref>. Simpler objects can be compressed to greater extents, because they exhibit self similarity. As a result, some hold that compression is general intelligence, meaning a general reinforcement learning agent like AIXI <ref type="bibr" target="#b10">[11]</ref> can use Solomonoff Induction <ref type="bibr" target="#b11">[12]</ref> to maximise expected reward across a wide range of environments. Yet for all that complexity seems to be at the heart of every matter, it has profound flaws. As a qualitative indicator of how subjectively difficult a system might be to understand, it makes perfect sense. It makes far less as an indicator of anything objective. Ockham's Razor is the epistemic principle that simpler statements are more likely to hold true. It can be understood as the claim that our subjective perceptions of complexity reflect an objective property of our environment. There is no obvious reason this should be the case, and yet it is <ref type="bibr" target="#b12">[13]</ref>. Simpler statements tend to be more accurate representations of reality. The aforementioned Solomonoff Induction formalises Ockham's Razor, meaning AIXI is based on the premise that simpler models are more accurate depictions of the environment than complex models of seemingly equal predictive power. AIXI is a superintelligence in the sense that it maximises Legg-Hutter intelligence <ref type="bibr" target="#b8">[9]</ref>. However, Jan Leike later showed that any claim regarding AIXI's performance is "entirely subjective" <ref type="bibr" target="#b13">[14]</ref>. Legg-Hutter intelligence is measured with respect to a fixed Universal Turing Machine (UTM), and AIXI is only optimal if it uses exactly the same UTM. This calls into question the viability of complexity based induction systems in interactive settings. Their performance is subjective, and from a pragmatic standpoint it is only objective performance claims that matter. Leike's result suggests there could be no correlation between objective performance and subjective complexity. This concurs with what seems intuitively obvious, that complexity is an aspect of interpretation. Complexity is a measure of form, not function. So why does the subjective perception of simplicity tend to correlate with objective performance?</p><p>What exactly is complexity supposed to indicate?: As it is used in AGI research <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15]</ref>, complexity is intended to help us infer the program which generated or caused data. If one can identify that which caused past data, then one can "generalise" to predict the outcomes in future interactions, to maximise performance <ref type="bibr" target="#b15">[16]</ref>. We are concerned with adaptation or "the ability to generalise" <ref type="bibr" target="#b16">[17]</ref>, not any specific circumstance. This is because any system can eventually identify cause given enough data and memory (by simply rote learning every outcome until it has a complete behavioural specification of the causal program). So assuming one can correctly infer cause, then we claim that the amount of data one requires to do so is the sole measure of performance<ref type="foot" target="#foot_0">foot_0</ref> . We refer to this as sample efficiency. The more sample efficiently one can infer cause, the greater one's ability to generalise and adapt to any desired end. Thus, we take intelligence to be a measure of the sample efficiency in generalisation.</p><p>1. Maximising simplicity of policies was proven unnecessary and insufficient to maximise sample efficiency <ref type="bibr">[18, prop. 3</ref>]. 2. Maximising policy "weakness" was proven necessary and sufficient to maximise sample efficiency <ref type="bibr">[18, prop. 1, 2]</ref> and identify cause <ref type="bibr" target="#b15">[16]</ref>. In experiments, weak policies outperformed simple by 110 -500%.</p><p>Our purpose here is to extend this work, and to establish:</p><p>1. Is complexity just an artefact of abstraction? 2. Why do sample efficiency and simplicity tend to be correlated?</p><p>Results: We begin by presenting a formalism. Our results are only meaningful if one accepts that our formalism is reflective of reality, so we provide an argument to the effect that it is (lest we be accused of straw-manning complexity). Second, we show that the complexity of all behaviours is equal in the absence of an abstraction layer (a general formalisation of any interpreter). In other words, complexity is a subjective "illusion". We further show that if the vocabulary is finite then weakness can confound simplicity and sample efficiency. Third, we argue that abstraction is goal directed. If the vocabulary is finite, and tasks uniformly distributed, then weak statements take simple forms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Formalism</head><p>The following definitions are shared with <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>, which apply them to biological and philosophical perspectives.</p><p>Definition 1 (environment).</p><p>-We assume a set Φ whose elements we call states.</p><p>-A declarative program is f ⊆ Φ, and we write P for the set of all declarative programs (the powerset of Φ). -By a truth or fact about a state ϕ, we mean f ∈ P such that ϕ ∈ f .</p><p>-By an aspect of a state ϕ we mean a set l of facts about ϕ s.t. ϕ ∈ l. By an aspect of the environment we mean an aspect l of any state, s.t. l ̸ = ∅. We say an aspect of the environment is realised<ref type="foot" target="#foot_1">foot_1</ref> by state ϕ if it is an aspect of ϕ.</p><p>Definition 2 (abstraction layer).</p><p>-We single out a subset v ⊆ P which we call the vocabulary of an abstraction layer. If v = P , then we say that there is no abstraction.</p><formula xml:id="formula_0">-Lv = {l ⊆ v : l ̸ = ∅} is a set of aspects in v.</formula><p>We call Lv a formal language, and l ∈ Lv a statement. -We say a statement is true given a state iff it is an aspect realised by that state.</p><p>-A completion of a statement x is a statement y which is a superset of x. If y is true, then x is true.</p><p>-The extension of a statement</p><formula xml:id="formula_1">3 x ∈ Lv is Ex = {y ∈ Lv : x ⊆ y}. Ex is the set of all completions of x. -The extension of a set of statements X ⊆ Lv is EX = x∈X Ex.</formula><p>-We say x and y are equivalent iff Ex = Ey.</p><p>(notation) E with a subscript is the extension of the subscript<ref type="foot" target="#foot_3">foot_3</ref> .</p><p>(intuitive summary) L v is everything which can be realised in this abstraction layer. The extension E x of a statement x is the set of all statements whose existence implies x, and so it is like a truth table.</p><p>Definition 3 (v-task). For a chosen v, a task α is a pair ⟨I α , O α ⟩ where:</p><p>-Iα ⊂ Lv is a set whose elements we call inputs of α.</p><p>-Oα ⊂ EI α is a set whose elements we call correct outputs of α.</p><p>I α has the extension E Iα we call outputs, and O α are outputs deemed correct. Γ v is the set of all tasks given v.</p><formula xml:id="formula_2">(generational hierarchy) A v-task α is a child of v-task ω if I α ⊂ I ω and O α ⊆ O ω . This is written as α ⊏ ω. If α ⊏ ω then ω is then a parent of α.</formula><p>⊏ implies a generational hierarchy of tasks. The level of a task α in this hierarchy is the largest k such there is a sequence ⟨α 0 , α 1 , ...α k ⟩ of k tasks such that α 0 = α and α i ⊏ α i+1 for all i ∈ (0, k). A child is "lower level" than its parents<ref type="foot" target="#foot_4">foot_4</ref> .</p><p>(notation) If ω ∈ Γ v , then we will use subscript ω to signify parts of ω, meaning one should assume ω = ⟨I ω , O ω ⟩ even if that isn't written.</p><p>(intuitive summary) To reiterate and summarise the above:</p><p>-An input is a possibly incomplete description of a world.</p><p>-An output is a completion of an input [def. 2].</p><p>-A correct output is a correct completion of an input.</p><p>Learning and inference definitions: Inference requires a policy and learning a policy requires a proxy, the definitions of which follow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 4 (inference).</head><p>-A v-task policy is a statement π ∈ Lv. It constrains how we complete inputs.</p><p>π is a correct policy iff the correct outputs Oα of α are exactly the completions π ′ of π such that π ′ is also a completion of an input.</p><p>-The set of all correct policies for a task α is denoted Πα. <ref type="foot" target="#foot_5">6</ref>Assume v-task ω and a policy π ∈ L v . Inference proceeds as follows:</p><p>1. we are presented with an input i ∈ Iω, and 2. we must select an output e ∈ Ei ∩ Eπ.</p><p>3. If e ∈ Oω, then e is correct and the task "complete". π ∈ Πω implies e ∈ Oω, but e ∈ Oω doesn't imply π ∈ Πω (an incorrect policy can imply a correct output).</p><p>(intuitive summary) To reiterate and summarise the above:</p><p>-A policy constrains how we complete inputs.</p><p>-A correct policy is one that constrains us to correct outputs.</p><p>In functionalist terms, a policy is a "causal intermediary".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 5 (learning).</head><p>-A proxy &lt; is a binary relation on statements, and the set of all proxies is Q.</p><p>-&lt;w is the weakness proxy. For statements l1, l2 we have</p><formula xml:id="formula_3">l1 &lt;w l2 iff |E l 1 | &lt; |E l 2 |.</formula><p>-&lt; d is the description length or simplicity proxy. We have</p><formula xml:id="formula_4">l1 &lt; d l2 iff |l1| &gt; |l2|.</formula><p>By the weakness of an extension we mean its cardinality. By the weakness of a statement, we mean the cardinality of its extension. Likewise, when we speak of simplicity with regards to a statement, we mean its cardinality. The complexity of an extension is the simplicity of the simplest statement of which it is an extension<ref type="foot" target="#foot_6">foot_6</ref> .</p><p>(generalisation) A statement l generalises to a v-task α iff l ∈ Π α . We speak of learning ω from α iff, given a proxy &lt;, π ∈ Π α maximises &lt; relative to all other policies in Π α , and π ∈ Π ω .</p><p>(probability of generalisation) We assume a uniform distribution over Γ v . If l 1 and l 2 are policies, we say it is less probable that l 1 generalizes than that l 2 generalizes, written l 1 &lt; g l 2 , iff, when a task α is chosen at random from Γ v (using a uniform distribution) then the probability that l 1 generalizes to α is less than the probability that l 2 generalizes to α.</p><p>(sample efficiency) Suppose app is the set of all pairs of policies. Assume a proxy &lt; returns 1 iff true, else 0. Proxy &lt; a is more sample efficient than</p><formula xml:id="formula_5">&lt; b iff   (l1,l2)∈app |(l 1 &lt; g l 2 ) -(l 1 &lt; a l 2 )| -|(l 1 &lt; g l 2 ) -(l 1 &lt; b l 2 )|   &lt; 0</formula><p>(optimal proxy) There is no proxy more sample efficient than &lt; w , so we call &lt; w optimal. This formalises the idea that "explanations should be no more specific than necessary" (see Bennett's razor in <ref type="bibr" target="#b17">[18]</ref>).</p><p>(intuitive summary) Learning is an activity undertaken by some manner of intelligent agent, and a task has been "learned" by an agent that knows a correct policy. Humans typically learn from "examples". An example of a task is a correct output and input. A collection of examples is a child task, so "learning" is an attempt to generalise from a child, to one of its parents. The lower level the child from which an agent generalises to parent, the "faster" it learns, the more sample efficient the proxy. The most sample efficient proxy is &lt; w .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Arguments and Results</head><p>The distinction between software and hardware is unsuitable for reasoning about cause <ref type="bibr" target="#b15">[16]</ref>. The performance of a software agent in an interactive setting is subjective <ref type="bibr" target="#b13">[14]</ref>, as its behaviour depends on hardware which interprets it. Hardware is an "abstraction layer" between software and the surrounding environment. If we are to understand complexity, we must understand what the concept entails in the absence of such abstraction layers. We must ascertain what is objective rather than subjective, and so we must begin at the level of the environment rather than the agent. To this end, previous work <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b22">23]</ref> proposed a "de facto" pancomputational <ref type="bibr" target="#b23">[24]</ref> model of all conceivable environments and aspects thereof, which we refine and extend. While the formalism used here is a departure from past work, it is equivalent with respect to those previously published proofs we reference <ref type="bibr" target="#b17">[18]</ref>. The formalism is not computational in the sense of relying on symbols, quantities, or any other high level abstraction interpreted by a human mind. The assumptions we make are extremely weak (they hold in all conceivable environments).</p><p>Axiom 1: When there are things, we call these things the environment 8 .</p><p>8 It might seem absurd to state something so minimal, but it is necessary to be precise about how minimal our assumptions are.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Axiom 2:</head><p>The environment has at least one "state" <ref type="foot" target="#foot_7">9</ref> . If there is more than one state, then there is at least one "dimension"<ref type="foot" target="#foot_8">foot_8</ref> a long which things can differ.</p><p>A dimension is a set of points, for example time. We do not make the additional claim that such sets of points must be ordered. Each state is the environment at a different point in one or more dimensions. States are "extended" along dimensions, meaning no two states can occupy the same points in all dimensions (in other words, states are like the environment perceived from different positions in time or some other dimensions by an omniscient observer). These are all the assumptions we need for our version of pancomputationalism. We don't even need to speculate about any internal structure states might have, or what dimensions might be. Any "fact" about a state's internal structure can be defined by its relation to other states. The existence of sets is implied by the existence of states (because there is more than one "thing" that exists), and a fact is just the set of states in which it holds (the truth conditions of a "declarative program").</p><p>Universality Claim: Axioms 1 and 2 hold for every conceivable environment.</p><p>As truth is defined in existential terms <ref type="foot" target="#foot_9">11</ref> , there is nothing which is not a fact of the environment, and no environment which does not amount to a set of facts. In other words, this is a minimalist formalism of everything. An environment could be like our own, or not. It could be deterministic, in which case states follow a sequence. It could be non-deterministic, in which case they don't <ref type="foot" target="#foot_10">12</ref> . It could be fantastical with magic and true names. It could even be a world constructed through the subjective experience of its inhabitants. All that matters is that an environment has states, and from the relations between them we obtain the set of all facts. Facts as relations between states let us avoid anything like a universal set, because states are otherwise irreducible. An aspect of a state is just a set of facts about that state. With aspects in hand, we can define abstraction. An aspect is akin to a logical statement. It has a truth value given a state. We also define its extension (def. 2), which is all other aspects of which it is a part. This implies a heirarchy or "lattice" of aspects. Intuitively, an abstraction layer is like a window through which one can view part of the environment. A laptop computer could function as an abstraction layer, as could all or part of the system in which an embodied and embedded organism enacts cognition <ref type="bibr" target="#b25">[26]</ref>. In precise terms an abstraction layer is implied by a vocabulary, which is a set of declarative programs. A vocabulary implies a formal language whose rules are determined by relations between states. An abstraction layer implies a set of "v-tasks" (def. 3) <ref type="foot" target="#foot_11">13</ref> , each of which is behaviour that defines a system. The formalisation of policies as causal intermediaries between inputs and outputs <ref type="bibr" target="#b28">[29]</ref> then develops this into a causal depiction of goal directed behaviour (assuming a policy is implied by the inputs and outputs). For example, an organism could be a policy for a v-task which is behaviour that organism might enact. Again, we must emphasise this is a first principles approach. We do not assume symbols and Turing machines. Inputs, outputs and policies are all just sets of declarative programs. Whether something is goal directed is determined by the relations between states. The environment makes only one sort of value judgement (existence or non-existence), and is otherwise impartial. Goal directed behaviour is a value judgement, so we formalise this impartiality as a uniform distribution over tasks. Finally, we must now define complexity. The claims we make in the rest of this paper pertain to this notion of complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Complexity of Extension:</head><p>The complexity of an extension is the cardinality of the simplest statement of which it is the extension (see def. 5). This is like other formal notions of complexity <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, but facilitates comparison of abstraction layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Implications for Complexity</head><p>Proposition 1 (subjectivity). If there is no abstraction, complexity can always be minimized without improving sample efficiency, regardless of the task.</p><p>Proof: In accord with definition 2, the absence of abstraction means the vocabulary is the set of all declarative programs, meaning v = P . It follows that for every l ∈ L v there exists f ∈ v such that l = f . Statements l and {f } are equivalent iff E l = E {f } , which is exactly the case here because l = f . <ref type="bibr">[18, prop. 1, 2]</ref> shows maximising weakness is necessary and sufficient to maximise the probability of generalisation, which means weakness maximises sample efficiency (is the optimal proxy). This means sample efficiency is determined by the cardinality of extension. For every correct policy l of every task in Γ v there exists f ∈ v s.t. E l = E {f } . Policy complexity can be minimised regardless weakness, because the simplest representation of every extension is simplicity 1. ■</p><p>In this sense, complexity is an illusion created by abstraction. In the absence of any particular abstraction, all behaviours (extensions) are implied by statements of the same complexity. To be clear, we are not repeating the claim made by others <ref type="bibr" target="#b13">[14]</ref> that if the interpreter used by a complexity based induction system matches one used to to compute an objective value for complexity, then that induction system will be optimal in the sense of eventually learning the correct policy 14 . We are claiming that if interpretation is truly objective, then v = P and complexity has nothing to do with intelligence 15 . There is no objective notion of complexity. However, when we take empirical measurements it is inevitably through an abstraction layer, for which v ̸ = P . In that context simpler forms have been observed to generalise more efficiently. This raises the question; what additional assumptions can we make that would explain the correlation?</p><p>Time, space and causal confounding: We now make the additional assumption that vocabularies are finite. Every aspect of the world in which we exist appears to be spatially extended, meaning no two things occupy the same space at the same time. For the sake of understanding complexity we assume this is true of all environments. We hold that this justifies the assumption of a finite vocabulary, because in our spatially extended environment the amount of information in a bounded system is finite <ref type="bibr" target="#b29">[30]</ref>.</p><p>Proposition 2 (confounding). If the vocabulary is finite, then policy weakness can confound 16 sample efficiency with policy simplicity.</p><p>Proof: We already have that policy weakness causes sample efficiency, in that it is necessary and sufficient to maximise it in order to maximise sample efficiency. Continuing from proof 1, in a finite vocabulary, there may not exist f ∈ v s.t. E l = E {f } , which means the complexity of all extensions will not be the same. If we choose any vocabulary in which weaker aspects take simpler forms, then simplicity will be correlated with weakness and so will also be correlated with sample efficiency. This means we would choose v s. Why confounding tends to occur: We now briefly argue that abstraction is goal directed. This means the tasks an abstraction layer tends to represent are those it is best suited to represent, which implies weak constraints take simple forms. There are several reasons an abstraction layer is biased toward particular goals, depending upon the context in which we consider complexity. In the case of a computer, a human has specifically designed each abstraction layer to express that which is needed for a purpose. What separates x86 from a higher level 14 To quote verbatim: "Legg-Hutter intelligence is measured with respect to a fixed UTM. AIXI is the most intelligent policy if it uses the same UTM." <ref type="bibr" target="#b13">[14]</ref> 15 Intelligence here meaning not just eventual generalisation, but the efficiency thereof. abstraction layer like Numpy is that the former has a more general intended purpose, expressing "weaker" constraints. We tend to construct abstraction layers to be as versatile as possible whilst satisfying a particular need. More generally natural selection favours adaptation, which means generalisation, which is maximised by preferring weaker policies <ref type="bibr" target="#b17">[18]</ref>. Biological cognition is not limited to the brain <ref type="bibr" target="#b30">[31]</ref>, meaning the mind is not neatly confined within a well defined neurological abstraction layer. Instead the multiscale competency architectures observed in living organisms <ref type="bibr" target="#b31">[32]</ref> amount to self organising abstraction layers.</p><p>Because natural selection favours adaptable organisms, these abstraction layers will be selected to represent the weakest policies which constitute fit behaviour (a weaker policy is more adaptable). More generally, we speculate that phase transitions motivate the emergence of self preserving goal directed behaviour, by destroying some physical structures and preserving others. Such goal directed abstraction must minimise the size of vocabularies at higher levels, whilst also maximising the weakness of the policies they can express (two opposing pressures). This is because a larger a vocabulary exponentially increases the space of outputs and policies <ref type="bibr" target="#b21">[22]</ref>, which may conflict with finite time and space constraints. A larger vocabulary would make inference and learning less tractable (more "complex" in the sense of being a more difficult search problem that takes up more time). To maximise the weakness of policies in higher levels of abstraction, while minimising the size of the vocabulary in which they're expressed, weaker policies must take simpler forms.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc>t. for all a, b ∈ L v , the simpler statement has the larger extension, meaning a &lt; w b ↔ a &lt; d b. For example, suppose P = {a, b, c. . . }, a = {1, 2, 4}, b = {1, 3, 4}, v = {a, b}, L v = {{a}, {b}, {a, b}}, then it follows {a, b} &lt; w {a}, {a, b} &lt; w {b}, {a, b} &lt; d {a}, {a, b} &lt; d {b}.■</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>16 A</head><label>16</label><figDesc>confounds B and C when for example A = "badly injured" causes B = "died" and C = "picked up by ambulance", and it looks like C causes B because p(B | C) &gt; p(B | ¬C), and yet it may be that p(B | C, A) &lt; p(B | ¬C, A).</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Using the same dataset, not different datasets which could necessarily imply a different set of sufficient causes and thus affect learning.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Realised meaning it is made real, or brought into existence.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>The relation to typical philosophical and linguistic notions of intension and extension is addressed at length in<ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>e.g. E l is the extension of l.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>Practical examples child and parent tasks are in a separately published paper with the publicly available experimental code<ref type="bibr" target="#b17">[18]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>To repeat the above definition in set builder notation:Πα = {π ∈ Lv : EI α ∩ Eπ = Oα}</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>For example, if we have a language Lv, and X ⊂ Lv is the set of all statements in Lv that all have the extension EX , then the complexity of EX is the cardinality of a statement x ∈ X s.t. there is not statement y ∈ X with smaller cardinality than x.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7"><p>Or if the reader prefers, there are as many environments as there are states. By "state" of the environment we mean the aforementioned things. If two states are not the same, then something is not the same.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_8"><p>By dimension, again, we just mean something which can differentiate states.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_9"><p>This does not mean "true as interpreted by an omniscient observer", although it does no harm to think of it that way if it helps intuition. For a practical example, the physical state of a transistor in a computer is a declarative program, but so is everything else that might exist. That's the point of deriving this form of pancomputationalism from first principles. We start from "things", and there is more than one state of things when something differs. How this relates to problems of consciousness is addressed elsewhere<ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b22">23]</ref>, and is beyond the scope of this paper.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_10"><p>In any case, the difference between deterministic and non-deterministic seems meaningless when you consider that DFAs and NFAs are equivalent<ref type="bibr" target="#b24">[25]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_11"><p>The notion of task used here descends from the mirror symbol hypothesis<ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>, however it is complemented by thematically similar research defining tasks in relation to machine learning and biology<ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Information Processing and Thermodynamic Entropy</title>
		<author>
			<persName><forename type="first">O</forename><surname>Maroney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">The Stanford Encyclopedia of Philosophy</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On tables of random numbers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kolmogorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sankhya: The Indian Journal of Statistics A</title>
		<imprint>
			<biblScope unit="page" from="369" to="376" />
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Modeling By Shortest Data Description*</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rissanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autom</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="465" to="471" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Barnsley</surname></persName>
		</author>
		<title level="m">Fractals Everywhere</title>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
	<note>2nd ed</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Theoretical physics: Complexity on the horizon</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gefter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">509</biblScope>
			<biblScope unit="issue">7502</biblScope>
			<biblScope unit="page" from="552" to="553" />
			<date type="published" when="2014-05">May 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Computational Complexity and Black Hole Horizons</title>
		<author>
			<persName><forename type="first">L</forename><surname>Susskind</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fortsch. Phys</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="24" to="43" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The meaning and origin of goal-directedness: a dynamical systems perspective</title>
		<author>
			<persName><forename type="first">F</forename><surname>Heylighen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BJLS</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="370" to="387" />
			<date type="published" when="2022-06">June 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Complexity and Self-organization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Heylighen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Encyclopedia of Library and Information Sciences</title>
		<imprint>
			<date type="published" when="2008-01">Jan. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Universal Intelligence: A Definition of Machine Intelligence</title>
		<author>
			<persName><forename type="first">S</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Minds and Machines</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="391" to="444" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Language Modeling Is Compression</title>
		<author>
			<persName><forename type="first">G</forename><surname>Deletang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Universal Artificial Intelligence: Sequential Decisions Based on Algorithmic Probability</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Complexity-based induction systems: Comparisons and convergence theorems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Solomonoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIT</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="422" to="432" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Ockham&apos;s Razors: A User&apos;s Manual</title>
		<author>
			<persName><forename type="first">E</forename><surname>Sober</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Cambridge Uni. Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bad Universal Priors and Notions of Optimality</title>
		<author>
			<persName><forename type="first">J</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 28th COLT</title>
		<meeting>The 28th COLT</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1244" to="1259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Machine Super Intelligence</title>
		<author>
			<persName><forename type="first">S</forename><surname>Legg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
		<respStmt>
			<orgName>Uni. of Lugano</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Emergent Causality and the Foundation of Consciousness</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Bennett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial General Intelligence</title>
		<imprint>
			<biblScope unit="page" from="52" to="61" />
			<date type="published" when="2023">2023</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">On the Measure of Intelligence</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The Optimal Choice of Hypothesis Is the Weakest, Not the Shortest</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Bennett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial General Intelligence</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="42" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Meat Meets Machine! Multiscale Competency Enables Causal Learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Bennett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>Under review</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Computational Dualism and Objective Superintelligence</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Bennett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial General Intelligence</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Philosophical Specification of Empathetic Ethical Artificial Intelligence</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Maruyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cognitive and Developmental Systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="292" to="300" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Symbol Emergence and the Solutions to Any Task</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Bennett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial General Intelligence</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="30" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On the Computation of Meaning, Language Models and Incomprehensible Horrors</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Bennett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial General Intelligence</title>
		<imprint>
			<biblScope unit="page" from="32" to="41" />
			<date type="published" when="2023">2023</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Piccinini</surname></persName>
		</author>
		<title level="m">Physical Computation: A Mechanistic Account</title>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2015-06">June 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Finite Automata and Their Decision Problems</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Rabin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1959">1959</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Thompson</surname></persName>
		</author>
		<title level="m">Mind in Life: Biology, Phenomenology, and the Sciences of Mind</title>
		<meeting><address><addrLine>Cambridge MA</addrLine></address></meeting>
		<imprint>
			<publisher>Harvard University Press</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">SAGE: Task-Environment Platform for Autonomy and Generality Evaluation</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Eberding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sheikhlar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Thórisson</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Explanatory models in neuroscience, Part 2: Functional intelligibility and the contravariance principle</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yamins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Systems Research</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page">101200</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Psychological Predicates</title>
		<author>
			<persName><forename type="first">H</forename><surname>Putnam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Art, mind, and religion</title>
		<imprint>
			<publisher>Uni. of Pittsburgh Press</publisher>
			<date type="published" when="1967">1967</date>
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Universal upper bound on the entropy-to-energy ratio for bounded systems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Bekenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. D</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="287" to="298" />
			<date type="published" when="1981-01">Jan. 1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The brain is not mental! coupling neuronal and immune cellular processing in human organisms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ciaunica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">V</forename><surname>Shmeleva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Levin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Integrative Neuroscience</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Collective intelligence: A unifying concept for integrating biology across scales and substrates</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mcmillen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Levin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications Biology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">378</biblScope>
			<date type="published" when="2024-03">Mar. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
