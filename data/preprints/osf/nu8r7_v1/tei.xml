<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Survey on Ambient Intelligence Contexts: A Context-Aware Taxonomy based on Deep Learning and Internet of Things Synergy</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Parsa</forename><forename type="middle">Pure</forename><surname>Hamedany</surname></persName>
							<email>parsapurehamedany@gmail.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Tehran</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Survey on Ambient Intelligence Contexts: A Context-Aware Taxonomy based on Deep Learning and Internet of Things Synergy</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0E4CDE6FDA51EBED70DB6958D1EBA400</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-22T20:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Ambient Intelligence</term>
					<term>Deep Learning</term>
					<term>Intelligent Environments</term>
					<term>Context-Aware Computing</term>
					<term>Internet of Things</term>
					<term>Cyber-Physical Systems</term>
					<term>Smart Environments 1</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep Learning (DL) and the Internet of Things (IoT) are critical components of modern Ambient Intelligence (AmI), which integrate state-of-the-art Artificial Intelligence (AI) and Information Communication Technologies. The recent synchronization of IoT-enabled big data generation (5G) and advanced reasoning of DL models, such as Multimodal Large Language Models, has created new opportunities and challenges for modern AmI. However, comprehensively analyzing and understanding the broader implications of AmI in the big picture is difficult because of its interdisciplinary nature. To address this intricacy, by adopting the Context-Aware Computing perspective, a systematic arrangement of AmI contexts is proposed. This survey develops a taxonomy of AmI contexts based on four key dimensions: Human, System, Space, and Time. Each of these dimensions is further split into sub-context categories. By organizing DL and IoT applications within this taxonomy, the study offers a systematic framework to understand and customize AmI systems based on requirements. The resulting context portfolios serve as a flexible conceptual-functional toolkit for researchers and practitioners, aiding them in selecting and adapting contexts for specific applications. This taxonomy aims to clarify the complex landscape of AmI and provide a foundation for future innovations in the field.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>We are living in the era of ubiquitous mobile interactive large Artificial Intelligence (AI) models in Internet of Things (IoT), learning highlevel complex contexts through Context-Aware Computing as Large Language Models (LLM) in the cloud connected to mobile phones via 5G. The trend of computing advancements has been accelerated, as Information Communication Technologies (ICT) panoramic expansion by Moore's law <ref type="bibr" target="#b86">[87]</ref>. The plethora of Big Data generation from World Wide Web (cyber) and pervasive IoTs (cyber-physical) in streams, simultaneous with huge progressions in processing capabilities in both software (e.g., algorithmic architectures) and hardware (e.g., microchip processors), made a big technological paradigm shift <ref type="bibr" target="#b132">[133,</ref><ref type="bibr" target="#b105">106]</ref>. In another view, advancements and cheapening of hardware processing capabilities have paved the infrastructural way for more affordability of heavy AI models, trained on large amounts of data like Deep Neural Networks (DNN) <ref type="bibr" target="#b305">[306,</ref><ref type="bibr" target="#b241">242]</ref>. Computer Vision (CV) and Natural Language Processing (NLP) tasks are being achieved by continuously evolving Deep Learning (DL) architectures uniquely. Whether generating realistic images, programming websites, solving mathematical problems, or telling the funny elements of memes, Multimodal Large Language Models (MLLM) handle them with only a few prompts (e.g., words or images) <ref type="bibr" target="#b300">[301,</ref><ref type="bibr" target="#b282">283,</ref><ref type="bibr" target="#b174">175]</ref>. Ambient Intelligence (AmI) as a subset of AI is impacted by the paradigm shift by DL and LLMs. Prior to State-Of-The-Art (SOTA) DL, such as MLLMs, if watching AI along with IoT, the modern facets of AmI would be revealed <ref type="bibr" target="#b69">[70]</ref>. That is observable in various applications, such as smart vehicles, smart homes, smart healthcare systems, or monitoring and controlling trajectories of billions of facilities in Industry 4.0. The term Ambient Intelligence, coined by Zelkha <ref type="bibr" target="#b302">[303]</ref>, has been defined in various ways, which in the most recent one, Dunne et al. <ref type="bibr" target="#b69">[70]</ref> states:</p><p>"AmI is the combining of AI and IoT with the ubiquity of mobile devices." AmI can also be defined based on its features of intelligence, sensitivity, ubiquity, transparency, adaptivity, and responsibility <ref type="bibr" target="#b59">[60]</ref>. In parallel, Intelligent Environments (IE) are defined by Augusto et al. <ref type="bibr" target="#b20">[21]</ref> as, "An Intelligent Environment is one in which the actions of numerous networked controllers (controlling different aspects of an environment) is orchestrated by self-programming pre-emptive processes (e.g., intelligent software agents) in such a way as to create an interactive holistic functionality that enhances occupants experiences.". Whether AmI is more than programmed autonomy in an environment, it requires tasks to be performed with a degree of intelligence (e.g., reasoning) like assistance without micro-management <ref type="bibr" target="#b86">[87,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b59">60]</ref>. Apparently, despite the rapid changes in AI and DL in recent years, there is no survey covering the SOTA AmI; while LLMs have been transforming DL into not only general-purpose reasoners but also emergently zero-shot learners without requiring model parameters changing (pretraining or finetuning) by prompting <ref type="bibr" target="#b286">[287,</ref><ref type="bibr" target="#b285">286]</ref>. Despite the recent technological transformations related to AmI in the last decade, they</p><p>have not yet been analyzed and interpreted contextually in the big picture as is depicted in Figure <ref type="figure">1</ref>. This lack of research is addressed here through a comprehensive survey of SOTA trends organized by AmI contexts. Since insufficient works are titled as AmI or IE with SOTA AI inclusiveness, it is aimed to survey them by their contexts focusing on DL. The context philosophy, as applied in Context-Aware Computing <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b213">214]</ref>, is adopted here to survey AmI because direct data on this topic is lacking.</p><p>Figure <ref type="figure">1</ref>: Ambient Intelligence (AmI) contexts' taxonomy. The taxonomy of AmI contexts is extended in four directions. Each direction represents a context and its sub-contexts at three more in-depth levels. By deepening into further levels of the taxonomy, their background color gets brighter to white. The first level of the taxonomy with four contexts of Human, System, Space, and Time are the most abstract contexts. The second level is about the major categories of each context. The paper organizationally follows the taxonomy structure, whether some subordinates (e.g., the lowest level) are not titled. The taxonomy aims to illustrate a big picture of AmI contexts as its framework, to view its variety more simplified.</p><p>Various definitions, categorizations, and applications have been submitted in context and context-awareness after the initial use of the "context-aware" term by Schilit et al., which eventually was for an environment <ref type="bibr" target="#b243">[244]</ref>. This paper follows Dey et al.'s <ref type="bibr" target="#b64">[65]</ref> definition of context by its greater generality and updates over the most official one <ref type="bibr" target="#b1">[2]</ref>, which is:</p><p>"Any information that can be used to characterize the situation of entities (i.e., whether a person, place, or object) that are considered relevant to the interaction between a user and an application, including the user and the application themselves. Context is typically the location, identity, and state of people, groups, and computational and physical objects."</p><p>Context-Awareness has plenty of ways to be accomplished, as DL and IoT are its two enabling technologies <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b213">214,</ref><ref type="bibr" target="#b235">236,</ref><ref type="bibr" target="#b195">196]</ref> contributing to awareness throughout any phase of the context lifecycle (acquisition, modeling, reasoning, dissemination) <ref type="bibr" target="#b213">[214]</ref>. As categorizing the contexts of an entity clarifies the most effective parts for designers, interpreting it as a contextual taxonomy of that entity can organize highlevel conceptual and practical comprehensions <ref type="bibr" target="#b1">[2]</ref>. The first level of AmI context taxonomy, as its structure, is inspired by previous classifications, which are reviewed in the next section. AmI, an interdisciplinary subject covering heterogeneous applications, will evolve into a transdisciplinary subject with immense intricacies if it becomes context-aware. IE subsystems are complex entities themselves as if they are potentially independent systems, and when consolidated, the new system becomes a complex Systems of System (SoS) <ref type="bibr" target="#b125">[126]</ref>.</p><p>In order to overcome these challenges and overview inclusively, the taxonomy of that entity (AmI) by categorizing its portions, will serve as a functional method to simplify complications. The methodological idea as the framework of this survey is adopted from the Context-Aware Computing paradigm <ref type="bibr" target="#b213">[214,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b64">65]</ref> in structured systems thinking while categorizing its contexts, to observe SOTA in AmI and IEs in abstraction. The AmI context taxonomy enables a systematic and less biased survey that identifies implications of future challenges and opportunities. Moreover, reviewing survey papers that deliberately classify up-to-date information about their addressed topics (contexts) serves as a reliable guideline for understanding the current state of the subject. Hence, the main resources of this study are recent survey review research.</p><p>This study aims to propose a modern comprehensive Ambient Intelligence context taxonomy, as is represented in Figure <ref type="figure">1</ref>, to survey recent trends and provide a toolkit for both researchers and practitioners. The approached notion here is formed through the consolidation of related cutting-edge technologies and the philosophy of Context-Aware Computing with systems thinking as its inspiration. The first contribution is to comprehend the current status per context, followed by systematically overviewing each concept and its interconnections. Then, address contexts and subsume each's incorporations. Finally, recognizing salient challenges and assessing precedents' hindsight to project future foresight extrapolation through discussion is represented. The SOTA of AI and ICT are considered respectively DL and IoT, which are mutually concatenated to Big Data. By reviewing context types found in previous works, the nominated one is derived as Human, System, Space, and Time, which are briefly introduced here:</p><p>• Human: how people can be recognized (who) and predicted contextually, whether in the fashion they appear, in the way they feel in a deeper context, or in the manner they behave. The manifestation of their appearance, expressions, and actions are cues (why) to bring metadata about their demography, emotions, attitudes, health, habits, and characters towards interacting.</p><p>• System: which assumed SOTA subsystems of IE systems are characterized by data, IoT, and DL. It incorporates the attributes of data collection, IoT layers, and some impactful DL algorithms and learning methods.</p><p>• Space: what types of IEs are and how space can be perceived by AmI using positional features (where) or physical knowledge (how). Physical information from sensing to perception further enhances spatial awareness and physically aware AI.</p><p>• Time: when time-based contexts of AmI can significantly affect IEs. By the one-way flow of time, as in series sequences or recurring temporal patterns, to notice phenomena and memorize them as for reasoning, time has much information to represent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related works</head><p>In this section, similar survey papers will be reviewed and then the related taxonomies in context-awareness will be analyzed. The related surveys are filtered from keywords of "Ambient Intelligence", "Intelligent Environments", "context-awareness", and "Smart Environment" with more attention to their recency. While a pleasant result was not obtained, research continued in surveys composed of AI and IoT related terms. The closest survey to the mentioned Key Performance Indicators was a CSUR publication, Dunne et al. <ref type="bibr" target="#b69">[70]</ref>, which has considered SOTA DL and IoT into AmI. However, almost no attention was paid to context-awareness, and no categorization was presented for IEs except listing related features, as the survey was done approximately four years ago (before the LLM paradigm). The other related paper is Gams et al. <ref type="bibr" target="#b86">[87]</ref> published five years ago, which makes it not covering SOTA. Instead of surveying AmI, it tended to define several applications in conceptual terms with an eye on the future, whether no referral to IE there was. Cai and Yang et al. <ref type="bibr" target="#b40">[41]</ref> focused on IoT's role in AmI overviewed architectures in sensing and local processing in multiple applications, however, like the priors, context-aware computing was not investigated. An older survey (2013) while more inclusive, Perera et al. <ref type="bibr" target="#b213">[214]</ref>, was IoT-based and highly heeded context-aware computing, caused by publish time, even if ahead of its time mentioned DL, could not support SOTA. Other related works are analyzed in Table <ref type="table">1</ref> by DL, IoT, AmI, IE, Context Awareness, Context Taxonomy, and Published Year parameters, whether covered or not.</p><p>Table 1: Related works by covering factors which CA is context-awareness, CT is context taxonomy, and PY is published year. Indicators show 'ü' as covered, 'Ø' as not covered, and '!' as vaguely noted. [264] [57] [13] [183] [40] [70] [261] [143] [239] [306] [269] [196] [87] [41] [182] [214] [21] [236] [60]</p><formula xml:id="formula_0">DL ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Ø Ø Ø IoT ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Ø Ø Ø CA Ø Ø ✓ Ø ! ! Ø Ø Ø ✓ Ø ! ! ! Ø ✓ ✓ ! ✓ AmI ! Ø ✓ Ø Ø ✓ Ø Ø ! ✓ Ø Ø ✓ ✓ Ø ✓ ✓ ✓ ✓ IE Ø Ø Ø Ø Ø ✓ Ø Ø Ø Ø Ø Ø Ø ✓ Ø Ø ✓ Ø ! CT Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø Ø ✓ ✓ Ø ✓</formula><p>PY 2023 2023 2022 2022 2022 2021 2021 2021 2021 2020 2020 2020 2019 2019 2018 2013 2013 2011 2009</p><p>Selecting context categorization can prepare an important connection between reasoning and each specific application <ref type="bibr" target="#b19">[20]</ref>. Whether the system must bring intelligence into an environment to interact with people, categorization has to be specialized for that field. In that setting, Feng et al. <ref type="bibr" target="#b81">[82]</ref> presented context categorization for AmI by bifurcating it into two major categories of user-centric and environmental context. The user-centric context contains the users' background, dynamic behavior, physiological state, and emotional state; besides, the environmental context comprises the physical environment, social environment, and computational environment. Another related context categorization by Augusto et al. <ref type="bibr" target="#b20">[21]</ref> in IEs divided context into user, environment, and system. Another influential categorization by its vision similarity is by Chen et al. <ref type="bibr" target="#b49">[50]</ref>, which categorized contexts into user context, physical context, computing context, and time context, rooted in Schilit et al. <ref type="bibr" target="#b243">[244]</ref>.</p><p>Other context categories in Context-Aware computing bring different notions of context philosophy and how its concepts can be discerned, which for more comparisons visiting Perera et al. <ref type="bibr" target="#b213">[214]</ref> is recommended. Abowd et al. <ref type="bibr" target="#b1">[2]</ref> classified contexts into characteristics of the entity (5Ws) that are identity (Who), location (Where), status (What), and time (When); in addition, those contexts would be used to figure out the reason (Why) a user took that specific action. Dourish et al. <ref type="bibr" target="#b66">[67]</ref> observed two views of context by noting the activity-oriented aspect, representational and interactional. The representational aspect of context is about the information that is stable and separable from activities in the environment. Although content and context are two segregated entities, the interactional form "instead argues that context arises from the activity" <ref type="bibr" target="#b66">[67]</ref>. Consequently, all activities and interactions of each user in the environment would be deemed as context. Operational and conceptual views of context are bifurcated by Van Bunningen et al. <ref type="bibr" target="#b269">[270]</ref>, which operational is about the procedure where context information is acquired, modeled, and treated. Albeit the conceptual category states the meaning and contextual interrelations by identifying user-centric contexts from ambient <ref type="bibr" target="#b81">[82]</ref>. The operational view was based on another categorization made by Henricksen et al. <ref type="bibr" target="#b109">[110]</ref> in four types of sensed, static, profiled, and derived. Van Bunningen et al. <ref type="bibr" target="#b269">[270]</ref> called derived context as high-level context and the past three types as lowlevel context. Calling a context as a high-level can although be related to context lifecycle in an IoT-based vision as Perera et al. <ref type="bibr" target="#b213">[214]</ref> brought up in four cyclic steps of acquisition, modeling, reasoning, and dissemination. Whether the raw data is received from sensors (context acquisition), then modeled, and after it is processed (context reasoning), can be called high-level context instead of low-level.</p><p>The exchange of views continues regarding a comprehensive context categorization while it is aimed to get ideas from the previous works to the concluded position in this paper, which is: Human, System, Space, and Time. That resembles Augusto et al. <ref type="bibr" target="#b20">[21]</ref> if Time context is added, correspondence with Chen et al. <ref type="bibr" target="#b49">[50]</ref> by transforming computing context to System, and more unabridged categories or less generality comparing the user-centric and environmental context types of Feng et al. <ref type="bibr" target="#b81">[82]</ref>.</p><p>In the following sections of this paper: Section (2) Human context in three subsections of appearance, sentiment, and behavior is viewed, Section (3) addresses System context in three subsections of data, IoT, and DL, Section (4) observes Space context in scope, location, and physics subsections, Section (5) Time context of AmI is mentioned in two flow (one-way) and cycle (two-way) forms of time, Section <ref type="bibr" target="#b5">(6)</ref> discussion about the study's configuration is presented, highlights the open issues and challenges of this study and AmI in general, points several possible future directions of this context, and ultimately brings the conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">HUMAN CONTEXT</head><p>Even if there is no background information for newcomers, their extractable information in the environment is the Human context's concentration. The way humans' status looks and how conjectures about their characteristics can be made by AI is discussed in the human context as is exemplified in Figure <ref type="figure" target="#fig_0">2</ref>. That is perceivable by different levels of human analysis from outside to inside and deepening into cognizance of users via SOTA technologies analyzing how people look, feel, and act. Here the Human contexts are classified into three aspects (1) Appearance (2) Sentiment (3) Behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Appearance</head><p>The appearance section is how people would seem outwardly, whereby they are occurred by their semblance and appearance characteristics estimation from the first interaction to infer. This section outlines appearance analysis in five parts: (1) attendance (2) age (3) gender (4) physique (5) fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Attendance</head><p>Attendance is when a person enters the environment, recognizes it as with object detection, counts her/him with other present people in the area, and detects motions <ref type="bibr" target="#b315">[316]</ref>. It also may be tracking the guy by any footprint, as by (multi) object tracking, until exiting <ref type="bibr" target="#b177">[178]</ref>. Each human would be detected by face (facial geometry, texture, skin color), motion (tracking), and body features <ref type="bibr" target="#b14">[15]</ref> for identification purposes. Detection can also be accompanied by tracking during the presence of people to record the whole trajectories of each person spatiotemporally. A way would be Indoor Positioning Systems (IPS) to attain people's attendance by tracking location via smart devices owned by them, which is not considered here but in the Space context. On a larger scale, crowd attendance by using counting and density estimation (crowd statistics) is possible <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b251">252]</ref>. The detection can be done simultaneously with authentication in which biometric methods seem more relevant to be done via face recognition, iris recognition, fingerprint, palm-print recognition, voice recognition, keystroke touch dynamics, and physiological signals <ref type="bibr" target="#b232">[233]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Age</head><p>Age group estimation approximates human age by biometric features <ref type="bibr" target="#b237">[238]</ref> as the human body has two age forms, chronological and biological, to distinguish. Chronological view is how old a person is by birthdate and biological is "based on the biological quality and functioning of tissues, apparatus and organs of an individual" <ref type="bibr" target="#b215">[216]</ref>. Because of the complexities of detecting biological age, mainly the chronological aspect is meant here. People in peer groups have more common than dissimilar groups, causing their needs to be akin to each other. That can be done by both image processing (facial, body) and sensors which the more attention is on facial analysis, but sensor-based approaches are feasible as gait analysis in recent years <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Gender</head><p>The conjecture and recognition of male, female, or other gender types based on physical-biological appearance is the point of this part. People of different genders have diverse societal roles, living experiences, and needs <ref type="bibr" target="#b267">[268]</ref>. Gender detection can be operated with visionbased and sensor-based approaches while gender detection via image is more common and accurate. By CV, gender can be classified by face <ref type="bibr" target="#b242">[243]</ref>, body skeleton <ref type="bibr" target="#b26">[27]</ref>, and gait <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b242">243]</ref>. It is also seen that tasks like this are achieved easily by DL with higher accuracy than humans <ref type="bibr" target="#b281">[282]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.4">Physique</head><p>Physical and structural features of the body such as height (stature), weight, shape, and Body Mass Index (BMI) estimation is the intention of the physique part. By knowing the position of the camera (height and angle) height of objects and humans' stature are measurable <ref type="bibr">[198,</ref><ref type="bibr">156]</ref>. Human body weight estimation with anthropometric features can prepare further analysis like BMI approximation and help individuals control and use it through health improvements and even clinical uses <ref type="bibr" target="#b128">[129]</ref>. 3D models of the body have many advantages and can be used in all sections of the human context, which is possible even with a single 2D image (depth sensor data can be used too) <ref type="bibr" target="#b211">[212]</ref> to do the human mesh recovery (3D body pose and shape estimation) <ref type="bibr" target="#b266">[267]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.5">Fashion</head><p>Detecting and classifying each individual's clothing, accessories, hair, and makeup styles is about this part. Fashion recognition can provide information about humans' lifestyles as how luxurious they prefer, which style is their type, or any sort of classification to find their needs caused by communication form of fashion <ref type="bibr" target="#b25">[26]</ref>. In order to reach the fashion context by multimedia, after landmark detection and pose estimation in the pixel computation level, understanding and assessing the stylistic features of a person's fashion is required for attaining fashion analysis at an ultimate level <ref type="bibr" target="#b253">[254]</ref>. In a more detailed view, Cheng et al. <ref type="bibr" target="#b53">[54]</ref> considered four aspects for enabling intelligent fashion through fashion detection <ref type="bibr" target="#b31">[32]</ref> at first, fashion analysis with attribute recognition <ref type="bibr" target="#b110">[111]</ref> and style learning <ref type="bibr" target="#b140">[141]</ref> secondly, then fashion synthesis with transferring style <ref type="bibr" target="#b121">[122]</ref> and pose transformation <ref type="bibr" target="#b178">[179,</ref><ref type="bibr" target="#b22">23]</ref>, finally fashion recommendation for compatibility <ref type="bibr" target="#b97">[98]</ref>, matching <ref type="bibr" target="#b254">[255]</ref>, and suggestions <ref type="bibr" target="#b47">[48]</ref>. In almost all intelligent fashion-related works, CV is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Sentiment</head><p>Indeed, AI neither feels nor understands emotions like a human, nor comprehends complicated human interrelation circumstances, nor understands how the grief of losing a loved one is. However, it can bring real-time decision-making about individuals' emotional state with context-aware multimodal emotion recognition systems in affective computing <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b295">296,</ref><ref type="bibr" target="#b258">259,</ref><ref type="bibr" target="#b280">281,</ref><ref type="bibr" target="#b219">220]</ref>, and in some ways, it can afford higher agility and ubiquity than humans in emotion recognition. According to Webster's New World College Dictionary, 4th Edition sentiment is: "a complex combination of feelings and opinions as a basis for or judgment; general emotionalized attitude". Emotion has been modeled in three discrete (categorical), dimensional, and componential manners <ref type="bibr" target="#b146">[147]</ref>. Discrete models consider emotions as separate basic emotional state categories (e.g., Ekman's six basic emotions) <ref type="bibr" target="#b76">[77]</ref>. Dimensional models consider emotions as cluster points in a multidimensional graph made of two dimensions (e.g., Russell's circumplex model of affect) <ref type="bibr" target="#b233">[234]</ref> or three dimensions (e.g., Mehrabian's pleasure-arousal-dominance) <ref type="bibr">[192]</ref>. Componential models are the combination of various factors that impact emotions (e.g., Plutchik's psychoevolutionary theory <ref type="bibr" target="#b216">[217]</ref>, Ortony-Clore-Collins (OCC) model of emotion <ref type="bibr" target="#b206">[207]</ref>). All in all, each research in this field picks one of these models as a structure to classify the output of emotion recognizer. The sentiment section consists of emotion (feelings) and notion (opinions) through affective computing that relates psychology and social sciences to cognitive and computer sciences by containing both emotion recognition and opinion mining <ref type="bibr" target="#b280">[281]</ref>. Emotion recognition methods detect how a person's emotional state is from physical and physiological contexts, however, opinion mining methods want to understand what people's thoughts and sentiments (e.g., negative, neutral, positive) are mainly from their linguistic signals <ref type="bibr" target="#b219">[220]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Body Language</head><p>Humans' postures and gestures can communicate nonverbally, revealing signals to other people. It also exposes a person's emotional condition and each body movement can be interpretable <ref type="bibr" target="#b212">[213]</ref>. Body language can communicate with diverse body parts, including facial expressions, hand gestures, body gestures, postures, and eye movements and any of these parts can be emotionally interpretable <ref type="bibr" target="#b203">[204]</ref>. Although body language is vigorously culture-dependent <ref type="bibr" target="#b137">[138]</ref> even in facial expressions <ref type="bibr" target="#b122">[123]</ref>, whether in hand gestures contradictions might be the opposite or offensive <ref type="bibr" target="#b212">[213]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.1">Face</head><p>Humans express their feelings and senses of their thoughts with their facial components' configuration and effect for reacting to stimuli, unconsciously or in a controlled manner. The face also is not only a multi-signal system but also a multi-message system, in which extracting messages brings a reliable non-verbal communication bridge <ref type="bibr" target="#b74">[75]</ref>. The face manifests three types of signals <ref type="bibr" target="#b74">[75]</ref>: static (e.g., skin pigmentation, face shape), slow changes by aging, and rapid with any movement of facial muscles (e.g., raising the eyebrows). The position of rapid signal changes provides Facial Expression Recognition input to sentimental analysis. One of the key factors in Facial Expression Recognition is the performance in a wild situation, which improving it needs an appropriate database <ref type="bibr" target="#b160">[161]</ref>. That means micro-expressions can facilitate emotion and affect recognition <ref type="bibr">[195]</ref> by quick (less than 0.5 s) facial emotional response <ref type="bibr" target="#b99">[100]</ref> caused by time-based attributes of emotional responses <ref type="bibr" target="#b75">[76]</ref>. All in all, detecting facial components' movements by not heeding contexts would not be enough, even if it is learned by any sophisticated computing algorithm <ref type="bibr" target="#b27">[28]</ref>. While vision-based works are prevalent, using other sensors (depth, EEG, infrared thermal, audio) caused by problems such as illumination variation and head pose in multimodal sensor data in facial expression recognition may impact performance <ref type="bibr" target="#b239">[240,</ref><ref type="bibr" target="#b41">42]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.2">Body</head><p>Posture is the unconscious body positioning with the skeletal muscles' contraction in the space <ref type="bibr" target="#b44">[45]</ref>. It contains the configuration of the head, body, arms, and legs with each other. While the head represents emotions naturally, the body demonstrates the intensity of emotions <ref type="bibr" target="#b73">[74]</ref>. The human body can be modeled in part-based or kinematic models by assuming the body as a configuration of separate parts or interconnected joints <ref type="bibr" target="#b203">[204]</ref>. In each procedure for recognizing emotion from body gestures, after detecting the human body, body pose detection and tracking has to be made for representation learning and emotion recognition <ref type="bibr" target="#b203">[204]</ref>. The walking style (gait) can even present particular gait kinematic patterns in separate emotional states and effects <ref type="bibr" target="#b294">[295,</ref><ref type="bibr">258]</ref>. Posture prediction can be made by anticipating the possible posture from a temporally incomplete time series to localize joints' positions <ref type="bibr" target="#b179">[180]</ref>. Human body posture and gesture recognition can be done by sensor-based (e.g., depth, accelerometer) and Radio Frequency-based (e.g., Wi-Fi, RFID), besides vision-based techniques <ref type="bibr" target="#b130">[131]</ref>; while radar-based gesture recognition is another new way for this task [250].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.3">Hand</head><p>The way hands move and the shape of hands represent specific meanings that can express people's feelings and send expressive messages about their thoughts <ref type="bibr" target="#b137">[138]</ref>. Hand gestures are a manual communication channel by sign language as a natural language recognizable via machine <ref type="bibr" target="#b226">[227,</ref><ref type="bibr" target="#b102">103]</ref>. Computers can recognize hand gestures and their meanings in various ways, but wearable glove-based, camera visionbased, and surface electromyography (EMG) are the mainly used approaches <ref type="bibr">[208,</ref><ref type="bibr" target="#b298">299]</ref>. Besides, other sensing technologies are available such as ultrasound and pressure sensors <ref type="bibr" target="#b102">[103]</ref>, while soft systems like bioelectronics, e-skin, e-tattoo, and soft circuits are playing the future role <ref type="bibr" target="#b129">[130]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.4">Eye</head><p>Eye tracking is the procedure of gaze analysis spatiotemporally by measuring where and how long the eyes are stared at and the pupil size <ref type="bibr" target="#b166">[167,</ref><ref type="bibr" target="#b144">145]</ref>. By analyzing eye movement and pupil behavior's related metrics, visual attention, emotional arousal (esp. stress), and cognitive workload can be depicted <ref type="bibr" target="#b252">[253]</ref>. Those metrics are fixations, eye movements, smooth pursuit, pupil size, and blinks <ref type="bibr" target="#b252">[253]</ref>. It can also be used to analyze mood from the reactions of individuals to each object <ref type="bibr" target="#b87">[88]</ref>. Eye-trackers have various types that are eye-attached, optical, and electric potential measurement <ref type="bibr" target="#b166">[167]</ref>, or from another point of view, they are head-mounted or remote <ref type="bibr" target="#b54">[55]</ref>. The main eye tracking techniques are classified as scleral search coil, infrared oculography, electrooculography (EOG), and video oculography <ref type="bibr" target="#b144">[145]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Verbal Language</head><p>Language as the most effective tool of humanity for communication and cognition plays a vital role for us and what we make. The NLP incorporates both Natural Language Understanding (NLU) and Natural Language Generating (NLG). NLG is about creating meaningful statements and NLU studies sound of words (phonology), structure of words (morphology), words' arrangement (syntax), meanings referring (semantics), and contextual inferring (pragmatics) <ref type="bibr" target="#b139">[140]</ref>. From another point of view, language analysis can be classified as discourse analysis and sentence analysis (syntax and semantics) while the theoretical aspect of language (e.g., grammar) is in a segregated category from computation linguistics <ref type="bibr" target="#b55">[56]</ref>. NLP has various tasks, but some more advanced ones acquire more attention by their diverse usages, like Sentiment Analysis (SA) and automatic speech recognition, which are brought up here. LLMs as SOTA in NLP are task-agnostic general purpose task solvers, supporting many tasks whether by zero-shot learning or finetuning downstream NLP tasks <ref type="bibr" target="#b311">[312]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.1">Speech</head><p>Speech is one of the most convenient and efficient communication ways that several sorts of information can be extracted from it nowadays such as the speaker's age, gender, language, accent, identity, health condition, emotional status, and subject of content <ref type="bibr">[200]</ref>. Text and speech processing except for their different input formats are distinct from paralinguistic speech variations (e.g., voice pitch, accent, speaking style, speed) <ref type="bibr" target="#b185">[186]</ref>; while language models are almost used in both of them, speech grammar rules are not used all the time <ref type="bibr" target="#b185">[186]</ref>. Paralanguage and language constituting speech and speech emotion recognition will be formed by considering its main features such as prosody, spectral, voice quality, and Teager-Kaiser Energy Operator features <ref type="bibr" target="#b283">[284]</ref>. Prosody is the study of syllables, intonation, stress, and rhythm; spectral features shape of the vocal tract by its frequency; voice quality is how vocal waveform is; Teager-Kaiser energy operation measures energy and frequency of voice signal amplitude.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.2">Opinion</head><p>To understand the meaning of a linguistic modality, many sides of NLP take part in its downstream tasks, whether in summarization, translation, conversational interactions (question-answering), or Sentiment Analysis (SA). LLMs support myriad NLU tasks such as SA and reasoning (e.g., inference, mathematics, comprehension) within conversation structure <ref type="bibr" target="#b200">[201]</ref>. They are also capable of doing SA even if the model cannot afford its standards. With LLM's ICL the SA accuracy is raised by demonstrating a few related examples <ref type="bibr" target="#b65">[66]</ref>. Among NLP tasks, SA is focused on comprehending the notion of a piece of information separated from reasoning in a query of facts. The SA or opinion mining field is an affective computing part which is one of the major NLP tasks. It is mostly performed on text data to understand what the author's thoughts and emotions are about by considering the polarity and aspects of text entities <ref type="bibr" target="#b48">[49,</ref><ref type="bibr">174]</ref>.</p><p>SA incorporates different levels in three main document, sentence, and aspect levels <ref type="bibr" target="#b165">[166,</ref><ref type="bibr" target="#b34">35]</ref>. The most uncomplicated level is the document level, which considers documents as one entity and tries to classify the document's discourse polarity (positive, negative, neutral) or subjectivity detection. Subjectivity detection filters information related to facts to determine the opinionated information <ref type="bibr" target="#b172">[173]</ref> for feeding polarity classification, whether syntactically or semantically <ref type="bibr" target="#b48">[49]</ref>. At the sentence level, the document text parses into sentences and each sentence would be semantically and syntactically analyzed as a distinct entity <ref type="bibr" target="#b48">[49]</ref>. More complicated and fine-grained aspect level analysis wants to find out what aspects are covered in the text content to recognize the opinion holder's main intention, further to polarity and subjectivity detection <ref type="bibr" target="#b244">[245]</ref>. Apart from subjectivity detection and aspect extraction, other remarkable SA tasks include opinion spam detection, implicit language detection (sarcasm), and cross-domain and cross-language sentiment classification <ref type="bibr" target="#b284">[285]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Physiological state</head><p>Human physiological processes contain plenty of reliable and unconscious information to analyze without the bias of pretense <ref type="bibr" target="#b141">[142,</ref><ref type="bibr" target="#b7">8]</ref>. Physiological information can be used for individuals' feelings recognition and monitoring from the brain, heart, blood circulatory, skin, muscle, breath, sweating, temperature, and eyes <ref type="bibr" target="#b70">[71]</ref>. The brain's electrical activities and its changes are chiefly recorded and measured by Electroencephalography (EEG) signals as other methods are used in medical applications as Magnetic Resonance Imaging (MRI) Computed Tomography (CT) techniques (e.g., functional-MRI, Diffusion Tensor Imaging, Single-Photon Emission Computed Tomography, Positron Emission Tomography). The circulatory system, especially heart rate and blood state, is analyzed in various ways with electrocardiography (ECG), photoplethysmography (PPG), Pulse Wave (PW), ballistocardiography (BCG), Blood Volume Pulse (BVP), blood oxygen saturation, and glucose. Skin related signals include Skin Temperature (SKT) and Galvanic Skin Response (GSR) or electrodermal activity (EDA) which is the electrical potential changes through sweating moisture. Other techniques to mention are electromyography (EMG) for muscles and nerves' electrical activity measurements, respiration (RSP) for analyzing breathing, and electrooculography (EOG) for the eye's retinal electrical analysis.</p><p>To do emotion recognition via physiological signals, typically after stimulation indicated to a person (e.g., music, image, video), signals' states will get measured for classification based on the emotion model <ref type="bibr" target="#b280">[281]</ref>. The most used signals in emotion recognition are EEG, ECG, EDA/ GSR, RSP, and EMG <ref type="bibr" target="#b152">[153,</ref><ref type="bibr" target="#b248">249,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b171">172]</ref> whereas others like PW and SKT were used less than the priors. Stress and mood recognition in everyday experience has been done from physiological signals <ref type="bibr" target="#b152">[153,</ref><ref type="bibr" target="#b236">237]</ref>. Training DL models on EEG, ECG, and EMG signals, multiple analysis tasks are applicable such as brain and heart functionality at diseases, sleep stage, age, gender, motion, and emotion <ref type="bibr" target="#b104">[105]</ref>. Each physiological signal has its proper sensor, but their acquisition tools for emotion recognition are classified as smart wearable, mounted, and external <ref type="bibr" target="#b7">[8]</ref>. Although physiological states might be used as a modality of multimodal affective computing, which beyond multi-physical modalities, get fused either as homogeneous multi-physiological or heterogeneous physical-physiological multimodalities <ref type="bibr" target="#b280">[281]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Behavior</head><p>Human behavior is taking any action and conducting manner of oneself or with others in interaction with the environment (objects, system) or people <ref type="bibr" target="#b82">[83]</ref>. In this section, the external observable and measurable aspect of behavior is considered as in the behavior analysis <ref type="bibr" target="#b21">[22]</ref>. Three aspects construct this section: (1) Human Activity Recognition (HAR) techniques to detect and analyze human behavior from sensory data <ref type="bibr" target="#b5">[6]</ref>, (2) personality computing which explains behavior psychologically by computing on personality trait theories <ref type="bibr" target="#b275">[276]</ref>, and (3) interactions as feedback and instructions. Acts of individuals represent their behave whether the behavior is a proper tool to measure personality, although personality traits are also related to sentiments as feelings and thoughts impact behavior <ref type="bibr">[293]</ref>. Relation and casual loops exist between the three of them, whereas personality computing tasks are related to how data and computation algorithms, but not the description and explanation still <ref type="bibr" target="#b214">[215]</ref>. By recognizing activities and analyzing them in their frequency quantity and contextual parameters, behavior patterns and habits will be discovered <ref type="bibr" target="#b150">[151]</ref>. That can result in obtaining behavior change <ref type="bibr" target="#b84">[85]</ref> and make anomaly detection tasks more enhanced [158] by considering the routines and contexts to interpret behavior as normal or abnormal, as in Ambient Assisted Living (AAL) <ref type="bibr" target="#b57">[58]</ref>. Since Kurt Lewin stated that behavior is a function of the person (history, personality, motivation) and environment (physical and social surroundings) (B = f (P, E)) <ref type="bibr" target="#b158">[159]</ref>, contextual parameters had been considered effectual on behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Activity</head><p>Human behavior analysis tasks can be classified based on the degrees of semantics and time frame into motion, action, activity, and behavior <ref type="bibr" target="#b46">[47]</ref>. Action and activity terms are almost interchangeable in HAR, which action is often simpler and has less duration compared to activity, whether an activity is mainly composed of actions <ref type="bibr" target="#b52">[53]</ref>. For instance, grabbing a glass is an action but making a coffee is an activity created by a sequence of actions. In HAR when the action is done decision is made, but in many cases, the decision has to be taken before occurring the event caused by urgency. For instance, the fall prediction of the elderly or the autonomous vehicles that have to decide swiftly what action to take prior to the accident happening. The solution to that problem is action or activity prediction which is about the future state by learning from unfinished cut from the end data, to estimate what the action before it is completely performed will be <ref type="bibr" target="#b147">[148]</ref>. This task can be achieved by human pose estimation if the objective is just predicting what the posture might be by analyzing the trajectory sequence of body joints' localization before the judgment execution <ref type="bibr" target="#b179">[180]</ref>. HAR is classified into motion-based (e.g., people counting, motion detection, tracking), action-based (e.g., gesture recognition, posture recognition, behavior recognition, fall detection, activities of daily living, AAL), and interaction-based (e.g., human, object) categories <ref type="bibr" target="#b117">[118]</ref>. This standpoint of HAR is also interpretable in an atomic to complicated hierarchical leveling manner <ref type="bibr" target="#b5">[6]</ref>; following motions and gestures, action recognition takes place, then interaction detection, and finally group activities with the highest level of complexity. Interaction is a reciprocal relation between humans and objects in three forms human-human (e.g., kissing) <ref type="bibr" target="#b234">[235]</ref>, human-object (e.g., wearing glasses) <ref type="bibr" target="#b91">[92]</ref>, and object-object (e.g., pot on the stove) <ref type="bibr" target="#b94">[95]</ref>.</p><p>Along with human-human, human-object, and human-scene interactions <ref type="bibr" target="#b117">[118]</ref>, any action in the environment can be considered an interaction. Interacting with the system can be further than settings changing in a smart mobile application and getting pervasive into the environment's embedded sensors and actuators using SOTA IoT. LLM agents understand and reason our instructions, they furthermore can adjust their reactions after our behavior as feedback towards planning future actions <ref type="bibr" target="#b279">[280]</ref>. Therefore, if they get finetuned on other behavioral attributes of users as their activities and personalities, it would bring a customized interactive framework. Whether by getting multimodal as MLLMs, many planning tasks can be achieved by interactions such as multimodal conversation (e.g., visual QA), saying story beneath images, and finding funny reasons of memes <ref type="bibr" target="#b300">[301,</ref><ref type="bibr" target="#b282">283]</ref>. So, models capable of solving high-level mathematical questions from images even without optical character recognition, might soon be able to infer human behavior into a cohesive conversation.</p><p>HAR also can be regarded with numerical scaling as individual, group, and crowd <ref type="bibr" target="#b293">[294]</ref>; in which groups consist of fewer individuals and have more intercorrelations than crowd <ref type="bibr" target="#b240">[241]</ref>. Group activity is a type of interaction among a group's members <ref type="bibr" target="#b293">[294]</ref> as standing in a queue, walking together, or other social activities <ref type="bibr" target="#b72">[73]</ref>. Crowd behavior has been studied in two views, microscopic (bottom-up) and macroscopic (top-down), by considering the crowd as multiple separate individuals or a monolith entity <ref type="bibr" target="#b265">[266]</ref>. Besides how to see a crowd, its analysis would be done by crowd scene analysis and crowd statistics <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b251">252]</ref>. In which crowd scene analysis is integrated by subtopics of crowd behavior recognition, motion tracking analysis and prediction, and group behavior analysis <ref type="bibr" target="#b30">[31]</ref>.</p><p>The most common categorization made in the HAR field is bifurcation according to its input data format to bring sensor-based and visionbased <ref type="bibr" target="#b61">[62]</ref> HAR, which has been stated as the main question of many works as sensor-based <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b278">279,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b50">51]</ref> and vision-based <ref type="bibr" target="#b124">[125,</ref><ref type="bibr" target="#b308">309,</ref><ref type="bibr">305,</ref><ref type="bibr" target="#b227">228,</ref><ref type="bibr" target="#b127">128]</ref> human action/activity recognition. Vision-based supports only RGB (Red Green Blue) and/or depth sensors (RGBD) while sensor-based contains all types of sensors that are capable of getting used in HAR whether the sensor is wearable, attached to any object, or device-free (environmental) <ref type="bibr" target="#b117">[118]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Personality</head><p>Personality is the inner characteristics in behavioral patterns that are shaped (e.g., habits) and shown by people in each particular situation. Personality computing is automatic personality judgment from the main observable stable factors of persons' behaviors about how the inner personality might be, based on models of personality trait theories <ref type="bibr" target="#b275">[276]</ref>. Whether personality computing as HAR does both the perception for predicting personality traits and the recognition automatically, it also includes automatic personality synthesis as a major problem <ref type="bibr" target="#b275">[276]</ref>.</p><p>There are different personality trait theories as 16PF (Sixteen Personality Factor) <ref type="bibr" target="#b45">[46]</ref>, MBTI (Myers-Briggs Type Indicator) <ref type="bibr">[199]</ref>, and PEN (Psychoticism, Extraversion, and Neuroticism) <ref type="bibr" target="#b77">[78]</ref>; but the five-factor model (Big Five) <ref type="bibr" target="#b188">[189]</ref> is the most used in personality trait recognition <ref type="bibr" target="#b312">[313]</ref>. The Big Five model contains the Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism factors and their opposites. Human personality is recognized from behavioral cues containing almost all the previous human contexts (e.g., face, body, speech, fashion) <ref type="bibr" target="#b192">[193,</ref><ref type="bibr" target="#b42">43]</ref>. Considering the point of view of this article's human context, personality detection has been done with all the sentiment section's parts and fashion cues. Recognizing personalities has applied based on varied data types (text, audio, static image, video, and physiologic) <ref type="bibr" target="#b131">[132,</ref><ref type="bibr" target="#b184">185,</ref><ref type="bibr" target="#b259">260,</ref><ref type="bibr" target="#b43">44]</ref> while SOTA works similarly to affective computing using a compound of this information in a multimodal fashion <ref type="bibr" target="#b192">[193,</ref><ref type="bibr" target="#b42">43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SYSTEM CONTEXT</head><p>A system is an organized entity composed of multiple interactive elements to achieve common goal(s) and if both the elements and their interactions have sophistication, it is a complex system <ref type="bibr" target="#b276">[277]</ref>. Elements of the system might be systems themselves as subsystems of the system, and if the subsystems can be independently operable, it is a system itself and the system is a System of Systems (SoS) <ref type="bibr" target="#b125">[126]</ref>. The IEs are a type of Embedded System and CPS which can be potentially a complex system, SoS, and a large-scale system, so it seems worth analyzing it as a System. The system will be built based on stakeholders' expectations and needs towards its requirements to be architected and designed. The system's design differs since it evolves from requirements, missions, purposes, applications, and forms which accordingly, functionalities get specified. Functional (logical) and physical requirements would lead to concept selection toward system's architecture through the nominated concepts. It is aimed by this survey that context portfolio (taxonomy) lets designers and architectures choose concepts, to provide solution-neutral space, at both abstract and technical levels, resulting in an appropriate allocation.</p><p>A traditional way for systems analysis is to understand the system's building blocks in functional and componential hierarchies, to perceive the utilities of each part in an organized arrangement. In the componential hierarchical decomposition of the system, the first element might be other systems that are independently operable each on their own. However, they are consolidated with integrated interconnections in SoS conforming to the same objective, coordinated and coherently <ref type="bibr" target="#b125">[126]</ref>. If the system is not a SoS, is composed of subsystems that are not fully independent runnable systems; whether it would be a complex system as human body parts like the brain, which is not independently operable but complex <ref type="bibr" target="#b148">[149]</ref>. The subsystem's subordinate, components (modules), are the constituent items of subsystem configuration, whether are made of subcomponents. Finally, in the most granularity of hierarchy, there are parts, which their functionality is only confined to be merged with others <ref type="bibr" target="#b148">[149]</ref>. For instance, a group of people is an SoS, a human is a system, the brain is a subsystem, the cerebellum is a component, the temporal lobe is a subcomponent, and granule cells are parts.</p><p>As mentioned above, any system based on its requirements shall have a particular concept selection for the architecture and design, which among alternatives in each level of hierarchy, there are trade-offs (e.g., cost, performance, risk, robustness, schedules). As a result, even for similar applications, stakeholders' expectations, and missions, it is highly challenging (illogical) to have one specific systematic framework; although, requirements in IEs are not analogous. Albeit, as the attention of this survey is SOTA in AmI, the roles of some systems are bolded more than others. Consequently, the concentration of this context is focused on specific DL and IoT systems since they have pivotal effectivity in SOTA AI and ICT, and modern AmI <ref type="bibr" target="#b69">[70]</ref>.</p><p>A system should be organized and developed on its logical and functional requirements in its life cycle. Besides, some recent technological opportunities such as real-time wireless systems in IoT (e.g., 5G) and Pre-Trained Models (PTM) in DL can be candidates in any AmI system, because of their generalization. Thus, the candidate concepts in this context are IoT and DL as systems of the IE systems, since they are building blocks of a modern AmI system <ref type="bibr" target="#b69">[70]</ref> as is illustrated in Figure <ref type="figure" target="#fig_1">3</ref>. Further to that, by the diversity of inputs (contexts) and the importance of proper data in any context, informational aspects are also analyzed in the System context. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data</head><p>Both IoT and DL are dependent on data directly and indirectly; IoT is a modern powerful tool to capture Big Data in high volume, variety (heterogeneity), and velocity (real-time) particularly where it should be utilized for obtaining contextual data <ref type="bibr" target="#b88">[89]</ref>. Although IoT is an adjusted infrastructure for big data acquisition, noisy raw sensory data is not prepared enough to be efficient on DL models. On the other side, better performance of DL is straightly affiliated to training data's quality and quantity, therefore, developing an apt dataset is crucial; as the datacentric AI and software 2.0 are getting heard from the AI community, a paradigm is going to shift towards there <ref type="bibr" target="#b303">[304,</ref><ref type="bibr" target="#b289">290]</ref>. This means, that if we have equal models with the same hyperparameters but enhanced data, performance rises while with the existing democratized transformable models, thinking about it would be worth indeed <ref type="bibr" target="#b209">[210]</ref>. Albeit, without considering this scenario or model-centric AI and physics-based simulations, high-quality data is decisively impactful for AI even in the big data era <ref type="bibr" target="#b209">[210]</ref>. Moreover, selecting data for each application differs from another to profile <ref type="bibr" target="#b279">[280]</ref> while some processes are within the same pipelines.</p><p>To reach acceptable data, after data acquisition (if needed), data should be preprocessed and improved <ref type="bibr" target="#b289">[290,</ref><ref type="bibr" target="#b231">232]</ref>. In a general systematic view of the data development lifecycle, requirements should be analyzed as the data must work for the goal it is following, afterward there would be design, implementation, testing, and maintenance <ref type="bibr" target="#b118">[119]</ref>. The sides of information that are discerned remarkable from the mentioned phenomena in this context, which regards data collection for DL's avail, are data acquisition, pre-processing, and multimodality. Without an existing proper dataset, required data should be collected. The first step in data collection is data acquisition, which is composed of discovery, augmentation, and generation of data; afterward, if necessary, data labeling and refinement must be done <ref type="bibr" target="#b231">[232]</ref>. Multimodality brings varied types of data into an integrated system to get the use out of assorted resources of information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Acquisition</head><p>When the prepared pertinent dataset is not discovered yet due to unavailability or expensiveness, the data must be generated depending on the requirements and design specifications, or be augmented which is called data acquisition <ref type="bibr" target="#b118">[119,</ref><ref type="bibr" target="#b231">232]</ref>. In data discovery, the goal is finding or sharing datasets from outsources such as collaborative sharing, web, and data lakes <ref type="bibr" target="#b231">[232]</ref>. Whether the data development is task exclusive, regular ways of data generation are experiments, crowdsourcing, and synthetic data generation, besides big data generators like IoT sensors <ref type="bibr" target="#b209">[210,</ref><ref type="bibr" target="#b231">232,</ref><ref type="bibr" target="#b88">89]</ref>. IoT data usually from various sensor nodes or in networks would generate at massive volume and velocity as stream data. Synthetic data can be generated via generative AI models as GANs or simulations' output data, besides data augmentation techniques <ref type="bibr" target="#b209">[210]</ref>. Data augmentation is reproducing new data from the existing data by making changes to samples, or generate established on them. In NLP, data augmentation methods can be viewed in three categories, paraphrasing, noising, and sampling <ref type="bibr">[160]</ref>. On the other hand, data augmentation in CV especially with image data is significant in semantic segmentation, image classification, and object detection <ref type="bibr" target="#b296">[297]</ref>. Augmentation methods in CV would be classified into manipulation, erasing, mixing, auto augments, feature augmentations, and deep generative models <ref type="bibr" target="#b296">[297]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Pre-processing</head><p>If the data is gathered, it should be preprocessed to get enhanced for higher performance of DL models <ref type="bibr" target="#b231">[232]</ref>. Preprocessing improvements can be done by labeling, validation, cleaning, sanitization, and integration. As the performance of DL models is better with labeled data, data labeling and annotation is a hot topic. But doing it manually even by crowdsourcing is expensively time-consuming, so, there are several automatic procedures. To mention some bolded ML-based solutions for lack of labeled data, there would be Semi-Supervised Learning, Active Learning, Weak Supervised Learning, and Self-Supervised Learning (SSL) <ref type="bibr" target="#b303">[304,</ref><ref type="bibr" target="#b289">290]</ref>. If the quality of labels is not in the standard domain because of their noise and incorrect labels, re-labeling is suggested <ref type="bibr" target="#b289">[290,</ref><ref type="bibr" target="#b231">232]</ref>. Another modification is data compression to reduce data size to utilize storage and other processing resources more efficiently <ref type="bibr" target="#b126">[127]</ref>. After collecting and shaping data, except labeling, other steps should take place such as validation (e.g., visualization), cleaning, sanitization, and integration <ref type="bibr" target="#b289">[290]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Multimodality</head><p>Modalities are just alike human perception mechanisms rooted in the five senses (observing, hearing, touching, smelling, and tasting), from raw to abstract forms of the same signal content <ref type="bibr" target="#b164">[165]</ref>. Raw modalities are aligned with raw sensory data without abstraction. For example, besides learning RGBD frames of videos that have audio in raw form, the data might have abstract modalities within it such as multi object recognition and emotion recognition (e.g., facial expression, body gesture, and speech). As a result, each context can be a modality and capable of getting used in a multimodal learning fashion whether in raw or abstract forms, if the abstract ones are homogeneous <ref type="bibr" target="#b23">[24]</ref>. Data integration by fusion of multiple datasets or modalities in a multimodal fashion like we humans, can make DL (Artificial Neural Networks) perceive more comprehensively <ref type="bibr" target="#b23">[24]</ref>. That similar diversity in modalities would lead to more natural interactions of machines with humans in HCI while traditional fusion techniques do not have much in common with SOTA, there are many fusion strategies. Whether in CV, a fusion of modalities leads to a higher comprehension of the model <ref type="bibr" target="#b133">[134]</ref>.</p><p>Multimodality, as mentioned in the Human sentiment context, is practical in myriad applications of ML such as for affective computing and smart healthcare to use multimodalities to be more inclusive <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b295">296,</ref><ref type="bibr" target="#b280">281,</ref><ref type="bibr" target="#b219">220]</ref>. A categorization of modalities is by their dimensionality, as in 1D signals and sequences, 2D images and audio, or 3D videos be considered <ref type="bibr" target="#b154">[155]</ref>. Although, the dimensionality of heterogeneous Big Data by fusion of multiple modalities can be beyond that, as an RGBD video with its audio and caption containing semantically high-level contexts like detected objects, all labeled. A multimodal ML model must handle the heterogeneity, interactions, and connections of modalities in different types of challenges as representation, alignment, generation, reasoning, transference, and fusion <ref type="bibr" target="#b164">[165,</ref><ref type="bibr" target="#b23">24]</ref>. Representation is features of each modality by their variety and interconnections to be learned, while alignment is detecting the relations between modalities. Generation challenge covers the compression of multimodal data, translating by mapping each modality to others, and generating other modalities out of the existing ones. Reasoning with this context is earning knowledge to infer and deduce via multimodalities. The transference challenge is about how to transfer the knowledge of each modality toward another's exploitation through representations and models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Internet of Things</head><p>IoT is the point of view to have objects (things) interconnected to a global network infrastructure (internet) dynamically integrated or fully embedded as one entity while ubiquitous <ref type="bibr">[137]</ref>. IoT can participate in data collection real-time such as data acquisition, almost all preprocessing steps, and providing abstract multimodality inside its architecture. The objective of this section is to get familiar with the constituents of IoT and how IoT-based system components conform and consist. To look at IoT from a finer perspective systematically, an IoT architecture's structure can be the way to analyze the elements' arrangements of the IoT-based system. One known architecture is the basic five-layered made of perception (physical), network (transportation), processing (middleware), application, and business layers <ref type="bibr" target="#b138">[139,</ref><ref type="bibr" target="#b246">247]</ref>. Advocating an architectural style is not the subject, but to indicate different aspects of IoT systematically to comprehend each layer's functions <ref type="bibr" target="#b9">[10]</ref>. Although IoT is an SoS and its architectures diverge by applications and business orientations, the components are removable from the system <ref type="bibr" target="#b85">[86,</ref><ref type="bibr" target="#b104">105]</ref>. Input of the system, perceived sensory data get transferred by networks to be gathered for process into information or more analysis will be utilized according to the application of the whole IoT system. While, that should be along with market demanding needs and business strategies. The processed contexts might return as adjusting feedback or operating commands to actuators, ultimately. The closest layer to the users is the perception's physical objects incorporating sensors, actuators, and devices (things). Network transports are almost through wireless internet-connected networks; the pivotal concept of Wireless Sensor Networks (WSN) is as intercommunicated sensor nodes <ref type="bibr" target="#b8">[9]</ref>. The sensor node in WSN is a device composed of processing and transportation layers (processor, storage, and transceiver modules) beside of perception layer (sensors) to convert analog sensory data to digital and send it wirelessly <ref type="bibr">[136]</ref>. The WSNs or just the perception layer after connecting to the internet will bring up the data to the gateways and middleware <ref type="bibr" target="#b104">[105]</ref>. Middleware is the mediator software between physical objects and applications, while the processing layer's other component is computer hardware <ref type="bibr" target="#b201">[202]</ref>. The processing would differ based on how far the computation site is; above the device, at a closer distance would be fog (edge) computing with more resource limits and more real-time, or cloud computing deeper and extended <ref type="bibr" target="#b169">[170]</ref>. Finally, after processing if it was considered, the made decision changes will recur via actuators in the environment (application interface).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Perception</head><p>The perception layer is made of sensors, actuators, and devices physically in the environment for identification, gathering contexts, and physical operations via sensors and actuators (devices) <ref type="bibr" target="#b104">[105]</ref>. Sensors measure physical parameters' qualities of the environment quantitatively by responding to changes in physical stimuli. They cover a wide range of sensing contexts' changes and reactions. The sensor is categorized into two major types if they are invasive or non-invasive sensors from an AI perspective. The invasive sensors should be wearable or attached to the human body, while non-invasive ones not <ref type="bibr" target="#b260">[261]</ref>. Non-invasive sensors can be installed in the environment or be used in the objects, whether they are vision-based or not <ref type="bibr" target="#b50">[51]</ref>. Vision-based approaches receive more attention in practice, caused by the heterogeneity of sensors that sophisticates the calculations. However, with RGB or RGB-D (red, green, blue, depth) data, CV offers various options; moreover, many tasks are unfeasible with vision-based approaches (e.g., EEG for brain electrical activities) and in some cases using other methods is less costly. As we are living in the Big Data era, proper data is a necessity for the system, and by providing it, objectives seem much more reachable while accomplishing each ML task is highly dependent on the data as in ImageNet <ref type="bibr" target="#b63">[64]</ref>. In addition, with the view of WSNs, sensors can be classified into Micro-Electro-Mechanical Systems (e.g., gyroscopes, accelerometers, magnetometers), Complementary Metal-Oxide-Semiconductors (e.g., humidity sensors, temperature sensors), and Light-Emitting Diodes (e.g., ambient light sensors, proximity sensors) <ref type="bibr" target="#b145">[146]</ref>.</p><p>An actuator is any object that can convert energy's form to produce a change for an operation, such as LEDs, speakers, displays, motors, thermostats, and soft actuators <ref type="bibr" target="#b246">[247,</ref><ref type="bibr" target="#b161">162]</ref>. The actuators like sensors have plenty of usage to perform any physical action in the environment whether it is a servo motor, a digital signage, or a shapeless arm in soft robotics. A device is hardware that drives software while has access or is integrated with sensors and/or actuators <ref type="bibr" target="#b104">[105]</ref>. Devices also potentially have other capabilities such as computing processors <ref type="bibr" target="#b221">[222]</ref> but each would be considered in the related layer of this architecture. A device might cover all five layers in itself as smartphones, but in the perception layer the data-perceiving physical object as a device seems to be the subject.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Network layer</head><p>The network, transport, or transmission layer connects parts and provides communication for them. It transmits sensory signals to the processing layer through network channels. Alongside the IoT paradigm, network protocols to link data, shifted toward wireless connectivity technologies. One view to analyze wireless communication methods is by their range <ref type="bibr">[137]</ref>. The short-range wireless methods are Radio Frequency Identification (RFID) and Near-Field Communication (NFC) which after them Bluetooth Low Energy (BLE) would take place. Wireless Local Area Networks (WLAN) are medium-range based such as 802.11 IEEE Wi-Fi protocols, whether ZigBee covers short to medium ranges. Lastly, the widest wireless range belongs to cellular technologies like Global System for Mobile communications (GSM) supporting 3G, LTE, 4G, and 5G protocols. Though, in network tradeoffs, other evaluation factors are significant as speed (latency, jitter), bandwidth, and energy consumption.</p><p>Communication types in any IoT device have four main strategies: Device to Device (D2D), Device to Gateway (D2G), Device to Cloud (D2C), and Device to Application (D2A) <ref type="bibr" target="#b256">[257]</ref>. That classification is separated from the technological infrastructure but represents the ways a smart IoT device can interact and exchange data. After all, if the device is not able to communicate independently, a gateway is the destination of its data to get transferred. The gateway translates and relays data for transmission to a data center and processing layer <ref type="bibr" target="#b246">[247]</ref>. It can be more than an intermediary proxy role and automatically does data filtering, preprocessing, processing, routing, and management depending on its resources <ref type="bibr" target="#b33">[34]</ref>. With this respect, WSN is the sensor nodes' configuration with routing nodes in the environment to collect sensory inputs and transfer them to gateways <ref type="bibr" target="#b183">[184]</ref>. WSN is not a subset of the network layer but includes both perception and network layers capable of supporting the processing layer. A WSN can use any wireless communication method in two major phases, within itself sensor node's transceiver and outside connection via gateway <ref type="bibr" target="#b145">[146]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Processing layer</head><p>The processing layer contains middleware and computing processors where aptly, further to data accumulation in storage, preprocessing to prepare information, computation, and analysis, the decision would be made there. After the sensed data is transferred via a device or gateway to the data center, a middleware would be to manage (e.g., abstraction) and join it to the application layer <ref type="bibr" target="#b246">[247]</ref>. A middleware bridges devices and software interfaces by purveying Application Programming Interfaces (API) and handling heterogeneities such as data types and device protocols by integration <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b80">81]</ref>. The middleware should not only provide network and syntactic interoperability but also semantically. Using semantic technologies in middleware (e.g., XML, OWL) allows data to be represented and related in a way the machine understands <ref type="bibr" target="#b306">[307,</ref><ref type="bibr" target="#b229">230]</ref>.</p><p>In recent years, computation can be done in multilevel layers such as device, edge, fog, cloud, or a hybrid combination <ref type="bibr" target="#b301">[302]</ref>. The IoT devices are the nearest and the most decentralized level for storage and processing while having the most constraints in resources. Whether a device like a smart gadget is limited in memory capacities and computing processors can do simpler tasks by itself privately. But, one conventional internet-based solution for limited capabilities is cloud computing infrastructure by serving massive ubiquitous computation power and storage remotely connected to the internet network. That made expensive powerful hardware (e.g., server farms, quantum computers) affordable by virtualization and sharing resources to run gigantic models and processing big data just by pay-per-use <ref type="bibr">[194]</ref>. Setting infrastructural resources in a deep centralized position makes the responding time-consuming based on the network characteristics; however, situating cloud computing resources closer to the end devices, handles that issue <ref type="bibr" target="#b301">[302]</ref>. Fog computing or edge computing does that by putting resources horizontally with smaller scale accessible for scenarios where IoT devices cannot operate and cloud computing has higher latency than the speed standard. Where fog or edge computing are in between, edge computing seems more edge at networks layer. All in all, it looks like a funnel where cloud computing has the most processing power and is far away from users despite the IoT devices <ref type="bibr" target="#b169">[170]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Application layer</head><p>The application layer is where the services are to provide the interface for users containing modules to monitor and control <ref type="bibr" target="#b228">[229]</ref>. As IoT can act as an internet infrastructure in CPSs, its applications cover a wide range of domains related to the service types and qualities they provide <ref type="bibr" target="#b17">[18]</ref>. IoT integration with pervasive computing, as the Internet of Everything (IoE), would be the enabling technology of various applications. One of the most important applications of IoT and AmI is healthcare, as patients' physiological and medical status real-time gets monitored remotely via sensor networks connected to the internet <ref type="bibr" target="#b2">[3]</ref>. AAL also can deliver healthcare services besides being used in smart homes, which is another application of IoT <ref type="bibr" target="#b57">[58]</ref>. While autonomously, it lets the elderly live independently longer and people with physical or mental impairment be safer and more empowered. Smart cities are to construct all fundamental components of an urban area more efficiently using IoT as a basis, which includes almost all other applications of IoT in itself <ref type="bibr" target="#b29">[30]</ref>.</p><p>Smart Homes using IoT further to being assistive (e.g., smart kitchen) and present living enhancements (e.g., caring), would be effective in power and energy efficiency management, even for a whole building <ref type="bibr" target="#b245">[246]</ref>. Smart Buildings as an infrastructural component of a smart city, vary in applications based on their uses as a smart office for administrative, a smart home for general, or a smart shopping mall with commercial retailing uses. Two mobility IoT tasks are smart transportation (e.g., traffic monitoring, smart vehicles, smart airport, smart port) and smart logistics <ref type="bibr" target="#b222">[223]</ref>; however, that (Mobile IoT) can be used in different modes of ground transportation, maritime (tankers), or in aviation industries. Another impressive related aspect is the Industrial Internet of Things (IIoT) which is applied in almost every aspect of Industry 4.0 in products' manufacturing to distribution processes <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b90">91,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b183">184]</ref>. Some important industries with bolded IIoT role in them are energy, grids, manufacturing, agriculture, and aquaculture. Environmental monitoring by remote sensing is also a major application of IoT, whether in the environment as surveillance, or ecosystem monitoring for atmospheric and biological purposes [269].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.5">Business layer</head><p>The business layer takes the managerial part to handle the system on its business model and market orientations, using monitoring analytics of the application layer's data to adopt strategies <ref type="bibr" target="#b138">[139,</ref><ref type="bibr" target="#b29">30]</ref>. The business layer on the top of all layers acts as an administrator to judge and control the system's performance through indicators to improve Business Intelligence or the business model <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b183">184]</ref>. As the engineering is a tradeoff between functional and financial criteria; which impacts technological tools' selection based on their benefits and disadvantages and funded capital to decide the most profitable application's services option. This aspect is not in the scope of this survey but is much important.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Deep Learning</head><p>Whether IoT plays the enabling infrastructure role for AmI, AI pushes autonomy to intelligence, while DL methods do the recognizing and understanding of highly complicated patterns apparently better than any rival. CV and NLP are two majorly important applications of ML that have various uses in modern AmI, which are essential in some cases. AI can be used in all IoT architecture layers for optimization and/or automation purposes further to privacy, security, and resource allocation uses as its effectiveness on Quality of Services (QoS) <ref type="bibr" target="#b142">[143]</ref>. Computing frameworks would use different ML methods by their characteristics, as edge computing is more limited than cloud computing. While, DL has numerous beneficiaries for IoT data like time-series analysis, noise toleration, real-time streaming data analysis, and handling heterogeneity in large-scale data <ref type="bibr">[239,</ref><ref type="bibr" target="#b181">182]</ref>. On the other hand, as DL algorithms' performance improves with training on larger data, IoT is a Big Data provider from multiple sensor networks and pervasive devices producing nonstop real-time data streams <ref type="bibr" target="#b39">[40]</ref>.</p><p>Computing systems need embedded robust software to program, but for processing and decision making at complex problems like chess game or multi object recognition, traditional functional programming alone does not work. Although ML was a huge step forward to do those tasks by learning the data algorithmically, with no programming on issues that cannot be solved even via massive coding. ML is the main subset of AI to learn the patterns and correlations in data using mathematics and statistics for its optimization algorithms. Classic ML algorithms like Support Vector Machine (SVM), K-Nearest Neighbor (KNN), Decision Tree, Random Forrest, Logistic Regression, and Kmeans somehow were competing with each other in SOTA ML until deep Artificial Neural Networks (ANN) outperformed prior ML methods on large datasets and proper computing power (e.g., GPU). However, some ML methods are getting used yet as SVM for its performance, Decision Tree by its interpretability and traceability, and they require less consumption in energy, amount of data, and processors than DL methods. Multi-layered deep NNs (DL) using optimization functions (e.g., Gradient Descent, Adam, Genetic Algorithm) demonstrated that can learn complex patterns and unlike classic ML are needless to hand-crafted feature extraction by doing it automatically. In addition, DL has more robustness, generalization, interoperability, and scalability, which regarding these reasons, the focus in AI is attracted on DL hitherto <ref type="bibr" target="#b154">[155,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>During writing this paper (2024) AI in practice is Artificial Narrow Intelligence (ANI) and there is no Artificial General Intelligence (AGI). That means all today's ML-based systems and models are capable of being specific on their objective function in limited subjects. They cannot learn every knowledge in a general way like an omniscient, even using new DL paradigms like In-Context Learning (ICL). Whether the ANI we have now (e.g., MLLMs) is likely the closest one to a future AGI, still there is not a clear description of AGI. But, even with today's AI at learning patterns on huge amounts of data and giant models, which can achieve multi tasks autonomously, omnipresent in real-time, scalable for new downstream tasks, with higher accuracy and precision than a human in some simpler tasks, and processing millions of rows of data in a fraction of time, are impossible for any person to do. If look at the ML just as a context reasoning method, there can be others as probabilistic logic-based, case-based, rule-based, and ontology-based <ref type="bibr" target="#b306">[307]</ref>, but are not in the scope of this study. It should be noted that there is absolutely no free lunch in AI to have a master key, and each technique must be analyzed in its context. In this section, DL, to be systematically analyzed as a conceptual and technical brief overview, is viewed by its two major aspects of (1) learning methods and (2) model architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Learning methods</head><p>Any learning type has mechanisms to confront issues from their requirements as many learning methods try to solve data-related problems quantitatively (e.g., samples) and qualitatively (e.g., unlabeled data) to be more efficient. Some learning procedures fit better in specified circumstances as Reinforcement Learning in interactive environments or controlling multi agents autonomously, concerning overfitting. As a primary goal of most learning methods is to modify pattern recognition and feature extraction automatically, some also follow more generalization and model's knowledge comprehension in more varied areas (e.g., Transfer Learning). To do so, in this section, there are impactful conceptual learning mechanisms that formed SOTA AI to glimpse. Any of these kinds of learning would be engineered to do a task, whether it is detection, classification, prediction, generation, or others which might be a mix of tasks. Although a DL model can include many learning methods and paradigms integrated into one architecture, while they are not completely separated from each other.</p><p>ML mainly had been differentiated from the labeled or unlabeled training data, which are the key factors to divide between the ML algorithms. Learning on labeled data is known as supervised learning, which learns mainly based on classification or regression techniques, while unsupervised learning uses unlabeled data as clustering, generative modeling, dimensionality reduction, and anomaly detection <ref type="bibr" target="#b35">[36]</ref>. Models' performance using labeled data usually is expected to be better than unlabeled, but as annotation and labeling costs are high, clean useful labeled data's accessibility is highly limited. Although, building models on unlabeled data is a hot topic and there are methods with sufficient efficiency as Self-Supervised Learning.</p><p>In classic ML data representations or features within the data, as a core element in ML, would be selected aided by domain expert's knowledge or other feature engineering methods manually; but representation learning in DL, with much less affiliation to feature engineering of labor, learns the underlying features autonomously <ref type="bibr" target="#b31">[32]</ref>. Representation learning methods try to extract characteristics of data by finding patterns, as in manifolds or coherences, through the NNs weights computation. Semi-supervised learning uses both labeled and unlabeled data to build a more efficient learning framework relying on unlabeled data, where labeled data is rare <ref type="bibr" target="#b270">[271]</ref>. For instance, wrapper methods are popular in semi-supervised learning, which is a supervised learning model on labeled data, predicting the labels of unlabeled data, within the pseudo-labeling process for further uses <ref type="bibr" target="#b270">[271]</ref>.</p><p>Reinforcement Learning (RL) framework uses one or more agents to explore the environment and take actions sequentially (exploration, exploitation) made on Markov Decision Process (MDP) optimization to learn each action state's reward function and maximize total reward value. The RL agent pursues a specific target to attain by exploiting the most rewarding situations according to its interactive intuitions based on its policy's setting <ref type="bibr" target="#b261">[262]</ref>. Actions' rewards depend on the dynamic contextual states related to environment and agent position; that can be optimized at prior feedback on posterior decisions even in a delayed rewarding for long term at value function, besides immediate actions' rewards <ref type="bibr" target="#b261">[262]</ref>. That causes the agent to choose actions based on value computations but not only the rewards as the policy determines, whether if the RL system is model-based to discover the environment's behavior, future prediction would be added. Models imitate the environment for planning the agent's activities, searching for the best policy via simulation, despite model-free approaches which learn by trial-and-error. That makes RL agents compelled to fail to learn (exploration-exploitation dilemma), which leads to RL's data inefficiency.</p><p>Deep Reinforcement Learning (DRL) is RL combined with DL methods, to benefit traditional RL in dynamic interactive sequences of decisions, and learning complex representations and optimization of DL in higher dimensional data of complex environments and complex agents' control <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b247">248]</ref>. DL in DRL can act as a function approximator for any RL element, whether is a model-based or model-free value function, policy, reward function, state transition function, planning, or exploration <ref type="bibr" target="#b163">[164,</ref><ref type="bibr" target="#b185">186]</ref>. DRL makes autonomous agents take consecutive strategies as per environmental situation, optimized by DL methods cumulatively <ref type="bibr" target="#b15">[16]</ref>; which actually started by coping with Atari games in high-level and boomed with winning Go game's champion <ref type="bibr" target="#b196">[197]</ref>.</p><p>Transfer learning (TL) is learning a new task pursuing the related prior earned knowledge of the model by generalizing it, like a person who could learn saxophone easier if knew how to play the flute, compared to a person without any musical experience <ref type="bibr" target="#b314">[315]</ref>. The TL does the adaptation and generalization of representations from the source domain, towards connecting it to other homogeneous or heterogeneous target domains, without building a new model from scratch <ref type="bibr" target="#b314">[315,</ref><ref type="bibr" target="#b287">288]</ref>. That makes higher performance when test data distribution distinctly differs from training data, on either conditional or marginal, since domain adaptation corrects differences of both by shifting source and target domains closer together <ref type="bibr" target="#b287">[288]</ref>. In DL architectures it is feasible to cut the latter layer(s) off, specifically the last output layer, then compound previous frozen layers' trained parameters weights with new data by replacing it with the NNs' last layer(s) and retrain the added layers.</p><p>The procedure of restating an already trained model into a new transferred usage is the finetuning and the reuse of a trained model for other tasks is pretraining. Pretraining as a side of TL, is to train a model usually as a parameter or initializer for future finetuning by the rise of data and computing power consumption <ref type="bibr" target="#b154">[155]</ref>. The Pre-Trained Model (PTM) on proper data would do the generalization for the newly transferred model with more data constraints, further to its less training power consumption. A type of finetuning to tune a model by learning it via a dataset of related instructed prompt queries is instruction-tuning, which is relevant in LLMs <ref type="bibr" target="#b311">[312,</ref><ref type="bibr" target="#b174">175]</ref>. If finetuning's aim is to filter the output of the model to not generate some sort of biased, unhelpful, harmful, or hallucinated responses, alignment-tuning would be made by methods such as Reinforcement Learning with Human Feedback (RLHF) with human alignment <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b311">312,</ref><ref type="bibr" target="#b208">209]</ref>.</p><p>Multi-Task Learning's (MTL) goal is to achieve multiple related ML tasks simultaneously by sharing representations jointly among them in parallel for more generalization <ref type="bibr" target="#b309">[310]</ref>. Tasks might have either equal importance or one main task and other auxiliaries, while a model can be a whole with multiple outputs or multiple models with one output. MLT is used mostly when the data has multiple labels, there is a need to get numerous outputs, many corresponding tasks have few amounts of data, or one dataset is sufficient for manifold tasks (data efficiency). For example, training a dataset containing images with multi-label of objects inside them for object detection task should do the recognition at the same time.</p><p>All these learning methods are learning algorithms for doing a task (inner learning), but meta-learning methods learn how to learn (learning to learn) with rectification from a higher stance (outer learning) <ref type="bibr">[114]</ref>. Meta-learning's target is to have a better learning way (e.g., transferability, generalization) with more data efficiency on fewer training instances <ref type="bibr">[114]</ref>. Representation generalization in meta-learning is directed to learn the learning types and algorithms' metadata as the input primarily <ref type="bibr">[273]</ref>. That metadata covers all data about training and algorithm features, such as hyperparameters, training time, model architecture, model parameters' weights, and evaluation metrics' results <ref type="bibr">[273]</ref>. Meta-learning also follows the NNs' reformation in deep meta-learning, based on their metrics, models, or optimizers respectively, further to more conventional algorithm type choosing and hyperparameter optimization in an agile task-agnostic fashion <ref type="bibr" target="#b115">[116]</ref>.</p><p>Federated learning can do the training process without direct reach to further supplementary data for updating or personalization on the main pretrained model, decentralized and asynchronously on plenty of devices collaboratively <ref type="bibr" target="#b262">[263]</ref>. Each device trains on its own data using its edge computing capacities locally and if it interacts with the main server, it sends the updated model parameters but data <ref type="bibr" target="#b134">[135]</ref>. That was an innovation after the growth of data islands affected by stricter legislations on data acquisition caused by privacy and security issues of users' data accessibility and lower latency in real-time decision making <ref type="bibr" target="#b167">[168]</ref>. Self-Supervised Learning (SSL) does the supervised learning itself only on unlabeled data by previously augmented labels for data, finding inherent similarities of samples to group them by their contrasts, or using both generative and contrastive techniques <ref type="bibr" target="#b175">[176]</ref>. Many of these self-supervisions form on data augmentation by making changes in data intentionally and putting the original data as the anchor for posterior labeled transformed data's probability estimation <ref type="bibr" target="#b123">[124]</ref>. That change can be a masked part of data (e.g., hiding a word in a sentence), adding noise (e.g., color changing), image rotation, or any other data augmentation or synthetic data generation technique. As SSL is needless to manual annotation, would be practical for pretrained representation learning on lots of unlabeled data.</p><p>There are some learning methods only seen in LLMs with a level of scale as In-Context Learning (ICL) and reasoning. ICL is a learning capability in LLMs in which by just feeding the model a few related samples as in a prompt, the model emergently shows extraordinarily inference of prompt's demonstrations and relatively generates outputs <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b286">287,</ref><ref type="bibr" target="#b311">312,</ref><ref type="bibr" target="#b174">175]</ref>. That is needless to any finetuning and parameters' update of the LLM by only analogy of contextual demonstration of prompted input. LLMs have reasoning ability to some extent when stepby-step guide it by a rationale as multi-stage questions, whether by prompting, finetuning, or pretraining <ref type="bibr" target="#b114">[115]</ref>. The majorly used reasoning method is Chain of Thoughts (CoT) <ref type="bibr" target="#b285">[286]</ref> to make LLMs reason by asking it step-by-step in a sequence of prompts or a prompt made of a series of intermediate requests, to generate both the output and the processes of reasoning (rationale). Another method is engineering the rationales to elicit the LLM for reasoning tasks and finetune the LLM on data as of scientific (e.g., Question-Answer, logics, philosophy) and mathematical (e.g., arithmetic, codes) rationales, to modify reasoning power <ref type="bibr" target="#b114">[115]</ref>. Another view is whether LLM's reasoning strategy is single-path as CoT or multi-path, which at each reasoning step multiple probable ways for the next reasoning step will be chosen <ref type="bibr" target="#b279">[280]</ref>. A salient example of a multi-path reasoning strategy is the Tree of Thoughts (ToT) <ref type="bibr" target="#b297">[298]</ref>, as does not consider one specific answer at each step for the further step of thought. ToT like tree branches in a bidimensional sequence of thoughts searches possibilities whether to lead to foresight planning. Planning with a determined initial purpose would be naïve by the dynamical perspective of the world, to tackle it further to planning by decomposing the task into sub-tasks, continuous (iterative) feedback whether from the environment, human, and the model (e.g., self-refinement), were approached recently <ref type="bibr" target="#b311">[312,</ref><ref type="bibr" target="#b279">280]</ref>.</p><p>Multimodal learning is learning from multiple modalities into one ML model with fused mapped representations <ref type="bibr" target="#b23">[24]</ref>. After representations' fusion and alignment, the multimodal model would extract knowledge by reasoning from relations, intermediates, and logical or causal inferences <ref type="bibr" target="#b164">[165]</ref>. Multimodal representations are in two forms of which joint representations that amalgamation of multimodalities into one representation simultaneously, while coordinated representations are by learning multiple modalities apart from each other but not detached at the end <ref type="bibr" target="#b23">[24]</ref>. Lastly, there are Few-Shot Learning, One-Shot Learning, and Zero-Shot Learning methods to have models that can learn with the least data. Few-shot learning would be done via some other learning methods, such as TL, meta-learning, data augmentation, and multimodal learning <ref type="bibr" target="#b255">[256]</ref>. Zero-shot learning tries to learn new unseen samples just in time by adapting the semantic information that formerly had learned, such as meta-learning and ICL <ref type="bibr" target="#b220">[221]</ref>. LLMs potentially have all the former learning as by finetuning or prompting whether to address, step-by-step reasoning ability as few-shot learning, ICL is caused by one-shot learning, and prompting is zero-shot learning <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b220">221,</ref><ref type="bibr" target="#b174">175]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Model Architectures</head><p>The advancements in NNs architectures were cohesively continuous in the last decade, as there were some tipping points. As well as new practical learning ideas in them, a few algorithms to some extent changed the paradigm while were introduced for one specific task, could impact various applications (e.g., Convolution, Transformer). DL techniques are generally categorized into Discriminative, Generative, and Hybrid network models <ref type="bibr" target="#b241">[242,</ref><ref type="bibr" target="#b247">248]</ref>. Discriminative models learn the probability of the output's affinity with the subjective data, based on their features and representations (e.g., the picture shows a cat with 80% likelihood). Generative models learn the probability of a statistical distribution in the data, which might be regarding or regardless of labels. Discriminative DL detects how probable an input belongs to a particular class, while generative DL generates outputs similar to their trained data. Hybrid methods are a combination of different methods to be adopted the competence of each for more development. A Hybrid DL model can be made of multi-Discriminative models or multi-Generative models, a compound of both Discriminative and Generative models, or other types of techniques with each other.</p><p>However, based on the anchor of our point of view (as an analyzer), this categorization will differ, as in this paper the focus is on SOTA, it appears that Transformer architecture <ref type="bibr" target="#b273">[274]</ref> is the tipping point in the last decade of DL architecture progressions. But that does not change the importance of prior architectures or make them useless at all. Some NN architectures in the history of DL are more prominent to get AI in its today's eminence, which are: Restricted Boltzmann Machines, Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), Auto-Encoder, Deep Belief Networks, and Generative Adversarial Networks (GAN). Some of these conventional architectures are covered in this section to get to SOTA DL concisely. Here, traditional ones that are still getting vastly used are mentioned first, and then the SOTA architectures are addressed while in the most abstract way.</p><p>Convolutional Neural Networks (CNN), a deep feedforward NN that proved its efficiency in the history of CV on one of the most effective datasets in ML, ImageNet <ref type="bibr" target="#b63">[64]</ref>. CNN aims to uncover patterns within related features (e.g., motifs in images) from each latter layer and connect them locally by convolution; the poolings compress features to dimensionality reduction towards a fully-connected layer for the reasoning <ref type="bibr" target="#b154">[155,</ref><ref type="bibr" target="#b96">97]</ref>. CNNs learn well on multi-dimensional data arrays including signals, sequences (entirely), and language in 1D, images and audio in 2D, and video in 3D <ref type="bibr" target="#b154">[155]</ref>. Some successful convolution-based architectures are AlexNet [150], VGGNet <ref type="bibr" target="#b250">[251]</ref>, and ResNet [109], in which their citation numbers partly demonstrate how impactful CNNs could be.</p><p>Recurrent Neural Networks (RNN) are another effective DL architectures to learn sequential patterns of arrays via recurrent cells and remember historical changes along data as NLP, videos, or trajectories through backpropagation through time <ref type="bibr" target="#b154">[155]</ref>. That had a major problem with long-term recalling which gradients in lengthy durations of learning (frequent time steps) would continuously get vanished until Long Short-Term Memory (LSTM) architecture <ref type="bibr" target="#b111">[112,</ref><ref type="bibr" target="#b112">113]</ref>. In the vanilla (first) LSTM <ref type="bibr" target="#b95">[96]</ref>, the most famous LSTM type and even mentioned as just the LSTM <ref type="bibr" target="#b271">[272]</ref>, each block uses three gates of forget (Gated Recurrent Units), input, and output, on the previous layer's memory cell. The forget gate calculates to choose unnecessary memories for deletion, the input gate controls which context to add in the cell and previous layer's output, and the output gate concludes the output of the LSTM block <ref type="bibr" target="#b95">[96]</ref>. Further to that, peepholes' connections make the LSTM cells capable of learning timings better via linking them to gates, thus the LSTM accumulates long-term states to reduce gradient vanishing of consecutive time steps <ref type="bibr" target="#b154">[155,</ref><ref type="bibr" target="#b95">96,</ref><ref type="bibr" target="#b271">272]</ref>.</p><p>Generative Adversarial Networks (GAN) is a game theory based constructive competition between two NN-based machines in which one of them is a generator and the other is a discriminator in connected parallel <ref type="bibr" target="#b93">[94]</ref>. Their interaction creates a control loop between machines to adjust the cost function; generator tries to generate outputs in a way that discriminator cannot recognize while discriminator guides by detecting fake generated ones from the original input data <ref type="bibr" target="#b92">[93]</ref>. This confrontation adheres to adjustments to get closer to Nash's equilibria, which are local saddle points of both machines' cost functions <ref type="bibr" target="#b100">[101]</ref>. Consequently, the generated outputs will be realistic enough to be unrecognizable for any discriminator (maybe humans), whether are fake or not, and capable of learning styles of different modalities in terms of domain adaptation as synthesis or manipulation <ref type="bibr" target="#b93">[94,</ref><ref type="bibr" target="#b100">101]</ref>.</p><p>After seven years of releasing the first Transformer model architecture (2017-2024) <ref type="bibr" target="#b273">[274]</ref>, it can be stated that Transformers are effectively prominent architectures in AI like CNNs and RNNs as were the basis of myriad further innovations and concepts. The SOTA AI are Transformer-based models since Transformers are called 'transformer', their flexible and transposable architectures have worked on almost all data modalities (e.g., text, image, video, audio, multimodality) and AI applications (NLP, CV, graph learning) <ref type="bibr">[121]</ref>. The vanilla Transformer is a sequence-to-sequence model predicated on the Attention mechanism, made of an encoder and a decoder; which each involves two layers of multi-head self-attention modules and position-wise feedforward fully connected networks where there are residual connections around each <ref type="bibr" target="#b273">[274]</ref>. The vanilla Transformer architecture is hinged completely on the self-attention mechanism to learn contextual interconnections of sequences, entirely from close converged relationships to track long-range sequential effects <ref type="bibr" target="#b170">[171,</ref><ref type="bibr">121]</ref>. That differs from conventional recurrence methods as Transformers try to compute the relations of sequences with each other in parallelized encoder and decoder blocks, while recurrence is just processing the series of sequences recursively.</p><p>Whether the vanilla Transformer is not efficient on 'not' large-scale data, has high complexity in computing very long strings of sequences, and is not adaptable to any downstream task; while further Transformer-based architectures are adjusted in each direction <ref type="bibr" target="#b170">[171]</ref>. Beside of these adjustments, Transformers pushed the TL's pretraining paradigm in a new way, as Transformers are in a neutral prejudgment position structurally despite CNNs and RNNs <ref type="bibr" target="#b170">[171,</ref><ref type="bibr">121]</ref>. That lack of inductive bias showed to be more appropriate for being in mainstream Pre-Trained Models (PTM) on large data, as is the overfitting cause for training on small datasets <ref type="bibr" target="#b313">[314]</ref>. Transformers architecturally can be classified by their models' usage namely, decoder models (e.g., BERT, LLaMA), encoder models (e.g., GPTs), and encode-decoder models (e.g., BART, T5) <ref type="bibr" target="#b170">[171]</ref>.</p><p>Foundation models as Large PTMs with huge parameters trained on large amounts of data, provide knowledge generalization and adaptation and make it possible to get higher performance on downstream tasks with limited domain data accessibility <ref type="bibr" target="#b313">[314]</ref>. The use of finetuning and prompting on restricted data (few-shot or zero-shot) of downstream tasks has considerable advantages <ref type="bibr" target="#b313">[314,</ref><ref type="bibr" target="#b106">107]</ref>. Adaptations by finetuning and prompting on PTMs as mainstream make building a new model from scratch unrequired for other downstream tasks as generalization increases. First, PTM is trained on large-scale data mainly by SSL and that generalization comes from varied big data while the attention mechanism of Transformers learns contextual relations within the dataset (e.g., word embeddings) <ref type="bibr">[121]</ref>. Moreover, that makes a large amount of labeled domain-specified data inessential for downstream tasks, which are rare and costly while PTMs can be trained on any data modality and different AI applications such as CV, NLP, and graph learning <ref type="bibr" target="#b313">[314]</ref>. Lastly, if the scale of the PTM increases enough (e.g., model parameters) as LLMs, not only raises downstream tasks' performance and data efficiency <ref type="bibr" target="#b38">[39]</ref> but can also cause emergent abilities such as ICL and reasoning <ref type="bibr" target="#b286">[287]</ref>.</p><p>Although there are plenty of LLMs currently, here Generative Pretrained Transformer (GPT) series as one of the most salient models are discussed which had made LLM-oriented properties (ICL, reasoning) for the first time (GPT-3). GPT-1 model architecture's core is decoder-only Transformer-based for text's long-term dependencies, in a way to have it in unsupervised SSL PTM, and supervised finetuning toward to be task-agnostic <ref type="bibr" target="#b106">[107,</ref><ref type="bibr" target="#b222">223,</ref><ref type="bibr" target="#b311">312]</ref>. GPT-2 <ref type="bibr" target="#b223">[224]</ref> is similar to GPT-1 architecturally with adjustments in layer normalization and residual network, besides scaled in model parameters (1.5B) <ref type="bibr" target="#b38">[39]</ref>. GPT-3 as a game changer, the largest scaled PTM of its time at model parameters (175B), revealed emergent abilities such as ICL, reasoning, and following instructions <ref type="bibr" target="#b286">[287]</ref>. Compared to the prior architecture, GPT-3's Transformer model used dense and sparse attention along with gradient noise guidance; while added to the massive model parameters, toward high performance in few-shot learning, and zero-shot learning <ref type="bibr" target="#b286">[287,</ref><ref type="bibr" target="#b311">312,</ref><ref type="bibr" target="#b200">201]</ref>. Also, GPT-3 was finetuned on Reinforcement Learning from Human Feedback (RLHF) <ref type="bibr" target="#b208">[209]</ref> for aligning the model to not generate harmful content as instruction tuning. Consequently, the domain adaptation steps went further to pretraining and finetuning with the addition of instruction tuning and prompting <ref type="bibr" target="#b200">[201,</ref><ref type="bibr" target="#b174">175]</ref>.</p><p>GPTs got multimodal (MLLM) in GPT-4, constructed on images in addition to texts in input training modalities, while it only generates text output with higher performance and reliability than the priors <ref type="bibr" target="#b3">[4]</ref>. Afterwards, GPT-4 Turbo is with higher knowledge domain and longer context support (tokens) with improvements in multimodality by adding audio (speech) modality to image and text <ref type="bibr" target="#b205">[206]</ref>. More recently, GPT-4o amalgamated more modalities in an end-to-end fashion, which as input has text, audio, image, and video, and as output, generates all except videos on one model <ref type="bibr">[317]</ref>. While LLMs and MLLMs do reasoning, planning is the next step; which, after decomposition of the whole issue into parts to understand and plan towards future decisions to be made, as in commonsense knowledge <ref type="bibr" target="#b282">[283]</ref>. Agents can do planning using feedback whether from the environment (physical or virtual), humans (prompts), or their model (e.g., GANs, actor-critic, selfrefinement) <ref type="bibr" target="#b279">[280]</ref>. The actor planner agents are not just limited to LLM agents as language-based task planners, but as vision-language multimodality can be on MLLMs <ref type="bibr" target="#b180">[181]</ref>. These robots can learn via interactions to plan for their reactions based on received (sensed) feedback <ref type="bibr" target="#b279">[280]</ref>. Those interactions might have been directed and taken action control policy as RL to let the model explore upon it, which the model would be a PTM as MLLMs <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b180">181]</ref>. A very impactful related example of an interactive environment with humans is ChatGPT, which finetuned on the GPT model series provides a conversational space of prompts and generations aligned by RLHF based on its proper data of both dialog and alignment samples <ref type="bibr" target="#b313">[314,</ref><ref type="bibr" target="#b311">312]</ref>. As the nature of TL works on three sides of pretraining, finetuning, and prompting, at each section the data is much more effective which in an interactive space if more feedback would be given to the model as data for finetuning, the model gets richer. Environmental feedback based on the environmental attributes can be different whether if it is a simulation, physics-based simulation, or dynamic physicsbased game environment with the collaboration of multi-agents of humans or programmed features or an AI-based agent <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b180">181,</ref><ref type="bibr" target="#b107">108,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b114">115]</ref>. That environment might be in the real world as a robotic arm after being pretrained on simulation, or a digital twin of its physical twin's real system <ref type="bibr" target="#b264">[265]</ref>.</p><p>Visual modality is necessary for environmental feedback while in conversational agents as GPT-4v enabled ChatGPT <ref type="bibr" target="#b205">[206]</ref>, multimodality is getting used ubiquitously these days. However, the Large Vision Models (LVM) as Segment Anything Model <ref type="bibr" target="#b143">[144]</ref> are like LLMs, while the visual modality is substituted with lingual (text) modality <ref type="bibr" target="#b277">[278]</ref>. These CV mainstream models can handle many downstream tasks as the Segment Anything Model generalized to do tasks like image analysis, multi object detection (plus segmentation), multi object tracking, and image editing <ref type="bibr" target="#b143">[144,</ref><ref type="bibr" target="#b277">278]</ref>. Properly engineered prompting as an interaction in LVMs is as effective as in LLMs (e.g., CoT, ToT), while in LVMs the prompt is also an image or a video to lead the model in the aimed direction <ref type="bibr" target="#b277">[278]</ref>, and with text prompts the model might not exactly get the idea well as for editing <ref type="bibr" target="#b176">[177]</ref>.</p><p>Moreover, some LVMs are made to surrogate reality in video physically trying to be at the quality of world simulation, which Sora as a text-to-video generator follows <ref type="bibr" target="#b37">[38]</ref>. They also can simulate real-world looking renders or imagine unreal scenes close to the physical reality by image synthesis, while cannot always afford reasonable physics-based knock-on effects <ref type="bibr" target="#b176">[177]</ref>. In addition to using the diffusion transformer model in Sora's architecture, diffusion models are being used in many CV tasks, especially in visual generations <ref type="bibr" target="#b176">[177]</ref>. Diffusion models similar to LLMs that mainly predict the unseen masked words add augmented noise into the data (e.g., image) randomly (e.g., by Gaussian probability distribution) <ref type="bibr" target="#b60">[61]</ref>. Afterward, a DL model learns to denoise the data to not only detect the original input data from noisy, but it also will generate based on its prediction. As this method is slower than GANs in image generation, it performs with higher quality <ref type="bibr" target="#b60">[61]</ref>. Although in image generation LVMs are unrecognizable to be fake, they are not still in a quality to generate video physically inclusively which deduces it is an absolutely complex job to do after that much other accomplishments. There are plenty of other successful model architectures not addressed in this section indeed, but a noteworthy point is here the purpose is to observe just a few architectures conceptually, while it is highly recommended to check referred resources. A noteworthy point is that some models are unclear about their architectures such as GPT-4 series.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SPACE CONTEXT</head><p>The spatial characteristics of how the environment is, the details of the surroundings' things, and the physical laws of space, are about the Space context. The place itself incorporates various information such as where it is by its geography (e.g., maps' data), how things are in it by their positioning, physical status of the environment (e.g., energy usage), what the applications of location are (e.g., commercial), the occurring spatial events of surroundings (e.g., weather), and the transportation conditions (e.g., public transportations). Whether analyzing any spatial attribute inclusively is challenging because of the generic philosophy of this topic, it is tried to consider the more related contexts of Space to AmI. Whether physics is the rules of nature, physical attributes of any space are vulnerable information in Space context. Getting aware of physical conditions as commonsense is what myriad animals have at their young ages while for high-level AI still is challenging. That physics-awareness path is another point of view mentioned in this context. Therefore, Space context is categorized into three sections at scale, location, and physics; among diverse schools of thought about observing the world as a "space".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Scope</head><p>The scope, size, complexity, and immersion are some parameters of the scale of an environment that are discussed in this part. Since the IEs would be assumed as cyber-physical embedded systems, they can be determined to be any spatial systems networked at multiple scales by computation powers distributed at any level of the system <ref type="bibr" target="#b101">[102]</ref>. 3D immersive cyber-physical environments can include both fully Virtual Realities (VR) in cyber, and the physical environments (original). Integration of them also as Augmented Realities (AR) and Mixed Reality (MR) to merge VR into the physical world has common directions with AmI <ref type="bibr" target="#b274">[275,</ref><ref type="bibr" target="#b18">19]</ref>. All types of environments can be three-dimensional (3D), while VR and AR can be in 2D as well. In the 21st century, a regular physical environment by the pervasiveness of digital devices (e.g., smart IoTs) potentially is a cyber-physical space.</p><p>The applications (objectives) of smart cyber-physical spaces (IEs) are various, similar to their scales, which scale of each diverges in quantitative and qualitative factors. For example, indoor and outdoor is one way to discuss space scale, though an indoor environment might be larger and more complicated than an outdoor one (e.g., hospital vs pickup truck's cargo bed). To start from micro-scale to macro-scale, a place where devices are in it has the capacity to be called an IE. Even though every smart environment needs them as components, one might say any space containing smart devices to be applicable to AmI is potentially a smart environment. A microscopic instance might be a smart vehicle that seems to be beyond just a thing, whether if connected to the internet an IoT. Then at the mesoscopic level, smart homes, stores, offices, gyms, high streets, schools, and buildings by having a proper infrastructure will further to smart environment towards becoming an IE. Macroscopic large-scale ones as are SoS, are malls, hospitals, hotels, museums, universities, industries (e.g., factories, agriculture, grid), airports, ports, and cities.</p><p>Any environment, wherever large it is, has boundaries and this delimitation has to be understood, especially for interacting with the ecosystem. As everything is a part of the ecosystem, the ambit of the IE and ecosystem must be distinguished and informed (e.g., causes and effects); where any change in the ecosystem is connected to the people and the environment in two-way, such as weather conditions and global warming. In some environments, there is a full enclosure (e.g., hospital) and in some, there is not (e.g., city). Whereas a place like an airport, while has its boundaries, embodies both closed indoor space from the ecosystem (terminal) and in touch with the ecosystem (runway). Any of these places are a separate subject and should be analyzed and studied by its parameters as a wide range point of view, like in Industry 4.0 <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b90">91,</ref><ref type="bibr" target="#b183">184,</ref><ref type="bibr" target="#b151">152,</ref><ref type="bibr" target="#b221">222,</ref><ref type="bibr" target="#b39">40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Location</head><p>The spatial position of humans and things and what is understandable from that metadata is about location. It mainly is considered as information about vertical and horizontal dimensions, while altitude might be necessary too. The main vision of positioning (i.e., localization) is whether it is indoor or outdoor. As the most used outdoor positioning method is the Global Navigation Satellite System (GNSS) signals as the Global Positioning System (GPS), it is not accurate in an indoor space and does not respond deep indoors <ref type="bibr">[157,</ref><ref type="bibr" target="#b16">17]</ref>. To solve this issue, Indoor Positioning Systems (IPS) such as Mobile Node-based Localization (MNL), Reference Node-based Localization (RNL), Inertial Measurement Units (IMU) (e.g., accelerometer, gyroscope, magnetometer), and Proximity-based detection, perform for navigation and tracking purposes <ref type="bibr" target="#b79">[80]</ref>. By the variation of requirements, different communication network methods such as Wi-Fi, RFID, and BLE would be used in IPSs whether omnipresent camera-based approaches work in both indoor and outdoor spaces in a variety of CCTV sizes <ref type="bibr" target="#b79">[80,</ref><ref type="bibr" target="#b210">211]</ref>.</p><p>In some scenarios, knowing the location of static objects is helpful, as in AAL and HAR <ref type="bibr" target="#b52">[53]</ref>, assuming them in a certain location on a plan layout, besides positioning movable ones. In order to reach location-awareness, location-enabled IoTs are needed pervasively where technologies like Low-Power Wide-Area Network (LPWAN) and 5G are recently ubiquitous <ref type="bibr" target="#b162">[163]</ref>. Monitoring location-enabled IoT devices carried by people express multiple contextual information in each context, such as spatial trajectories and paths (e.g., traffic condition, process management, inventory control). Where the environment itself is placed and how that is, are other qualities of the location. This perspective has the ability of vast inclusion and complexity; as how the neighborhood and geographical attributes of the environment are even economically and culturally (e.g., residential or commercial or industrial, economical states, urban or rural, near or far (to another location), weather attributes). Similar to scales' diversities, in this context, inclusiveness needs research on each subject specifically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Physics</head><p>Physics rules are the laws of nature and perceiving them makes humans and animals aware of universal realities which can potentially be the exact same direction for machines. The world might physically be a unity of matter, energy, and interactions between the first two. Their conditions (e.g., gaseous substance, liquid substance, or solid matter) and interaction forms (e.g., potential, kinetic, thermal, electrical, chemical) are in a relationship with effective parameters (e.g., temperature, intensity, velocity, frequency). Since we are living in the material world, waves with diverse energy levels are almost everywhere in the cosmos, therefore, any environment has diversified waves through itself, either electromagnetic or mechanical (medium dependent). But as a matter of fact, we humans are limited to feeling all wave frequencies unarmed. Whether if it is Radio Frequency or x-rays and gamma rays in electromagnetic, or mechanical infrasonic and ultrasonic in Hertz  or Decibel  out of our sensible domain. However, our tools (e.g., IoT, WSN) do sense. As we sense, at construction there are standards for insulation of thermal, moisture, sound, and high energy radiations. With the amazing advancements of AI, now it is feasible for some models to reason step-by-step, solve problems, and plan future actions. Though they are still not learning it the way biological living entities are and have challenges in some simple ones, such as commonsense, they can be modeled to be more aware of physics laws. Here, firstly an abstract vision of sensing and monitoring is overviewed, and then an outlooked side of AI's physical perception is covered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Monitoring</head><p>Physical properties of a system should get sensed and measured for documentation, if unfeasible, must be estimated from other measured attributes <ref type="bibr" target="#b264">[265]</ref>. Measuring application differs depending on its utilizations for the model, whether for its input, update model states (realtime), monitoring, or reasoning. Physical properties in intuitive physics are bifurcated into clear observable properties and latent properties <ref type="bibr" target="#b68">[69]</ref>, which observable properties examples include position, velocity, and geometrical shape. Latent physical properties require reasoning, such as density, mass, and friction. Furthermore, monitoring can be done for many physical subjects as energy and pollution. The monitoring capabilities of IoT and WSN are extensive as their main applications are for monitoring healthcare and industries <ref type="bibr" target="#b305">[306,</ref><ref type="bibr" target="#b182">183,</ref><ref type="bibr">239]</ref>. Monitoring the physical environmental conditions by IoT and WSN can be done in all matter substances, whether in air, water, or soil <ref type="bibr">[269]</ref>. Smart environments' monitoring has usages in quality control, quality monitoring, and forecasting majorly in air and water quality, which below a level of quality would be pollution <ref type="bibr">[269,</ref><ref type="bibr" target="#b151">152]</ref>. Moreover, an important issue is to manage waste within the lifecycle as it has an impact on any type of pollution and also energy, for all people by effects of the ecosystem <ref type="bibr" target="#b151">[152]</ref> With IoT-based systems real-time metering of energy consumption monitoring is feasibly affordable, even for individuals globally <ref type="bibr" target="#b116">[117]</ref>. That can contribute to the decision-making process to raise optimization in effective energy management using ML where the smart grids are the result of this vision <ref type="bibr" target="#b132">[133]</ref>. That incorporates the massive power generators and microgrids of the produced electricity by solar panels on the roof of houses towards consumption trends of households <ref type="bibr" target="#b305">[306]</ref>. Pollution also as another example can be sensed and monitored using WSN and IoT in any three phases as air pollution, water pollution (abnormal pH or toxic), and soil pollution (degraded or salty) <ref type="bibr">[269]</ref>. Air pollution is a serious problem for everyone to monitor <ref type="bibr" target="#b119">[120]</ref>, though water and soil pollution is more directly decisive for particular smart environments such as smart agriculture, industries, and smart cities <ref type="bibr">[269]</ref>. Air condition of the environment is further than pollution purifying, temperature, lighting, oxygen concentration, and humidity are other parameters that WSN can frequently afford <ref type="bibr" target="#b151">[152]</ref>. Beside of these types of pollution, hazardous radiation is another challenge, especially in spaces with more variety and ubiquity of wireless electromagnetic devices' exposure <ref type="bibr" target="#b0">[1]</ref>. One implication of them is the high specific absorption rate of electromagnetic fields affecting humans biologically (e.g., tissues) by their heat whether jointly, as from Radio Frequencies <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Perception</head><p>Apart from what we cannot sense while machines can, they can do omnipresent management of what physical condition we need or like to have. Whether they have still problems with perception of physics (e.g., commonsense) spatiotemporally as in autonomous driving. To understand the system's physical state and behaviors, the first step is the physics laws. Physical priors' knowledge as physical laws and rules of the world in a frame of AI relationship would be viewed as Physics-Informed ML (PIML) <ref type="bibr" target="#b107">[108]</ref>. PIML is categorized into Partial Differential Equations (PDE), symmetry constraints, and intuitive physics <ref type="bibr">[326]</ref>. Physics priors' categories from another point of view within CV are (1) equations and constraints, (2) data fusion, (3) representations, (4) physical or statistical property, (5) physical variables, and (6) hybridization <ref type="bibr" target="#b24">[25]</ref>. The attention here is more on intuitive physics, but high inductive biased physical priors as PDEs have plenty of usages in PIML as scientific discoveries <ref type="bibr" target="#b107">[108]</ref>. In this section, physics priors' perception is covered in reasoning ability and the modeling methods for acquiring it are discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.1">Modeling</head><p>The AmI's Physics-Awareness requires linking the physical state of the environment to the digital state of the model. That connection can be done in three modeling methods if it is completely physics-based, data-driven, or a hybrid amalgamation of both. From less need for data to higher data requirements would be physics-based, hybrid, and data-driven methods <ref type="bibr" target="#b107">[108]</ref>. The physics prior knowledge would be a physical awareness enabler in the three ways of modeling, while empirical data are useful in PIML too <ref type="bibr" target="#b107">[108,</ref><ref type="bibr" target="#b24">25]</ref>. Whether physics-awareness is not only dependent on data and properties parameters based on priors, it (e.g., PDEs) can be derived into the architecture as in Physics-Informed Neural Networks (PINN) <ref type="bibr" target="#b225">[226]</ref>. If the physical modeling is for CPSs, it is categorized to have modeling based on state machines, rules, and agents <ref type="bibr" target="#b114">[115]</ref>.</p><p>Fully physics-based modeling is designed to build a model as in computer-aided engineering, a software based on physical prior like PDEs and geometries would be made and initialized for the model's utilization as in simulation <ref type="bibr" target="#b264">[265]</ref>. In abstract exemplar, a physics-based model can be made out of geometrical measurements of objects, while in more high-level modeling, the solid body of an object by its strain can be analyzed. The other types of physics-based analysis and simulation also can be about thermal flow, fluid flow, kinematics, dynamics, and multiphysics <ref type="bibr" target="#b264">[265]</ref>. Data-driven approaches such as DL by high-level representation learning, in CV for example, can learn relations in video data as multi object recognition and tracking in a way to capture physical priors, while it would not satisfyingly perceive physics. Other ones would be SSL on augmented rotated image data to learn which rotation is a natural image without any knowledge about the gravity of the earth, and data-driven models' capability of learning systems dynamics from sensory data <ref type="bibr" target="#b264">[265]</ref>. Hybrid methods are nevertheless to be composed of selected good qualities of any modeling approaches. DL and physics hybridization can handle both physics' forward problems (e.g., weather forecasting) and inverse problems (e.g., PDE discovery) <ref type="bibr" target="#b107">[108]</ref>, as in models like PINNs <ref type="bibr" target="#b225">[226]</ref>. For example, in the Human context, for analyzing Humans' behavior and sentiment, DL can learn the physical data or dataset labeled on physical attributes like body skeleton models <ref type="bibr" target="#b26">[27]</ref>. While some physical constraints as speed limits can help, physics priors in many scenarios do not work in prediction as in pedestrians' trajectories, which are dependent on psychological state and biophysical constraints beside many other spatiotemporal contexts <ref type="bibr" target="#b133">[134]</ref>. Adding biological constraints as prior physical knowledge of human anatomy as in body posture and gesture to the DL model is better learnable than pure data-driven <ref type="bibr" target="#b24">[25]</ref>.</p><p>As physics-based models of CPSs are robust and interpretable, they are not flexibly intelligent, as data-driven and hybrid methods are <ref type="bibr" target="#b224">[225]</ref>. Physics-based models are better models of a physical system if they inclusively have all system dynamics measured, get updated, and modeled principally on all levels (micro to macro, inside to outside) of the system's behavior <ref type="bibr" target="#b224">[225]</ref>. That is toughly difficult by the costs and the dynamically unpredictable nature of the world to make it much time consuming to be updated. Whether the changes would be noticed and perceived, its complexity puts interpretability and accuracy at risk <ref type="bibr" target="#b224">[225]</ref>. On the other side, model-based methods (e.g., physics-based models) may need no recorded data for modeling a system except for calibration while data-driven ones (e.g., DL) are data-oriented <ref type="bibr" target="#b24">[25]</ref>. If data-driven models have proper data of the task, they can accurately afford it without any other model if there is no need for high-level reasoning while physical perception requires it.</p><p>A salient subject to find both physical-digital relationships and a variety of hybridization is twinning. AI-based digital twins are the intersection points of real-world systems' data (e.g., from IoTs), physics-based models (e.g., multiphysics simulations), and AI, to not only analyze systems (e.g., counterfactual what-if simulations) but also the finding optimized decisions in each state space <ref type="bibr" target="#b18">[19]</ref>. Digital twins follow the goal of being the inclusive twin of a real physical system, no matter which system it is. That covers the relationships too, whether the physical to virtual (digital) or virtual to physical enabling, to measure and model the physical system's objects and dynamics <ref type="bibr" target="#b264">[265]</ref>. Digital twins cover many hybrid modeling approaches as in geometrical modeling (laser scanning, VR, AR), physics modeling (structural analysis, flow analysis, kinematics and dynamics, multiphysics), data-driven modeling (degradation, surrogate, dynamic system identification), and PIML <ref type="bibr" target="#b264">[265]</ref>.</p><p>Hybrid models can be incorporated in either data and architecture or both in cooperation to have both physics-based and data-driven models made <ref type="bibr" target="#b107">[108,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b264">265,</ref><ref type="bibr" target="#b114">115]</ref>. In data hybridization, one common approach is to have the output of a physics-based model such as a firstprinciple multiphysics simulation, as the input of a data-driven model like DL models to learn, because that synthetic data is less limited <ref type="bibr" target="#b107">[108,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b114">115,</ref><ref type="bibr" target="#b264">265]</ref>. It can also be stated that having physics-based simulations as data generators is easier than comprehending the exact system dynamics <ref type="bibr" target="#b114">[115]</ref>. The hybrid model can be designed by using the data-driven models to learn datasets that are composed of physical knowledge about a specific context, as physics QA about advanced complex school tests <ref type="bibr" target="#b200">[201,</ref><ref type="bibr" target="#b62">63]</ref>. Multimodality and data fusion of multiple domains' heterogeneous contexts as historical, preprocessed (e.g., labeled), and empirical, is another way to cover more physically inclusive data. If one aspect of those is physics-oriented, would lead to being more physically aware <ref type="bibr" target="#b24">[25]</ref>. Architectural adjustments to inject physics priors (PDEs) into a data-driven model like DNNs' (DL) loss function can work on both physics' forward and inverse problems, as in PINNs <ref type="bibr" target="#b225">[226,</ref><ref type="bibr" target="#b107">108]</ref>. When these unidimensional physics-awareness enabling methods (data, architecture) get combined, several hybrid methods will be made. One near example is having physical data in PINNs <ref type="bibr" target="#b107">[108]</ref>, while innovations are more.</p><p>Hybridization can be in different levels of architecture such as model, loss function (objective), regularization function, and optimizer <ref type="bibr" target="#b107">[108,</ref><ref type="bibr" target="#b133">134,</ref><ref type="bibr" target="#b224">225,</ref><ref type="bibr" target="#b24">25]</ref>. Intuitive physics can be utilized in data-driven models' architectures as constraints in kinetic momentums or energy conservation <ref type="bibr" target="#b107">[108]</ref>. The recent hybrid techniques of data and architectural inclusiveness can be viewed within learning methods of TL, SSL, prompting, and their amalgamation <ref type="bibr" target="#b264">[265,</ref><ref type="bibr" target="#b107">108,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr">292,</ref><ref type="bibr" target="#b174">175]</ref>. TL is mostly based on pretraining on big general data (e.g., in LLMs) or large synthetic datasets (e.g., from physics-based simulations or digital twins), then finetune on real-world physical data (e.g., empirical) with higher access limitation (e.g., in PIML) <ref type="bibr" target="#b107">[108]</ref>. The synthetic data might be from the agent's exploration and exploitation with trial and error in the digital environment (simulation or digital twin) through methods like DRL <ref type="bibr" target="#b264">[265]</ref>. In SSL large synthetic data would be learned by predicting each time-step, whether if TL gets added, the pretrained SSL model gets finetuned on another limited data. Prompting majorly specified to foundation models can be done by physical datasets as physics QA and exams or any physics prior knowledge, prompted on large pretrained model as LLMs. Another hybrid way is to have an AI agent (e.g., RL) to explore and exploit via action, feedback, and reaction whether in simulation, digital twin, or the real physical twin (e.g., controlling, path planning) <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b264">265]</ref>. In AI-based digital twins, the interacting feedback might come from the real physical system (physical twin) to learn and react as in controlling and path planning tasks <ref type="bibr" target="#b264">[265]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.2">Reasoning</head><p>Intuitive physics is about the commonsense that we humans and many animals have to reason how our surrounding mechanisms work from understanding systems' behavior to reacting to them <ref type="bibr" target="#b107">[108]</ref>. Reasoning physics is a strategy to take physics priors into AI whether as post-processing or inference <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b107">108,</ref><ref type="bibr" target="#b24">25]</ref>. Reasoning with commonsense is about learning physical properties as underlying concepts toward manipulating them <ref type="bibr" target="#b62">[63]</ref>. The latent physical properties (e.g., density) have to be inferred after observation <ref type="bibr" target="#b68">[69]</ref> enabling the model to physically interpret the scene <ref type="bibr" target="#b133">[134]</ref>, however, the physical reasoning is not that simple. Physical reasoning tasks in intuitive physics whether in detection, prediction, causality, or inferring, can be categorized into Physical Properties, Physical Interaction Outcome, Physical Trajectories, Physical Dynamics, Visual State, Violation of Expectation, and counterfactual reasoning <ref type="bibr" target="#b68">[69]</ref>. Reasoning also can be done by hybrid methods, whether by a dataset or benchmark made for physical reasoning, whether in visual (2D or 3D) or text to be learned by datadriven models <ref type="bibr" target="#b62">[63]</ref>. Physical reasoning can also be made by large models such as LVMs, LLMs, and MLLMs as mentioned in the DL System context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">TIME CONTEXT</head><p>The philosophy of time and chronological analysis as a physical fact should be viewed by human's perception of time beside of the way machines can read and process it. As vast as modern philosophy and physics (also in thermodynamics), there is time all across the universe to interpret it through standpoints. However, to analyze how IEs can understand time and considering its related issues as a natural interaction, checking it with AmI's systematic settings seems more useful. Though, getting inspiration from humans' and animals' time perception, works from philosophy, psychology, neuroscience, cognitive science, and interdisciplinary approaches (e.g., neurophysiology, neuropsychology, and cognitive psychology), would help.</p><p>Time is interpretable in terms of the human perception of time via the peripheral signals (stimuli/cues) that our somatic and cognitive (mental) channels sense from space-time and process in neural (nervous) systems. The process of 'becoming' is defendable in that category <ref type="bibr">[291]</ref>. A taxonomy of temporal experiences would be a guiding signal for time perception as the 'elementary time experiences' taxonomy by Ernst <ref type="bibr">Pöppel [219]</ref>. That categorizes time experiences as subjective phenomena such as duration estimation, subjective present, temporal organization, temporal processing units (simultaneity, successiveness, and temporal order), and continuum of time <ref type="bibr">[219,</ref><ref type="bibr" target="#b217">218]</ref>. Duration estimation is how much time we think had taken for an event in the past subjectively. Subjective present is about the short episodic intervals of the present (nearly 3 seconds) which is specious. Temporal organization is how we plan and anticipate the possible future actions. Temporal processing units are the successive sequences of events to represent the non-simultaneity and the latency of our cognitive procedures to perceive now (present) is near real-time. Temporal continuity is about the subjectivity of past to future as the passage of time.</p><p>Our elementary experiences apparently have common with the chronological concept that AmI can have. To exemplify, pure real-time even with 5G (even 6G) is impossible since it is near real-time. As mentioned in the DL architectures section, a major objective of the model can just be about temporal processing units (e.g., RNNs). On the other side, assessing neuropsychological research on time perception towards computational models and robotics <ref type="bibr" target="#b28">[29]</ref> might be a gleaming peephole in the direction of comprehending how AI may perceive time. Another way to address Time context is patterns of time, which are made upon the repeating spatiotemporal occurrences that shaped the lives of all living creatures of the earth.</p><p>The world is always changing and the human brain has been trying to extract patterns from Pareidolia to spatiotemporal periodic events, even if they are not iterative as cyclic constituents. Some are by our chronological perception before (past), now (present), and then (future), while some are realities of our life as the daily routine of earth rotation at circadian rhythm. These recurring phenomena might revert to one of the previous states or not, which on that basis this part is bifurcated. A pattern might be just algorithmic and not conclude in a complete cycle, but still recur as the succession of future into present and be passed finally, while a past space-time will never be repeated twice. Various phenomena repeat as stationary travelers as the earth's gravity causes recurrence alternate of days (morning, noon, evening, night), moon's rotation, seasons, and years. Here cyclic aspect and flowing style of temporal patterns are separated as being one-way or two-way modality of it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Flow</head><p>Watching time as a flow of spatial spreading might turn our minds towards the process of continuous events in order, to change the states of objects. That would lead to observing the world as dynamic processes of 'becoming' transformers, except looking at it as tremendous stable permanent objects. Technologies like AI are not outliers too (with that perspective), by their inclusion with becoming objects through time <ref type="bibr" target="#b58">[59]</ref>; as the escalation of related aspects such as big data, IoTs, algorithms (e.g., NN architectures), learning methods, and processing capabilities (e.g., hardware and computing layers) are following Moore's law up.</p><p>As time passage continues from the past to the future direction, the present is untouchable in pure real-time, whether either humans, fauna, or machines. Caused by our limitations, as in sensing, processing, and acting, humans have latencies of about several milliseconds to catch the present tense, namely subjective present <ref type="bibr">[219]</ref>. That specious illusion of real-time attendance in the present will be magnified in machines if long queues of latencies are stacked up as lag and jitter. Their slowness of speed would not only be rooted in the transference or hardware specifications but also might be an effect of other causes, like bandwidth or architectural inefficiency. Delay issues further to increasing the speed of network or architectural modifications can be handled by the model itself as model compression or distributed local processing like federated learning <ref type="bibr" target="#b312">[313,</ref><ref type="bibr" target="#b167">168]</ref>. Besides, handling stream data in near real-time has been becoming a timeliness norm, whether it is listening to a piece of music or monitoring the generated big event log data of a large factory from all processes of the supply chain toward sales. All in all, although we are entities with near real-time reflection, we seem thirsty for omnipresent real-time immersive environments with telepresence via technologies such as 5G, quantum computing, now-casting, and so on <ref type="bibr" target="#b83">[84]</ref>.</p><p>If data has a sequential structure in which each random data point (stochastic process) is related to its prior and records of the dataset, while these orders are indexed on a timestamp, it is called a time series. AI's main channel to interact with time is by learning timeseries data. As in the System context remarked, an impactful part of ML is focused on learning these types of patterns. Spatial time-series is spatiotemporal data, which is a salient joint between Space and Time contexts. A substance of these datatypes is trajectories to track the route of an entity (e.g., an agent), timestamped. If the data continuously is generated and sent (for processing) in streaming data and the ordered timestamped, is a time-series data stream <ref type="bibr" target="#b10">[11]</ref>. That unbounded data streams in high velocity and scale, toward having minimum latency, unlike batch processing needs online stream processing.</p><p>A vision in the flow of time is analysis's timing perspective, whether retrospective or prospective; as AI with the power to accomplish the majority of tasks can be interpreted within this angle. The utilization of stream data is usually at real-time processing for tasks that require rapid reactions as forecasting a probable dangerous event in alerting systems <ref type="bibr" target="#b10">[11]</ref>. The use of DL (and its hybridization with other techniques) in forecasting by learning both weak signals and complex patterns of time series seems to get better performance than other methods <ref type="bibr" target="#b32">[33]</ref>. As deep forecasting needs a high-volume sequential data in the time-series, the quantity of observations counts in the amount of data <ref type="bibr" target="#b32">[33]</ref>. Anomaly detection or anomaly prediction are retrospective and prospective tasks respectively, in time-series analysis which identify irregular fluctuations if outlier data points or sequences do not look like the others or fit in their context <ref type="bibr" target="#b10">[11]</ref>. Those are more indispensable tasks when there is real-time data streaming, caused by its data poisoning sensitivity <ref type="bibr" target="#b10">[11]</ref>. Whether a recognized anomalous signal might be a prediction to prevent a possible catastrophic problem in the future as a consequence of security attacks prospectively <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Cycle</head><p>Many phenomena have periodicity in cyclic trends with complete oscillations, whether deterministic as circular clocks' graduations, or stochastic as weather. This type of recurrence includes all information about routines as timepieces, calendars, or any timestamped historical information. AI not only learns deterministic patterns, but can also do pattern mining from stochastic events for time-series analysis tasks such as forecasting (e.g., in action, event, weather), anomaly detection, classification, monitoring, and reasoning. Periodicity in time-series data can be derived by frequency transformations to turn the time domain into a frequency domain <ref type="bibr" target="#b299">[300]</ref>. That makes complex patterns more perceivable for ML using techniques like convolution theorem and decomposing series to frequency components manifesting representations as seasonality and global dependencies <ref type="bibr" target="#b299">[300]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Timescale</head><p>While duration is a one-way factor, its measuring units (timing) are repetitive in different timescales which from micro scale to macro are as microsecond, millisecond, second, minute, hour, day, week, month, year, and decade <ref type="bibr" target="#b28">[29]</ref>. The impact of having a range of multiscale periods is noticeable in Context-Awareness (time-awareness) <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b49">50]</ref> which can be indexed within or further to the above timescales. Those to mention would be time of the day (working hours, off-hours, morning, noon, afternoon, evening, night, midnight), days of week (working, weekdays), holidays (e.g., weekends or occasions), event time (e.g., calendar events, visiting an event, attendance duration in an event, event's unveiling time).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Event</head><p>Time and event are separated phenomena, but as our ancestors distinguished time by trends of changes, the relationship between time and event is bidirectional as we perceive time by events and event perception comes with its temporal facets <ref type="bibr" target="#b153">[154]</ref>. Events as dynamic positional changes are directly interrelated with other contexts too as they happen and influence Systems like the environment (Space) and people (Human) <ref type="bibr" target="#b310">[311]</ref>. Beyond timestamping, learning, and reasoning deterministic recurrence of events, there is much attention on the recognition and prediction of stochastic events. It might be correct that each detection or forecasting task in ML tries to extract patterns out of what betides among the training data representations. Events besides their occurrence probabilities might be sequential whether a procedural time-series trajectory event data (e.g., event logs) has both attributes. An event in event sequence data is the most detailed scale, while sequences of events represent more inclusive semantics about the entity's journey with temporal and chronological features (e.g., order, successiveness, and duration) <ref type="bibr" target="#b103">[104]</ref>. Event sequence analysis is proper beyond the mentioned tasks (detection and prediction) to be used for anomaly detection, recommendation, and causality analysis (e.g., reasoning) <ref type="bibr" target="#b103">[104]</ref>.</p><p>To determine chronological changes, Change Point Detection (CPD) works on how to analyze temporal behavior variations inside signals of data with ML techniques <ref type="bibr" target="#b13">[14]</ref>. Online CPD focuses only on real-time change recognition as anomaly detection in natural disaster recognition as soon as possible, whether these tasks can be done already by event prediction. Event is a very common term corresponding to its time association, embraces various topics which event prediction covers healthcare, media, transportation, politics, economics, natural disasters, crime, entertainment, and business applications <ref type="bibr" target="#b310">[311]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Memory</head><p>Without memorizing, time is not comprehensible as a major subject in time-series analysis is adjusting the long-term dependencies learning, to acquire minimum gradient vanishing in DL. Memory decay problem and long-range dependencies seem to be modified by using modern DL architectures as Transformers but still are not solved enough <ref type="bibr" target="#b299">[300,</ref><ref type="bibr" target="#b288">289]</ref>. Managing plenty of historical profiles about multimodal contexts is another important problem that a variety of methods work to modify, like SSL, which tries to take steps for independence from labeled data even in time-series data <ref type="bibr" target="#b307">[308]</ref>. Additionally, memory in real-time streams is not accessible via conventional processing fashions for working memory <ref type="bibr" target="#b10">[11]</ref> and they should merge with long-term memory and attention to perceive time better <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION</head><p>The context categorization taxonomy indicated how vast AmI is and which capacities are to be expanded because of the diversity in this configuration. Perspectives within contexts have the potential to get deeper into them by applications while having independence, are not separated islands, and are interconnected. Hence, overlaps show that an improvement in context 'A' may affect context 'B' as the systematic presumption of this survey (DL &amp; IoT are SOTA of AI &amp; ICT). As these sophistications are, they synergize with each other exactly by those linkages under the AmI umbrella. Whether the purpose of this paper is to address the point of view of intelligible related contexts of AmI concurrently includes conceptual and technical features, rather than just setting boundaries. Some interrelationships in Human context for example are behavioral analysis for gender classification <ref type="bibr" target="#b168">[169]</ref>, audio recorded by cameras for facial expressions <ref type="bibr" target="#b239">[240]</ref>, EEG for facial expression <ref type="bibr" target="#b7">[8]</ref>, BVP stress recognition <ref type="bibr" target="#b189">[190]</ref>, and emotional information (e.g., facial expression and gait) as cues for action prediction <ref type="bibr" target="#b147">[148]</ref>. Another path to interpret contexts' links is to contrast dichotomies for concluding the high valued priorities which for example any context can be seen from the Human contextual positions (human-centric); as how any IE system must serve for humanity's sake (humanism). Whether it might be for the System's wise that how contexts might be analyzed as a system's conceptualized role in its architecture design. All in all, while the contexts are distinct, they conceivably are potentially each other's anchors for further interpretation, and data about one context is probably metadata for another's induction.</p><p>Notwithstanding, having the taxonomy of contextual concepts denotes the variety of requirements of an IE system, organized to assess the best matching portfolio, to balance tradeoffs with specifications. Similar to resource allocation, system contexts shall be selected in concept, heterogeneity of varieties, and depth levels, until an engineered Context-Aware system is developed. A very noteworthy factor in this regard is to make a harmonious balance between contexts' tradeoffs in an orchestration fashion by myriad differentiations in concept and functionality, as being all-inclusive seems impossible. Another basis of this study, changes in recent years, led its references to be mainly review surveys of the last five years. 5G and low energy-consuming IoT technologies paved the path for more practical real-time infrastructures. It eventually seems if there is a sufficiently proper dataset for a specific task, it can suffice (software 2.0) and DL will handle it better than any other method in high-level contexts <ref type="bibr" target="#b289">[290]</ref>. DL is also capable of offering personalized services via more and more interactions with users nonstop (instruction and prompt tuned), which by preparations can lead to customized smart experiences of IEs.</p><p>Out-of-the-box is not necessarily unavailable in the box but might be a hybrid exploitation of accessible contexts and tools as SoS, to create novel incredible intelligent systems at different scopes. MLLMs (e.g., ChatGPT) as SOTA AI and the closest systems to AGI, are hybrid models with systematically several learning contexts' inclusion. Such foundation models are mainly pretrained (TL) in SSL fashion on large-scale multimodal input data within a large model (e.g., in parameters), which would lead that task-agnostic (MTL) generalization to support few-shot, one-shot, and zero-shot learning whether by finetuning or prompting. LLMs have also emerged the ICL (as a form of meta learners) and reasoning abilities which in some scenarios might lead to being eventually zero-shot planners <ref type="bibr" target="#b286">[287,</ref><ref type="bibr" target="#b279">280]</ref>. They can have RL (e.g., RLHF) to have continuous (online) changing policies or reward (loss) functions to be more and more personalized outputs. These foundation models, architecturally, are also a united compilation of previously presented models' elements <ref type="bibr" target="#b36">[37]</ref>.</p><p>It must have good reasons to go through constructing such a powerful entity as a machine, in which a tool like a knife not only can cut autonomously but also decides consecutively on its strategic logic. First of all, in almost all applications that IoT and AI might have, an IE made by those would, whereby an intelligent crane-wise power, homogenized with humans' living area, can help to build eutopia as much as a dystopia. It presumably seems we humans are condemned to moving there as we are in the midway of the journey, riding hastily. Meanwhile, the hurry is rewarding caused by plenty of economic and social beneficiaries.</p><p>The Philanthropic point of view of IEs seems to be sufficiently effective in varied usages which if only briefly looked at healthcare will be: (1) medical applications from healthcare systems, smart hospitals in high-level embedded systematic facilities, efficient utilization, or detection of regular disease universally, as people with the highest healthcare limitations can use with least costs (2) AAL application of AmI to help people in their routine Activities of Daily Living (ADL) and recognize an emergency event for people with health and medical conditions, as elderly for a longer dependent living (e.g., fall detection) and impairments such as mobility impairments, Autism, Alzheimer, intellectual disabilities (e.g., dawn syndrome), hearing loss, blindness, and other special diseases (e.g., Multiple Sclerosis). (3) educating about medical circumstances using recommendation systems as very likely detected illness scenarios or quickly needed reactions in specific emergency occurrences. Another philanthropic aspect, by the pervasiveness and low (possibly zero) marginal cost of copying a cyberphysical technology in return for tremendous profits, shall impact seriously the societies' welfare circumstances, likewise the most needful human beings.</p><p>As optimization and personalization are two consequents of context-aware AmI, there are several usages of it which just to name a few would be: education, security, industry 4.0 (e.g., factories, agriculture, grid), infrastructures (e.g., smart cities), entertainment (e.g., immersive environments, gaming), commerce (e.g., marketing, shopping), education (e.g., personalized gamification), and workplace (e.g., make more convenient synergies). Moreover, the commercial and industrial utilizations are obvious even if not lead to immersive fascinating joyful pleasure for a long time, or move toward optimized agile production with few marginal costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Challenges &amp; Open issues</head><p>Majorly works in each context are application-oriented (e.g., Industry 4.0, AAL), but the concept they are analyzing may be generalizable while are not expressed that way in broadly covering works. Whether the interconnections in the AmI context are highly diversified, by semiheuristic methodology, sophistication is in a wide range of searching keywords at interchangeable parts. On the other side, for the exchange of knowledge and research comprehensively, it is required to consider these diverse aspects of contexts, whether the fields are divergent; it is attempted to cover these heterogeneities under the umbrella terms of Ambient Intelligence and Intelligent Environments. That is why this research cannot be organized in a systematic literature review format just by overviewing fixed keywords to neither cover SOTA nor comprehensive.</p><p>Stability is not a feature of SOTA and purely all-inclusiveness does not exist at all, as in this study, each context is extendable. Human context might have additive parts, for example: expectation (e.g., user taste, QoS), culture, assisting, caring, cognition, ownership, compliance hierarchy (access permission), and safety. The other example to mention for System context extension would be further system parts (e.g., Robotics, embedded CPS), design related factors (e.g., requirement-oriented, specification-oriented, system engineering), economic related factors (e.g., financial tradeoffs, supply &amp; demand, marketing), or management related factors (e.g., business, processes, organization, construction).</p><p>Tradeoffs between IE contexts are beyond managing the resources, about contexts' orchestration by their conceptual and physical characteristics. That harmony depends on the system's metrics' priorities and constraints for allocation and control, which are systematically rooted in the requirements and specifications of the system's stakeholders. Each tradeoff, including its evaluation metrics by their Key Performance Indicators, should be analyzed including cost, accuracy, precision, reliability, energy consumption, power efficiency, security, privacy, and interoperability. The result is unique for each system to analyze indeed.</p><p>Safety and security are terrifically serious factors for humanity as one of the biggest concerns in Embedded Systems, such as CPS and IoT <ref type="bibr" target="#b286">[287]</ref>, where the cyber systems would take actions physically to impact people's lives. It was observed that major security issues are presented in the contexts this article surveyed because of their importance. The security facet of IoT can be analyzed through confidentiality, integrity, availability, identification, authentication, privacy, and trust features to be considered in all IoT layers <ref type="bibr" target="#b169">[170]</ref>. Great technologies and protocols come just to solve security problems such as data and model decentralization (e.g., federated learning <ref type="bibr" target="#b134">[135,</ref><ref type="bibr" target="#b262">263,</ref><ref type="bibr" target="#b167">168]</ref>), highlevel authenticating <ref type="bibr" target="#b232">[233]</ref>, encryption <ref type="bibr" target="#b169">[170]</ref> (e.g., blockchains), smart surveillance (e.g., natural disaster, violence, crime, terrorism) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b117">118,</ref><ref type="bibr" target="#b179">180,</ref><ref type="bibr" target="#b310">311]</ref>, attack management <ref type="bibr" target="#b186">[187,</ref><ref type="bibr" target="#b101">102]</ref>, or not using vision-based sensor networks <ref type="bibr" target="#b278">[279,</ref><ref type="bibr" target="#b189">190,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b50">51]</ref>. The lack of security and privacy puts the whole system at risk, whether it is reliable or not, which, if below confidentiality standards, it is better not to continue its duty. The risks seem unlimited as exploiting theft information footprints and malicious attackers might take any harmful action. However, it may have new faces as defying AGI to take its beneficiaries. Unethical apocalyptic scenarios might be more important than the cognition of a maverick AGI; as exploitation of malicious totalitarian illiberal owners to treat humanity as pigeons of skinner-box just for their own sake.</p><p>Fairness problem is more general than these scenarios and exists everywhere caused by its nature in which anyone has her/his own definition. A major reason for fairness and bias in ML can be in data, algorithm, and user interactions <ref type="bibr">[191,</ref><ref type="bibr" target="#b118">119]</ref>, as it looks possible to be in all contexts of AmI. To these concerns and probable futuristic ones, as AmI is potentially much more powerful, it is better to have fundamental standards before it pervasively gets used, regardless of its fairness considerations. A modern IE facing serious consequences must have cyber-physically comprehensive up-to-date legislation in major applications, in parallel with AI's.</p><p>As mentioned in the scope context, the scale of IEs is extensive by size or application, therefore scalability and interoperability should grab attention. Interoperability as is a key performance factor in IoT <ref type="bibr" target="#b206">[207]</ref> and SoS <ref type="bibr" target="#b125">[126,</ref><ref type="bibr" target="#b202">203]</ref> to cover heterogeneity in functionality, in DL, generalization and adaptation (e.g., in TL) is a significant topic. Though by scaling except growth in size, the complexity will rise, as in LLMs, emerging unplanned abilities might occur as in the physical phase transition <ref type="bibr" target="#b286">[287,</ref><ref type="bibr" target="#b311">312,</ref><ref type="bibr" target="#b200">201]</ref>.</p><p>The degree of freedom of AmI as how much power it can have even in a high-leveled reliability standard might shift it from an independent to maverick's secretive hypocrisy is a thread, as that self-governance was potentially autonomy. If not liars, both LLMs and MLLMs are great hallucinators, while detecting original from fake would be hard <ref type="bibr" target="#b282">[283]</ref>. An entity equipped with the ability to comprehend us and any related context of ours more accurately and agiler than us does not appear in a sci-fi drama in this era. If an intelligent CPS is constructed on physical world scenarios to have meta-learning commonsense physically, can act as a structure creator (e.g., using 3D printers and robots). As instructions and manuals (how to) are probably available in such large model's training data, the degree of freedom subject is more salient if that AmI would get or be offered access to a variety of empowering resources (e.g., financial, logistics). Ultimately, a noteworthy point is a system with the above-mentioned specifications is not available now and is only embodied. while how much feasible that is or when to achieve it is not clear for sure, though is possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Future works</head><p>The represented taxonomy configuration method is semi-heuristic, any modification in future works as a customized context taxonomies for specific applications is probable. Because of the rapidly changing nature of technology, it was observed that even philosophical assumptions about technology get adjusted or converted as well as the context taxonomies and computation paradigms. I think there will be further application-oriented research in AmI contexts, settled on particular context portfolio settings in theory and practice. Also, as there is an issue in multimodal learning about information fusion, in context-aware AmI, it is noteworthy to research how these contexts will be in an intelligent system, embedded.</p><p>What the future of AmI will be is mutually hinged on the future of its constituents as AI, ICT, data sciences, and robotics, plus their applications and connections in different fields. For example: (1) if a new method for cancer diagnosis tests is discovered, hence affects medicine after that data (tests) collected, AI models will learn it; (2) if a robot capable to lift things with numerous shapes and weights to bear on many surfaces like stairs reliably be invented, by training on DL (e.g., DRL) to do assistive tasks, changes IEs if produced scaled;</p><p>(3) toward AGI as by progressions in MLLMs that have high quality multimodal ICL and generalized physics-awareness (e.g., commonsense), might lead to trustable interactive adaptable agents even in real world physical tasks. Soon, with more advanced algorithms and architectures in the DL context, software 2.0 looks bolder whether if the proper data is available, the model can learn it <ref type="bibr" target="#b289">[290]</ref>. With the continuum of Moore's law in hardware processors, it seems large models get larger and larger, and inefficient but highly prone to improve methods like DRL and larger foundation models be more feasible. Another major trend to be expected is more systematic transdisciplinary collaborations to bring new paradigm shifts which might be for example: <ref type="bibr" target="#b0">(1)</ref> from Social or Industrial IoT (SIoT, IIoT) toward Social or Industrial AmI (e.g., industry 4.0) (2) from Context-Aware Recommendation Systems to Multimodal Context-Aware Recommendation Systems of IEs (3) from MLLMs to Cyber-Physical Multimodal Large Models (e.g., trained on physical modalities, physics-based twin, robotic enabler). Moreover, there will be more customized foundation models for each application with higher fairness and reliability.</p><p>Whether foundation models are SOTA in AI, there is still a lack of a large multimodal model trained in interactive (multimodal feedback) dynamic physics-supported environment to do reasoning about the physical world physically <ref type="bibr" target="#b107">[108,</ref><ref type="bibr" target="#b62">63]</ref>. Such models comprehend difficult complex problems differently or might be more inclusive in the wild, always changing real-world space for precise interventions <ref type="bibr" target="#b107">[108]</ref>; no matter if it is not exactly clear whether LLMs and MLLMs really do reasoning or it is just a property of heuristics <ref type="bibr" target="#b114">[115]</ref>. As discussed in the physics context, physics-based simulations and digital twins are proper ways to have agents within complex environments to explore and learn with multiple feedback on their policies to get more ready for being used in wild. Learning by multimodal interactions with rewards like nature's gamification as worked in many RL-based works, in order to reach at AGI level might help to be a physical AGI which not necessarily is intimidating.</p><p>The future of AmI as General Ambient Intelligence if would not be in one large DL model, systematic thinking views as SoS might be the solution to scale faster by utilizing a variety of systems and contexts as embedded hybrid intelligent CPSs' integration. The combination of different services linking to each other is as probable as we experience it in smartphone apps to have personalization of our multiple needs so routinely. Whether that customized recommendation is a favorite music, clip, movie, series, or anything we may want to buy, such as foods, clothes, accessories, or even skills we might like to learn. Any operating system supporter IoT (e.g., android) is likely to bring these services into a part of an environment if embedded; like smart kitchens, whether if a necessary ingredient in the refrigerator is finished, it alerts or even orders autonomously.</p><p>By surveying SOTA DL contexts, it can be induced that if a system as IEs with harmonious orchestrated context portfolio using good qualities of prior works in each context to modify, synchronous with pioneering courage (e.g., scaling in GPT-3), can succeed even in AI. Each significant technology after its emergence paved the way for the next ones by collaborating with other available technologies, playing supportive infrastructure or complementary cooperation whether if impactful enough, regards a paradigm shift. New paradigms in the past vanished in a fashion in which collectors would think those tech products would be counted as antiques or just disappear. While this time is different, as the case is the data (contexts). Data will not disappear because it always is utilizable if the entities in which the dataset recorded them do not exist anymore. Overall, the AGI or Physical AGI seems to be context-aware complex SoSs in interactive multimodal feedbacking space with large scale (e.g., in model parameter and data), resulting in transdisciplinary synergetic collaborations. Achieving that is in its best possible form through the history of AI by observing the incredible potentials of SOTA as LLMs, LVMs, and MLLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Conclusion</head><p>In this study, the contexts of AmI were surveyed by proposing a comprehensive taxonomy to configure a conceptual-functional framework through four dimensions: Human, System, Space, and Time. The taxonomy framework, by categorizing AmI contexts' heterogeneous nature and mapping interconnections, aims to be used for future hybrid context portfolio allocations in complex intelligent systems, such as IEs. Using a review of over 300 publications in a systematic taxonomy, majorly recent surveys, this study captures the current trends and various facets of SOTA in AmI more comprehensively. The rapid advancements in all contexts' domains and their technological progressions in just a few years indicate the continued influence of Moore's law. By increasing the value of the DL and IoT synergizing utilizations, modern context-aware AmI can potentially become a major technology paradigm prospectively. This framework provides a structured foundation for researchers and practitioners to navigate the complexities of AmI systems and leverage their capabilities in future innovations. Future research could further explore and modify how this taxonomy can be applied in real-world systems while investigating the role of emerging technologies in context-aware AmI will be key to unlocking its full potential. The emergence of unprecedented capabilities is unignorable and unforeseeable, as it was for LLMs, emphasizing the dynamic evolving nature of the field.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The Human contexts in AmI. These contexts can be diversely comprehensive as a brief presence in the AmI might bring various metadata of the Human with the available technologies such as AI and IoT. The goal of this Figure is to visualize some Human contexts as examples. All images used in this image is generated by an AI foundation model (ChatGPT 4o).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The covering domain of Ambient Intelligence discussed in this paper. Any part of Deep Learning as foundation models can be Ambient Intelligence enabler.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="10,107.10,446.64,397.80,281.28" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>No funds or any other support is received. All three Figures infographic were produced by <rs type="person">Elahe Kordlou</rs>. The images in Figure <ref type="figure">2</ref> were generated by ChatGPT 4o. the grammatical check is done by Grammarly and ProWritingAid.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Wireless electromagnetic radiation assessment based on the specific absorption rate (SAR): A review case study</title>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Abdul-Al</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed S I</forename><surname>Amar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Issa</forename><surname>Elfergani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Littlehales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naser</forename><surname>Ojaroudi Parchin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasir</forename><surname>Al-Yasir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chan</forename><surname>Hwang See</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuhairiah</forename><surname>Zainal Abidin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Alibakhshikenari</surname></persName>
		</author>
		<idno type="DOI">10.3390/electronics11040511</idno>
		<ptr target="https://doi.org/10.3390/electronics11040511" />
	</analytic>
	<monogr>
		<title level="j">Electronics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">511</biblScope>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards a better understanding of context and context-awareness</title>
		<author>
			<persName><surname>Gregory D Abowd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Anind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nigel</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pete</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><surname>Steggles</surname></persName>
		</author>
		<idno type="DOI">10.1007/3-540-48157-5_29</idno>
		<ptr target="https://doi.org/10.1007/3-540-48157-5_29" />
	</analytic>
	<monogr>
		<title level="m">Handheld and Ubiquitous Computing: First International Symposium, HUC&apos;99</title>
		<meeting><address><addrLine>Karlsruhe, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1999-09-27">1999. September 27-29, 1999 Proceedings 1, 1999</date>
			<biblScope unit="page" from="304" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Industry 4.0 and health: Internet of things, big data, and cloud computing for healthcare 4.0</title>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Aceto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Valerio Persico</surname></persName>
		</author>
		<author>
			<persName><surname>Pescapé</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jii.2020.100129</idno>
		<ptr target="https://doi.org/10.1016/j.jii.2020.100129" />
	</analytic>
	<monogr>
		<title level="j">J. Ind. Inf. Integr</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">100129</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Gpt-4 technical report</title>
		<author>
			<persName><forename type="first">Josh</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lama</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilge</forename><surname>Akkaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florencia</forename><surname>Leoni Aleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janko</forename><surname>Altenschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shyamal</forename><surname>Anadkat</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2303.08774</idno>
		<idno>arXiv Prepr. arXiv2303.08774</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2303.08774" />
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning age from gait: A survey</title>
		<author>
			<persName><forename type="first">Tee</forename><surname>Timilehin B Aderinola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thian</forename><forename type="middle">Song</forename><surname>Connie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Chuen</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Yau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Beng</surname></persName>
		</author>
		<author>
			<persName><surname>Teoh</surname></persName>
		</author>
		<idno type="DOI">10.1109/access.2021.3095477</idno>
		<ptr target="https://doi.org/10.1109/access.2021.3095477" />
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="100352" to="100368" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Human activity analysis: A review</title>
		<author>
			<persName><forename type="first">Jake</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<idno type="DOI">10.1145/1922649.1922653</idno>
		<ptr target="https://doi.org/10.1145/1922649.1922653" />
	</analytic>
	<monogr>
		<title level="j">Acm Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="43" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Wearable sensor-based gait analysis for age and gender estimation</title>
		<author>
			<persName><forename type="first">Md</forename><surname>Atiqur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahman</forename><surname>Ahad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thanh</forename><forename type="middle">Trung</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anindya</forename><surname>Das Antar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masud</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tahera</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daigo</forename><surname>Muramatsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasushi</forename><surname>Makihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sozo</forename><surname>Inoue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasushi</forename><surname>Yagi</surname></persName>
		</author>
		<idno type="DOI">10.3390/s20082424</idno>
		<ptr target="https://doi.org/10.3390/s20082424" />
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">2424</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A systematic survey on multimodal emotion recognition using learning algorithms</title>
		<author>
			<persName><forename type="first">Naveed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaher</forename><surname>Al Aghbari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shini</forename><surname>Girija</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.iswa.2022.200171</idno>
		<ptr target="https://doi.org/10.1016/j.iswa.2022.200171" />
	</analytic>
	<monogr>
		<title level="j">Intell. Syst. with Appl</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">200171</biblScope>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Wireless sensor networks: a survey</title>
		<author>
			<persName><forename type="first">Weilian</forename><surname>Ian F Akyildiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yogesh</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erdal</forename><surname>Sankarasubramaniam</surname></persName>
		</author>
		<author>
			<persName><surname>Cayirci</surname></persName>
		</author>
		<idno type="DOI">10.1016/s1389-1286(01)00302-4</idno>
		<ptr target="https://doi.org/10.1016/s1389-1286(01)00302-4" />
	</analytic>
	<monogr>
		<title level="j">Comput. networks</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="393" to="422" />
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A comprehensive review of internet of things: Technology stack, middlewares, and fog/edge computing interface</title>
		<author>
			<persName><forename type="first">Omer</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamad</forename><forename type="middle">Khairi</forename><surname>Ishak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Kamran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liaquat</forename><surname>Bhatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Imran</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ki-Il</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.3390/s22030995</idno>
		<ptr target="https://doi.org/10.3390/s22030995" />
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">995</biblScope>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Time series big data: a survey on data stream frameworks, analysis and algorithms</title>
		<author>
			<persName><forename type="first">Ana</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susana</forename><surname>Brás</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susana</forename><surname>Sargento</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filipe</forename><forename type="middle">Cabral</forename><surname>Pinto</surname></persName>
		</author>
		<idno type="DOI">10.1186/s40537-023-00760-1</idno>
		<ptr target="https://doi.org/10.1186/s40537-023-00760-1" />
	</analytic>
	<monogr>
		<title level="j">J. Big Data</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">83</biblScope>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A state-of-the-art survey on deep learning theory and architectures</title>
		<author>
			<persName><forename type="first">Md</forename><surname>Zahangir Alom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tarek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Taha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Yakopcic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paheding</forename><surname>Westberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mst</forename><surname>Sidike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahmudul</forename><surname>Shamima Nasrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">C</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><surname>Van Essen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A S</forename><surname>Abdul</surname></persName>
		</author>
		<author>
			<persName><surname>Awwal</surname></persName>
		</author>
		<author>
			<persName><surname>Vijayan</surname></persName>
		</author>
		<author>
			<persName><surname>Asari</surname></persName>
		</author>
		<idno type="DOI">10.3390/electronics8030292</idno>
		<ptr target="https://doi.org/10.3390/electronics8030292" />
	</analytic>
	<monogr>
		<title level="j">electronics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">292</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Context-Aware Edge-Based AI Models for Wireless Sensor Networks-An Overview</title>
		<author>
			<persName><forename type="first">Ahmed A</forename><surname>Al-Saedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselka</forename><surname>Boeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emiliano</forename><surname>Casalicchio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Exner</surname></persName>
		</author>
		<idno type="DOI">10.3390/s22155544</idno>
		<ptr target="https://doi.org/10.3390/s22155544" />
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">5544</biblScope>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A survey of methods for time series change point detection</title>
		<author>
			<persName><forename type="first">Samaneh</forename><surname>Aminikhanghahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diane</forename><forename type="middle">J</forename><surname>Cook</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10115-016-0987-z</idno>
		<ptr target="https://doi.org/10.1007/s10115-016-0987-z" />
	</analytic>
	<monogr>
		<title level="j">Knowl. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="339" to="367" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Human detection techniques for real time surveillance: a comprehensive survey</title>
		<author>
			<persName><forename type="first">Mohd</forename><surname>Aquib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ansari</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Dushyant</forename><forename type="middle">Kumar</forename><surname>Singh</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11042-020-10103-4</idno>
		<ptr target="https://doi.org/10.1007/s11042-020-10103-4" />
	</analytic>
	<monogr>
		<title level="j">Multimed. Tools Appl</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="8759" to="8808" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning: A brief survey</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Arulkumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Peter Deisenroth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miles</forename><surname>Brundage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anil</forename><forename type="middle">Anthony</forename><surname>Bharath</surname></persName>
		</author>
		<idno type="DOI">10.1109/msp.2017.2743240</idno>
		<ptr target="https://doi.org/10.1109/msp.2017.2743240" />
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="26" to="38" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A comprehensive review of indoor/outdoor localization solutions in IoT era: Research challenges and future perspectives</title>
		<author>
			<persName><forename type="first">M</forename><surname>Safar</surname></persName>
		</author>
		<author>
			<persName><surname>Asaad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Halgurd</surname></persName>
		</author>
		<author>
			<persName><surname>Maghdid</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.comnet.2022.109041</idno>
		<ptr target="https://doi.org/10.1016/j.comnet.2022.109041" />
	</analytic>
	<monogr>
		<title level="j">Comput. Networks</title>
		<imprint>
			<biblScope unit="volume">212</biblScope>
			<biblScope unit="page">109041</biblScope>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Internet of Things applications: A systematic review</title>
		<author>
			<persName><forename type="first">Parvaneh</forename><surname>Asghari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir Masoud</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamid</forename><surname>Haj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seyyed</forename><surname>Javadi</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.comnet.2018.12.008</idno>
		<ptr target="https://doi.org/10.1016/j.comnet.2018.12.008" />
	</analytic>
	<monogr>
		<title level="j">Comput. Networks</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="page" from="241" to="261" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Digital Twin: Benefits, use cases, challenges, and opportunities</title>
		<author>
			<persName><forename type="first">Mohsen</forename><surname>Attaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bilge</forename><surname>Gokhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Celik</forename></persName>
		</author>
		<idno type="DOI">10.1016/j.dajour.2023.100165</idno>
		<ptr target="https://doi.org/10.1016/j.dajour.2023.100165" />
	</analytic>
	<monogr>
		<title level="j">Decis. Anal. J</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">100165</biblScope>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A survey on the evolution of the notion of context-awareness</title>
		<author>
			<persName><forename type="first">Asier</forename><surname>Augusto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dean</forename><surname>Aztiria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Unai</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName><surname>Alegre</surname></persName>
		</author>
		<idno type="DOI">10.1080/08839514.2018.1428490</idno>
		<ptr target="https://doi.org/10.1080/08839514.2018.1428490" />
	</analytic>
	<monogr>
		<title level="j">Appl. Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">7-8</biblScope>
			<biblScope unit="page" from="613" to="642" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Intelligent environments: a manifesto</title>
		<author>
			<persName><forename type="first">Vic</forename><surname>Juan C Augusto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diane</forename><surname>Callaghan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Achilles</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ichiro</forename><surname>Kameas</surname></persName>
		</author>
		<author>
			<persName><surname>Satoh</surname></persName>
		</author>
		<idno type="DOI">10.1186/2192-1962-3-12</idno>
		<ptr target="https://doi.org/10.1186/2192-1962-3-12" />
	</analytic>
	<monogr>
		<title level="j">Human-centric Comput. Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Some still-current dimensions of applied behavior analysis</title>
		<author>
			<persName><forename type="first">Montrose</forename><forename type="middle">M</forename><surname>Donald M Baer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><forename type="middle">R</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><surname>Risley</surname></persName>
		</author>
		<idno type="DOI">10.1901/jaba.1987.20-313</idno>
		<ptr target="https://doi.org/10.1901/jaba.1987.20-313" />
	</analytic>
	<monogr>
		<title level="j">J. Appl. Behav. Anal</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="313" to="327" />
			<date type="published" when="1987">1987. 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Synthesizing images of humans in unseen poses</title>
		<author>
			<persName><forename type="first">Guha</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><forename type="middle">V</forename><surname>Dalca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fredo</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Guttag</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2018.00870</idno>
		<ptr target="https://doi.org/10.1109/cvpr.2018.00870" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="8340" to="8348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multimodal machine learning: A survey and taxonomy</title>
		<author>
			<persName><forename type="first">Tadas</forename><surname>Baltrušaitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaitanya</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<idno type="DOI">10.1109/tpami.2018.2798607</idno>
		<ptr target="https://doi.org/10.1109/tpami.2018.2798607" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="423" to="443" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Physics-informed computer vision: A review and perspectives</title>
		<author>
			<persName><forename type="first">Chayan</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kien</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clinton</forename><surname>Fookes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Karniadakis</surname></persName>
		</author>
		<idno type="DOI">10.1145/3689037</idno>
		<idno>arXiv2305.18035</idno>
		<ptr target="https://doi.org/10.1145/3689037" />
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv Prepr.</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Graphic design as communication</title>
		<author>
			<persName><forename type="first">Malcolm</forename><surname>Barnard</surname></persName>
		</author>
		<idno type="DOI">10.4324/9781315015385</idno>
		<ptr target="https://doi.org/10.4324/9781315015385" />
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Routledge</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gender classification on 2D human skeleton</title>
		<author>
			<persName><forename type="first">Paola</forename><surname>Barra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carmen</forename><surname>Bisogni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Nappi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Freire-Obregón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Modesto</forename><surname>Castrillón-Santana</surname></persName>
		</author>
		<idno type="DOI">10.1109/biosmart.2019.8734198</idno>
		<ptr target="https://doi.org/10.1109/biosmart.2019.8734198" />
	</analytic>
	<monogr>
		<title level="m">2019 3rd International Conference on Bio-engineering for Smart Technologies (BioSMART)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Emotional expressions reconsidered: Challenges to inferring emotion from human facial movements</title>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barrett</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Adolphs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stacy</forename><surname>Marsella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleix</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><forename type="middle">D</forename><surname>Pollak</surname></persName>
		</author>
		<idno type="DOI">10.1177/1529100619832930</idno>
		<ptr target="https://doi.org/10.1177/1529100619832930" />
	</analytic>
	<monogr>
		<title level="j">Psychol. Sci. public Interes</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="68" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Time perception: A review on psychological, computational, and robotic models</title>
		<author>
			<persName><forename type="first">Hamit</forename><surname>Basgol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inci</forename><surname>Ayhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emre</forename><surname>Ugur</surname></persName>
		</author>
		<idno type="DOI">10.1109/tcds.2021.3059045</idno>
		<ptr target="https://doi.org/10.1109/tcds.2021.3059045" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cogn. Dev. Syst</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="301" to="315" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">IoT-enabled smart cities: A review of concepts, frameworks and key technologies</title>
		<author>
			<persName><forename type="first">Pierfrancesco</forename><surname>Bellini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Nesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gianni</forename><surname>Pantaleo</surname></persName>
		</author>
		<idno type="DOI">10.3390/app12031607</idno>
		<ptr target="https://doi.org/10.3390/app12031607" />
	</analytic>
	<monogr>
		<title level="j">Appl. Sci</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">1607</biblScope>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Recent trends in crowd analysis: A review</title>
		<author>
			<persName><forename type="first">Mounir</forename><surname>Bendali-Braham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Germain</forename><surname>Forestier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lhassane</forename><surname>Idoumghar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Alain</forename><surname>Muller</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.mlwa.2021.100023</idno>
		<ptr target="https://doi.org/10.1016/j.mlwa.2021.100023" />
	</analytic>
	<monogr>
		<title level="j">Mach. Learn. with Appl</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">100023</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<idno type="DOI">10.1109/tpami.2013.50</idno>
		<ptr target="https://doi.org/10.1109/tpami.2013.50" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep learning for time series forecasting: Tutorial and literature survey</title>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Benidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Syama</forename><surname>Sundar Rangapuram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Flunkert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danielle</forename><surname>Maddix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caner</forename><surname>Turkmen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Gasthaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bohlke-Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Salinas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Stella</surname></persName>
		</author>
		<idno type="DOI">10.1145/3533382</idno>
		<ptr target="https://doi.org/10.1145/3533382" />
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="36" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A systematic literature review on IoT gateways</title>
		<author>
			<persName><forename type="first">Gunjan</forename><surname>Beniwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anita</forename><surname>Singhrova</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jksuci.2021.11.007</idno>
		<ptr target="https://doi.org/10.1016/j.jksuci.2021.11.007" />
	</analytic>
	<monogr>
		<title level="j">J. King Saud Univ. Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="9541" to="9563" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A comprehensive survey on sentiment analysis: Approaches, challenges and trends</title>
		<author>
			<persName><forename type="first">Marouane</forename><surname>Birjali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Kasri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abderrahim</forename><surname>Beni-Hssane</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.knosys.2021.107134</idno>
		<ptr target="https://doi.org/10.1016/j.knosys.2021.107134" />
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Syst</title>
		<imprint>
			<biblScope unit="volume">226</biblScope>
			<biblScope unit="page">107134</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nasser</surname></persName>
		</author>
		<author>
			<persName><surname>Nasrabadi</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-0-387-45528-0</idno>
		<ptr target="https://doi.org/10.1007/978-0-387-45528-0" />
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">On the opportunities and risks of foundation models</title>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Drew</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simran</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><surname>Sydney Von Arx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeannette</forename><surname>Michael S Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emma</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><surname>Brunskill</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2108.07258</idno>
		<idno>arXiv2108.07258</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2108.07258" />
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv Prepr.</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Video generation models as world simulators</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Depue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Schnurr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Troy</forename><surname>Luhman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Luhman</surname></persName>
		</author>
		<ptr target="https//openai.com/research/video-generation-models-as-world-simulators3" />
		<imprint>
			<date type="published" when="2024">2024. 2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><surname>Tom B Brown</surname></persName>
		</author>
		<idno>ArXiv2005.14165</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv Prepr</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Machine learning-enabled internet of things (iot): Data, applications, and industry perspective</title>
		<author>
			<persName><forename type="first">Jamal</forename><surname>Bzai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furqan</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arwa</forename><surname>Dhafer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miroslav</forename><surname>Bojović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Imran</forename><surname>Saleh M Altowaijri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rashid</forename><surname>Khan Niazi</surname></persName>
		</author>
		<author>
			<persName><surname>Mehmood</surname></persName>
		</author>
		<idno type="DOI">10.3390/electronics11172676</idno>
		<ptr target="https://doi.org/10.3390/electronics11172676" />
	</analytic>
	<monogr>
		<title level="j">Electronics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">2676</biblScope>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">IoT-based architectures for sensing and local data processing in ambient intelligence: research and industrial trends</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelo</forename><surname>Genovese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincenzo</forename><surname>Piuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Scotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mel</forename><surname>Siegel</surname></persName>
		</author>
		<idno type="DOI">10.1109/i2mtc.2019.8827110</idno>
		<ptr target="https://doi.org/10.1109/i2mtc.2019.8827110" />
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Instrumentation and Measurement Technology Conference (I2MTC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Facial expression recognition using computer vision: A systematic review</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Canedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J R</forename><surname>António</surname></persName>
		</author>
		<author>
			<persName><surname>Neves</surname></persName>
		</author>
		<idno type="DOI">10.3390/app9214678</idno>
		<ptr target="https://doi.org/10.3390/app9214678" />
	</analytic>
	<monogr>
		<title level="j">Appl. Sci</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">4678</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Toward an integrative approach to nonverbal personality detection: Connecting psychological and artificial intelligence research</title>
		<author>
			<persName><forename type="first">Davide</forename><surname>Cannata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">M</forename><surname>Breil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Lepri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitja</forename><forename type="middle">D</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis O'</forename><surname>Hora</surname></persName>
		</author>
		<idno type="DOI">10.1037/tmb0000054</idno>
		<ptr target="https://doi.org/10.1037/tmb0000054" />
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Feature learning from spectrograms for assessment of personality traits</title>
		<author>
			<persName><forename type="first">Marc-André</forename><surname>Carbonneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazid</forename><surname>Attabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ghyslain</forename><surname>Gagnon</surname></persName>
		</author>
		<idno type="DOI">10.1109/taffc.2017.2763132</idno>
		<ptr target="https://doi.org/10.1109/taffc.2017.2763132" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affect. Comput</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="31" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Posture and posturology, anatomical and physiological profiles: overview and current state of art</title>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Carini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margherita</forename><surname>Mazzola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiara</forename><surname>Fici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salvatore</forename><surname>Palmeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimo</forename><surname>Messina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Provvidenza</forename><surname>Damiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Tomasello</surname></persName>
		</author>
		<idno type="DOI">10.23750/abm.v88i1.5309</idno>
		<ptr target="https://doi.org/10.23750/abm.v88i1.5309" />
	</analytic>
	<monogr>
		<title level="j">Acta Bio Medica Atenei Parm</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The sixteen personality factor questionnaire (16PF)</title>
		<author>
			<persName><forename type="first">E P</forename><surname>Heather</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">D</forename><surname>Cattell</surname></persName>
		</author>
		<author>
			<persName><surname>Mead</surname></persName>
		</author>
		<idno type="DOI">10.4135/9781849200479.n7</idno>
		<ptr target="https://doi.org/10.4135/9781849200479.n7" />
	</analytic>
	<monogr>
		<title level="j">SAGE Handb. Personal. theory Assess</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="135" to="159" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A review on vision techniques applied to human behaviour analysis for ambientassisted living</title>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaaraoui</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Pau</forename><surname>Climent-Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Flórez-Revuelta</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2012.03.005</idno>
		<ptr target="https://doi.org/10.1016/j.eswa.2012.03.005" />
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="10873" to="10888" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Fashion recommendation systems, models and methods: A review</title>
		<author>
			<persName><forename type="first">Samit</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><surname>Saiful Hoque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naimur</forename><surname>Rahman Jeem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manik</forename><surname>Chandra Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepayan</forename><surname>Bardhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edgar</forename><surname>Lobaton</surname></persName>
		</author>
		<idno type="DOI">10.3390/informatics8030049</idno>
		<ptr target="https://doi.org/10.3390/informatics8030049" />
	</analytic>
	<monogr>
		<title level="m">Informatics</title>
		<imprint>
			<publisher>MDPI</publisher>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="volume">49</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Distinguishing between facts and opinions for sentiment analysis: Survey and challenges</title>
		<author>
			<persName><forename type="first">Iti</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><forename type="middle">E</forename><surname>Welsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Herrera</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.inffus.2017.12.006</idno>
		<ptr target="https://doi.org/10.1016/j.inffus.2017.12.006" />
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="65" to="77" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">A survey of context-aware mobile computing research</title>
		<author>
			<persName><forename type="first">Guanling</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Kotz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep learning for sensor-based human activity recognition: Overview, challenges, and opportunities</title>
		<author>
			<persName><forename type="first">Kaixuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dalin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lina</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhao</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3447744</idno>
		<ptr target="https://doi.org/10.1145/3447744" />
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="40" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">activity recognition</title>
		<author>
			<persName><forename type="first">Liming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Hoey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">D</forename><surname>Nugent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diane</forename><forename type="middle">J</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwen</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1109/tsmcc.2012.2198883</idno>
		<ptr target="https://doi.org/10.1109/tsmcc.2012.2198883" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man, Cybern. Part C (Applications Rev</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="790" to="808" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Human activity recognition and behaviour analysis</title>
		<author>
			<persName><forename type="first">Liming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">D</forename><surname>Nugent</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-19408-6</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-19408-6" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Fashion meets computer vision: A survey</title>
		<author>
			<persName><forename type="first">Wen-Huang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sijie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chieh-Yun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shintami</forename><surname>Chusnul Hidayati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3447239</idno>
		<ptr target="https://doi.org/10.1145/3447239" />
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="41" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Appearance-based gaze estimation with deep learning: A review and benchmark</title>
		<author>
			<persName><forename type="first">Yihua</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haofei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiwei</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1109/tpami.2024.3393571</idno>
		<ptr target="https://doi.org/10.1109/tpami.2024.3393571" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Natural language processing</title>
		<author>
			<persName><forename type="first">K R</forename><surname>Kr ; Chowdhary</surname></persName>
		</author>
		<author>
			<persName><surname>Chowdhary</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-81-322-3972-7_19</idno>
		<ptr target="https://doi.org/10.1007/978-81-322-3972-7_19" />
	</analytic>
	<monogr>
		<title level="j">Fundam. Artif. Intell</title>
		<imprint>
			<biblScope unit="page" from="603" to="649" />
			<date type="published" when="1442">1442. 2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A survey of internet of things and cyber-physical systems: standards, algorithms, applications, security, challenges, and future directions</title>
		<author>
			<persName><forename type="first">Tai</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><surname>Chui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Brij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varsha</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadia</forename><surname>Arya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ammar</forename><surname>Nedjah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priyanka</forename><surname>Almomani</surname></persName>
		</author>
		<author>
			<persName><surname>Chaurasia</surname></persName>
		</author>
		<idno type="DOI">10.3390/info14070388</idno>
		<ptr target="https://doi.org/10.3390/info14070388" />
	</analytic>
	<monogr>
		<title level="j">Information</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">388</biblScope>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Ambient assisted living: a review of technologies, methodologies and future perspectives for healthy aging of population</title>
		<author>
			<persName><forename type="first">Grazia</forename><surname>Cicirelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Marani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Petitti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Annalisa</forename><surname>Milella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiziana D'</forename><surname>Orazio</surname></persName>
		</author>
		<idno type="DOI">10.3390/s21103549</idno>
		<ptr target="https://doi.org/10.3390/s21103549" />
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">3549</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Time machines: Artificial intelligence, process, and narrative</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Coeckelbergh</surname></persName>
		</author>
		<idno type="DOI">10.1007/s13347-021-00479-y</idno>
		<ptr target="https://doi.org/10.1007/s13347-021-00479-y" />
	</analytic>
	<monogr>
		<title level="j">Philos. Technol</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1623" to="1638" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Ambient intelligence: Technologies, applications, and opportunities</title>
		<author>
			<persName><forename type="first">Diane</forename><forename type="middle">J</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">C</forename><surname>Augusto</surname></persName>
		</author>
		<author>
			<persName><surname>Vikramaditya</surname></persName>
		</author>
		<author>
			<persName><surname>Jakkula</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.pmcj.2009.04.001</idno>
		<ptr target="https://doi.org/10.1016/j.pmcj.2009.04.001" />
	</analytic>
	<monogr>
		<title level="j">Pervasive Mob. Comput</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="277" to="298" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Diffusion models in vision: A survey</title>
		<author>
			<persName><surname>Florinel-Alin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vlad</forename><surname>Croitoru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Hondru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tudor</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
		<idno type="DOI">10.1109/tpami.2023.3261988</idno>
		<ptr target="https://doi.org/10.1109/tpami.2023.3261988" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="10850" to="10869" />
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Sensor-based and vision-based human activity recognition: A comprehensive survey</title>
		<author>
			<persName><forename type="first">Minh</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyungbok</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><surname>Jalil Piran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheol</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Hee</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyeonjoon</forename><surname>Moon</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2020.107561</idno>
		<ptr target="https://doi.org/10.1016/j.patcog.2020.107561" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page">107561</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Benchmarks for automated commonsense reasoning: A survey</title>
		<author>
			<persName><forename type="first">Ernest</forename><surname>Davis</surname></persName>
		</author>
		<idno type="DOI">10.1145/3615355</idno>
		<ptr target="https://doi.org/10.1145/3615355" />
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="41" />
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvprw.2009.5206848</idno>
		<ptr target="https://doi.org/10.1109/cvprw.2009.5206848" />
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009. 2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A conceptual framework and a toolkit for supporting the rapid prototyping of context-aware applications</title>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">D</forename><surname>Anind K Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Abowd</surname></persName>
		</author>
		<author>
			<persName><surname>Salber</surname></persName>
		</author>
		<idno type="DOI">10.1207/s15327051hci16234_02</idno>
		<ptr target="https://doi.org/10.1207/s15327051hci16234_02" />
	</analytic>
	<monogr>
		<title level="j">Human-Computer Interact</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2-4</biblScope>
			<biblScope unit="page" from="97" to="166" />
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<author>
			<persName><forename type="first">Qingxiu</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2301.00234</idno>
		<idno>arXiv2301.00234</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2301.00234" />
		<title level="m">A survey on in-context learning</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">What we talk about when we talk about context</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Dourish</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00779-003-0253-8</idno>
		<ptr target="https://doi.org/10.1007/s00779-003-0253-8" />
	</analytic>
	<monogr>
		<title level="j">Pers. ubiquitous Comput</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="19" to="30" />
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">A survey on machine learning approaches for modelling intuitive physics</title>
		<author>
			<persName><forename type="first">Jiafei</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arijit</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheston</forename><surname>Tan</surname></persName>
		</author>
		<idno type="DOI">10.1109/tetci.2022.3141105</idno>
		<idno>arXiv2202.06481</idno>
		<ptr target="https://doi.org/10.1109/tetci.2022.3141105" />
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv Prepr.</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">A survey of embodied ai: From simulators to research tasks</title>
		<author>
			<persName><forename type="first">Jiafei</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samson</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Li Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyuan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheston</forename><surname>Tan</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2202.06481</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2202.06481" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Emerg. Top. Comput. Intell</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="230" to="244" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">A survey of ambient intelligence</title>
		<author>
			<persName><forename type="first">Rob</forename><surname>Dunne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Harper</surname></persName>
		</author>
		<idno type="DOI">10.1145/3447242</idno>
		<ptr target="https://doi.org/10.1145/3447242" />
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Human emotion recognition: Review of sensors and methods</title>
		<author>
			<persName><forename type="first">Andrius</forename><surname>Dzedzickis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artūras</forename><surname>Kaklauskas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vytautas</forename><surname>Bucinskas</surname></persName>
		</author>
		<idno type="DOI">10.3390/s20030592</idno>
		<ptr target="https://doi.org/10.3390/s20030592" />
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">592</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Emotion recognition from physiological signal analysis: A review</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Egger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Ley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sten</forename><surname>Hanke</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.entcs.2019.04.009</idno>
		<ptr target="https://doi.org/10.1016/j.entcs.2019.04.009" />
	</analytic>
	<monogr>
		<title level="j">Electron. Notes Theor. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">343</biblScope>
			<biblScope unit="page" from="35" to="55" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Joint learning of social groups, individuals action and sub-group activities in videos</title>
		<author>
			<persName><forename type="first">Mahsa</forename><surname>Ehsanpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alireza</forename><surname>Abedin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fatemeh</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javen</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58545-7_11</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58545-7_11" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020-08-23">2020. August 23-28, 2020. 2020</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="177" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Facial expression and emotion</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Ekman</surname></persName>
		</author>
		<idno type="DOI">10.2466/pms.1967.24.3.711</idno>
		<ptr target="https://doi.org/10.2466/pms.1967.24.3.711" />
	</analytic>
	<monogr>
		<title level="j">Am. Psychol</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">384</biblScope>
			<date type="published" when="1993">1993. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Handbook of cognition and emotion</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dalgleish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mick</forename><surname>Power</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Head and body cues in the judgment of emotion: A reformulation</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wallace</forename><forename type="middle">V</forename><surname>Friesen</surname></persName>
		</author>
		<idno type="DOI">10.1037/0003-066x.48.4.384</idno>
		<ptr target="https://doi.org/10.1037//0003-066x.48.4.384" />
	</analytic>
	<monogr>
		<title level="j">Percept. Mot. Skills</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="711" to="724" />
			<date type="published" when="1967">1967. 1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Unmasking the face: A guide to recognizing emotions from facial clues</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wallace</forename><forename type="middle">V</forename><surname>Friesen</surname></persName>
		</author>
		<idno type="DOI">10.1002/0470013494.ch3</idno>
		<ptr target="https://doi.org/10.1002/0470013494.ch3" />
	</analytic>
	<monogr>
		<title level="j">Ishk</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">A model for personality</title>
		<author>
			<persName><forename type="first">Hans</forename><surname>Jurgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eysenck</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">A survey of crowd counting and density estimation based on convolutional neural network</title>
		<author>
			<persName><forename type="first">Zizhu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangming</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yudong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaowei</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2021.02.103</idno>
		<ptr target="https://doi.org/10.1016/j.neucom.2021.02.103" />
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">472</biblScope>
			<biblScope unit="page" from="224" to="251" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">A survey on indoor positioning systems for IoT-based applications</title>
		<author>
			<persName><forename type="first">Pooyan</forename><surname>Shams Farahsari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amirhossein</forename><surname>Farahzadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javad</forename><surname>Rezazadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alireza</forename><surname>Bagheri</surname></persName>
		</author>
		<idno type="DOI">10.1109/jiot.2022.3149048</idno>
		<ptr target="https://doi.org/10.1109/jiot.2022.3149048" />
	</analytic>
	<monogr>
		<title level="j">IEEE Internet Things J</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="7680" to="7699" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Middleware technologies for cloud of things: a survey</title>
		<author>
			<persName><forename type="first">Amirhossein</forename><surname>Farahzadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pooyan</forename><surname>Shams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javad</forename><surname>Rezazadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reza</forename><surname>Farahbakhsh</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.dcan.2017.04.005</idno>
		<ptr target="https://doi.org/10.1016/j.dcan.2017.04.005" />
	</analytic>
	<monogr>
		<title level="j">Digit. Commun. Networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="176" to="188" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Towards context-aware data management for ambient intelligence</title>
		<author>
			<persName><forename type="first">Ling</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M G</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willem</forename><surname>Apers</surname></persName>
		</author>
		<author>
			<persName><surname>Jonker</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-30075-5_41</idno>
		<ptr target="https://doi.org/10.1007/978-3-540-30075-5_41" />
	</analytic>
	<monogr>
		<title level="m">Database and Expert Systems Applications: 15th International Conference</title>
		<meeting><address><addrLine>DEXA; Zaragoza, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004-08-30">2004. 2004. August 30-September 3, 2004. 2004</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="422" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Handbook of applied behavior analysis</title>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">W</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cathleen</forename><forename type="middle">C</forename><surname>Piazza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><forename type="middle">S</forename><surname>Roane</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Guilford Publications</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Digital time: Latency, real-time, and the onlife experience of everyday time</title>
		<author>
			<persName><forename type="first">Luciano</forename><surname>Floridi</surname></persName>
		</author>
		<idno type="DOI">10.1007/s13347-021-00472-5</idno>
		<ptr target="https://doi.org/10.1007/s13347-021-00472-5" />
	</analytic>
	<monogr>
		<title level="j">Philos. Technol</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="407" to="412" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">A context-aware approach for long-term behavioural change detection and abnormality prediction in ambient assisted living</title>
		<author>
			<persName><forename type="first">Abdur</forename><surname>Rahim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Forkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ibrahim</forename><surname>Khalil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zahir</forename><surname>Tari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebti</forename><surname>Foufou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelaziz</forename><surname>Bouras</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2014.07.007</idno>
		<ptr target="https://doi.org/10.1016/j.patcog.2014.07.007" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="628" to="641" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Internet of things as system of systems: A review of methodologies, frameworks, platforms, and tools</title>
		<author>
			<persName><forename type="first">Giancarlo</forename><surname>Fortino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudio</forename><surname>Savaglio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giandomenico</forename><surname>Spezzano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengchu</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1109/tsmc.2020.3042898</idno>
		<ptr target="https://doi.org/10.1109/tsmc.2020.3042898" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man, Cybern. Syst</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="223" to="236" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Artificial intelligence and ambient intelligence</title>
		<author>
			<persName><forename type="first">Matjaz</forename><surname>Gams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irene</forename><surname>Yu-Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aki</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrés</forename><surname>Härmä</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Muñoz</surname></persName>
		</author>
		<author>
			<persName><surname>Tam</surname></persName>
		</author>
		<idno type="DOI">10.3233/ais-180508</idno>
		<ptr target="https://doi.org/10.3233/ais-180508" />
	</analytic>
	<monogr>
		<title level="j">J. Ambient Intell. Smart Environ</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="86" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Emotional targeting using digital signage systems and facial recognition at the point-of-sale</title>
		<author>
			<persName><forename type="first">Marion</forename><surname>Garaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Udo</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricarda</forename><forename type="middle">C</forename><surname>Rainer</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jbusres.2020.10.065</idno>
		<ptr target="https://doi.org/10.1016/j.jbusres.2020.10.065" />
	</analytic>
	<monogr>
		<title level="j">J. Bus. Res</title>
		<imprint>
			<biblScope unit="volume">131</biblScope>
			<biblScope unit="page" from="747" to="762" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Big data for internet of things: a survey</title>
		<author>
			<persName><forename type="first">Mouzhi</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hind</forename><surname>Bangui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barbora</forename><surname>Buhnova</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.future.2018.04.053</idno>
		<ptr target="https://doi.org/10.1016/j.future.2018.04.053" />
	</analytic>
	<monogr>
		<title level="j">Futur. Gener. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="601" to="614" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Deepfashion2: A versatile benchmark for detection, pose estimation, segmentation and re-identification of clothing images</title>
		<author>
			<persName><forename type="first">Yuying</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruimao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2019.00548</idno>
		<ptr target="https://doi.org/10.1109/cvpr.2019.00548" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="5337" to="5345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Industry 4.0, digitization, and opportunities for sustainability</title>
		<author>
			<persName><forename type="first">Morteza</forename><surname>Ghobakhloo</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jclepro.2019.119869</idno>
		<ptr target="https://doi.org/10.1016/j.jclepro.2019.119869" />
	</analytic>
	<monogr>
		<title level="j">J. Clean. Prod</title>
		<imprint>
			<biblScope unit="volume">252</biblScope>
			<biblScope unit="page">119869</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Detecting and recognizing human-object interactions</title>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="8359" to="8367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Generative adversarial networks</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.1145/3422622</idno>
		<ptr target="https://doi.org/10.1145/3422622" />
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="139" to="144" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">The&quot; something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susanne</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heuna</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ingo</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Mueller-Freitag</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2017.622</idno>
		<ptr target="https://doi.org/10.1109/iccv.2017.622" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="5842" to="5850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">LSTM: A search space odyssey</title>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rupesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><surname>Koutník</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Bas R Steunebrink</surname></persName>
		</author>
		<author>
			<persName><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1109/tnnls.2016.2582924</idno>
		<ptr target="https://doi.org/10.1109/tnnls.2016.2582924" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. neural networks Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2222" to="2232" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Recent advances in convolutional neural networks</title>
		<author>
			<persName><forename type="first">Jiuxiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianyang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingxing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2017.10.013</idno>
		<ptr target="https://doi.org/10.1016/j.patcog.2017.10.013" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="354" to="377" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Personalized fashion compatibility modeling via metapath-guided heterogeneous graph learning</title>
		<author>
			<persName><forename type="first">Fangkai</forename><surname>Weili Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuemeng</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haokun</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung-Hsing</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3477495.3532038</idno>
		<ptr target="https://doi.org/10.1145/3477495.3532038" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th international ACM SIGIR conference on research and development in information retrieval</title>
		<meeting>the 45th international ACM SIGIR conference on research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="page" from="482" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Internet of Things (IoT): A vision, architectural elements, and future directions</title>
		<author>
			<persName><forename type="first">Jayavardhana</forename><surname>Gubbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajkumar</forename><surname>Buyya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slaven</forename><surname>Marusic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marimuthu</forename><surname>Palaniswami</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.future.2013.01.010</idno>
		<ptr target="https://doi.org/10.1016/j.future.2013.01.010" />
	</analytic>
	<monogr>
		<title level="j">Futur. Gener. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1645" to="1660" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Macro-and micro-expressions facial datasets: A survey</title>
		<author>
			<persName><forename type="first">Hajer</forename><surname>Guerdelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudio</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walid</forename><surname>Barhoumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haythem</forename><surname>Ghazouani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Berretti</surname></persName>
		</author>
		<idno type="DOI">10.3390/s22041524</idno>
		<ptr target="https://doi.org/10.3390/s22041524" />
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1524</biblScope>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">A review on generative adversarial networks: Algorithms, theory, and applications</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonggang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieping</forename><surname>Ye</surname></persName>
		</author>
		<idno type="DOI">10.1109/tkde.2021.3130191</idno>
		<ptr target="https://doi.org/10.1109/tkde.2021.3130191" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3313" to="3332" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">A survey on concepts, applications, and challenges in cyber-physical systems</title>
		<author>
			<persName><forename type="first">Volkan</forename><surname>Gunes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Givargis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Vahid</surname></persName>
		</author>
		<idno type="DOI">10.3837/tiis.2014.12.001</idno>
		<ptr target="https://doi.org/10.3837/tiis.2014.12.001" />
	</analytic>
	<monogr>
		<title level="j">KSII Trans. Internet Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="4242" to="4268" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Human-machine interaction sensing technology based on hand gesture recognition: A review</title>
		<author>
			<persName><forename type="first">Lin</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongxing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ligang</forename><surname>Yao</surname></persName>
		</author>
		<idno type="DOI">10.1109/thms.2021.3086003</idno>
		<ptr target="https://doi.org/10.1109/thms.2021.3086003" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Human-Machine Syst</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="300" to="309" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Survey on visual analysis of event sequence data</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shunan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuochen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Smiti</forename><surname>Kaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Gotz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Cao</surname></persName>
		</author>
		<idno type="DOI">10.1109/tvcg.2021.3100413</idno>
		<ptr target="https://doi.org/10.1109/tvcg.2021.3100413" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="5091" to="5112" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Comparison of IoT platform architectures: A field study based on a reference architecture</title>
		<author>
			<persName><forename type="first">Jasmin</forename><surname>Guth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uwe</forename><surname>Breitenbücher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Falkenthal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Leymann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Reinfurt</surname></persName>
		</author>
		<idno type="DOI">10.1109/ciot.2016.7872918</idno>
		<ptr target="https://doi.org/10.1109/ciot.2016.7872918" />
	</analytic>
	<monogr>
		<title level="m">Cloudification of the Internet of Things (CIoT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016. 2016. 2016</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Big data and IoT-based applications in smart environments: A systematic review</title>
		<author>
			<persName><forename type="first">Yosra</forename><surname>Hajjaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wadii</forename><surname>Boulila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Riadh</forename><surname>Imed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Imed</forename><surname>Farah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName><surname>Hussain</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cosrev.2020.100318</idno>
		<ptr target="https://doi.org/10.1016/j.cosrev.2020.100318" />
	</analytic>
	<monogr>
		<title level="j">Comput. Sci. Rev</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page">100318</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Pre-trained models: Past, present and future</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxian</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqi</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.aiopen.2021.08.002</idno>
		<ptr target="https://doi.org/10.1016/j.aiopen.2021.08.002" />
	</analytic>
	<monogr>
		<title level="j">AI Open</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="225" to="250" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<title level="m" type="main">Physics-informed machine learning: A survey on problems, methods and applications</title>
		<author>
			<persName><forename type="first">Zhongkai</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengyang</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2211.08064</idno>
		<idno>arXiv2211.08064</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2211.08064" />
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2016.90</idno>
		<ptr target="https://doi.org/10.1109/cvpr.2016.90" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title level="m" type="main">A framework for context-aware pervasive computing applications</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Henricksen</surname></persName>
		</author>
		<idno type="DOI">10.14264/106832</idno>
		<ptr target="https://doi.org/10.14264/106832" />
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Learning and recognition of clothing genres from full-body images</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shintami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuang-Wen</forename><surname>Hidayati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Huang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Lung</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><surname>Hua</surname></persName>
		</author>
		<idno type="DOI">10.1109/tcyb.2017.2712634</idno>
		<ptr target="https://doi.org/10.1109/tcyb.2017.2712634" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1647" to="1659" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">The vanishing gradient problem during learning recurrent neural nets and problem solutions</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
		<ptr target="https://doi.org/10.1162/neco.1997.9.8.1735" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Uncertainty, Fuzziness Knowledge-Based Syst</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">02</biblScope>
			<biblScope unit="page" from="107" to="116" />
			<date type="published" when="1998">1998. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1142/s0218488598000094</idno>
		<ptr target="https://doi.org/10.1142/s0218488598000094" />
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Meta-learning in neural networks: A survey</title>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antreas</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Micaelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
		<idno type="DOI">10.1109/tpami.2021.3079209</idno>
		<ptr target="https://doi.org/10.1109/tpami.2021.3079209" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="5149" to="5169" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<monogr>
		<title level="m" type="main">Towards reasoning in large language models: A survey</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Chuan</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.findings-acl.67</idno>
		<idno>arXiv2212.10403</idno>
		<ptr target="https://doi.org/10.18653/v1/2023.findings-acl.67" />
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv Prepr</note>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">A survey of deep meta-learning</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Huisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">N</forename><surname>Van Rijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aske</forename><surname>Plaat</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10462-021-10004-4</idno>
		<ptr target="https://doi.org/10.1007/s10462-021-10004-4" />
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Rev</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="4483" to="4541" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Smart and intelligent energy monitoring systems: A comprehensive literature survey and future research guidelines</title>
		<author>
			<persName><forename type="first">Tanveer</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><surname>Fath U Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khan</forename><surname>Ullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungmin</forename><surname>Muhammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><surname>Rho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eenjun</forename><surname>Ullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jihoon</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sung</forename><forename type="middle">Wook</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><surname>Baik</surname></persName>
		</author>
		<idno type="DOI">10.1002/er.6093</idno>
		<ptr target="https://doi.org/10.1002/er.6093" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Energy Res</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="3590" to="3614" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<monogr>
		<title level="m" type="main">Different approaches for human activity recognition: A survey</title>
		<author>
			<persName><forename type="first">Zawar</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><forename type="middle">Emma</forename><surname>Zhang</surname></persName>
		</author>
		<idno>arXiv1906.05074</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv Prepr</note>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Towards accountability for machine learning datasets: Practices from software engineering and infrastructure</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Smart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christina</forename><surname>Greer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oddur</forename><surname>Kjartansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parker</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<idno type="DOI">10.1145/3442188.3445918</idno>
		<ptr target="https://doi.org/10.1145/3442188.3445918" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the 2021 ACM Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="560" to="575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Low cost air pollution monitoring systems: A review of protocols and enabling technologies</title>
		<author>
			<persName><forename type="first">Zeba</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lirong</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jii.2019.100123</idno>
		<ptr target="https://doi.org/10.1016/j.jii.2019.100123" />
	</analytic>
	<monogr>
		<title level="j">J. Ind. Inf. Integr</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">100123</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">A comprehensive survey on applications of transformers for deep learning tasks</title>
		<author>
			<persName><forename type="first">Saidul</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanae</forename><surname>Elmekki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Elsebai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamal</forename><surname>Bentahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nagat</forename><surname>Drawel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaith</forename><surname>Rjoub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Witold</forename><surname>Pedrycz</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2023.122666</idno>
		<ptr target="https://doi.org/10.1016/j.eswa.2023.122666" />
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="page">122666</biblScope>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2017.632</idno>
		<ptr target="https://doi.org/10.1109/cvpr.2017.632" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Facial expressions of emotion are not culturally universal</title>
		<author>
			<persName><forename type="first">Rachael</forename><forename type="middle">E</forename><surname>Jack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G B</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Garrod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><forename type="middle">G</forename><surname>Caldara</surname></persName>
		</author>
		<author>
			<persName><surname>Schyns</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1200155109</idno>
		<ptr target="https://doi.org/10.1073/pnas.1200155109" />
	</analytic>
	<monogr>
		<title level="j">Proc. Natl. Acad. Sci</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="7241" to="7244" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">A survey on contrastive self-supervised learning</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Jaiswal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Ashwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">Zaki</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debapriya</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fillia</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><surname>Makedon</surname></persName>
		</author>
		<idno type="DOI">10.3390/technologies9010002</idno>
		<ptr target="https://doi.org/10.3390/technologies9010002" />
	</analytic>
	<monogr>
		<title level="j">Technologies</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Robust human activity recognition from depth video using spatiotemporal multifused features</title>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Jalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeon-Ho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong-Joong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaharyar</forename><surname>Kamal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daijin</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2016.08.003</idno>
		<ptr target="https://doi.org/10.1016/j.patcog.2016.08.003" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="295" to="308" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<monogr>
		<title level="m" type="main">Systems of systems engineering: principles and applications</title>
		<author>
			<persName><forename type="first">Mo</forename><surname>Jamshidi</surname></persName>
		</author>
		<idno type="DOI">10.1201/9781420065893</idno>
		<ptr target="https://doi.org/10.1201/9781420065893" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">CRC press</note>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">A survey on data compression techniques: From the perspective of data quality, coding schemes, data type and applications</title>
		<author>
			<persName><forename type="first">Uthayakumar</forename><surname>Jayasankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vengattaraman</forename><surname>Thirumal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhavachelvan</forename><surname>Ponnurangam</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jksuci.2018.05.006</idno>
		<ptr target="https://doi.org/10.1016/j.jksuci.2018.05.006" />
	</analytic>
	<monogr>
		<title level="j">J. King Saud Univ. Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="119" to="140" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Vision-based human action recognition: An overview and real world challenges</title>
		<author>
			<persName><forename type="first">Imen</forename><surname>Jegham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anouar</forename><surname>Ben Khalifa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ihsen</forename><surname>Alouani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><forename type="middle">Ali</forename><surname>Mahjoub</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.fsidi.2019.200901</idno>
		<ptr target="https://doi.org/10.1016/j.fsidi.2019.200901" />
	</analytic>
	<monogr>
		<title level="j">Forensic Sci. Int. Digit. Investig</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">200901</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Body Weight Analysis from Human Body Images</title>
		<author>
			<persName><forename type="first">Min</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Guo</surname></persName>
		</author>
		<idno type="DOI">10.1109/tifs.2019.2904840</idno>
		<ptr target="https://doi.org/10.1109/tifs.2019.2904840" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Forensics Secur</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="2676" to="2688" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Emerging wearable interfaces and algorithms for hand gesture recognition: A survey</title>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peiqi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P L</forename><surname>Benny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">B</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><surname>Shull</surname></persName>
		</author>
		<idno type="DOI">10.1109/rbme.2021.3078190</idno>
		<ptr target="https://doi.org/10.1109/rbme.2021.3078190" />
	</analytic>
	<monogr>
		<title level="j">IEEE Rev. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="85" to="102" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">A survey on artificial intelligence in posture recognition</title>
		<author>
			<persName><forename type="first">Xiaoyan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuojin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuihua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yudong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.32604/cmes.2023.027676</idno>
		<ptr target="https://doi.org/10.32604/cmes.2023.027676" />
	</analytic>
	<monogr>
		<title level="j">Comput. Model. Eng. Sci. C</title>
		<imprint>
			<biblScope unit="volume">137</biblScope>
			<biblScope unit="page">35</biblScope>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">First impressions: A survey on vision-based apparent personality trait analysis</title>
		<author>
			<persName><forename type="first">C S Jacques</forename><surname>Julio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yağmur</forename><surname>Junior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Güçlütürk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Umut</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Güçlü</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Andujar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><forename type="middle">Jair</forename><surname>Baró</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Escalante</surname></persName>
		</author>
		<author>
			<persName><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A J</forename><surname>Marcel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Van Gerven</surname></persName>
		</author>
		<author>
			<persName><surname>Van Lier</surname></persName>
		</author>
		<idno type="DOI">10.1109/taffc.2019.2930058</idno>
		<ptr target="https://doi.org/10.1109/taffc.2019.2930058" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affect. Comput</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="75" to="95" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Internet of things applications as energy internet in smart grids and smart environments</title>
		<author>
			<persName><forename type="first">Yasin</forename><surname>Kabalci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ersan</forename><surname>Kabalci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeevikumar</forename><surname>Padmanaban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Holm-Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frede</forename><surname>Blaabjerg</surname></persName>
		</author>
		<idno type="DOI">10.3390/electronics8090972</idno>
		<ptr target="https://doi.org/10.3390/electronics8090972" />
	</analytic>
	<monogr>
		<title level="j">Electronics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">972</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Incorporating physics into data-driven computer vision</title>
		<author>
			<persName><forename type="first">Achuta</forename><surname>Kadambi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Celso</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mani</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
		<idno type="DOI">10.1038/s42256-023-00662-0</idno>
		<ptr target="https://doi.org/10.1038/s42256-023-00662-0" />
	</analytic>
	<monogr>
		<title level="j">Nat. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="572" to="580" />
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Advances and open problems in federated learning</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Kairouz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Avent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurélien</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Bennis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitin</forename><surname>Arjun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kallista</forename><surname>Bhagoji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Bonawitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Cormode</surname></persName>
		</author>
		<author>
			<persName><surname>Cummings</surname></persName>
		</author>
		<idno type="DOI">10.1561/2200000083</idno>
		<ptr target="http://dx.doi.org/10.1561/2200000083" />
	</analytic>
	<monogr>
		<title level="j">Found. trends® Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="210" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Applications of wireless sensor networks: an up-to-date survey</title>
		<author>
			<persName><forename type="first">Dionisis</forename><surname>Kandris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Nakas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitrios</forename><surname>Vomvas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grigorios</forename><surname>Koulouras</surname></persName>
		</author>
		<idno type="DOI">10.3390/asi3010014</idno>
		<ptr target="https://doi.org/10.3390/asi3010014" />
	</analytic>
	<monogr>
		<title level="j">Appl. Syst. Innov</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">A-Z survey of Internet of Things: Architectures, protocols, applications, recent advances, future directions and recommendations</title>
		<author>
			<persName><forename type="first">'</forename><surname>Wafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khalid</forename><forename type="middle">A</forename><surname>Kassab</surname></persName>
		</author>
		<author>
			<persName><surname>Darabkh</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jnca.2020.102663</idno>
		<ptr target="https://doi.org/10.1016/j.jnca.2020.102663" />
	</analytic>
	<monogr>
		<title level="j">J. Netw. Comput. Appl</title>
		<imprint>
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="page">102663</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<monogr>
		<author>
			<persName><forename type="first">Adam</forename><surname>Kendon</surname></persName>
		</author>
		<idno type="DOI">10.1017/cbo9780511807572</idno>
		<ptr target="https://doi.org/10.1017/cbo9780511807572" />
		<title level="m">Gesture: Visible Action as Utterance</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Future internet: the internet of things architecture, possible applications and key challenges</title>
		<author>
			<persName><forename type="first">Rafiullah</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarmad</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rifaqat</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahid</forename><surname>Khan</surname></persName>
		</author>
		<idno type="DOI">10.1109/fit.2012.53</idno>
		<ptr target="https://doi.org/10.1109/fit.2012.53" />
	</analytic>
	<monogr>
		<title level="m">2012 10th international conference on frontiers of information technology</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="257" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Natural language processing: state of the art, current trends and challenges</title>
		<author>
			<persName><forename type="first">Diksha</forename><surname>Khurana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Koli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiran</forename><surname>Khatter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sukhdev</forename><surname>Singh</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11042-022-13428-4</idno>
		<ptr target="https://doi.org/10.1007/s11042-022-13428-4" />
	</analytic>
	<monogr>
		<title level="j">Multimed. Tools Appl</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="3713" to="3744" />
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Hipster wars: Discovering elements of fashion styles</title>
		<author>
			<persName><forename type="first">Kota</forename><surname>Hadi Kiapour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><surname>Berg</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10590-1_31</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-10590-1_31" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014: 13th European Conference</title>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014-09-06">2014. September 6-12, 2014. 2014</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="472" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Emotion recognition based on physiological changes in music listening</title>
		<author>
			<persName><forename type="first">Jonghwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elisabeth</forename><surname>André</surname></persName>
		</author>
		<idno type="DOI">10.1109/tpami.2008.26</idno>
		<ptr target="https://doi.org/10.1109/tpami.2008.26" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2067" to="2083" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Application of deep learning for quality of service enhancement in internet of things: A review</title>
		<author>
			<persName><forename type="first">Nasser</forename><surname>Kimbugwe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingrui</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moses</forename><surname>Ntanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyebambe</forename></persName>
		</author>
		<idno type="DOI">10.3390/en14196384</idno>
		<ptr target="https://doi.org/10.3390/en14196384" />
	</analytic>
	<monogr>
		<title level="j">Energies</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">6384</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Segment anything</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Mintun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhila</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chloe</forename><surname>Rolland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Gustafson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Spencer</forename><surname>Whitehead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv51070.2023.00371</idno>
		<ptr target="https://doi.org/10.1109/iccv51070.2023.00371" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
			<biblScope unit="page" from="4015" to="4026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Eye tracking algorithms, techniques, tools, and applications with an emphasis on machine learning and Internet of Things technologies</title>
		<author>
			<persName><surname>Ahmad F Klaib</surname></persName>
		</author>
		<author>
			<persName><surname>Nawaf O Alsrehin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wasen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haneen</forename><forename type="middle">O</forename><surname>Melhem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aws</forename><forename type="middle">A</forename><surname>Bashtawi</surname></persName>
		</author>
		<author>
			<persName><surname>Magableh</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2020.114037</idno>
		<ptr target="https://doi.org/10.1016/j.eswa.2020.114037" />
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">166</biblScope>
			<biblScope unit="page">114037</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">An overview of Wireless Sensor Networks towards internet of things</title>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Kocakulak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ismail</forename><surname>Butun</surname></persName>
		</author>
		<idno type="DOI">10.1109/ccwc.2017.7868374</idno>
		<ptr target="https://doi.org/10.1109/ccwc.2017.7868374" />
	</analytic>
	<monogr>
		<title level="m">IEEE 7th annual computing and communication workshop and conference (CCWC)</title>
		<imprint>
			<date type="published" when="2017">2017. 2017. 2017</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Modeling emotions for affect-aware applications</title>
		<author>
			<persName><forename type="first">Agata</forename><surname>Kołakowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agnieszka</forename><surname>Landowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariusz</forename><surname>Szwoch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wioleta</forename><surname>Szwoch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michał</forename><forename type="middle">R</forename><surname>Wróbel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Syst. Dev. Appl</title>
		<imprint>
			<biblScope unit="page" from="55" to="69" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Human action recognition and prediction: A survey</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-022-01594-9</idno>
		<ptr target="https://doi.org/10.1007/s11263-022-01594-9" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1366" to="1401" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<monogr>
		<title level="m" type="main">Systems engineering principles and practice</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kossiakoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">M</forename><surname>Biemer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">J</forename><surname>Seymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Flanigan</surname></persName>
		</author>
		<idno type="DOI">10.1002/9781119516699</idno>
		<ptr target="https://doi.org/10.1002/9781119516699" />
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1145/3065386</idno>
		<ptr target="https://doi.org/10.1145/3065386" />
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="84" to="90" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Learning and managing context enriched behavior patterns in smart homes</title>
		<author>
			<persName><forename type="first">Paula</forename><surname>Lago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudia</forename><surname>Roncancio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudia</forename><surname>Jiménez-Guarín</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.future.2018.09.004</idno>
		<ptr target="https://doi.org/10.1016/j.future.2018.09.004" />
	</analytic>
	<monogr>
		<title level="j">Futur. Gener. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page" from="191" to="205" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Advancement of environmental monitoring system using IoT and sensor: A comprehensive analysis</title>
		<author>
			<persName><forename type="first">Binod</forename><surname>Suprava Ranjan Laha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saumendra</forename><surname>Kumar Pattanayak</surname></persName>
		</author>
		<author>
			<persName><surname>Pattnaik</surname></persName>
		</author>
		<idno type="DOI">10.3934/environsci.2022044</idno>
		<ptr target="https://doi.org/10.3934/environsci.2022044" />
	</analytic>
	<monogr>
		<title level="j">AIMS Environ. Sci</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="771" to="800" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Toward emotion recognition from physiological signals in the wild: approaching the methodological issues in real-life data collection</title>
		<author>
			<persName><forename type="first">Fanny</forename><surname>Larradet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radoslaw</forename><surname>Niewiadomski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giacinto</forename><surname>Barresi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darwin</forename><forename type="middle">G</forename><surname>Caldwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonardo</forename><forename type="middle">S</forename><surname>Mattos</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2020.01111</idno>
		<ptr target="https://doi.org/10.3389/fpsyg.2020.01111" />
	</analytic>
	<monogr>
		<title level="j">Front. Psychol</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">1111</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<monogr>
		<title level="m" type="main">The experience and perception of time</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Poidevin</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature14539</idno>
		<ptr target="https://doi.org/10.1038/nature14539" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Human height estimation by color deep learning and depth 3D conversion</title>
		<author>
			<persName><forename type="first">Dong-Seok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong-Soo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seok</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Chan</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soon-Kak</forename><surname>Kwon</surname></persName>
		</author>
		<idno type="DOI">10.3390/app10165531</idno>
		<ptr target="https://doi.org/10.3390/app10165531" />
	</analytic>
	<monogr>
		<title level="j">Appl. Sci</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">5531</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<monogr>
		<author>
			<persName><forename type="first">Alfred</forename><surname>Leick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lev</forename><surname>Rapoport</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Tatarnikov</surname></persName>
		</author>
		<idno type="DOI">10.1002/9781119018612</idno>
		<ptr target="https://doi.org/10.1002/9781119018612" />
		<title level="m">GPS satellite surveying</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Non-intrusive human activity recognition and abnormal behavior detection on elderly people: A review</title>
		<author>
			<persName><forename type="first">Athanasios</forename><surname>Lentzas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Vrakas</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10462-019-09724-5</idno>
		<ptr target="https://doi.org/10.1007/s10462-019-09724-5" />
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Rev</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="1975" to="2021" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<monogr>
		<title level="m" type="main">Principles of topological psychology</title>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Lewin</surname></persName>
		</author>
		<idno type="DOI">10.1037/10019-000</idno>
		<ptr target="https://doi.org/10.1037/10019-000" />
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Read Books Ltd</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Data augmentation approaches in natural language processing: A survey</title>
		<author>
			<persName><forename type="first">Bohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutai</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.aiopen.2022.03.001</idno>
		<ptr target="https://doi.org/10.1016/j.aiopen.2022.03.001" />
	</analytic>
	<monogr>
		<title level="j">Ai Open</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="71" to="90" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Soft actuators for real-world applications</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aniket</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amirreza</forename><surname>Aghakhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdon</forename><surname>Pena-Francesch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Metin</forename><surname>Sitti</surname></persName>
		</author>
		<idno type="DOI">10.1109/taffc.2020.2981446</idno>
		<ptr target="https://doi.org/10.1109/taffc.2020.2981446" />
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Mater</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="235" to="249" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Deep facial expression recognition: A survey</title>
		<author>
			<persName><forename type="first">Shan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41578-021-00389-7</idno>
		<ptr target="https://doi.org/10.1038/s41578-021-00389-7" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affect. Comput</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1195" to="1215" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">Toward location-enabled IoT (LE-IoT): IoT positioning techniques, error sources, and error mitigation</title>
		<author>
			<persName><forename type="first">You</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhouzheng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kejie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/jiot.2020.3019199</idno>
		<ptr target="https://doi.org/10.1109/jiot.2020.3019199" />
	</analytic>
	<monogr>
		<title level="j">IEEE Internet Things J</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="4035" to="4062" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning: An overview</title>
		<author>
			<persName><forename type="first">Yuxi</forename><surname>Li</surname></persName>
		</author>
		<idno>arXiv1701.07274</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv Prepr.</note>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">Foundations &amp; trends in multimodal machine learning: Principles, challenges, and open questions</title>
		<author>
			<persName><forename type="first">Paul Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<idno type="DOI">10.1145/3656580</idno>
		<ptr target="https://doi.org/10.1145/3656580" />
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">Systematic reviews in sentiment analysis: a tertiary study</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Ligthart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cagatay</forename><surname>Catal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bedir</forename><surname>Tekinerdogan</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10462-021-09973-3</idno>
		<ptr target="https://doi.org/10.1007/s10462-021-09973-3" />
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Rev</title>
		<imprint>
			<biblScope unit="page" from="1" to="57" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">Emotion recognition using eye-tracking: taxonomy, review and current challenges</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Mountstephens</surname></persName>
		</author>
		<author>
			<persName><surname>Teo</surname></persName>
		</author>
		<idno type="DOI">10.3390/s20082384</idno>
		<ptr target="https://doi.org/10.3390/s20082384" />
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">2384</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">Federated learning in mobile edge networks: A comprehensive survey</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cong</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinh Thai</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutao</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying-Chang</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dusit</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyan</forename><surname>Niyato</surname></persName>
		</author>
		<author>
			<persName><surname>Miao</surname></persName>
		</author>
		<idno type="DOI">10.1109/comst.2020.2986024</idno>
		<ptr target="https://doi.org/10.1109/comst.2020.2986024" />
	</analytic>
	<monogr>
		<title level="j">IEEE Commun. Surv. tutorials</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="2031" to="2063" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">Human gender classification: a review</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingxiao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyao</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1504/ijbm.2016.10003589</idno>
		<ptr target="https://doi.org/10.1504/ijbm.2016.10003589" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Biom</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="275" to="300" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">A survey on internet of things: Architecture, enabling technologies, security and privacy, and applications</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanlin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.1109/jiot.2017.2683200</idno>
		<ptr target="https://doi.org/10.1109/jiot.2017.2683200" />
	</analytic>
	<monogr>
		<title level="j">IEEE internet things J</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1125" to="1142" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">A survey of transformers</title>
		<author>
			<persName><forename type="first">Tianyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.aiopen.2022.10.001</idno>
		<ptr target="https://doi.org/10.1016/j.aiopen.2022.10.001" />
	</analytic>
	<monogr>
		<title level="j">AI open</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="111" to="132" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">Review of studies on emotion recognition and judgment based on physiological signals</title>
		<author>
			<persName><forename type="first">Wenqian</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.3390/app13042573</idno>
		<ptr target="https://doi.org/10.3390/app13042573" />
	</analytic>
	<monogr>
		<title level="j">Appl. Sci</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">2573</biblScope>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">Sentiment analysis and subjectivity</title>
		<author>
			<persName><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1201/9781420085938-36</idno>
		<ptr target="https://doi.org/10.1201/9781420085938-36" />
	</analytic>
	<monogr>
		<title level="j">Handb. Nat. Lang. Process</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="627" to="666" />
			<date type="published" when="2010">2010. 2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<monogr>
		<title level="m" type="main">Sentiment analysis: Mining opinions, sentiments, and emotions</title>
		<author>
			<persName><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1017/9781108639286</idno>
		<ptr target="https://doi.org/10.1017/9781108639286" />
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing</title>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroaki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="DOI">10.1145/3560815</idno>
		<ptr target="https://doi.org/10.1145/3560815" />
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">Self-supervised learning: Generative or contrastive</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fanjin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1109/tkde.2021.3090866</idno>
		<ptr target="https://doi.org/10.1109/tkde.2021.3090866" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="857" to="876" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<monogr>
		<title level="m" type="main">Sora: A review on background, technology, limitations, and opportunities of large vision models</title>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiling</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chujie</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoxi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengqing</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanchi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2402.17177</idno>
		<idno>arXiv2402.17177</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2402.17177" />
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv Prepr.</note>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">Multiple object tracking: A literature review</title>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tae-Kyun</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.artint.2020.103448</idno>
		<ptr target="https://doi.org/10.1016/j.artint.2020.103448" />
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">293</biblScope>
			<biblScope unit="page">103448</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">Pose guided person image generation</title>
		<author>
			<persName><forename type="first">Liqian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="DOI">10.1109/icip40778.2020.9190773</idno>
		<ptr target="https://doi.org/10.1109/icip40778.2020.9190773" />
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">A survey of human action recognition and posture prediction</title>
		<author>
			<persName><forename type="first">Nan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhixuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiu-Ming</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beijyan</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="DOI">10.26599/tst.2021.9010068</idno>
		<ptr target="https://doi.org/10.26599/tst.2021.9010068" />
	</analytic>
	<monogr>
		<title level="j">Tsinghua Sci. Technol</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="973" to="1001" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<monogr>
		<title level="m" type="main">A Survey on Vision-Language-Action Models for Embodied AI</title>
		<author>
			<persName><forename type="first">Yueen</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zixing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuzheng</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianye</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2405.14093</idno>
		<idno>arXiv2405.14093</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2405.14093" />
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv Prepr.</note>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">Machine learning for Internet of Things data analysis: A survey</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Saeid Mahdavinejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammadreza</forename><surname>Rezvan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammadamin</forename><surname>Barekatain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peyman</forename><surname>Adibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Payam</forename><surname>Barnaghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit P</forename><surname>Sheth</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.dcan.2017.10.002</idno>
		<ptr target="https://doi.org/10.1016/j.dcan.2017.10.002" />
	</analytic>
	<monogr>
		<title level="j">Digit. Commun. Networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="161" to="175" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">A comprehensive review on artificial intelligence/machine learning algorithms for empowering the future IoT toward 6G era</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Rezwanul Mahmood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panagiotis</forename><surname>Abdul Matin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sotirios</forename><forename type="middle">K</forename><surname>Sarigiannidis</surname></persName>
		</author>
		<author>
			<persName><surname>Goudos</surname></persName>
		</author>
		<idno type="DOI">10.1109/access.2022.3199689</idno>
		<ptr target="https://doi.org/10.1109/access.2022.3199689" />
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="87535" to="87562" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">Applications of wireless sensor networks and internet of things frameworks in the industry revolution 4.0: A systematic literature review</title>
		<author>
			<persName><forename type="first">Mamoona</forename><surname>Majid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaista</forename><surname>Habib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdul</forename><forename type="middle">Rehman</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Rizwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautam</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thippa</forename><surname>Reddy Gadekallu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Wei</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.3390/s22062087</idno>
		<ptr target="https://doi.org/10.3390/s22062087" />
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2022">2022. 2022. 2087</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<analytic>
		<title level="a" type="main">Deep learning-based document modeling for personality detection from text</title>
		<author>
			<persName><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<idno type="DOI">10.1109/mis.2017.23</idno>
		<ptr target="https://doi.org/10.1109/mis.2017.23" />
	</analytic>
	<monogr>
		<title level="j">IEEE Intell. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="74" to="79" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<analytic>
		<title level="a" type="main">Automatic speech recognition: a survey</title>
		<author>
			<persName><forename type="first">Mishaim</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><forename type="middle">Kamran</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khawar</forename><surname>Mehmood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Imran</forename><surname>Makhdoom</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11042-020-10073-7</idno>
		<ptr target="https://doi.org/10.1007/s11042-020-10073-7" />
	</analytic>
	<monogr>
		<title level="j">Multimed. Tools Appl</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="9411" to="9457" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b186">
	<analytic>
		<title level="a" type="main">Embedded system design: embedded systems foundations of cyber-physical systems, and the internet of things</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Marwedel</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-60910-8</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-60910-8" />
	</analytic>
	<monogr>
		<title level="j">Springer Nature</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main">Deep learning, reinforcement learning, and world models</title>
		<author>
			<persName><forename type="first">Yutaka</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maneesh</forename><surname>Sahani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eiji</forename><surname>Uchibe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Morimoto</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neunet.2022.03.037</idno>
		<ptr target="https://doi.org/10.1016/j.neunet.2022.03.037" />
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="page" from="267" to="275" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b188">
	<analytic>
		<title level="a" type="main">Validation of the five-factor model of personality across instruments and observers</title>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">T</forename><surname>Robert R Mccrae</surname></persName>
		</author>
		<author>
			<persName><surname>Costa</surname></persName>
		</author>
		<idno type="DOI">10.1037/0022-3514.52.1.81</idno>
		<ptr target="https://doi.org/10.1037//0022-3514.52.1.81" />
	</analytic>
	<monogr>
		<title level="j">J. Pers. Soc. Psychol</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">81</biblScope>
			<date type="published" when="1987">1987. 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<analytic>
		<title level="a" type="main">Camera measurement of physiological vital signs</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Mcduff</surname></persName>
		</author>
		<idno type="DOI">10.1145/3558518</idno>
		<ptr target="https://doi.org/10.1145/3558518" />
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1" to="40" />
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b190">
	<analytic>
		<title level="a" type="main">A survey on bias and fairness in machine learning</title>
		<author>
			<persName><forename type="first">Ninareh</forename><surname>Mehrabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><surname>Morstatter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nripsuta</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
		<idno type="DOI">10.1145/3457607</idno>
		<ptr target="https://doi.org/10.1145/3457607" />
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b191">
	<analytic>
		<title level="a" type="main">An approach to environmental psychology</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Mehrabian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Massachusetts Inst. Technol</title>
		<imprint>
			<date type="published" when="1974">1974. 1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b192">
	<analytic>
		<title level="a" type="main">Recent trends in deep learning based personality detection</title>
		<author>
			<persName><forename type="first">Yash</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10462-019-09770-z</idno>
		<ptr target="https://doi.org/10.1007/s10462-019-09770-z" />
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Rev</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2313" to="2339" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b193">
	<analytic>
		<title level="a" type="main">The NIST Definition of Cloud Computing</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Mell</surname></persName>
		</author>
		<idno type="DOI">10.6028/nist.sp.800-145</idno>
		<ptr target="https://doi.org/10.6028/nist.sp.800-145" />
	</analytic>
	<monogr>
		<title level="j">Natl. Inst. Stand. Technol</title>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b194">
	<monogr>
		<title level="m" type="main">A review on facial micro-expressions analysis: datasets, features and metrics</title>
		<author>
			<persName><forename type="first">Walied</forename><surname>Merghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><forename type="middle">K</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moi</forename><surname>Hoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yap</forename></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1805.02397</idno>
		<idno>arXiv Prepr. arXiv1805.02397</idno>
		<ptr target="https://doi.org/10.48550/arXiv.1805.02397" />
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b195">
	<analytic>
		<title level="a" type="main">A survey on machine learning in Internet of Things: Algorithms, strategies, and applications</title>
		<author>
			<persName><forename type="first">Seifeddine</forename><surname>Messaoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abbas</forename><surname>Bradai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hashim</forename><forename type="middle">Raza</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pham</forename><surname>Bukhari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olfa</forename><surname>Quang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Ben Ahmed</surname></persName>
		</author>
		<author>
			<persName><surname>Atri</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.iot.2020.100314</idno>
		<ptr target="https://doi.org/10.1016/j.iot.2020.100314" />
	</analytic>
	<monogr>
		<title level="j">Internet of Things</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">100314</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b196">
	<monogr>
		<title level="m" type="main">Playing atari with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1312.5602</idno>
		<idno>arXiv1312.5602</idno>
		<ptr target="https://doi.org/10.48550/arXiv.1312.5602" />
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b197">
	<analytic>
		<title level="a" type="main">Height estimation from a single camera view</title>
		<author>
			<persName><forename type="first">Mahdi</forename><surname>Momeni-K</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sotirios</forename><surname>Ch Diamantas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Ruggiero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Siciliano</surname></persName>
		</author>
		<idno type="DOI">10.5220/0003866203580364</idno>
		<ptr target="https://doi.org/10.5220/0003866203580364" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision Theory and Applications</title>
		<imprint>
			<publisher>SciTePress</publisher>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="358" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b198">
	<monogr>
		<title level="m" type="main">The Myers-Briggs Type Indicator</title>
		<author>
			<persName><surname>Myers</surname></persName>
		</author>
		<idno type="DOI">10.1037/14404-000</idno>
		<ptr target="https://doi.org/10.1037/14404-000" />
		<imprint>
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b199">
	<analytic>
		<title level="a" type="main">Speech recognition using deep neural networks: A systematic review</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Bou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nassif</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ismail</forename><surname>Shahin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Imtinan</forename><surname>Attili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Azzeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khaled</forename><surname>Shaalan</surname></persName>
		</author>
		<idno type="DOI">10.1109/access.2019.2896880</idno>
		<ptr target="https://doi.org/10.1109/access.2019.2896880" />
	</analytic>
	<monogr>
		<title level="j">IEEE access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="19143" to="19165" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b200">
	<monogr>
		<title level="m" type="main">A comprehensive overview of large language models</title>
		<author>
			<persName><forename type="first">Humza</forename><surname>Naveed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asad</forename><surname>Ullah Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Saqib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saeed</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Usman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveed</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajmal</forename><surname>Mian</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2307.06435</idno>
		<idno>arXiv2307.06435</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2307.06435" />
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv Prepr.</note>
</biblStruct>

<biblStruct xml:id="b201">
	<analytic>
		<title level="a" type="main">IoT middleware: A survey on issues and enabling technologies</title>
		<author>
			<persName><forename type="first">Anne</forename><forename type="middle">H</forename><surname>Ngu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Gutierrez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vangelis</forename><surname>Metsis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Nepal</surname></persName>
		</author>
		<author>
			<persName><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><surname>Sheng</surname></persName>
		</author>
		<idno type="DOI">10.1109/jiot.2016.2615180</idno>
		<ptr target="https://doi.org/10.1109/jiot.2016.2615180" />
	</analytic>
	<monogr>
		<title level="j">IEEE Internet Things J</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b202">
	<analytic>
		<title level="a" type="main">Systems of systems engineering: basic concepts, model-based techniques, and research directions</title>
		<author>
			<persName><forename type="first">Claus</forename><surname>Ballegaard Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Gorm Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jim</forename><surname>Woodcock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Peleska</surname></persName>
		</author>
		<idno type="DOI">10.1145/2794381</idno>
		<ptr target="https://doi.org/10.1145/2794381" />
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="41" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b203">
	<analytic>
		<title level="a" type="main">Survey on emotional body gesture recognition</title>
		<author>
			<persName><forename type="first">Fatemeh</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Ciprian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dorota</forename><surname>Corneanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Kamińska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Sapiński</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gholamreza</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName><surname>Anbarjafari</surname></persName>
		</author>
		<idno type="DOI">10.1109/taffc.2018.2874986</idno>
		<ptr target="https://doi.org/10.1109/taffc.2018.2874986" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affect. Comput</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="505" to="523" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b204">
	<analytic>
		<title level="a" type="main">Interoperability in internet of things: Taxonomies and open challenges</title>
		<author>
			<persName><forename type="first">Mahda</forename><surname>Noura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Atiquzzaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Gaedke</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11036-018-1089-9</idno>
		<ptr target="https://doi.org/10.1007/s11036-018-1089-9" />
	</analytic>
	<monogr>
		<title level="j">Mob. networks Appl</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="796" to="809" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b205">
	<monogr>
		<title level="m" type="main">4V (ision) system card</title>
		<author>
			<persName><forename type="first">G P T</forename><surname>Openai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b206">
	<monogr>
		<title level="m" type="main">The cognitive structure of emotions</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ortony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerald</forename><forename type="middle">L</forename><surname>Clore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allan</forename><surname>Collins</surname></persName>
		</author>
		<idno type="DOI">10.1017/9781108934053</idno>
		<ptr target="https://doi.org/10.1017/9781108934053" />
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b207">
	<analytic>
		<title level="a" type="main">Hand gesture recognition based on computer vision: a review of techniques</title>
		<author>
			<persName><forename type="first">Munir</forename><surname>Oudah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Al-Naji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javaan</forename><surname>Chahl</surname></persName>
		</author>
		<idno type="DOI">10.3390/jimaging6080073</idno>
		<ptr target="https://doi.org/10.3390/jimaging6080073" />
	</analytic>
	<monogr>
		<title level="j">J. Imaging</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">73</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b208">
	<analytic>
		<title level="a" type="main">Training language models to follow instructions with human feedback</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="27730" to="27744" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b209">
	<analytic>
		<title level="a" type="main">Data-centric Engineering: integrating simulation, machine learning and statistics. Challenges and opportunities</title>
		<author>
			<persName><forename type="first">Indranil</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><forename type="middle">K</forename><surname>Lachlan R Mason</surname></persName>
		</author>
		<author>
			<persName><surname>Matar</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ces.2021.117271</idno>
		<ptr target="https://doi.org/10.1016/j.ces.2021.117271" />
	</analytic>
	<monogr>
		<title level="j">Chem. Eng. Sci</title>
		<imprint>
			<biblScope unit="volume">249</biblScope>
			<biblScope unit="page">117271</biblScope>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b210">
	<analytic>
		<title level="a" type="main">Collaborative indoor positioning systems: A systematic review</title>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Pascacio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Casteleyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joaquin</forename><surname>Torres-Sospedra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Simona Lohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jari</forename><surname>Nurmi</surname></persName>
		</author>
		<idno type="DOI">10.3390/s21031002</idno>
		<ptr target="https://doi.org/10.3390/s21031002" />
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">1002</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b211">
	<analytic>
		<title level="a" type="main">Expressive body capture: 3d hands, face, and body from a single image</title>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nima</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitrios</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName><surname>Black</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2019.01123</idno>
		<ptr target="https://doi.org/10.1109/cvpr.2019.01123" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="10975" to="10985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b212">
	<monogr>
		<title level="m" type="main">The definitive book of body language: The hidden meaning behind people&apos;s gestures and expressions</title>
		<author>
			<persName><forename type="first">Barbara</forename><surname>Pease</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allan</forename><surname>Pease</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Bantam</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b213">
	<analytic>
		<title level="a" type="main">Context aware computing for the internet of things: A survey</title>
		<author>
			<persName><forename type="first">Charith</forename><surname>Perera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arkady</forename><surname>Zaslavsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Christen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitrios</forename><surname>Georgakopoulos</surname></persName>
		</author>
		<idno type="DOI">10.1109/surv.2013.042313.00197</idno>
		<ptr target="https://doi.org/10.1109/surv.2013.042313.00197" />
	</analytic>
	<monogr>
		<title level="j">IEEE Commun. Surv. tutorials</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="414" to="454" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b214">
	<analytic>
		<title level="a" type="main">Personality computing: New frontiers in personality assessment</title>
		<author>
			<persName><forename type="first">Le</forename><surname>Vy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phan</forename></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">F</forename><surname>Rauthmann</surname></persName>
		</author>
		<idno type="DOI">10.1111/spc3.12624</idno>
		<ptr target="https://doi.org/10.1111/spc3.12624" />
	</analytic>
	<monogr>
		<title level="j">Soc. Personal. Psychol. Compass</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">12624</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b215">
	<analytic>
		<title level="a" type="main">Biological age estimation using an eHealth system based on wearable sensors</title>
		<author>
			<persName><forename type="first">Paola</forename><surname>Pierleoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Belli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Concetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Palma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federica</forename><surname>Pinti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Raggiunto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luisiana</forename><surname>Sabbatini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Valenti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Monteriù</surname></persName>
		</author>
		<idno type="DOI">10.1007/s12652-019-01593-8</idno>
		<ptr target="https://doi.org/10.1007/s12652-019-01593-8" />
	</analytic>
	<monogr>
		<title level="j">J. Ambient Intell. Humaniz. Comput</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="4449" to="4460" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b216">
	<analytic>
		<title level="a" type="main">The nature of emotions: Human emotions have deep evolutionary roots, a fact that may explain their complexity and provide tools for clinical practice</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Plutchik</surname></persName>
		</author>
		<idno type="DOI">10.1511/2001.4.344</idno>
		<ptr target="https://doi.org/10.1511/2001.4.344" />
	</analytic>
	<monogr>
		<title level="j">Am. Sci</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="344" to="350" />
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b217">
	<analytic>
		<title level="a" type="main">Time perception</title>
		<author>
			<persName><forename type="first">Ernst</forename><surname>Poppel</surname></persName>
		</author>
		<idno type="DOI">10.1016/s1364-6613(97)01008-5</idno>
		<ptr target="https://doi.org/10.1016/s1364-6613(97)01008-5" />
	</analytic>
	<monogr>
		<title level="j">Handb. Sens. Physiol</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="713" to="729" />
			<date type="published" when="1978">1978. 1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b218">
	<analytic>
		<title level="a" type="main">A hierarchical model of temporal perception</title>
		<author>
			<persName><forename type="first">Ernst</forename><surname>Pöppel</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-46354-9_23</idno>
		<ptr target="https://doi.org/10.1007/978-3-642-46354-9_23" />
	</analytic>
	<monogr>
		<title level="j">Trends Cogn. Sci</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="56" to="61" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b219">
	<analytic>
		<title level="a" type="main">A review of affective computing: From unimodal analysis to multimodal fusion</title>
		<author>
			<persName><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajiv</forename><surname>Bajpai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Hussain</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.inffus.2017.02.003</idno>
		<ptr target="https://doi.org/10.1016/j.inffus.2017.02.003" />
	</analytic>
	<monogr>
		<title level="j">Inf. fusion</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="98" to="125" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b220">
	<analytic>
		<title level="a" type="main">A review of generalized zeroshot learning methods</title>
		<author>
			<persName><forename type="first">Farhad</forename><surname>Pourpanah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moloud</forename><surname>Abdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chee</forename><surname>Peng Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi-Zhao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q M Jonathan</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1109/tpami.2022.3191696</idno>
		<ptr target="https://doi.org/10.1109/tpami.2022.3191696" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="4051" to="4070" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b221">
	<analytic>
		<title level="a" type="main">Edge computing in industrial internet of things: Architecture, advances and challenges</title>
		<author>
			<persName><forename type="first">Tie</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiancheng</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaolong</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Atiquzzaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dapeng</forename><forename type="middle">Oliver</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1109/comst.2020.3009103</idno>
		<ptr target="https://doi.org/10.1109/comst.2020.3009103" />
	</analytic>
	<monogr>
		<title level="j">IEEE Commun. Surv. Tutorials</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2462" to="2488" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b222">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b223">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b224">
	<analytic>
		<title level="a" type="main">Driven by data or derived through physics? a review of hybrid physics guided machine learning techniques with cyber-physical system (cps) focus</title>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chandan</surname></persName>
		</author>
		<author>
			<persName><surname>Sahu</surname></persName>
		</author>
		<idno type="DOI">10.1109/access.2020.2987324</idno>
		<ptr target="https://doi.org/10.1109/access.2020.2987324" />
	</analytic>
	<monogr>
		<title level="j">IEEe Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="71050" to="71073" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b225">
	<analytic>
		<title level="a" type="main">Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations</title>
		<author>
			<persName><forename type="first">Maziar</forename><surname>Raissi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paris</forename><surname>Perdikaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jcp.2018.10.045</idno>
		<ptr target="https://doi.org/10.1016/j.jcp.2018.10.045" />
	</analytic>
	<monogr>
		<title level="j">J. Comput. Phys</title>
		<imprint>
			<biblScope unit="volume">378</biblScope>
			<biblScope unit="page" from="686" to="707" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b226">
	<analytic>
		<title level="a" type="main">Sign language recognition: A deep survey</title>
		<author>
			<persName><forename type="first">Razieh</forename><surname>Rastgoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kourosh</forename><surname>Kiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Escalera</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2020.113794</idno>
		<ptr target="https://doi.org/10.1016/j.eswa.2020.113794" />
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">164</biblScope>
			<biblScope unit="page">113794</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b227">
	<analytic>
		<title level="a" type="main">Transfer learning enhanced vision-based human activity recognition: a decadelong analysis</title>
		<author>
			<persName><forename type="first">Abhisek</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Maheshkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raman</forename><surname>Kolekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adel</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName><surname>Hafiane</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jjimei.2022.100142</idno>
		<ptr target="https://doi.org/10.1016/j.jjimei.2022.100142" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Inf. Manag. Data Insights</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">100142</biblScope>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b228">
	<analytic>
		<title level="a" type="main">A survey on Internet of Things architectures</title>
		<author>
			<persName><forename type="first">Partha</forename><surname>Pratim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ray</forename></persName>
		</author>
		<idno type="DOI">10.1016/j.jksuci.2016.10.003</idno>
		<ptr target="https://doi.org/10.1016/j.jksuci.2016.10.003" />
	</analytic>
	<monogr>
		<title level="j">J. King Saud Univ. Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="291" to="319" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b229">
	<analytic>
		<title level="a" type="main">Middleware for internet of things: a survey</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Abdur Razzaque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marija</forename><surname>Milojevic-Jevric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Palade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siobhán</forename><surname>Clarke</surname></persName>
		</author>
		<idno type="DOI">10.1109/jiot.2015.2498900</idno>
		<ptr target="https://doi.org/10.1109/jiot.2015.2498900" />
	</analytic>
	<monogr>
		<title level="j">IEEE Internet things J</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="70" to="95" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b230">
	<analytic>
		<title level="a" type="main">Deep learning in physiological signal data: A survey</title>
		<author>
			<persName><forename type="first">Beanbonyka</forename><surname>Rim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nak-Jun</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sedong</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Hong</surname></persName>
		</author>
		<idno type="DOI">10.3390/s20040969</idno>
		<ptr target="https://doi.org/10.3390/s20040969" />
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">969</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b231">
	<analytic>
		<title level="a" type="main">A survey on data collection for machine learning: a big data-ai integration perspective</title>
		<author>
			<persName><forename type="first">Yuji</forename><surname>Roh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geon</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">Euijong</forename><surname>Whang</surname></persName>
		</author>
		<idno type="DOI">10.1109/tkde.2019.2946162</idno>
		<ptr target="https://doi.org/10.1109/tkde.2019.2946162" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1328" to="1347" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b232">
	<analytic>
		<title level="a" type="main">A survey on biometric authentication: Toward secure and privacy-preserving identification</title>
		<author>
			<persName><forename type="first">Zhang</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.1109/access.2018.2889996</idno>
		<ptr target="https://doi.org/10.1109/access.2018.2889996" />
	</analytic>
	<monogr>
		<title level="j">IEEE access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="5994" to="6009" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b233">
	<analytic>
		<title level="a" type="main">A circumplex model of affect</title>
		<author>
			<persName><forename type="first">Russell</forename><surname>James</surname></persName>
		</author>
		<idno type="DOI">10.1037/h0077714</idno>
		<ptr target="https://doi.org/10.1037/h0077714" />
	</analytic>
	<monogr>
		<title level="j">J. Pers. Soc. Psychol</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page">1161</biblScope>
			<date type="published" when="1980">1980. 1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b234">
	<analytic>
		<title level="a" type="main">Spatio-temporal relationship match: Video structure comparison for recognition of complex human activities</title>
		<author>
			<persName><forename type="first">S</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><forename type="middle">K</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName><surname>Aggarwal</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2009.5459361</idno>
		<ptr target="https://doi.org/10.1109/iccv.2009.5459361" />
	</analytic>
	<monogr>
		<title level="m">IEEE 12th international conference on computer vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009. 2009. 2009</date>
			<biblScope unit="page" from="1593" to="1600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b235">
	<analytic>
		<title level="a" type="main">Ambient intelligence: A survey</title>
		<author>
			<persName><forename type="first">Fariba</forename><surname>Sadri</surname></persName>
		</author>
		<idno type="DOI">10.1145/1978802.1978815</idno>
		<ptr target="https://doi.org/10.1145/1978802.1978815" />
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="66" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b236">
	<analytic>
		<title level="a" type="main">Emotion recognition for everyday life using physiological signals from wearables: A systematic literature review</title>
		<author>
			<persName><forename type="first">Stanisław</forename><surname>Saganowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bartosz</forename><surname>Perz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">G</forename><surname>Polak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Przemysław</forename><surname>Kazienko</surname></persName>
		</author>
		<idno type="DOI">10.1109/taffc.2022.3176135</idno>
		<ptr target="https://doi.org/10.1109/taffc.2022.3176135" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affect. Comput</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1876" to="1897" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b237">
	<analytic>
		<title level="a" type="main">Facial Asymmetry-Based Age Group Estimation: Role in Recognizing Age-Separated Face Images</title>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Sajid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Imtiaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Usama</forename><surname>Taj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naeem</forename><surname>Ijaz Bajwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ratyal</forename><surname>Iqbal</surname></persName>
		</author>
		<idno type="DOI">10.1111/1556-4029.13798</idno>
		<ptr target="https://doi.org/10.1111/1556-4029.13798" />
	</analytic>
	<monogr>
		<title level="j">J. Forensic Sci</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1727" to="1749" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b238">
	<analytic>
		<title level="a" type="main">Deep learning for the internet of things: Potential benefits and use-cases</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Tausifa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Saleem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chishti</forename><surname>Ahsan</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.dcan.2020.12.002</idno>
		<ptr target="https://doi.org/10.1016/j.dcan.2020.12.002" />
	</analytic>
	<monogr>
		<title level="j">Digit. Commun. Networks</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="526" to="542" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b239">
	<analytic>
		<title level="a" type="main">A review on automatic facial expression recognition systems assisted by multimodal sensor data</title>
		<author>
			<persName><forename type="first">Najmeh</forename><surname>Samadiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangyan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Borui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi-Hung</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.3390/s19081863</idno>
		<ptr target="https://doi.org/10.3390/s19081863" />
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">1863</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b240">
	<analytic>
		<title level="a" type="main">Revisiting crowd behaviour analysis through deep learning: Taxonomy, anomaly detection, crowd emotions, datasets, opportunities and prospects</title>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Francisco Luque Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siham</forename><surname>Hupont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Tabik</surname></persName>
		</author>
		<author>
			<persName><surname>Herrera</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.inffus.2020.07.008</idno>
		<ptr target="https://doi.org/10.1016/j.inffus.2020.07.008" />
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="318" to="335" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b241">
	<analytic>
		<title level="a" type="main">Deep learning: a comprehensive overview on techniques, taxonomy, applications and research directions</title>
		<author>
			<persName><forename type="first">H</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName><surname>Sarker</surname></persName>
		</author>
		<idno type="DOI">10.1007/s42979-021-00815-1</idno>
		<ptr target="https://doi.org/10.1007/s42979-021-00815-1" />
	</analytic>
	<monogr>
		<title level="j">SN Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">420</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b242">
	<analytic>
		<title level="a" type="main">How computers see gender: An evaluation of gender classification in commercial facial analysis services</title>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Klaus Scheuerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><forename type="middle">M</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jed</forename><forename type="middle">R</forename><surname>Brubaker</surname></persName>
		</author>
		<idno type="DOI">10.1145/3359246</idno>
		<ptr target="https://doi.org/10.1145/3359246" />
	</analytic>
	<monogr>
		<title level="j">Proc. ACM Human-Computer Interact</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="33" />
			<date type="published" when="2019">2019. 2019</date>
			<publisher>CSCW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b243">
	<analytic>
		<title level="a" type="main">Disseminating active map information to mobile hosts</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marvin</forename><forename type="middle">M</forename><surname>Schilit</surname></persName>
		</author>
		<author>
			<persName><surname>Theimer</surname></persName>
		</author>
		<idno type="DOI">10.1109/65.313011</idno>
		<ptr target="https://doi.org/10.1109/65.313011" />
	</analytic>
	<monogr>
		<title level="j">IEEE Netw</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="22" to="32" />
			<date type="published" when="1994">1994. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b244">
	<analytic>
		<title level="a" type="main">Survey on aspect-level sentiment analysis</title>
		<author>
			<persName><forename type="first">Kim</forename><surname>Schouten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Flavius</forename><surname>Frasincar</surname></persName>
		</author>
		<idno type="DOI">10.1109/tkde.2015.2485209</idno>
		<ptr target="https://doi.org/10.1109/tkde.2015.2485209" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="813" to="830" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b245">
	<analytic>
		<title level="a" type="main">A systematic content review of artificial intelligence and the internet of things applications in smart home</title>
		<author>
			<persName><forename type="first">Samad</forename><surname>Sepasgozar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reyhaneh</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leila</forename><surname>Farahzadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farimah</forename><surname>Moezzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Shirowzhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sanee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Ebrahimzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><surname>Aye</surname></persName>
		</author>
		<idno type="DOI">10.3390/app10093074</idno>
		<ptr target="https://doi.org/10.3390/app10093074" />
	</analytic>
	<monogr>
		<title level="j">Appl. Sci</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">3074</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b246">
	<analytic>
		<title level="a" type="main">Internet of things: architectures, protocols, and applications</title>
		<author>
			<persName><forename type="first">Pallavi</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Smruti</surname></persName>
		</author>
		<author>
			<persName><surname>Sarangi</surname></persName>
		</author>
		<idno type="DOI">10.1155/2017/9324035</idno>
		<ptr target="https://doi.org/10.1155/2017/9324035" />
	</analytic>
	<monogr>
		<title level="j">J. Electr. Comput. Eng</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9324035</biblScope>
			<date type="published" when="2017">2017. 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b247">
	<analytic>
		<title level="a" type="main">Review of deep learning algorithms and architectures</title>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Shrestha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ausif</forename><surname>Mahmood</surname></persName>
		</author>
		<idno type="DOI">10.1109/access.2019.2912200</idno>
		<ptr target="https://doi.org/10.1109/access.2019.2912200" />
	</analytic>
	<monogr>
		<title level="j">IEEE access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="53040" to="53065" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b248">
	<analytic>
		<title level="a" type="main">A review of emotion recognition using physiological signals</title>
		<author>
			<persName><forename type="first">Lin</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangmin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.3390/s18072074</idno>
		<ptr target="https://doi.org/10.3390/s18072074" />
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018">2018. 2018. 2074</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b249">
	<analytic>
		<title level="a" type="main">Flexible gesture input with radars: systematic literature review and taxonomy of radar sensing integration in ambient intelligence environments</title>
		<author>
			<persName><forename type="first">Alexandru-Ionuţ</forename><surname>Şiean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristian</forename><surname>Pamparău</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Sluÿters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu-Daniel</forename><surname>Vatavu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Vanderdonckt</surname></persName>
		</author>
		<idno type="DOI">10.1007/s12652-023-04606-9</idno>
		<ptr target="https://doi.org/10.1007/s12652-023-04606-9" />
	</analytic>
	<monogr>
		<title level="j">J. Ambient Intell. Humaniz. Comput</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="7967" to="7981" />
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b250">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1409.1556</idno>
		<idno>arXiv1409.1556</idno>
		<ptr target="https://doi.org/10.48550/arXiv.1409.1556" />
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv Prepr.</note>
</biblStruct>

<biblStruct xml:id="b251">
	<analytic>
		<title level="a" type="main">A survey of recent advances in cnn-based single image crowd counting and density estimation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vishwanath</surname></persName>
		</author>
		<author>
			<persName><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName><surname>Patel</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patrec.2017.07.007</idno>
		<ptr target="https://doi.org/10.1016/j.patrec.2017.07.007" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="3" to="16" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b252">
	<analytic>
		<title level="a" type="main">Review of eye tracking metrics involved in emotional and cognitive processes</title>
		<author>
			<persName><forename type="first">Vasileios</forename><surname>Skaramagkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giorgos</forename><surname>Giannakakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanouil</forename><surname>Ktistakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Manousos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Karatzanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evanthia</forename><surname>Nikolaos S Tachos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kostas</forename><surname>Tripoliti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitrios</forename><forename type="middle">I</forename><surname>Marias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manolis</forename><surname>Fotiadis</surname></persName>
		</author>
		<author>
			<persName><surname>Tsiknakis</surname></persName>
		</author>
		<idno type="DOI">10.1109/rbme.2021.3066072</idno>
		<ptr target="https://doi.org/10.1109/rbme.2021.3066072" />
	</analytic>
	<monogr>
		<title level="j">IEEE Rev. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="260" to="277" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b253">
	<analytic>
		<title level="a" type="main">When multimedia meets fashion</title>
		<author>
			<persName><forename type="first">Sijie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<idno type="DOI">10.1109/mmul.2018.2875860</idno>
		<ptr target="https://doi.org/10.1109/mmul.2018.2875860" />
	</analytic>
	<monogr>
		<title level="j">IEEE Multimed</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="102" to="108" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b254">
	<analytic>
		<title level="a" type="main">GP-BPR: Personalized compatibility modeling for clothing matching</title>
		<author>
			<persName><forename type="first">Xuemeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianjing</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunkai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin-Shun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<idno type="DOI">10.1145/3343031.3350956</idno>
		<ptr target="https://doi.org/10.1145/3343031.3350956" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM international conference on multimedia</title>
		<meeting>the 27th ACM international conference on multimedia</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="320" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b255">
	<analytic>
		<title level="a" type="main">A comprehensive survey of few-shot learning: Evolution, applications, challenges, and opportunities</title>
		<author>
			<persName><forename type="first">Yisheng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Puyu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subrota</forename><forename type="middle">K</forename><surname>Mondal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jyoti</forename><forename type="middle">Prakash</forename><surname>Sahoo</surname></persName>
		</author>
		<idno type="DOI">10.1145/3582688</idno>
		<ptr target="https://doi.org/10.1145/3582688" />
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">13s</biblScope>
			<biblScope unit="page" from="1" to="40" />
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b256">
	<analytic>
		<title level="a" type="main">A systematic review of IoT communication strategies for an efficient smart environment</title>
		<author>
			<persName><forename type="first">Alireza</forename><surname>Souri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aseel</forename><surname>Hussien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahdi</forename><surname>Hoseyninezhad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monire</forename><surname>Norouzi</surname></persName>
		</author>
		<idno type="DOI">10.1002/ett.3736</idno>
		<ptr target="https://doi.org/10.1002/ett.3736" />
	</analytic>
	<monogr>
		<title level="j">Trans. Emerg. Telecommun. Technol</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">3736</biblScope>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b257">
	<analytic>
		<title level="a" type="main">Automatic affect perception based on body gait and posture: A survey</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Stephens-Fripp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fazel</forename><surname>Naghdy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Stirling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Golshah</forename><surname>Naghdy</surname></persName>
		</author>
		<idno type="DOI">10.1007/s12369-017-0427-6</idno>
		<ptr target="https://doi.org/10.1007/s12369-017-0427-6" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Soc. Robot</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="617" to="641" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b258">
	<analytic>
		<title level="a" type="main">What is missing in the study of emotion expression?</title>
		<author>
			<persName><forename type="first">Elisa</forename><surname>Straulino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristina</forename><surname>Scarpazza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luisa</forename><surname>Sartori</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2023.1158136</idno>
		<ptr target="https://doi.org/10.3389/fpsyg.2023.1158136" />
	</analytic>
	<monogr>
		<title level="j">Front. Psychol</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">1158136</biblScope>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b259">
	<analytic>
		<title level="a" type="main">ASCERTAIN: Emotion and personality recognition using commercial sensors</title>
		<author>
			<persName><forename type="first">Ramanathan</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><surname>Wache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mojtaba</forename><surname>Khomami Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Radu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Vieriu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicu</forename><surname>Winkler</surname></persName>
		</author>
		<author>
			<persName><surname>Sebe</surname></persName>
		</author>
		<idno type="DOI">10.1109/taffc.2016.2625250</idno>
		<ptr target="https://doi.org/10.1109/taffc.2016.2625250" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affect. Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="147" to="160" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b260">
	<analytic>
		<title level="a" type="main">Sensors and artificial intelligence methods and algorithms for human-computer intelligent interaction: A systematic mapping study</title>
		<author>
			<persName><forename type="first">Boštjan</forename><surname>Šumak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saša</forename><surname>Brdnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maja</forename><surname>Pušnik</surname></persName>
		</author>
		<idno type="DOI">10.3390/s22010020</idno>
		<ptr target="https://doi.org/10.3390/s22010020" />
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b261">
	<analytic>
		<title level="a" type="main">Reinforcement learning: an introduction</title>
		<author>
			<persName><surname>Richard S Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">A Bradford B</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b262">
	<analytic>
		<title level="a" type="main">A survey on deep transfer learning</title>
		<author>
			<persName><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenchang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunfang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/tnnls.2022.3160699</idno>
		<ptr target="https://doi.org/10.1109/tnnls.2022.3160699" />
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks and Machine Learning-ICANN 2018: 27th International Conference on Artificial Neural Networks</title>
		<meeting><address><addrLine>Rhodes, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018-10-04">2018. October 4-7, 2018. 2018</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="270" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b263">
	<analytic>
		<title level="a" type="main">DeepThink IoT: the strength of deep learning in internet of things</title>
		<author>
			<persName><forename type="first">Divyansh</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaur</forename><surname>Saini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srikant</forename><surname>Srinivasan</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10462-023-10513-4</idno>
		<ptr target="https://doi.org/10.1007/s10462-023-10513-4" />
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Rev</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="14663" to="14730" />
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b264">
	<analytic>
		<title level="a" type="main">A comprehensive review of digital twin-part 1: modeling and twinning enabling technologies</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Thelen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoge</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Fink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sayan</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><surname>Byeng D Youn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sankaran</forename><surname>Michael D Todd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00158-022-03425-4</idno>
		<ptr target="https://doi.org/10.1007/s00158-022-03425-4" />
	</analytic>
	<monogr>
		<title level="j">Struct. Multidiscip. Optim</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page">354</biblScope>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b265">
	<analytic>
		<title level="a" type="main">A literature review on video analytics of crowded scenes</title>
		<author>
			<persName><forename type="first">Myo</forename><surname>Thida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leng</forename><surname>Yoke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pau</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">How-Lung</forename><surname>Climent-Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Eng</surname></persName>
		</author>
		<author>
			<persName><surname>Remagnino</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-41512-8_2</idno>
		<ptr target="https://doi.org/10.1007/978-3-642-41512-8_2" />
	</analytic>
	<monogr>
		<title level="j">Intell. Multimed. Surveill. Curr. Trends Res</title>
		<imprint>
			<biblScope unit="page" from="17" to="36" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b266">
	<analytic>
		<title level="a" type="main">Recovering 3d human mesh from monocular images: A survey</title>
		<author>
			<persName><forename type="first">Yating</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/tpami.2023.3298850</idno>
		<ptr target="https://doi.org/10.1109/tpami.2023.3298850" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b267">
	<analytic>
		<title level="a" type="main">The nature of gender</title>
		<author>
			<persName><forename type="first">Udry</forename><surname>Richard</surname></persName>
		</author>
		<idno type="DOI">10.2307/2061790</idno>
		<ptr target="https://doi.org/10.2307/2061790" />
	</analytic>
	<monogr>
		<title level="j">Demography</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="561" to="573" />
			<date type="published" when="1994">1994. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b268">
	<analytic>
		<title level="a" type="main">Advances in smart environment monitoring systems using IoT and sensors</title>
		<author>
			<persName><forename type="first">Silvia</forename><surname>Liberata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ullo</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ram</forename><surname>Sinha</surname></persName>
		</author>
		<idno type="DOI">10.3390/s20113113</idno>
		<ptr target="https://doi.org/10.3390/s20113113" />
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">3113</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b269">
	<analytic>
		<title level="a" type="main">Context for ubiquitous data management</title>
		<author>
			<persName><forename type="first">Ling</forename><surname>Arthur H Van Bunningen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter M G</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><surname>Apers</surname></persName>
		</author>
		<idno type="DOI">10.1109/udm.2005.7</idno>
		<ptr target="https://doi.org/10.1109/udm.2005.7" />
	</analytic>
	<monogr>
		<title level="m">International Workshop on Ubiquitous Data Management</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b270">
	<analytic>
		<title level="a" type="main">A survey on semi-supervised learning</title>
		<author>
			<persName><forename type="first">E</forename><surname>Jesper</surname></persName>
		</author>
		<author>
			<persName><surname>Van Engelen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Holger</surname></persName>
		</author>
		<author>
			<persName><surname>Hoos</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10994-019-05855-6</idno>
		<ptr target="https://doi.org/10.1007/s10994-019-05855-6" />
	</analytic>
	<monogr>
		<title level="j">Mach</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="373" to="440" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b271">
	<analytic>
		<title level="a" type="main">A review on the long short-term memory model</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Van Houdt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Mosquera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gonzalo</forename><surname>Nápoles</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10462-020-09838-1</idno>
		<ptr target="https://doi.org/10.1007/s10462-020-09838-1" />
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Rev</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="5929" to="5955" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b272">
	<monogr>
		<title level="m" type="main">Meta-learning: A survey</title>
		<author>
			<persName><forename type="first">Joaquin</forename><surname>Vanschoren</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1810.03548</idno>
		<idno>arXiv1810.03548</idno>
		<ptr target="https://doi.org/10.48550/arXiv.1810.03548" />
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b273">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<idno>arXiv1706.03762</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv Prepr</note>
</biblStruct>

<biblStruct xml:id="b274">
	<analytic>
		<title level="a" type="main">Are ambient intelligence and augmented reality two sides of the same coin? Implications for human-computer interaction</title>
		<author>
			<persName><forename type="first">Radu-Daniel</forename><surname>Vatavu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3491101.3519710</idno>
		<ptr target="https://doi.org/10.1145/3491101.3519710" />
	</analytic>
	<monogr>
		<title level="m">CHI Conference on Human Factors in Computing Systems Extended Abstracts</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b275">
	<analytic>
		<title level="a" type="main">A survey of personality computing</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Vinciarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gelareh</forename><surname>Mohammadi</surname></persName>
		</author>
		<idno type="DOI">10.1109/taffc.2014.2330816</idno>
		<ptr target="https://doi.org/10.1109/taffc.2014.2330816" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affect. Comput</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="291" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b276">
	<monogr>
		<author>
			<persName><surname>David D Walden</surname></persName>
		</author>
		<title level="m">Systems engineering handbook: A guide for system life cycle processes and activities</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note>No Title</note>
</biblStruct>

<biblStruct xml:id="b277">
	<analytic>
		<title level="a" type="main">Review of large vision models and visual prompt engineering</title>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sigang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haixing</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiushi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songyao</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.metrad.2023.100047</idno>
		<ptr target="https://doi.org/10.1016/j.metrad.2023.100047" />
	</analytic>
	<monogr>
		<title level="j">Meta-Radiology</title>
		<imprint>
			<biblScope unit="page">100047</biblScope>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b278">
	<analytic>
		<title level="a" type="main">Deep learning for sensor-based activity recognition: A survey</title>
		<author>
			<persName><forename type="first">Jindong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuji</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohui</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisha</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patrec.2018.02.010</idno>
		<ptr target="https://doi.org/10.1016/j.patrec.2018.02.010" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="3" to="11" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b279">
	<analytic>
		<title level="a" type="main">A survey on large language model based autonomous agents</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueyang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingsen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiakai</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11704-024-40231-1</idno>
		<ptr target="https://doi.org/10.1007/s11704-024-40231-1" />
	</analytic>
	<monogr>
		<title level="j">Front. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">186345</biblScope>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b280">
	<analytic>
		<title level="a" type="main">A systematic review on affective computing: Emotion models, databases, and recent advances</title>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Liotta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuyong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weifeng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.inffus.2022.03.009</idno>
		<ptr target="https://doi.org/10.1016/j.inffus.2022.03.009" />
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="19" to="52" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b281">
	<analytic>
		<title level="a" type="main">Deep neural networks are more accurate than humans at detecting sexual orientation from facial images</title>
		<author>
			<persName><forename type="first">Yilun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Kosinski</surname></persName>
		</author>
		<idno type="DOI">10.1037/pspa0000098</idno>
		<ptr target="https://doi.org/10.1037/pspa0000098" />
	</analytic>
	<monogr>
		<title level="j">J. Pers. Soc. Psychol</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page">246</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b282">
	<monogr>
		<title level="m" type="main">Exploring the reasoning abilities of multimodal large language models (mllms): A comprehensive survey on emerging trends in multimodal reasoning</title>
		<author>
			<persName><forename type="first">Yiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotian</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xudong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiteng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bohan</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianbo</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanzeng</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2401.06805</idno>
		<idno>arXiv2401.06805</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2401.06805" />
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv Prepr.</note>
</biblStruct>

<biblStruct xml:id="b283">
	<analytic>
		<title level="a" type="main">A comprehensive review of speech emotion recognition systems</title>
		<author>
			<persName><forename type="first">Taiba</forename><surname>Majid Wani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teddy</forename><surname>Surya Gunawan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Syed</forename><surname>Asif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Qadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mira</forename><surname>Kartiwi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliathamby</forename><surname>Ambikairajah</surname></persName>
		</author>
		<idno type="DOI">10.1109/access.2021.3068045</idno>
		<ptr target="https://doi.org/10.1109/access.2021.3068045" />
	</analytic>
	<monogr>
		<title level="j">IEEE access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="47795" to="47814" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b284">
	<analytic>
		<title level="a" type="main">A survey on sentiment analysis methods, applications, and challenges</title>
		<author>
			<persName><forename type="first">Mayur</forename><surname>Wankhade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Annavarapu</forename><surname>Chandra Sekhara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaitanya</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><surname>Kulkarni</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10462-022-10144-1</idno>
		<ptr target="https://doi.org/10.1007/s10462-022-10144-1" />
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Rev</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="5731" to="5780" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b285">
	<monogr>
		<title level="m" type="main">Emergent abilities of large language models</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<idno>arXiv2206.07682</idno>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv Prepr</note>
</biblStruct>

<biblStruct xml:id="b286">
	<analytic>
		<title level="a" type="main">Chain-of-thought prompting elicits reasoning in large language models</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2206.07682</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2206.07682" />
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="24824" to="24837" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b287">
	<analytic>
		<title level="a" type="main">A survey of transfer learning</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taghi</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingding</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1186/s40537-016-0043-6</idno>
		<ptr target="https://doi.org/10.1186/s40537-016-0043-6" />
	</analytic>
	<monogr>
		<title level="j">J. Big data</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="40" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b288">
	<monogr>
		<title level="m" type="main">Transformers in time series: A survey</title>
		<author>
			<persName><forename type="first">Qingsong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaoli</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqing</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2202.07125</idno>
		<idno>arXiv2202.07125</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2202.07125" />
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b289">
	<analytic>
		<title level="a" type="main">Data collection and quality challenges in deep learning: A data-centric ai perspective</title>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">Euijong</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuji</forename><surname>Roh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwanjun</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jae-Gil</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00778-022-00775-9</idno>
		<ptr target="https://doi.org/10.1007/s00778-022-00775-9" />
	</analytic>
	<monogr>
		<title level="j">VLDB J</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="791" to="813" />
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b290">
	<monogr>
		<title level="m" type="main">Process and reality</title>
		<author>
			<persName><forename type="first">Alfred</forename><surname>North</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Whitehead</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Simon and Schuster</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b291">
	<analytic>
		<title level="a" type="main">Integrating scientific knowledge with machine learning for engineering and environmental systems</title>
		<author>
			<persName><forename type="first">Jared</forename><surname>Willard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Steinbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vipin</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="DOI">10.1145/3514228</idno>
		<ptr target="https://doi.org/10.1145/3514228" />
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b292">
	<analytic>
		<title level="a" type="main">Affect, behaviour, cognition and desire in the Big Five: An analysis of item content and structure</title>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Wilt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Revelle</surname></persName>
		</author>
		<idno type="DOI">10.1002/per.2002</idno>
		<ptr target="https://doi.org/10.1002/per.2002" />
	</analytic>
	<monogr>
		<title level="j">Eur. J. Pers</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="478" to="497" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b293">
	<analytic>
		<title level="a" type="main">A comprehensive review of group activity recognition in videos</title>
		<author>
			<persName><forename type="first">Li-Fang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo-Xuan</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11633-020-1258-8</idno>
		<ptr target="https://doi.org/10.1007/s11633-020-1258-8" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Autom. Comput</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="334" to="350" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b294">
	<analytic>
		<title level="a" type="main">Emotion recognition from gait analyses: Current research and future directions</title>
		<author>
			<persName><forename type="first">Shihao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiping</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edith</forename><surname>Ngai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C M</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><surname>Leung</surname></persName>
		</author>
		<idno type="DOI">10.1109/tcss.2022.3223251</idno>
		<ptr target="https://doi.org/10.1109/tcss.2022.3223251" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput. Soc. Syst</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="363" to="377" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b295">
	<analytic>
		<title level="a" type="main">Emotion recognition for multiple context awareness</title>
		<author>
			<persName><forename type="first">Dingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shunli</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liuzhen</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihua</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-19836-6_9</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-19836-6_9" />
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="page" from="144" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b296">
	<monogr>
		<title level="m" type="main">Image data augmentation for deep learning: A survey</title>
		<author>
			<persName><forename type="first">Suorong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weikang</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengchen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furao</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2204.08610</idno>
		<idno>arXiv2204.08610</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2204.08610" />
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b297">
	<analytic>
		<title level="a" type="main">Tree of thoughts: Deliberate problem solving with large language models</title>
		<author>
			<persName><forename type="first">Shunyu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Izhak</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b298">
	<analytic>
		<title level="a" type="main">A systematic review on hand gesture recognition techniques, challenges and applications</title>
		<author>
			<persName><forename type="first">Mais</forename><surname>Yasen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaidah</forename><surname>Jusoh</surname></persName>
		</author>
		<idno type="DOI">10.7717/peerj-cs.218</idno>
		<ptr target="https://doi.org/10.7717/peerj-cs.218" />
	</analytic>
	<monogr>
		<title level="j">PeerJ Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">218</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b299">
	<monogr>
		<title level="m" type="main">A survey on deep learning based time series analysis with frequency transformation</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longbing</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoujin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhendong</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2302.02173</idno>
		<idno>arXiv2302.02173</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2302.02173" />
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv Prepr.</note>
</biblStruct>

<biblStruct xml:id="b300">
	<monogr>
		<title level="m" type="main">A survey on multimodal large language models</title>
		<author>
			<persName><forename type="first">Shukang</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaoyou</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sirui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2306.13549</idno>
		<idno>arXiv2306.13549</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2306.13549" />
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv Prepr.</note>
</biblStruct>

<biblStruct xml:id="b301">
	<analytic>
		<title level="a" type="main">All one needs to know about fog computing and related edge computing paradigms: A complete survey</title>
		<author>
			<persName><forename type="first">Ashkan</forename><surname>Yousefpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caleb</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tam</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><surname>Kadiyala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fatemeh</forename><surname>Jalali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amirreza</forename><surname>Niakanlahiji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">P</forename><surname>Jue</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.sysarc.2019.02.009</idno>
		<ptr target="https://doi.org/10.1016/j.sysarc.2019.02.009" />
	</analytic>
	<monogr>
		<title level="j">J. Syst. Archit</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="289" to="330" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b302">
	<monogr>
		<title level="m" type="main">The future of information appliances and consumer devices</title>
		<author>
			<persName><surname>Zelkha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998. 1998</date>
			<publisher>Palo Alto Ventur</publisher>
			<pubPlace>Palo Alto, Calif.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b303">
	<monogr>
		<title level="m" type="main">Data-centric artificial intelligence: A survey</title>
		<author>
			<persName><forename type="first">Daochen</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pervaiz</forename><surname>Zaid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwei-Herng</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhimeng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaochen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2303.10158</idno>
		<idno>arXiv2303.10158</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2303.10158" />
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv Prepr.</note>
</biblStruct>

<biblStruct xml:id="b304">
	<analytic>
		<title level="a" type="main">A comprehensive survey of vision-based human action recognition methods</title>
		<author>
			<persName><forename type="first">Hong-Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Xiang</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duan-Sheng</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.3390/s19051005</idno>
		<ptr target="https://doi.org/10.3390/s19051005" />
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">1005</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b305">
	<analytic>
		<title level="a" type="main">Empowering things with intelligence: a survey of the progress, challenges, and opportunities in artificial intelligence of things</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<idno type="DOI">10.1109/jiot.2020.3039359</idno>
		<ptr target="https://doi.org/10.1109/jiot.2020.3039359" />
	</analytic>
	<monogr>
		<title level="j">IEEE Internet Things J</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="7789" to="7817" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b306">
	<analytic>
		<title level="a" type="main">Middleware for the Internet of Things: A survey on requirements, enabling technologies, and solutions</title>
		<author>
			<persName><forename type="first">Jingbin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Dong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.sysarc.2021.102098</idno>
		<ptr target="https://doi.org/10.1016/j.sysarc.2021.102098" />
	</analytic>
	<monogr>
		<title level="j">J. Syst. Archit</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page">102098</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b307">
	<analytic>
		<title level="a" type="main">Self-supervised learning for time series analysis: Taxonomy, progress, and prospects</title>
		<author>
			<persName><forename type="first">Kexin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingsong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaoli</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongyao</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guansong</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongjin</forename><surname>Song</surname></persName>
		</author>
		<idno type="DOI">10.1109/tpami.2024.3387317</idno>
		<ptr target="https://doi.org/10.1109/tpami.2024.3387317" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b308">
	<analytic>
		<title level="a" type="main">A review on human activity recognition using vision-based method</title>
		<author>
			<persName><forename type="first">Shugang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1155/2017/3090343</idno>
		<ptr target="https://doi.org/10.1155/2017/3090343" />
	</analytic>
	<monogr>
		<title level="j">J. Healthc. Eng</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3090343</biblScope>
			<date type="published" when="2017">2017. 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b309">
	<analytic>
		<title level="a" type="main">A survey on multi-task learning</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/tkde.2021.3070203</idno>
		<ptr target="https://doi.org/10.1109/tkde.2021.3070203" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="5586" to="5609" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b310">
	<analytic>
		<title level="a" type="main">Event prediction in the big data era: A systematic survey</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.1145/3450287</idno>
		<ptr target="https://doi.org/10.1145/3450287" />
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b311">
	<monogr>
		<title level="m" type="main">A survey of large language models</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Wayne Xin Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yupeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingqian</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beichen</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zican</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Dong</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2303.18223</idno>
		<idno>arXiv2303.18223</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2303.18223" />
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv Prepr.</note>
</biblStruct>

<biblStruct xml:id="b312">
	<analytic>
		<title level="a" type="main">Deep personality trait recognition: a survey</title>
		<author>
			<persName><forename type="first">Xiaoming</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqing</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2022.839619</idno>
		<ptr target="https://doi.org/10.3389/fpsyg.2022.839619" />
	</analytic>
	<monogr>
		<title level="j">Front. Psychol</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">839619</biblScope>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b313">
	<monogr>
		<title level="m" type="main">A comprehensive survey on pretrained foundation models: A history from bert to chatgpt</title>
		<author>
			<persName><forename type="first">Ce</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangjing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiben</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifang</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2302.09419</idno>
		<idno>arXiv2302.09419</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2302.09419" />
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv Prepr.</note>
</biblStruct>

<biblStruct xml:id="b314">
	<analytic>
		<title level="a" type="main">A comprehensive survey on transfer learning</title>
		<author>
			<persName><forename type="first">Fuzhen</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keyu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongbo</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongchun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengshu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1109/jproc.2020.3004555</idno>
		<ptr target="https://doi.org/10.1109/jproc.2020.3004555" />
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="76" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b315">
	<analytic>
		<title level="a" type="main">Object detection in 20 years: A survey</title>
		<author>
			<persName><forename type="first">Zhengxia</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keyan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenwei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieping</forename><surname>Ye</surname></persName>
		</author>
		<idno type="DOI">10.1109/jproc.2023.3238524</idno>
		<ptr target="https://openai.com/index/hello-gpt-4o/" />
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="257" to="276" />
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
