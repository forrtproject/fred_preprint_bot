<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The diffusion of online misinformation in India</title>
				<funder ref="#_j5TkupG">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_7Kp9Meq">
					<orgName type="full">Crime and Violence Initiative at J-PAL</orgName>
				</funder>
				<funder ref="#_EhEJmA4">
					<orgName type="full">Center for Effective Global Action</orgName>
					<orgName type="abbreviated">CEGA</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>American Economic Association</publisher>
				<availability status="unknown"><p>Copyright American Economic Association</p>
				</availability>
				<date type="published" when="2024-07-20">July 20, 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Jimmy</forename><surname>Narang</surname></persName>
							<email>jimmy.narang@usc.edu</email>
						</author>
						<author>
							<persName><forename type="first">Aprajit</forename><surname>Maha- Jan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Arun</forename><surname>Chandrasekhar</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ben</forename><surname>Golub</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Emily</forename><surname>Breza</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Frederico</forename><surname>Finan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jeremy</forename><surname>Magruder</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Stefano</forename><surname>Dellavigna</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kaveh</forename><surname>Danesh</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ao</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Stephanie</forename><surname>Bonds</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Arlen</forename><surname>Guarin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sharad</forename><surname>Hotha</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Saandra</forename><surname>Nandakumar</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ananya</forename><surname>Kumar</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lokanath</forename><surname>Naidugari</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Anand</forename><surname>Yadav</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Meghna</forename><surname>Kane</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Praful</forename><surname>Shankar</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Srinjita</forename><surname>Roy</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Taha</forename><surname>Ibrahim</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Swarupjit</forename><surname>Palit</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dhruvikaa</forename><surname>Ahuja</surname></persName>
						</author>
						<title level="a" type="main">The diffusion of online misinformation in India</title>
					</analytic>
					<monogr>
						<imprint>
							<publisher>American Economic Association</publisher>
							<date type="published" when="2024-07-20">July 20, 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">880ACD350E35B0FB668070A4E2CD0EF7</idno>
					<idno type="DOI">10.1257/rct.6474-1.0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-10-22T06:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Do people believe a news story more if it is shared by a friend? Should they? We investigate this using lab-in-field experiments in India with 800 pairs of friends and a custom social-media platform. We find sharers can distinguish true from false stories but share both equally, making sharing uninformative about a story's truth. Receivers, however, interpret sharing as a sign of truth: they overestimate how well sharers' beliefs predict veracity; discount how factors besides belief influence sharing decisions; and update the most on stories they least believed initially. Altogether, stories gain (unmerited) credibility from being shared by a friend.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>"I knew the video was fake, but I still went ahead and shared it because it concerns our kids," said one villager, talking about the kidnapping scare. "I have never seen a ghost, but whenever I'm riding my bike at night and see smoke in the distance, I get worried. I never saw God, but I often go to a temple and fold my hands" -so it was with the rumours of child-snatchers -"I've got to take care of my family."</p><p>-The strange case of WhatsApp and the child-kidnappers 1 . <ref type="bibr" target="#b15">Lalwani (2021)</ref> People forward news stories on social media to friends and family for a variety of reasonsto inform, to entertain, to signal something about themselves or even to themselves. The perceived truthfulness of a story can influence their sharing decisions, but its importance can vary. In some cases, people share a story because they strongly believe to be true; but in other cases, they share a story even when unsure of its truth, because it feels sufficiently important or urgent (such as the child-trafficking rumor above), or because its truthfulness feels secondary (e.g., if it is an entertaining bit of news).</p><p>But do people receiving these forwarded stories appreciate such distinctions? Consider the recipient of the child-trafficking rumor, who must now form an opinion of its veracity. Part of her opinion is likely based on the story's contents and claims, and how well they fit with her priors about the world. But another part is likely based on what she thinks of her friend and why they shared it. If she is unaware of (or does not fully account for) her friend's doubts, or overestimates this friend's tendency to share truthful news stories in general, she may interpret her friend's decision to share the story as a sign of its truth rather than its "shareability" (a notion we make precise later), and come to trust it more than she should. This study investigates if there's a discrepancy between the perceived and real "signal value" of sharing -i.e., what people think a friend's decision to share a story signals about its truth, and what it does in practice. In doing so, it addresses some fundamental but surprisingly understudied questions in the literature, namely: do people believe a story more when it is forwarded to them by a friend? If so, by how much? and what is a benchmark for how much they "should" trust it for that reason? Answering these questions is important, because if receivers over-interpret sharing as a sign of veracity, a false story can start off being poorly believed but widely shared, and end up being widely believed because it was shared.</p><p>Whether that happens in practice is not guaranteed nor well-tested 2 . On one hand, if receivers trust their friends' ability to discern truths from falsehoods, and view their decision to forward 1 The quote refers to a WhatsApp rumor that went viral in India in 2018. The rumor claimed that child traffickers were on the prowl and warned residents to be vigilant of strangers. It led to at least 70 deaths by lynching across the country. See <ref type="bibr" target="#b11">Goel, Raj, and Ravichandran (2018)</ref>; Buzzfeed-News (2018); <ref type="bibr" target="#b15">Lalwani (2021)</ref> for details. 2 In response to lynchings related to the child-kidnapping rumor, WhatsApp added a label to mark frequently forwarded stories (see <ref type="bibr" target="#b28">WhatsApp-Blog 2018;</ref><ref type="bibr" target="#b26">TechCrunch 2018)</ref>, under the assumption that such labeling would reduce trust in these stories.</p><p>a story as a proxy for belief, they are likely to increase their trust in the forwarded story as well<ref type="foot" target="#foot_0">3</ref> . On the other hand, people may trust their friends' intentions but not their judgement; and infer nothing (or even falsehood) from the latter's sharing choices. Receivers' beliefs might remain unchanged for an entirely different reason: on account of being friends with the sharers, receivers may already be on the same page about most news stories (i.e., have strongly correlated priors) so the latter's sharing decisions may have little marginal impact.</p><p>Although a vast scientific literature has emerged in the last decade to study online misinformation (see <ref type="bibr" target="#b16">Lazer et al. 2018;</ref><ref type="bibr">Pennycook and Rand 2021;</ref><ref type="bibr" target="#b2">Aridor et al. 2024</ref> for cross-disciplinary overviews), the focus has often been on why false or misleading stories get shared. Much less is known about whether recipients' beliefs change upon receiving such stories from a friend, let alone whether recipients accurately account for heterogeneity in beliefs/sharing motivations when receiving stories from the same friend. (Even from the sharers' perspective, there is little exploration of how doubt/uncertainty about a story's accuracy figures into sharing decisions.)</p><p>Collecting such evidence is challenging for multiple reasons. Direct messaging is often encrypted end-to-end, which prevents researchers from observing what stories get forwarded or the comments that accompany them. This is a key reason why misinformation in low and middle income countries -where WhatsApp is the most popular form of social media -remains understudied <ref type="bibr" target="#b4">(Badrinathan and Chauchard 2023;</ref><ref type="bibr" target="#b17">Madrigal 2018)</ref>. Even if such access were possible, it would not reveal how strongly sharers believed stories they shared versus those they didn't; nor would it reveal how receivers' beliefs change upon receipt.</p><p>Moreover, since receivers would always know who shared a story with them, it would be difficult to identify how much of their trust stemmed from a story's contents versus their opinion about the sharer. Random assignment of sharing labels (e.g., "forwarded by friend xyz") -or even artificially incentivizing a friend to share a story -could undermine the study's validity, since receivers may not find such shares credible. And finally, since receivers may exhibit any number of belief-updating biases -updating differently on stories that confirm vs. contradict their priors -it would not be straightforward to map raw changes in their beliefs to an estimate of how informative they think their friends' decisions are (i.e., to the perceived signal value of sharing).</p><p>To overcome these challenges, we conduct a set of lab-in-field experiments in India using a custom social media environment 4 built for the study. About 800 pairs (𝑁 ≈ 1600) of real-life friends from roughly 30 colleges across the country sign up as participants. In each pair, one person is randomly assigned to be the "sharer" and her friend becomes the "receiver". The sharer scrolls through news stories of unspecified veracity on her device, and can share as many as she likes with her partner<ref type="foot" target="#foot_2">5</ref> . She also reports how likely she thinks each story is true, and answers some supplementary questions about her choices. This data allows us to measure the relationship between sharers' beliefs about a story's accuracy and their sharing decisions, and estimate how well either predict a story's truth. (It also allows us to observe any text/message the sharer adds to the story).</p><p>Next, the receiver scrolls through the same set of stories -without knowing the sharer's decisions -and reports how likely he thinks each story is true (i.e., his prior). He then observes a signal about each story's veracity and reports his updated belief (i.e., his posterior).</p><p>Since we control the platform, we can vary the type of signal he observes. For one-third of the stories selected at random, we reveal whether the sharer chose to share the story or not<ref type="foot" target="#foot_3">6</ref> . For another third, we reveal the sharer's belief about the story's veracity instead. And for the remaining third, we provide a computer-generated guess -called the "BOT's clue" -which simply declares if the story is TRUE or FALSE. The clue is drawn independently at random, and has a 4 in 5 chance of being right.</p><p>To identify the impact of sharing, we compare receivers' beliefs about a story before and after learning their friend's sharing decisions <ref type="foot" target="#foot_4">7</ref> . To identify what value receivers ascribe to their friends' sharing decisions -i.e., how likely they think a story is true because their friend shared it -we benchmark changes in receivers' beliefs from learning the sharer's decisions to those from learning the BOT's clues. (That way, we do not need to assume receivers are Bayesian). And by comparing how receivers update from directly learning the sharers' beliefs instead of decisions, we identify if receivers' (mis)inference stems from incorrectly assessing the sharer's belief in the forwarded story, versus mis-assessing how informative that belief was about a story's truth. (For additional robustness, we also ask receivers to guess the sharer's beliefs, with and without incentives for accuracy).</p><p>We extensively pilot various design elements of the study, such as the sequencing of questions, the number of stories shown at a time, and whether participants are reminded of their previous answers (see Section 3 for details). We also vary the methods used to decide which stories are seen by participants. One set of participants see stories selected for balance (i.e., true and false stories are balanced across a range of topics, partisan slants, and plausibility) while the other set see stories drawn at random from leading newspapers and fact-checking websites. Moreover, roughly one third of teams take part in the study from home, completing it at their own pace over the course of 1-2 days; while the remaining teams participate from a "lab" (a temporary venue on campus) in a 90 minute session observed by research assistants. Unless otherwise noted, these implementation details do not meaningfully alter the results outlined below.</p><p>SHARERS. We find that sharers' beliefs are a useful signal of a story's veracity, but their sharing decisions are not. The more strongly sharers believe a story to be true, the more likely it is in fact, true. That discernment, however, does not translate into sharing decisions: sharers are about as likely to forward true stories as false ones ( 𝑃 𝑟(𝑠ℎ𝑟|𝑇 ) 𝑃 𝑟(𝑠ℎ𝑟|𝐹 ) ≈ 1.04) to their friends. The pattern is robust and holds across experimental settings (at home or in a lab), story topics and types (e.g. favorable / unfavorable / irrelevant to one's political identity) and story-selection algorithms<ref type="foot" target="#foot_5">8</ref> . There are two reasons this gap emerges between the signal value of sharers' beliefs vs. decisions. First, though stronger belief is associated with higher sharing rates (See row 2, Table <ref type="table" target="#tab_0">1</ref>), it is neither necessary nor sufficient for sharing. Even poorly believed stories are shared about 25% of the time in the sample, and strongly believed stories are not shared about 40% of the time. Second, across all levels of belief, false stories are at least as likely to be shared as true stories. That is, even accounting for perceived credibility, false stories possess slightly greater shareability -a catch-all term we use to capture factors other than belief that influence sharing.</p><p>To get a qualitative sense of these factors we ask sharers to explain some of their sharing decisions to us. In most cases, sharers forward a story because they perceive it to be useful or interesting to the receiver (44%), or because they want to express support for -or outrage about -the underlying claims (20%). These reasons hold even when the veracity of these stories is in doubt. (For example, among stories shared to express concern or anger, sharers' beliefs are four times more likely at or below 0.5 than above). And yet, sharers rarely add their own comments: less than 20% of shares are accompanied by a message, and less than a fourth of those messages convey the sharer's belief.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RECEIVERS.</head><p>Receivers treat their friend's decision to share a story as a sign of its truth, and overestimate the value of that signal. On average, receivers' beliefs increase by 4 percentage points (𝑠.𝑒. 0.4 𝑝𝑝) when a story is forwarded by their friend. The change is not uniform, and is concentrated among stories receivers least believed originally<ref type="foot" target="#foot_6">9</ref> . Benchmarking against their updates on BOT-generated signals, we estimate that receivers perceive nearly 2:1 odds a story is true if shared by their friend ( ŝ𝑝𝑐𝑣𝑑 = 2.23, 𝐶𝐼 0.95 (1.95 -2.54)), when in fact, the odds are scarcely better than noise.</p><p>Moreover, receivers show almost no sign of accounting for that friend's sharing motivations in their updates. That is, at a given prior, receivers' beliefs in forwarded stories increase by the same amount irrespective of the sharer's belief in them. This is problematic, since sharers' beliefs contain a useful signal of veracity but their sharing decisions do not. We can think of this phenomenon as a form of omitted variable bias, where the omitted variable is the sharer's belief, or equivalently, the component of sharing unexplained by belief (shareability). We refer to it as "shareability neglect". On one hand this "neglect" is unsurprising, since the sharer's belief -unless spelled out in a message -is unknown to the receiver. But on the other hand, we might expect receivers to at least partly intuit a story's shareability from its contents, and make some distinction when updating their beliefs<ref type="foot" target="#foot_7">10</ref> .</p><p>To better understand mechanisms driving receivers' updating behavior, we ask them to guess the sharer's beliefs after revealing their sharing decisions and guess decisions after revealing the sharer's beliefs, with and without incentives for accuracy. We find that receivers underestimate how often poorly-believed stories are shared (thus over-attributing sharing to belief rather than "shareability"), while overestimating how well sharer's beliefs predict a story's veracity<ref type="foot" target="#foot_8">11</ref> .</p><p>Literature &amp; Contributions.</p><p>Our findings contribute to multiple literatures. On the sharers' side, by breaking down a story's virality into its perceived credibility (or "truthiness") and its shareability, we provide a common framework to understand how seemingly disparate types of misinformation -home remedies for covid, get-rich scams, fear-inducing rumors or moral panics, even politically favorable claims<ref type="foot" target="#foot_9">12</ref> -gain credibility from sharing. The central idea is that such stories lend themselves to being shared even at low levels of belief, which receivers fail to adequately consider when forming their own posteriors.</p><p>Our study's main contributions are in tying sharers' decisions to changes in receivers' beliefs.</p><p>As mentioned above, empirical evidence on how sharing affects recipients' beliefs is lacking -especially when sharers are one's close friends. Our study complements two recent papers, however: Serra-Garcia and Gneezy (2021) and <ref type="bibr" target="#b13">Guess et al. (2023)</ref>. In Serra-Garcia and Gneezy (2021), the authors use a set of lab experiments to show that recipients overestimate their ability to detect lies shared by (unknown) senders, causing false stories to gain credibility from sharing. In <ref type="bibr" target="#b13">Guess et al. (2023)</ref>, the authors partner with Facebook to hide re-shared stories from treated users' newsfeeds (i.e., both the story and the fact it was re-shared by a friend are hidden). They do not find significant impact on beliefs, but that does not necessarily contradict our findings: for one, the authors measure receivers' beliefs about political claims broadly and not about specific stories; and for another, they hypothesize that Facebook's algorithm may have substituted hidden stories on treated users' newsfeeds with others of similar content.</p><p>To the best of our knowledge, the study is the first to provide evidence for at least three mechanisms through which misinformation gains trust among recipients: (1) receivers, on average, interpret friends' sharing as a sign of a story's truth even though it isn't;</p><p>(2) receivers cannot distinguish between stories shared for their credibility versus shareability, and (3) receivers update in non-bayesian, conservative ways that cause small changes overall, but large changes among stories they least believed. (We identify and test a fourth mechanism in Appendix Section 8.2: on any social media platform, receivers never observe if their friend chose not to share a story, and therefore miss out on potential negative signals about their veracity).</p><p>The study also contributes to the experimental literature through its design. First, by comparing receivers' belief-updates on computer-generated signals to updates on human-generated decisions, we control for several factors that might otherwise bias our estimates of what receivers infer from their friends' decisions. Second, lab (or lab-in-field) studies on social media often elicit sharing choices as hypotheticals (e.g., "Would you share this story if you saw it in real life?" <ref type="bibr" target="#b23">Pennycook and Rand 2019;</ref><ref type="bibr" target="#b22">Pennycook et al. 2020)</ref>, or in settings where the sharer does not know the recipient or vice-versa (e.g. Serra-Garcia and Gneezy 2021). To the best of our knowledge, our study is unique in using sharing between real-life friends, thereby providing a better approximation of participants' real-life behavior/incentives. (Indeed, in about 40% of cases in our sample, sharers report taking their friends' interests into consideration when making their sharing decisions). An additional advantage of this design is we are able to measure what friends think about the same news stories before communicating with each other. Our finding that friends' priors are weakly correlated (~0.2) is of interest on its own.</p><p>Finally, we contribute to the literature on misinformation in low-and-middle income countries (LMICs). With a few notable exceptions <ref type="bibr" target="#b3">(Badrinathan 2021;</ref><ref type="bibr" target="#b1">Arechar et al. 2022)</ref>, most studies about the subject are conducted with subjects from high-income countries, and may not generalize to other parts of the world -or indeed, to platforms popular in them <ref type="bibr" target="#b4">(Badrinathan and Chauchard 2023;</ref><ref type="bibr" target="#b7">Blair et al. 2023)</ref>. For instance, WhatsApp -the platform of choice in India, Brazil and many LMICs -is a peer-to-peer messaging app that resembles "SMS on</p><p>Steroids" <ref type="bibr" target="#b10">(Garimella and Tyson 2018)</ref>. Unlike Facebook, Twitter or Instagram, it has no centralized newsfeed, no advertised or suggested posts, and provides no feedback to the poster on engagement metrics (e.g., number of likes or shares); thus eliminating most platform-specific tricks to keep people enganged. And yet, it has proved as capable of spreading harmful misinformation as any other platform <ref type="bibr" target="#b11">(Goel, Raj, and Ravichandran 2018;</ref><ref type="bibr">Times 2018;</ref><ref type="bibr" target="#b17">Madrigal 2018)</ref>. Our study highlights mechanisms of how that might happen.</p><p>The rest of this paper is organized as follows: Section 2 describes the study's theoretical and empirical framework. Section 3 describes the experimental design. Section 4 and Section 5 discuss results on the sharers' and receivers' side respectively. Section 6 concludes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Theoretical and Empirical Framework</head><p>The structure of this section is as follows: Section 2.1 develops a microeconomic model of the relationship between a sharer's belief about a story's truth and their sharing decisions. Section 2.2 describes how we estimate the signal value of sharer's decisions (or beliefs) from the data. Section 2.3 discusses how receivers update their beliefs about a story based on the sharer's actions. Details of the corresponding empirical estimation -i.e. of translating receivers' belief-updates into a perceived signal value of sharing -are discussed later, in Section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">A Model for Sharing Decisions</head><p>We start with a simple model of the sharer's decisions. Let (𝑆, 𝑅) denote a pair of sharer (he/him) and receiver (she/her) respectively. News story 𝑗 appears on 𝑆's screen at random, and he must decide whether to share it with 𝑅 or not. Let 𝑢(𝑠ℎ𝑟 𝑗 | 𝑣 𝑗 ), 𝑠ℎ𝑟 𝑗 ∈ {1, 0} denote his utility from sharing (respectively, not sharing) the story 𝑗 conditional on knowing its veracity 𝑣 𝑗 ∈ {𝑇 , 𝐹 }.</p><p>At the moment of sharing, 𝑆 does not know if 𝑗 is true but has some belief about the likelihood of its truth. His sharing decision is a (conscious or subconscious) gamble: he shares the story if and only if his expected utility from sharing -at least in that moment -exceeds that from not sharing. That is:</p><formula xml:id="formula_0">𝑠ℎ𝑟 𝑗 = 1 ⟺ ∑ 𝑣∈{𝑇 ,𝐹 } 𝑢(𝑠ℎ𝑟 𝑗 = 1 | 𝑣 𝑗 = 𝑣) ⋅ 𝑝𝑟(𝑣 𝑗 = 𝑣) ≥ ∑ 𝑣 𝑢(𝑠ℎ𝑟 𝑗 = 0 | 𝑣 𝑗 = 𝑣) ⋅ 𝑝𝑟(𝑣 𝑗 = 𝑣)</formula><p>Let Δ𝑢 𝑗 (𝑣) = 𝑢(𝑠ℎ𝑟 𝑗 = 1 | 𝑣) -𝑢(𝑠ℎ𝑟 𝑗 = 0 | 𝑣) denote the net utility from sharing rather than not sharing story 𝑗 given its veracity, and let 𝑏𝑒𝑙 𝑗 = 𝑝𝑟(𝑣 𝑗 = 𝑇 ) denote the sharer's belief about the likelihood of 𝑗's truth. Then, we can rewrite the above equation as:</p><formula xml:id="formula_1">𝑠ℎ𝑟 𝑗 = 1 ⟺ Δ𝑢 𝑗 (𝑇 ) ⋅ 𝑏𝑒𝑙 𝑗 + Δ𝑢(𝐹 ) 𝑖𝑗 ⋅ (1 -𝑏𝑒𝑙 𝑗 ) ≥ 0 ⟺ (Δ𝑢 𝑗 (𝑇 ) -Δ𝑢 𝑗 (𝐹 )) ⏟⏟⏟⏟⏟⏟⏟⏟⏟ 𝛽 1 𝑗 ⋅𝑏𝑒𝑙 𝑗 + Δ𝑢 𝑗 (𝐹 ) ⏟ 𝛽 0 𝑗 ≥ 0<label>(1)</label></formula><p>We make two assumptions about net utility: Δ𝑢 𝑗 (𝑇 ) ≥ Δ𝑢 𝑗 (𝐹 ) and Δ𝑢 𝑗 (𝐹 ) ≤ 0. That is, conditional on sharing, the sharer weakly prefers the story be true; and the sharer derives no special joy from misleading the receiver. We label these the "no disinformation" assumptions 13 .</p><p>if Δ𝑢 𝑗 (𝑇 ) ≠ Δ𝑢 𝑗 (𝐹 ), we can rewrite Equation 1 as:</p><formula xml:id="formula_2">𝑠ℎ𝑟 𝑗 = 1 ⟺ 𝑏𝑒𝑙 𝑗 ≥ 𝑏𝑒𝑙 * 𝑗 , where 𝑏𝑒𝑙 * 𝑗 = { -Δ𝑢 𝑗 (𝐹 )</formula><p>Δ𝑢 𝑗 (𝑇 )-Δ𝑢 𝑗 (𝐹 ) if Δ𝑢 𝑗 (𝑇 ) ≠ Δ𝑢 𝑗 (𝐹 )</p><formula xml:id="formula_3">0 if Δ𝑢 𝑗 (𝑇 ) = Δ𝑢 𝑗 (𝐹 ) = 0 (2)</formula><p>Equation 2 gives us the sharer's decision rule: Sharer 𝑆 shares the story 𝑗 if and only if his belief in its veracity exceeds some threshold 𝑏𝑒𝑙 * 𝑗 . The equation decomposes the sharer's decision into a component based on the story's perceived accuracy (𝑏𝑒𝑙 𝑗 ) and a thereshold encapsulating the story's shareability (𝑏𝑒𝑙 * 𝑗 ), net of belief. From the sharer's rule, it follows that even a poorly believed story may be widely shared if 𝑏𝑒𝑙 * is sufficiently low. To return to our opening example, a person may share a rumor about child kidnappers despite doubting its veracity because the net utility from warning neighborsshould the rumor turn out to be true, Δ𝑢(𝑇 ) -greately exceeds the net disutility from warning when the rumor is false (-Δ𝑢(𝐹 )).</p><p>Note that beliefs and thresholds need not be independent. For instance, 𝑏𝑒𝑙 and 𝑏𝑒𝑙 * might be negatively correlated for politically favorable claims: people might be more likely to believe such claims, but holding belief constant, they may also derive greater pleasure from sharing them with others -especially if their network consists of fellow partisans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Estimating the true signal value of sharing</head><p>In the data, for each sharer 𝑖 we observe if they chose to share the story (𝑠ℎ𝑟 𝑖𝑗 ) and how likely they thought the story was true (𝑏𝑒𝑙 𝑖𝑗 ). We estimate what those choices/beliefs signal about a story's truth as follows:</p><p>The Signal Value of Sharing a Story. Let 𝑠 𝑜𝑟 (𝑠ℎ𝑟 = 1) denote the odds that the sharer forwards (shares) a true story rather than a false one when presented with an equal mix 14 . That is,</p><formula xml:id="formula_4">𝑠 𝑜𝑟 (𝑠ℎ𝑟 = 1) = 𝑝𝑟(𝑠ℎ𝑟 = 1 | 𝑣 = 𝑇 ) 𝑝𝑟(𝑠ℎ𝑟 = 1 | 𝑣 = 𝐹 ) (3)</formula><p>13 The assumptions are less restrictive than they might appear: sharers can be indifferent to a story's truth (Δ𝑢 𝑗 (𝑇 ) = Δ𝑢 𝑗 (𝐹 )), or refuse to share a story even when certain of its truth (Δ𝑢 𝑗 (𝑇 ) &lt; 0), say, because they find it boring or do not wish to spam their friends. 14 A word on notation: we use the suffix "or" (𝑠 𝑜𝑟 ) when expressing a signal as an odds ratio. When expressing it as a probability, we assume a prior of 0.5, since all participants in our experiment see exactly a 50/50 split of true and false stories. That is, 𝑠 = 𝑠 𝑜𝑟 1+𝑠 𝑜𝑟 .</p><p>To estimate this value from the data we use its sample analogue, which is simply the ratio of the probabilities of sharing true vs. false stories across sharers 𝑖 and stories 𝑗 in the experiment:</p><formula xml:id="formula_5">ŝ𝑜𝑟 (𝑠ℎ𝑟 = 1) = ∑ 𝑖𝑗 𝑠ℎ𝑟 𝑖𝑗 = 1 | 𝑣 𝑗 = 𝑇 ∑ 𝑖𝑗 𝑠ℎ𝑟 𝑖𝑗 = 1 | 𝑣 𝑗 = 𝐹 ⋅ ∑ 𝑖𝑗 𝑣 𝑗 = 𝐹 ∑ 𝑖𝑗 𝑣 𝑗 = 𝑇 (4)</formula><p>If we need to estimate 𝑠 𝑜𝑟 (𝑠ℎ𝑟) for a particular category (say, politically favorable stories) we do so by subsetting the data to that category and computing the signal value within it.</p><p>The Signal Value of Sharers' Beliefs: Analogously, the signal value of the sharer reporting a belief 𝑥 is the ratio of how likely they would report that belief among true vs. false stories.</p><p>That is, 𝑠 𝑜𝑟 (𝑏𝑒𝑙 = 𝑥) = 𝑝𝑟(𝑏𝑒𝑙=𝑥|𝑣=𝑇 ) 𝑝𝑟(𝑏𝑒𝑙=𝑥|𝑣=𝐹 ) Since beliefs are reported on a continuous scale, we approximate the signal value of reporting 𝑥 by that of its neighborhood 𝑁 𝑥 = 𝑥 ± 0.05. (For binned scatter plots, 𝑁 𝑥 is just the bin containing 𝑥).</p><formula xml:id="formula_6">ŝ𝑜𝑟 (𝑏𝑒𝑙 = 𝑥) = ∑ 𝑖𝑗 𝟙(𝑏𝑒𝑙 𝑖𝑗 ∈ 𝑁 𝑥 ) | 𝑣 𝑗 = 𝑇 ∑ 𝑖𝑗 𝟙(𝑏𝑒𝑙 𝑖𝑗 ∈ 𝑁 𝑥 ) | 𝑣 𝑗 = 𝐹 ⋅ ∑ 𝑖𝑗 𝑣 𝑗 = 𝐹 ∑ 𝑖𝑗 𝑣 𝑗 = 𝑇<label>(5)</label></formula><p>Finally, although not generated by the sharer, receivers in our experiment also observe a third kind of signal: a computer-generated clue that declares if the story is true or false, and is correct with 80% probability each time. By design, the signal value of the BOT's clue is constant and common knowledge:</p><formula xml:id="formula_7">𝑠 𝑜𝑟 (𝑏𝑜𝑡 = 𝑏) = 𝑝𝑟(𝑏𝑜𝑡 = 𝑏 | 𝑣 = 𝑇 ) 𝑝𝑟(𝑏𝑜𝑡 = 𝑏 | 𝑣 = 𝐹 ) = { 4, if 𝑏 = 𝑇 1/4, if 𝑏 = 𝐹 (6)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">A Model for Receiver's Updates</head><p>For each receiver 𝑖 and story 𝑗, we know the receiver's prior (𝑝𝑟𝑖 𝑖𝑗 ), the unique action/signal they observe, 𝑎 𝑖𝑗 (one of: the sharer's belief<ref type="foot" target="#foot_10">15</ref> , the sharer's decision, or a BOT's clue), and the posterior 𝑝𝑜𝑠𝑡 𝑖𝑗 they reach. We do not observe the value receivers assign to the signal (𝑠 𝑝𝑐𝑣𝑑 (𝑎 𝑖𝑗 )), and we must infer it from the data.</p><p>If 𝑓 denotes the receiver's belief-updating function, we have:</p><formula xml:id="formula_8">𝑝𝑜𝑠𝑡 𝑖𝑗 = 𝑓 (𝑝𝑟𝑖𝑜𝑟 𝑖𝑗 , 𝑠 𝑝𝑐𝑣𝑑 (𝑎 𝑖𝑗 )) ⟹ 𝑠 𝑝𝑐𝑣𝑑 (𝑎 𝑖𝑗 ) = 𝑓 -1 (𝑝𝑜𝑠𝑡 = 𝑝𝑜𝑠𝑡 𝑖𝑗 )| 𝑝𝑟𝑖=𝑝𝑟𝑖 𝑖𝑗<label>(7)</label></formula><p>Estimating 𝑠 𝑝𝑐𝑣𝑑 would be straightforward if we assumed receivers were Bayesian, in which case 𝑓 and 𝑓 -1 would be well defined. However, participants in our experiment are not. In line with a long literature in Psychology and Economics (See Benjamin 2019 for a review), participants' behavior in our experiment is consistent with base-rate neglect, confirmation bias, and conservatism <ref type="bibr" target="#b18">(Möbius et al. 2022)</ref>. Thus, we leave 𝑓 unspecified for now but make some basic assumptions about its structure for tractability. For example, we assume 𝑓 is monotonically increasing in both prior and signal, so 𝑓 can be inverted if 𝑝𝑜𝑠𝑡, 𝑝𝑟𝑖𝑜𝑟 are known. Later, in Section 5.2 we will pick functional forms for 𝑓 and calibrate parameters by tracing participants' repsonses to the BOT's signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Study Design</head><p>The study was carried out between November 2021 and June 2022 in 25 colleges across the country, covering a mix of urban and rural settings, elite and non-elite institutions, and state versus private ownership see Appendix 9.1. It was advertised to students through emails, posters, visits to campus, and messages to groups on Facebook and WhatsApp<ref type="foot" target="#foot_11">16</ref> .</p><p>Students were invited to sign up with a friend they knew in real life and connected to on social media; individual sign ups were not allowed. During registration, teams provided information about themselves (demographics, social media usage, political leanings) and how well they knew each other. For instance, they guessed their partner's political leanings, and how often that partner tended to share true vs. false news stories (the information was used only used to pick an appropriate signal-strength for BOT's generated clues). The main experiment took place 0-3 days after registration ended.</p><p>The experiment was conducted either in person or remotely. The decision often came down to logistical constraints, such as a college's covid restrictions or academic calendar. For the remote version, participants were emailed a link to the study (on a date of their choosing), and had 48 hours to finish their tasks. For the in-person version, participants arrived at a designated venue on campus, and completed the study in a single, 90 minute session. In either case, participants used their own phones or devices to complete the study.</p><p>Each setting had its advantages and disadvantages. The remote setting facilitated more natural interactions, since friends could share stories from home at their own pace over 1-2 days. However, it raised concerns about participants looking up a story's veracity, misunderstanding instructions, or communicating in ways that might undermine the study's internal validity.</p><p>On the other hand, the in-person setting was less natural, but allowed research assistants to monitor participants' compliance with the study's protocols, and clarify instructions as needed.</p><p>In the end, however, the setting did not meaningfully affect behavior. Remote participants were no better at discerning story veracity, and sharing rates were similar across settings.</p><p>Table <ref type="table">2</ref> summarizes the participating sample. Nearly all (&gt;95%) participants were 18-25 years old, with 41% identifying as female. The average household income (among the two-third participants who reported it) was about 600,000 INR per annum<ref type="foot" target="#foot_12">17</ref> , placing them within the top 10% of Indian households <ref type="bibr" target="#b14">(Jha and Basole 2023;</ref><ref type="bibr" target="#b6">Bharti et al. 2024)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Platform &amp; Experiment Design</head><p>The experiments were conducted on a custom social-media style platform that participants could access using a URL or QR code. The goal was to mimic the basic functions of WhatsApp, but add features and restrictions needed for the study. For instance, all participants could scroll through a series of news previews (headlines with lede images and captions), but only sharers could forward stories to their partner (more details below). Researchers could control what stories were shown, when beliefs were elicited, whether participants were reminded of previous answers, and change other experimental parameters as needed. Figure <ref type="figure" target="#fig_0">1</ref> shows some screenshots. We now describe the experimental design for the in-person version of the study. Procedures for the remote version are similar, unless otherwise specified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sharer's Workflow</head><p>At the start of the experiment, one participant from each team was randomly assigned to be the "sharer", while the other became the "receiver". The sharer completed their part ("workflow") before the receiver began. The sharer's workflow included four rounds:</p><p>Round 1: Sharing Choices. The sharer scrolled through 20 to 24 news previews on his screen (see Figure <ref type="figure" target="#fig_0">1a</ref>), and could share any -and as many -of them with his partner as he liked. Stories varied across sharers, but all sharers saw an equal mix of true and false stories.</p><p>(Details on story selection follow in Section 3.4). The sharer was given no special incentives for sharing, and simply asked to match his real-life behavior as best as possible. He was also informed that forwarded stories may appear on his partner's screen with a caption like "[Name] shared this story with you", just as they would on any regular social media platform.</p><p>Round 2: Beliefs. Next, the sharer saw the stories again (in a randomized order), and reported how likely he thought each story was true, on a scale from 0 (surely false) to 100 (surely true)<ref type="foot" target="#foot_13">18</ref> . He was not reminded of his previous sharing decisions, so as to reduce the likelihood of reporting beliefs that rationalize those choices.</p><p>Round 3: Explanations. Next, the sharer saw a random subset of 𝐾 ≈ 4 stories, and was asked to explain -in one or two sentences -why he decided to share or not share them. (He was reminded of his sharing decisions but not his beliefs).</p><p>Round 4: Misc. The questions in this round varied, but were used to validate the study's procedures. For example, sharers were shown a subset of stories from previous rounds and asked if they had ever seen them before the experiment. This allowed us to check how well stories selected for the study reflected participants' real-life experience.</p><p>This completed the sharer's workflow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Receiver's Workflow</head><p>The receiver's workflow also consisted of four rounds, as follows:</p><p>Round 1: Receiver's Priors: The receiver was shown the same 𝑀 stories as her partner <ref type="foot" target="#foot_14">19</ref> , and asked how likely she thought each story was true. (This is identical to the "beliefs" round for the sharer).</p><p>Rounds 2-4 (Signal Rounds): The next three rounds had a common structure and could appear in any order. In each round, the receiver reviewed about one -third stories from the first round (drawn at random without replacement), but each story was now accompanied by a signal about its veracity. Having observed the signal, the receiver reported her updated belief (i.e., her posterior) about it. The rounds were named after the signal shown, as described below:</p><p>Reveal Sharer's Decision (RSD): The receiver learned whether their partner shared the story. That is, each story was accompanied either by the caption: "[partner name] shared this story with you" or "[partner name] saw the story but chose not to share it". Having learned the sharer's decision, the receiver reported her updated belief (posterior) <ref type="foot" target="#foot_15">20</ref> . Supplementary question: Then, she was asked to guess how likely the sharer thought the story was true<ref type="foot" target="#foot_16">21</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reveal Sharer's Belief (RSB).</head><p>The receiver learned their partner's belief about the story. That is, each story was accompanied by the caption "[partner name] thinks this story is [X]% likely to be true". The receiver then reported her updated belief (posterior). Supplementary question: Next, the receiver was asked to guess if her partner had chosen to share the story.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reveal Robot's Clue (BOT).</head><p>For the remaining third of stories, the receiver was given a computer-generated clue that stated "this story is TRUE" or "this story is FALSE". Each clue was drawn independently at random, with an 80% chance of being correct. Having seen this clue, the receiver reported her posterior.</p><p>Supplementary questions were incentivized for a subset of participants. Receivers were paid an additional Rs. 10 (about 50 cents adjusted for purchasing power) if they guessed the sharer's decision correctly, and Rs. 10 if they guessed the sharer's belief within 10 percentage points. This completed the receiver's workflow.</p><p>After completing the study, all participants were shown which stories were true and which were false. True stories were linked to their original articles, while false stories were accompanied by pointers to third-party fact-checks -or in cases where we had made up a story -to their original, unedited version. Participants were paid a flat amout of Rs. 250 each (Rs. 500 per team)<ref type="foot" target="#foot_17">22</ref> , but only after they had read through the fact checks, and only if both team members had completed their respective workflows. This was to reduce receiver-side attrition, especially in the remote setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Story Selection</head><p>All participants saw between 20-24 stories in the experiment (exactly half of which were true), but these stories were drawn from a pool of contemporary news and social media forwards that was updated every couple of weeks. Stories were selected using one of two methods, described below. About three-fourths of teams saw stories selected for "balance", while the remaining saw stories selected for "representativeness". Overall, the procedures resulted in 109 unique stories shown across the study's duration.</p><p>Selection for Balance: In this method (motivated by <ref type="bibr" target="#b19">Mosleh, Pennycook, and Rand 2022)</ref>, research assistants manually selected stories to reflect a range of topics, political leanings, and plausibility. In any set of twenty stories shown to a participant, about twelve (60%) were related to politics, religion or group-identity; four (20%) covered health or finance; and the remaining were a miscellaneous mix of sports, entertainment or science. (Half the stories within each group were true). Furthermore, within the political/identity-related set, true and false stories were distributed equally among narratives that favored the Bharatiya Janata Party (BJP) and those that disfavored it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Selection for Representativeness:</head><p>In this method (motivated by Serra-Garcia and Gneezy 2021), stories were drawn in a manner that reduced the researcher's input, and reflected a more "natural" distribution of what participants might see on their feed. As always, exactly half the stories shown to any participant were true and the other half false, but there was no attempt to balance them on other parameters. True stories were selected at random from the front webpage of four leading Indian newspapers (and the "most shared" column from one of them), while false stories were selected from the homepages of five leading fact-checking websites.</p><p>Table <ref type="table" target="#tab_2">3</ref> summarizes participants' mean beliefs and sharing-rates among true and false stories. Across story-selection method and experimental setting, participants' mean belief in true stories is higher than in false stories. Moreover, sharers' and receivers' mean beliefs are similar as well. (The story-wise correlation between sharers' &amp; receivers' mean priors is 0.9).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results: Discernment and Sharing Behavior</head><p>We start with results about sharers. Figure <ref type="figure" target="#fig_1">2a</ref> illustrates sharers' average beliefs in, and likelihood of sharing, true versus false stories. On average, sharers believe true stories more strongly than false stories (a difference of around 15pp); but share them at roughly equal rates (a difference below 1pp). This gap between participants' discernment in belief and sharing echoes recent findings in the literature. For instance, in a large online experiment (N = 34,000) spanning 16 countries including India, <ref type="bibr" target="#b1">Arechar et al. (2022)</ref> assign online participants to one of two treatments: one where they guess a story's veracity, and another where they rate their willingness to share the story in real life. In every country, the gap between accuracy ratings is larger than the gap in sharing intentions. Our design allows us to dig deeper however, since we work with realized (instead of hypothetical) sharing decisions, and can explore how those decisions relate to the strength of the sharer's own belief.</p><p>To that end, Section 4.1 examines the relationship between the sharer's belief (i.e., how likely they think a story is true) and whether the story is actually true. Section 4.2 explores the relationship between the sharer's beliefs and their sharing decisions. And Section 4.3 examines what sharing decisions signal about a story's truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Sharers' beliefs as signals of veracity</head><p>Figure <ref type="figure" target="#fig_1">2b</ref> plots the probability that a story is true conditional on the sharer's own belief in its accuracy. We see a monotonically increasing relationship: as the sharer's belief rises from 0 (certainly false) to 1 (certainly true), the probability the story is true rises from about 25% to 60%. Additionally, when the sharer has no idea about the story's truth -i.e., when his 𝑏𝑒𝑙 = 0.5 -the story has a 50% chance of being true. This pattern -that stronger beliefs signal a greater likelihood a story is true -holds across story selection methods, stories' favorability to the sharers' politics and experimental settings (see Table <ref type="table" target="#tab_4">6</ref>, column 3).</p><p>Since sharers and receivers are randomly assigned (within team), receivers' discernment is on average no different from sharers'. Surprisingly however, within a pair of friends, the correlation between the sharer and receiver's belief about the same news story is quite low: 0.207. That is, pairs of friends appear to have meaningfully different beliefs about the same news. This is of benefit to the receiver: not only is the sharer's belief accurate on average, it is likely different from her own.</p><p>To summarize:</p><p>RESULT S1 (Signal value of sharer's belief): The sharer's belief in a story is an informative and useful signal of its veracity.</p><p>(i) The more strongly the sharer believes a story is true, the more likely the story is actually true. (𝑠 ′ (𝑏𝑒𝑙) &gt; 0, 𝑠(𝑏𝑒𝑙 = 0.5) = 0.5) (ii) Withing a pair of friends, sharers' beliefs are weakly correlated with their partners', making them a useful (non-redundant) signal to the receiver.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Sharing vs. believing a story</head><p>Figure <ref type="figure" target="#fig_1">2c</ref> shows the likelihood of sharing a story story conditional on the sharer's belief in it. Across true and false stories, the relationship is monotonically increasing but convex, with sharing rates rising slowly at first (until 𝑏𝑒𝑙 = 0.5) and rapidly thereafter. It is worth noting that even stories with low credibility are shared quite often; and on the flip side, several stories with high credibility are not shared. 23 .</p><p>To better understand sharers' motivations, we ask them explain a random subset of their sharing decisions. Table <ref type="table" target="#tab_3">5</ref> categorizes their explanations 24 . In most cases, people share a story because they regard it as interesting or useful (44%) to the receiver, or because they want to express anger at -or support for -the underlying claim (20%). But why do they share stories they themselves suspect are more likely false than true (i.e., at 𝑏𝑒𝑙 ≤ 0.5)? In about 3% of cases this appears to be an unintentional error (they say so), and in another 6% it is because they find the story funny or amusing. But the main reasons remain the same. Stories that arouse concern or anger are an exception, and significantly more likely to be cited as a reason for sharing at beliefs below 0.5 (11%) than above (3%).</p><p>For example, when sharing a headline about a minister who declared there is "no place in the country if you don't speak Hindi", a participant stated "Why we should speak Hindi? It is <ref type="bibr">[sic]</ref> national language but we give respect to our mother tongue Kannada" (reported belief: 36%).</p><p>Others shared a post about an unemployment scheme ("young citizens can sign up to earn Rs. 3500 per month"), because they wanted "to help <ref type="bibr">[their]</ref> friends" (belief: 20%) or "So she [the recipient] could get benefited" (belief: 30%). Sharing a headline about a minor's assault by a muslim cleric (belief: 40%), a participant said "It appeared to be brutal <ref type="bibr">[…]</ref> I felt it should be shared so that my friends will get to know about this incident". And a participant forwarded a headline about ISKCON (a Hindu non-profit organization) donating food at the Ukranian border because he and his friend "share almost every news update related to that [conflict]" (belief: 50%) 25 .</p><p>And yet, in almost none of these cases did sharers convey their doubts to the receiver in an accompanying message. Accompanying messages were rare to begin with: only 19% of shares contained a comment/message from the sharer, and under 4% of all shares contained a message that conveyed the sharer's own belief in the story's veracity. To summarize:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RESULT S2 (Sharing vs. believing):</head><p>(i) The stronger the sharer's belief in a news story, the more likely they are to share it. However, belief is neither necessary nor sufficient for sharing. (ii) Sharers rarely hint at their own belief in accompanying messages to the receiver.</p><p>23 Since sharers report their beliefs after sharing decisions, it is possible they overstate their belief in shared stories to rationalize their choices. We attempt to reduce this bias by eliciting all 20-24 sharing decisions before asking sharers for their beliefs; and we do not remind them of their choices at that time. Still, in so far as a bias exists, it understates the extent to which people share stories despite low beliefs 24 The categories were constructed manually by research assistants reading through all explanations.</p><p>25 All the stories above -except for the unemployment scheme -were true. See Supplementary Figure <ref type="figure" target="#fig_0">11</ref> for screenshots</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Sharing as a signal of veracity</head><p>True stories are more likely to be believed (Result S1), and believed stories are more likely to be shared (Result S2). And yet, true stories are only slightly more likely to be shared than false ones (41.7 % vs. 39.8% respectively). Thus, sharing is a weak sign of veracity: When presented with an equal mix of true and false stories, the probability that a shared story is true is about 51% (i.e., ( ŝ (𝑠ℎ𝑟 = 1) = 0.51, 𝑠.𝑒.0.007). There are two reasons why this gap emerges between what sharers' beliefs vs. decisions signal about a story's veracity. One we've seen already: plenty of stories get shared despite low beliefs. But a second reason, as shown in Figure <ref type="figure" target="#fig_1">2c</ref>, is that false stories are more likely to be shared at any level of the sharer's belief. That is, they possess greater shareability. The gap is more pronounced among stories selected for representativeness rather than for balance. See appendix 9.</p><p>In fact, suppose the receiver knew the sharer's belief. Would she learn anything additional about the story's veracity from knowing the story was shared? In other words, does the decision to share have any signal value net of the sharer's belief? To answer this, we test the following specification:</p><formula xml:id="formula_9">𝑣𝑒𝑟 𝑗 = 𝛼 + 𝛽 1 ⋅ 𝑠ℎ𝑟 𝑖𝑗 + 𝛽 2 ⋅ 𝑏𝑒𝑙 𝑖𝑗 + 𝛽 3 ⋅ (𝑏𝑒𝑙 𝑖𝑗 ⋅ 𝑠ℎ𝑟 𝑖𝑗 ) + 𝑋 𝑖𝑗 + 𝜖 𝑖𝑗<label>(8)</label></formula><p>Where 𝑣𝑒𝑟 𝑗 = 1 if the story is true, 𝑏𝑒𝑙 𝑖𝑗 ∈ [0, 1] is sharer 𝑖's belief in story 𝑗's veracity, and 𝑠ℎ𝑟 𝑖𝑗 = 1 if sharer 𝑖 shared story 𝑗. 𝑋 𝑖𝑗 are controls for the study's setting, story selection-method or story's political concordance. Table <ref type="table" target="#tab_4">6</ref> presents the results. In col (3), we cannot reject that the sharer's choice has no additional predictive power (𝛽 1 = -0.01, 𝑠.𝑒.0.02; 𝛽 3 = -0.04, 𝑠.𝑒.0.02) about story veracity. Including participant-fixed effects (col 4) changes the interaction coefficient 𝛽 3 to -0.07(𝑠.𝑒.0.03). That is, if anything, shared stories are slightly less likely to be true once we control for the sharer's belief.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RESULT S3 (Signal value of sharing):</head><p>The decision to share a story is uninformative (or very weakly informative) of that story's truth.</p><p>(i) Given an equal mix of true and false stories, the probability a shared story is true is about 51% (𝑠(𝑠ℎ𝑟 = 1) = 0.51, 𝑠.𝑒.0.01) (ii) Controlling for sharer's beliefs, false stories are more likely to be shared than true stories -i.e., they have higher shareability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results: Receivers</head><p>We now turn to describing results on the receivers' side. In Section 5.1 we test if receivers' belief in a story increases when it is shared by their friend. In Section 5.2 we use changes in receivers' beliefs to estimate how much they trust their friends' sharing decisions. In Section 5.3 we test if receivers distinguish between forwarded stories that their friend strongly believed vs. those they didn't. Section 5.4 explores the mechanisms behind receivers' misinferences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Updating on Sharer's Decision(s)</head><p>Recall that receivers report their priors for about 20-24 news stories, and then see their friend's sharing decisions for a subset (about one-third) of them. That is, they see if their friend chose to share the story (𝑠ℎ𝑟 = 1) or not (𝑠ℎ𝑟 = 0). A minority are given no additional information and simply asked for their belief a second time.</p><p>To identify the impact of sharing, we compare the receiver's belief about the same story before and after learning their friend's decision. Thus, identification is within-person and within-story: it comes from randomly withholding information about the friend's choice, and assuming the receiver's beliefs would have, on average, remained unchanged in the absence of that information -at least during the experiment. (The mean change in the "no info" case is indeed zero).</p><p>Figure <ref type="figure" target="#fig_4">3a</ref> shows the mean change in receivers' beliefs in each case (changes in response to the BOT's signals are also shown). We see receivers' beliefs increase by an average of 4 percentage points (s.e. 0.4 pp) upon learning their friend shared a story. The corresponding decrease in belief from learning the opposite is about 2 pp. These changes are not uniformly distributed: receivers' priors matters tremendously. Figure <ref type="figure" target="#fig_4">3b</ref> plots receivers' beliefs before and after learning their friend shared a story (updates from a different round -where they learn about the BOT's signals -are also shown). Focusing our discussion on the case where 𝑠ℎ𝑟 = 1, we see the change is largest when receivers' beliefs are at their lowest.<ref type="foot" target="#foot_18">26</ref> .</p><p>Once we control for receivers' priors, other parameters -such as the story's selection procedure or its political valence -appear to have little impact on their belief-updates. Table <ref type="table" target="#tab_6">8</ref> (cols 1-2) regresses receivers' posteriors on priors conditional on learning the story was shared. Receivers' updates are larger in the remote setting (reasons are unclear), but the coefficient on priors remains unchanged, and other coefficients are not statistically different from zero. Thus,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RESULT R1 (Sharing impacts receivers' beliefs):</head><p>Learning that a story was shared by their friend affects receivers' beliefs.</p><p>(i) On average, receivers' beliefs increase by 4 percentage points upon learning a story was shared by their friend. (ii) The impact is not uniform: updates are large among stories receivers least believed, and small among those they already suspected were true.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">The perceived signal-value of sharing</head><p>Next, we turn to estimating the value receivers ascribe to their friends' decisions. Specifically, we estimate 𝑠 𝑝𝑐𝑣𝑑 𝑜𝑟 (𝑠ℎ𝑟 = 1) -the odds, as perceived by the receiver, that a story is true if shared by their friend. Before discussing how we perform that estimation, it is worth discussing why we do so at all.</p><p>The main reason is generalizability: participants' beliefs (and changes therein) may be specific to stories shown in the study, but their opinion of their friend's discernment is not. That opinion is likely shaped outside the experiment, and unlikely to change within it. Therefore, estimating 𝑠 𝑝𝑐𝑣𝑑 𝑜𝑟 provides a portable measure of the impact of sharing on recipients' beliefs. Additionally, it allows us to benchmark 𝑠 𝑝𝑐𝑣𝑑 𝑜𝑟 against 𝑠 𝑜𝑟 -i.e., against how much receivers should update from their friends' sharing decisions, if they assessed them correctly.</p><p>We now return to describing how 𝑠 𝑝𝑐𝑣𝑑 𝑜𝑟 is estimated. The principle challenge is that receivers do not update their beliefs as Bayesians. Figure <ref type="figure" target="#fig_4">3b</ref> plots a bin-scatter of receivers' posteriors (y-axis) against their priors (x-axis) after learning the BOT thinks the story is TRUE (correct with probability 0.8). The Bayesian benchmark -what posteriors would look like if receivers updated as perfect Bayesians -is also plotted for comparison. Figure <ref type="figure" target="#fig_4">3c</ref> reproduces Figure <ref type="figure" target="#fig_4">3b</ref>, but all quantities (priors, signal, posteriors) are converted to log odds. In either case, it is clear that receivers' updates on the BOT's signals are nowhere close to the Bayesian benchmark. 27   Since we do not know the "true" form of the receivers' updating function, we choose and calibrate a model on each scale 28 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Model 1: Quasilinear updating</head><p>A straight line fits receivers' updating pattern in Figure <ref type="figure" target="#fig_4">3b</ref> reasonably well (𝑅 2 𝑎𝑑𝑗 = 0.57), and adding a quadratic term (𝑝𝑟𝑖𝑜𝑟 2 ) or or an interaction term for whether the signal confirms or contradicts priors (𝑝𝑟𝑖𝑜𝑟 &gt; 0.5) leaves the adjusted R-squared unchanged (see Table <ref type="table" target="#tab_5">7</ref>). Thus, we assume that the receiver's posterior is linear in their prior, but not necessarily in the signal. That is, the receivers' updating function takes the form:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑝𝑜𝑠𝑡𝑒𝑟𝑖𝑜𝑟 = 𝑓(𝑝𝑟𝑖𝑜𝑟, 𝑠) ∶= 𝑚(𝑠) ⋅ 𝑝𝑟𝑖𝑜𝑟 + 𝑐(𝑠)</head><p>Where 𝑚(𝑠) and 𝑐(𝑠) map a signal 𝑠 to the line's slope and intercept respectively. We do not know 𝑚() and 𝑐(), but we impose some "reasonable" restrictions on them. For instance, we require that 𝑓 is increasing in signal strength (𝑚 ′ (𝑠) &gt; 0), and that uninformative signals do not affect the receiver's beliefs (𝑚(0.5) = 1, 𝑐(0.5) = 0). (The full set of restrictions and 27 We remain agnostic to why receivers update this way: for example, the slightly negative updating near 𝑝𝑟𝑖𝑜𝑟 = 1 are consistent with both base-rate-neglect and with right-censoring of noise. That receivers' updates are frequently below the Bayesian benchmark is consistent with conservative updating (see <ref type="bibr" target="#b18">Möbius et al. 2022</ref>). 28 Thus, the x axis denotes 𝑙𝑛 ( 𝑝𝑟𝑖𝑜𝑟 1-𝑝𝑟𝑖𝑜𝑟 ). Each scale has its relative merits and demerits: receivers report their beliefs on a linear scale (and probably think in it too), but the logistic scale makes it easier to observe deviations from the Bayesian benchmark and measure signal strengths. On the other hand, the logit scale exaggerates noise at extreme priors (0 or 1) and shrinks it in the middle (near prior 0.5), in ways participants are unlikely to empirically.</p><p>procedures are described in appendix Section 8.1). After picking the simplest such functions 𝑐() and 𝑚(), we use moments from receivers' updates on signal BOT = T, and end up with the following equation to invert a signal's strength:</p><formula xml:id="formula_10">𝑠 𝑝𝑐𝑣𝑑 𝑞𝑙 = 0.3 * β0 𝑠 β0 𝑏𝑜𝑡=𝑇 + 0.5 given 𝑠 ≥ 0.5 (9)</formula><p>Where β0 𝑠 is the constant (intercept) term from an OLS regressing the receiver's posteriors on priors conditional on observing the signal 𝑠. We use Equation <ref type="formula">9</ref>as our baseline specification to translate receivers' updates into a signal strength.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Model 2: Logistic updating (Grether's Model)</head><p>In our second model we assume receivers follow Grether's specification <ref type="bibr" target="#b12">(Grether 1980)</ref>. That is, 𝑙𝑜𝑔𝑖𝑡(𝑝𝑜𝑠𝑡𝑒𝑟𝑖𝑜𝑟) = 𝛽 𝑝 ⋅ 𝑙𝑜𝑔𝑖𝑡(𝑝𝑟𝑖𝑜𝑟) + 𝛽 𝑠 ⋅ 𝑙𝑜𝑔𝑖𝑡(𝑠) ⏟⏟⏟⏟⏟ 𝛾 𝑠 <ref type="bibr">(10)</ref> Note that the model predicts 𝛽 𝑝 -the slope on receiver's priors -is independent of the signal strength. Fortunately, this is consistent with our data: estimating Equation <ref type="formula">10</ref>separately for 𝑏𝑜𝑡 = 𝑇 and 𝑠ℎ𝑟 = 1, we find 𝛽 𝑝 is 0.76 (0.02) and 0.77 (0.02) respectively. We can also see this in Figure <ref type="figure" target="#fig_4">3c</ref>, where the lines of best fit for 𝑏𝑜𝑡 = 𝑇 and 𝑠ℎ𝑟 = 1 are parallel. Equating 𝛽 𝑠 = 𝛾 𝑠 /𝑙𝑜𝑔𝑖𝑡(𝑠) across those two signals, we obtain:</p><formula xml:id="formula_11">𝑠 𝑝𝑐𝑣𝑑 𝑙𝑔𝑡 = 𝑖𝑛𝑣.𝑙𝑜𝑔𝑖𝑡 ( γ𝑠 ⋅ 𝑙𝑜𝑔𝑖𝑡 (𝑠(𝑏𝑜𝑡 = 𝑇 )) γ𝑏𝑜𝑡=𝑇 ) (<label>11</label></formula><formula xml:id="formula_12">)</formula><p>Where γ𝑠 is the constant (intercept) term from regressing logit posteriors on logit priors, conditional on observing the signal 𝑠.</p><p>* * * Table <ref type="table" target="#tab_7">9</ref> (panel A) shows our estimates using both methods. We find 𝑠 𝑝𝑐𝑣𝑑 𝑞𝑙 (𝑠ℎ𝑟 = 1) = 0.689 using the quasilinear method and 𝑠 𝑝𝑐𝑣𝑑 𝑙𝑔𝑡 (𝑠ℎ𝑟 = 1) = 0.693 using Grether's model (bootstrapped standard errors are under 0.001 in either case). The estimates are quite similar: receivers appear to update as if there is nearly a 69% chance that a story is true if shared by their friend false ones.<ref type="foot" target="#foot_19">29</ref> . Thus, RESULT R2 (perceived vs. actual signal value of sharing): Receivers update as if there's a 69% chance a story is true if shared by their friend, when in fact, the chance is close to 51%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Shareability Neglect</head><p>Receivers over-infer veracity from their friends' sharing decisions (result R2), but do they update more on stories their friend strongly believed, and less on stories their friend did not believe as strongly but shared anyway? The distinction matters because -as we saw in Results S1 &amp; S2 -sharers' beliefs carry a useful signal about the story's veracity, but sharing decisions do not.</p><p>To be clear, we do not expect receivers to perfectly intuit sharers' unstated beliefs and motivations. What we might expect, however, is for them to make some distinction in the right direction. For example, the receiver might appreciate that their friend could share a rumor about child-traffickers despite strong misgivings about its veracity, and update less in response. We test for evidence of such discounting in multiple ways.</p><p>Method 1: First, we test if receivers' updates are sensitive to sharers' latent beliefs. To do so, we regress receivers' posteriors on their priors conditional on knowing the story was shared (from round RSD), but include a term for the sharer's belief (𝑏𝑒𝑙 𝑖𝑗 )<ref type="foot" target="#foot_20">30</ref> :</p><formula xml:id="formula_13">𝑝𝑜𝑠𝑡 𝑖𝑗 = 𝛽 0 + 𝛽 𝑅 ⋅ 𝑝𝑟𝑖𝑜𝑟 𝑖𝑗 + 𝛽 𝑆 ⋅ 𝑏𝑒𝑙 𝑖𝑗 + 𝑋 𝑖𝑗 + 𝜖 𝑖𝑗 | 𝑠ℎ𝑟 𝑖𝑗 = 1</formula><p>(𝑝𝑟𝑖𝑜𝑟 𝑖𝑗 is receiver 𝑖's prior about story 𝑗, and 𝑋 𝑖𝑗 are controls for experimental setting, story selection method, and story favorability). Table <ref type="table" target="#tab_6">8</ref> (col 4) shows the results. Receivers' updates are slightly larger for stories the sharer strongly believed (𝛽 𝑆 = 0.024 (0.017)) but we cannot reject they are different from zero. We can also test if receivers make a coarser distinction: namely, between stories where the sharer's belief was above vs. below 0.5<ref type="foot" target="#foot_21">31</ref> . To do so we recreate the specification above but replace 𝑏𝑒𝑙 𝑖𝑗 with a dummy for 𝑏𝑒𝑙 𝑖𝑗 &gt; 0.5. (observations with belief = 0.5 are dropped). Table <ref type="table" target="#tab_6">8</ref> (col 3) shows the results. Once again, we find no heterogeneity in receivers' updates (𝛽 𝑆 = 0.001 (0.012)).</p><p>Method 2: In this method, instead of testing for heterogeneity in receivers' updates by sharers' beliefs, we test for heterogeneity across stories of high vs. low shareability. The idea is that even if receivers cannot tell their partner's belief, they might still form a rough guess based on a story's contents. We generate the classification as follows:</p><p>• Step 1: we restrict the data to stories seen by at least 100 participants (76 stories out of 109), estimate Equation 1 for each story, and compute its sharing threshold as b𝑒𝑙 * 𝑗 = -β0 𝑗 / ( β0 𝑗 + β1 𝑗 ). •</p><p>Step 2: we label stories with thresholds below the median as "high" shareability and those above as "low" shareability. Figure <ref type="figure">4a</ref> shows the results of this procedure. (Stories with high shareability are clearly more likely to be shared at any level of belief).</p><p>If receivers distinguished between these two types of stories, we should see smaller updates for stories with high-shareability. However, we see no such differentiation in Figure <ref type="figure">4c</ref>, which plots receivers' updates across the two groups of stories.  In the first series (solid dots), receivers learn that the story was shared, but do not see how strongly the sharer believed the story. In the second series (hollow squares), receivers see the sharer's belief, but are not told that the sharer chose to share the story. In both cases, the series plot the mean change in the receivers' beliefs (Y axis) against the sharer's belief (X axis) binned at width 0.2. All standard errors are clustered at the participant level.</p><p>We see that the mean change for 𝑠ℎ𝑟 = 1 is consistently positive, illustrating that receivers infer shared stories are more likely to be true. The relatively flat slope of the series illustrates shareability neglect: irrespective of the sharer's belief (x-axis), the mean change in receivers' beliefs remains roughly the same. In contrast, when the sharer's belief is visible, receivers' update their beliefs in the directions one would expect: receivers' beliefs increase when sharers' 𝑏𝑒𝑙 &gt; 0.5, decrease when 𝑏𝑒𝑙 &lt; 0.5, and remain unchanged when 𝑏𝑒𝑙 = 0.5. Thus, in summary:</p><p>RESULT R3 (Shareability neglect): At a given prior, receivers update identically on forwarded stories that sharers strongly believed and those they did not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Mechanisms</head><p>In this subsection we explore why receivers overinfer veracity from sharing and exhibit shareability neglect. For example, we would like to know if the gap between the perceived and real signal value of sharing (0.69 vs. 0.51) arises because:</p><p>• Receivers overestimate how strongly their friends believe forwarded stories (e.g. the sharer's belief was 75%, but the receiver assumes it was 90%), or, • Receivers overestimate how well their friend's beliefs predict a story's truth. (e.g., the receiver correctly guesses the sharer's belief was 100%, but assumes this indicates a 90% chance the story is true. However, the sharer is right only 60% of the time when he is certain about a story's truth).</p><p>We would also like to understand the drivers of shareability neglect. Does it stem from inattention (receivers can guess sharer's beliefs accurately if they thought about them) or does it reflect a genuine difficulty in second-guessing friends' beliefs? To answer these questions we look into two datasets: (1) receivers' responses to supplementary questions (where they are asked to guess sharers' beliefs/decisions) (2) receivers' updates from learning sharers' beliefs directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Second Order Beliefs</head><p>Recall that receivers guess the sharer's belief after learning the sharer's decision (𝑠ℎ𝑟 = 0 or 𝑠ℎ𝑟 = 1) in the round RSD. Figure <ref type="figure" target="#fig_6">6</ref> plots receivers' guesses versus the sharer's actual belief. We focus on unincentivized guesses first. Receivers can somewhat guess sharer's beliefs: stronger beliefs correspond to higher guesses, but the relationship is noisy and weak (slope 0.17, s.e. 0.03). The gap between guesses for 𝑠ℎ𝑟 = 1 and 𝑠ℎ𝑟 = 0 however, is large and constant. Receivers think sharers' beliefs are higher by 9.7 pp (s.e. 3.2) among shared stories, and the extent to which they do so is almost independent of the sharer's actual belief. This is consistent with both overinference and shareability neglect.</p><p>Providing modest incentives for accuracy appears to improve performance: the gap between 𝑠ℎ𝑟 = 0 and 𝑠ℎ𝑟 = 1 gets smaller, and the slope gets closer to 1, suggesting that shareability neglect stems at partly -but not entirely -from receivers' not thinking carefully about why their friend may have shared a story.</p><p>A similar picture emerges from examining receivers' guesses about sharers' decisions (See supplementary Figure <ref type="figure" target="#fig_10">10</ref>). Receivers assume a stronger correlation between sharers' beliefs and decisions than there is. When the sharer's belief is below 0.5, receivers make more type II than type I errors, mispredicting 56% of shared stories as not-shared, vs. 32% of not-shared stories as shared. And when the sharer's belief is above 0.5, receivers make more type I errors than type II, mispredicting 52% of not-shared stories as shared, vs. 35% of shared stories as not-shared. Providing incentives improves performance but the broader pattern remains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Perceived signal value of sharers' beliefs</head><p>Next, we estimate what value receivers assign to sharers' beliefs when they do observe them; and contrast it with the true signal value of sharers' beliefs. Instead of estimating the entire continuous distribution 𝑠 𝑝𝑐𝑣𝑑 (𝑏𝑒𝑙 = 𝑥), 𝑥 ∈ [0, 1], we bin sharers' beliefs into four quarters, and compare the true vs. perceived value in each bin. The "true" signal value of sharers' beliefs is calculate using Equation <ref type="formula" target="#formula_6">5</ref>, and that of the receivers' perception is calculated using Equation <ref type="formula">9</ref>and Equation <ref type="formula">10</ref>.</p><p>Table <ref type="table" target="#tab_7">9</ref> (panel B) presents the comparison. We see receivers' perceptions (using either model) are consistently higher than the true value of these signals. For example, when sharers' belief is ≥ 0.75, receivers update as if that range of confidence signals an 80-84% chance the story is true, when in fact, the chance is about 59%. In so far as receivers assume sharers' beliefs to be above 0.75 (or 0.5), this leads them to overestimate the signal value of sharing as well. Note however that receivers also tend to underestimate the sharer's belief when it is especially high (see Figure <ref type="figure" target="#fig_6">6</ref>), undercutting the influence of this channel.</p><p>Overall, the picture that emerges is nuanced, with no single mechanism/factor being the dominant contributor to receivers' misinference. Receivers overinfer veracity from sharing because they overestimate the sharer's belief when it is low; and overestimate the informativeness of that belief. This biases 𝑠 𝑝𝑐𝑣𝑑 | 𝑏𝑒𝑙 &lt; 0.5 upwards. However, since receivers' guesses are noisy overall, they underestimate sharers beliefs when they are high, which reduces 𝑠 𝑝𝑐𝑣𝑑 | 𝑏𝑒𝑙 &gt; 0.5, while still keeping it positive. Together, this manifests itself as a 'neglect' of sharers' beliefs, and flattens the signal value of sharing. At least part of this neglect can be undone if receivers are incentivized to think carefully the sharer's beliefs/motivations when sharing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>This study demonstrates how misinformation can gain credibility from sharing, even in communities where individuals can generally distinguish truths from falsehoods. Several factors contribute to this phenomenon. First, individuals forward false stories as often as true ones, even when they (correctly) doubt their accuracy. Second, sharers seldom express that doubt clearly in an accompanying message, which makes things harder for the receiver. Third, receivers incorrectly interpret sharing as a sign of a story's truth, partly because they underestimating how often stories are shared at low beliefs, and partly from overestimating how informative a friend's beliefs are about truth. And fourth, receivers exhibit shareability neglect: they treat their friend's decision as a relatively uniform signal, ignoring how motives besides belief -altruism, fear, ideology -heterogeneously affect sharing decisions.</p><p>A policy implication of our findings is that encouraging sharers to convey their beliefs (or at least, their doubts) when forwarding stories would benefit aggregate learning. It would also nudge sharers to think about a story's truth before sharing, which prior research has found to disproportionately reduce sharing of misinformation <ref type="bibr">(Pennycook et al. (2021)</ref>). Conversely, it would also help if receivers, when forming an opinion about a story, took a moment to consider why their friend may have shared it.</p><p>But these suggestions have their limitations. In the appendix Sec 8.3 we conduct counterfactual simulations where sharers precisely convey their beliefs about every story they forward. (We also cover other signaling possibilities). While it prevents receivers from strongly believing some false stories, the overall difference is modest. This is for two reasons. One is selection: sharers forward only a fraction of what they see, and receivers never know which stories the sharer saw but (perhaps wisely) chose not to share. The second is asymmetric discernment: participants (sharers and receivers alike) are much better at classifying true stories as true than false stories as false. Therefore, even when receivers learn sharers' beliefs, their trust in falsehoods does not decrease relative to true stories. The policy is more likely to be effective in environments where distrust is as strong a signal of falsehood as trust is of truth; and beliefs are exchanged not just about shared stories, but some unshared ones.       <ref type="bibr">= 2000)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tables</head><p>not necessarily the signal), 𝑓 takes the form:</p><p>𝑓(𝑝, 𝑠) = 𝑚(𝑠) ⋅ 𝑝 + 𝑐(𝑠)</p><p>We now make the following assumptions about 𝑓:</p><p>1. An uninformative signal leads to no change. That is, 𝑓(𝑝, 0.5) = 𝑝 ⟹ 𝑚(0.5) = 1, 𝑐(0.5) = 0 2. Positive signals (𝑠 &gt; 0.5) increase receiver's beliefs. But if the receiver is already at the maximum possible belief (𝑝 = 1), they have no further impact. Thus,</p><formula xml:id="formula_14">∀𝑠 &gt; 0.5, 𝑖.𝑓(1, 𝑠) = 1 ⟹ 𝑐(𝑠) = 1 -𝑚(𝑠) ⟹ 𝑓(𝑝, 𝑠) = 𝑚(𝑠) ⋅ 𝑝 + 1 -𝑚(𝑠) 𝑖𝑖. 𝜕𝑓 𝜕𝑠 &gt; 0 ⟹ 𝑚 ′ (𝑠) &lt; 0</formula><p>Essentially, for positive signals we assume 𝑓 always passes through (1,1) 33 , but the intercept increases as the strength of the signal increases. That is, for negative signals we assume that 𝑓 passes through (0,0) and the slope decreases as the signal grows "stronger" (i.e., as 𝑠 draws closer to 0).</p><p>4. Finally, although we do not need it for identification, we can also assume receivers' updates are symmetric around 0.5 (as hinted at in Figure <ref type="figure">7</ref> above). That is, ∀𝑝, 𝑠 ∶ 𝑓(𝑝, 𝑠) -𝑝 = (1 -𝑝) -𝑓(1 -𝑝, 1 -𝑠) ⟹ 𝑓(𝑝, 𝑠) = 1 -𝑓(1 -𝑝, 1 -𝑠) 33 In truth, participants' mean posterior at 𝑝𝑟𝑖𝑜𝑟 = 1 is slightly below 1, which can result from measurement error and right-censoring. Accounting for this using tobit (see Table <ref type="table" target="#tab_5">7</ref>) does not affect our estimates.</p><p>In summary, 𝑓 takes the form: This still leaves 𝑚(𝑠), 𝑐(𝑠) unspecified, however. We pick the simplest form that satisifies the conditions above. For positive signals, we assume 𝑐(𝑠) is proportional to the signal's "strength", i.e., its distance from 0.5. In our experiment however, we make this information explicit. Thus, analogous to our procedures for 𝑠ℎ𝑟 = 1, we can estimate how receivers would update upon learning their friend chose not to share a story (i.e., by comparing their updates on 𝑠ℎ𝑟 = 0 against those from 𝐵𝑂𝑇 = 𝐹 ). Using these procedures, we find that receivers perceive the decision to not share a story has a signal value of 𝑠 𝑝𝑐𝑣𝑑 (𝑠ℎ𝑟 = 0) = 0.43(𝑠.𝑒.0.03) (Grether's specification) or 0.38(𝑠.𝑒.0.02) (Quasilinear model). The true value of this signal is in fact 0.49(𝑠.𝑒.0.01). Thus, once again receivers appear to incorrectly treat not sharing a as a sign of falsehood, but the magnitude of the bias is much smaller. In other words, receivers at better at recognizing that not-sharing a story is a sign that it might be uninteresting rather than false.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Counterfactuals</head><p>In this section we construct estimates for what receivers' posteriors would look under different signaling regimes. We take sharers' and receivers' prior beliefs as given (i.e., as they are in the data), and also take sharing decisions as given. What we vary however, are attributes of signaling: selection (whether receivers obtain signals about all stories or just the forwarded ones), coarseness (whether receivers observe sharers' beliefs or just sharing decisions), and perception (how accurate receivers perceive various signals to be). The full list is as follows:</p><p>• RSD.shr / Baseline: Receivers learn their friend shared a story (𝑠ℎ𝑟 = 1). They do not know about stories their friend saw but didn't share (𝑠ℎ𝑟 = 0). • RSB.shr: Receivers see the sharer's belief for any story they share. (That is, receiver knows 𝑏𝑒𝑙 when 𝑠ℎ𝑟 = 1) • RSD.all: Receivers see the sharer's decision for all stories (𝑠ℎ𝑟 = 0 and 𝑠ℎ𝑟 = 1).</p><p>• RSB.all: Receivers see the sharer's beliefs for all stories, shared or unshared.</p><p>• BOT.all: Receivers see high-precision computer-generated signals for all stories. Signals are correct 80% of the time indepedent of the story. This is the equivalent of learning the (binary) opinion of an unbiased, reasonably well-informed friend. • RSD.acc: same as RSD.all, except receivers know what 𝑠ℎ𝑟 = 1 or 𝑠ℎ𝑟 = 0 truly indicate.</p><p>Receivers' posteriors for RSD* are estimated using Equation <ref type="formula">9</ref>. Posteriors for RSB* are estimated by regressing receivers' posteriors on sharer's priors and their own priors, from the revealed-belief (RSB) round. We assume receivers' beliefs remain unchanged in stories that weren't shared. Figure <ref type="figure">8</ref> plots how the distribution of receivers' beliefs shifts in each case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We start with RSD.acc (not shown in the figure). If receivers know the true signal-value of their friends' sharing decisions, nearly all updating is eliminated. This is as expected, since sharers' decisions are uninformative about truth.</p><p>Next, we compare the baseline case (forward as usual) to RSB.shr (forward, but state belief precisely) in Figure <ref type="figure">8a</ref>. There is no difference in posteriors among true stories, but false stories are slightly less likely to be strongly believed under RSB.shr. (The CDF for RSB.shr is to the left of the baseline when bel &gt; 0.5). However belief in false stories appears to have increased overall. This occurs because of asymmetric discernment: sharers (and receivers) are much better at identifying true stories as true than false stories as false. In fact (as we saw in Figure <ref type="figure" target="#fig_1">2a</ref>) sharers' mean belief in false stories is slightly greater than 0.5; and learning those beliefs pushes receivers' trust upward as well.</p><p>Next, in Figure <ref type="figure">8b</ref> we address selection. We compare the baseline to RSD.all (receivers see both the decision to share and not-share); and to RSB.all (receivers learn sharers' beliefs in all stories, irrespective of sharing). Unlike the baseline or RSB.shr, these scenarios actually reduce belief in false stories when it is high (above 0.5), and are no worse than either when belief is low. At the same time, they cause an overall regression to the mean across strory veracity (this is to be expected, since sharers/receivers are randomly assigned). Thus, under RSB.all and RSD.all, reduction in believing false stories comes at the cost of reduced confidence in true stories as well.</p><p>Unfortunately, the only treatment that appears to "work" -i.e., increase belief in true stories and reduce belief in false stories -is the BOT's signal. BOT's signals are especially impactful at decreasing belief in false stories: unlike sharers, the BOT's signals are just as accurate about false stories as they are about true stories.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Screenshots of participants' workflows during the experiment. Participants see each story as a preview (headline, lede image and caption). Some captions are not shown in Figure 1b and Figure 3 for clarity.</figDesc><graphic coords="12,239.24,299.39,133.53,220.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: This figure plots sharers' beliefs in (and sharing rates of) stories presented to them. Each sharer sees 20-24 news stories, exactly half of which are true. 2a shows their mean belief and sharing rates, grouped by story veracity. 2b shows the probability a story is true given sharer's belief in it. Beliefs are elicited on a scale 0 (surely false) -100 (surely true) and translated to probabilities. 2c shows the probability a story is shared, given sharer's belief. Sharing choices are binary. All standard errors are clustered at the sharer level.</figDesc><graphic coords="17,83.46,323.53,222.55,211.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: 3a plots the mean change in receivers' beliefs about a story after they observe a signal about its truth. (The receiver observes at most one signal per story). Signal 𝑠ℎ𝑟 = 1 (𝑠ℎ𝑟 = 0) correspond to the receiver learning that their friend chose to share (or respectively, not share) the story. Signals 𝑏𝑜𝑡 = 𝑇 (𝑏𝑜𝑡 = 𝐹 ) correspond to the BOT declaring the story is true (respectively, false). The BOT's clue is generated independently at random and is correct with probability 0.8. The signal (no info) corresponds to the case where the receiver is not shown any signal and is simply asked to report their belief about the story a second time. 3b plots receivers' posteriors vs. priors (instead of mean change) for two types of signals: 𝑠ℎ𝑟 = 1 and 𝑏𝑜𝑡 = 𝑇 ; a binscatter and fitted line are plotted for each case. 3c recreates 3b; but the scales are transformed from probabilities to log-odds. Shaded regions indicate 95% confidence intervals for fitted lines.</figDesc><graphic coords="21,83.46,301.74,222.55,211.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 4: 4a, 4b plot the sharer's belief in a story versus their likelihood of sharing it. 4a fits a quadratic curve seperately for each story, whereas 4b groups stories by shareability (high vs. low) and plots a binscatter for each group. Figure 4c plots receivers' updates (priors vs. posteriors) after learning that their partner chose to share the story. Standard errors for 4b, 4c are clustered at the participant level. Data is restricted to stories that have at least a 100 observations.</figDesc><graphic coords="25,194.73,327.64,222.55,211.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Method 3 :</head><label>3</label><figDesc>Finally, we examine what happens if receivers directly observe sharers' beliefs instead of their sharing decisions. Figure5plots this comparison. We plot two series: one for average change from learning the story was shared (𝑠ℎ𝑟 = 1) in the round RSD, and the other from learning the sharer's beliefs in round RSB, restricted to shared stories 32 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: This figure contrasts receivers' updates from learning two different types of signals about sharedstories. In the first series (solid dots), receivers learn that the story was shared, but do not see how strongly the sharer believed the story. In the second series (hollow squares), receivers see the sharer's belief, but are not told that the sharer chose to share the story. In both cases, the series plot the mean change in the receivers' beliefs (Y axis) against the sharer's belief (X axis) binned at width 0.2. All standard errors are clustered at the participant level.</figDesc><graphic coords="26,83.46,235.64,445.10,267.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: This figure shows the receiver's guess about the sharer's belief (y-axis) versus the sharer's actual belief (x-axis), conditional on knowing the latter's sharing decision (𝑠ℎ𝑟 = 1 or 0). A line of best fit (OLS) is plotted separately for each case. Guesses are either incentivized for accuracy (right panel) or not (left panel).</figDesc><graphic coords="28,83.46,72.00,445.11,222.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>3. Analogously, negative signals (𝑠 &lt; 0.5) decrease receivers' beliefs. But if the receiver is already at the minimum, they have no further impact. Thus,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>𝑝𝑜𝑠𝑡𝑒𝑟𝑖𝑜𝑟 = 𝑓(𝑝𝑟𝑖𝑜𝑟, 𝑠) = { 𝑚(𝑠) ⋅ 𝑝𝑟𝑖𝑜𝑟 + 1 -𝑚(𝑠), if 𝑠 ∈ [0.5, 1] -(a) 𝑚(𝑠) ⋅ 𝑝𝑟𝑖𝑜𝑟, if 𝑠 ∈ [0, 0.5] -(b) (12)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>' updates from learning their friend chose not to share On most social media platforms it is impossible to tell if a friend never saw a story, versus saw but chose not to share it. As a result, potentially negative signals about a story's veracitysuch as a friend's refusal to share it -are systematically hidden from receivers. The absence of such signals can lead receivers into a what-you-see-is-all-there-is bias<ref type="bibr" target="#b9">(Enke and Zimmermann (2017)</ref>), where they forget the possibility of such signals or downweigh them.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: This figure shows the probability receivers guess their partner's (binary) sharing decision correctly, after learning how likely the latter thought the story was true. Separate bars are plotted for the case where the sharer's belief was above vs. below 0.5. (Observations with belief = 0.5 are dropped). Guesses on the right panel are incentivized for accuracy, and on the left panel are not.</figDesc><graphic coords="49,83.46,215.90,445.11,222.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Study Administration: On the left, a map shows the locations of the colleges in which the study was conducted. On the right, a picture of participants in the in-person version of the experiment.</figDesc><graphic coords="51,83.46,248.25,445.09,181.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="41,83.46,135.82,445.11,342.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="45,83.46,72.00,445.11,222.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="45,83.46,312.41,445.11,222.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="45,83.46,552.82,445.11,222.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Probability that a story is shared or true conditional on the sharer's belief. Sharers see an equal mix of true and false stories. Standard errors are in brackets.</figDesc><table><row><cell>Sharer's</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Belief</cell><cell>0.00</cell><cell>0.25</cell><cell>0.50</cell><cell>0.75</cell><cell>1.00</cell></row><row><cell>Pr. story is</cell><cell>0.26 (0.11)</cell><cell>0.42 (0.18)</cell><cell>0.50 (0.01)</cell><cell>0.55 (0.12)</cell><cell>0.60 (0.01)</cell></row><row><cell>true</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Pr. story is</cell><cell>0.25 (0.01)</cell><cell>0.26 (0.02)</cell><cell>0.31 (0.01)</cell><cell>0.48 (0.02)</cell><cell>0.62 (0.01)</cell></row><row><cell>shared</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4 :</head><label>4</label><figDesc>Participants' mean beliefs (and sharing rates) in true vs. false stories. All standard errors (unclustered, not shown) are below 0.01.</figDesc><table><row><cell>Story Type</cell><cell cols="2">Mean Belief</cell><cell cols="2">Sharing Rate</cell><cell>N obs</cell></row><row><cell></cell><cell>True</cell><cell>False</cell><cell>True</cell><cell>False</cell><cell></cell></row><row><cell>All</cell><cell>0.64</cell><cell>0.51</cell><cell>0.42</cell><cell>0.40</cell><cell>9096</cell></row><row><cell>By story selection</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>balanced</cell><cell>0.63</cell><cell>0.52</cell><cell>0.43</cell><cell>0.39</cell><cell>6132</cell></row><row><cell>representative</cell><cell>0.65</cell><cell>0.50</cell><cell>0.40</cell><cell>0.42</cell><cell>2965</cell></row><row><cell>By story favorability</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>neutral</cell><cell>0.65</cell><cell>0.53</cell><cell>0.45</cell><cell>0.43</cell><cell>3992</cell></row><row><cell>pleasant</cell><cell>0.65</cell><cell>0.53</cell><cell>0.42</cell><cell>0.42</cell><cell>1353</cell></row><row><cell>unpleasant</cell><cell>0.61</cell><cell>0.48</cell><cell>0.38</cell><cell>0.41</cell><cell>1422</cell></row><row><cell>By study venue</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>in-person</cell><cell>0.63</cell><cell>0.52</cell><cell>0.43</cell><cell>0.43</cell><cell>6566</cell></row><row><cell>remote</cell><cell>0.65</cell><cell>0.50</cell><cell>0.37</cell><cell>0.31</cell><cell>2531</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Story Characteristics and Observations</figDesc><table><row><cell cols="2">Full Sample</cell><cell></cell><cell cols="2">Balanced Selection</cell><cell></cell><cell cols="2">Representative Selection</cell></row><row><cell>(All)</cell><cell></cell><cell cols="2">Remote</cell><cell cols="2">In-Person</cell><cell></cell><cell>In-Person</cell></row><row><cell>True</cell><cell>False</cell><cell>True</cell><cell>False</cell><cell>True</cell><cell>False</cell><cell>True</cell><cell>False</cell></row><row><cell>(1)</cell><cell>(2)</cell><cell>(3)</cell><cell>(4)</cell><cell>(5)</cell><cell>(6)</cell><cell>(7)</cell><cell>(8)</cell></row><row><cell cols="4">Panel A: Observations</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Explanations for Sharing Decisions</figDesc><table><row><cell>Reason</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Sharers' Choices and Beliefs as Predictors of Story VeracityThis table presents the regression of the story's veracity on sharing choice of the sharers. Shared is a dummy variable taking value 1 when the story is shared, 0 otherwise. Sharer's Prior indicates sharer's belief on the scale of 0 to 1 (continuous).All Std. errors are clustered at the sharer level. Levels of significance are defined as:</figDesc><table><row><cell cols="4">Dependent var: Story is TRUE (=1)</cell></row><row><cell>(1)</cell><cell>(2)</cell><cell>(3)</cell><cell>(4)</cell></row></table><note><p>Note: * p&lt;0.05; * * p&lt;0.01; * * * p&lt;0.001</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>Receivers' updates on computer-generated ("BOT")  signals. This table presents the regression of receiver's posterior on their prior when presented with a computer-generated signal. Columns 1 to 4 present the results when the BOT indicates that the story is true, while columns 5 to 8 present the results when the BOT indicates the story is false. (The BOT's guess is correct with probability 0.8). Columns 1 to 3 and 5 to 7 present the OLS estimates. Column 4 presents the estimates of a TOBIT regression censored towards right at 100% belief. Column 8 presents the estimates of a TOBIT regression censored towards left at 0% belief. When the BOT signal is True, Polarity takes value 1 if the participant's prior is greater than 50% and 0 otherwise. When the BOT signal is False, Polarity takes value 1 if the participant's prior is less than or equal to 50% and 0 otherwise. Concordance indicates if the given story is favorable (+1) or unfavorable (-1) or neutral (0) to the participant's stated political belief/bias. Standard Errors are clustered at the Participant level in all the regression results above.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="4">Dependent variable: Receiver's Posterior</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Signal: BOT says T</cell><cell></cell><cell></cell><cell cols="2">Signal: BOT says F</cell><cell></cell></row><row><cell></cell><cell>OLS</cell><cell>OLS</cell><cell>OLS</cell><cell>TOBIT</cell><cell>OLS</cell><cell>OLS</cell><cell>OLS</cell><cell>TOBIT</cell></row><row><cell></cell><cell>(1)</cell><cell>(2)</cell><cell>(3)</cell><cell>(4)</cell><cell>(5)</cell><cell>(6)</cell><cell>(7)</cell><cell>(8)</cell></row><row><cell>Prior</cell><cell>0.69  *  *  *</cell><cell></cell><cell>0.49  *  *  *</cell><cell>0.69  *  *  *</cell><cell>0.70  *  *  *</cell><cell></cell><cell>0.70  *  *  *</cell><cell>0.70  *  *  *</cell></row><row><cell></cell><cell>(0.02)</cell><cell></cell><cell>(0.12)</cell><cell>(0.02)</cell><cell>(0.02)</cell><cell></cell><cell>(0.03)</cell><cell>(0.02)</cell></row><row><cell>Logit(Prior)</cell><cell></cell><cell>0.74  *  *  *</cell><cell></cell><cell></cell><cell></cell><cell>0.71  *  *  *</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>(0.02)</cell><cell></cell><cell></cell><cell></cell><cell>(0.03)</cell><cell></cell><cell></cell></row><row><cell>Polarity</cell><cell></cell><cell></cell><cell>0.04</cell><cell></cell><cell></cell><cell></cell><cell>-0.03</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>(0.05)</cell><cell></cell><cell></cell><cell></cell><cell>(0.06)</cell><cell></cell></row><row><cell>Concordance</cell><cell></cell><cell></cell><cell>0.005</cell><cell></cell><cell></cell><cell></cell><cell>0.01</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>(0.02)</cell><cell></cell><cell></cell><cell></cell><cell>(0.02)</cell><cell></cell></row><row><cell>Prior x Polarity</cell><cell></cell><cell></cell><cell>-0.01</cell><cell></cell><cell></cell><cell></cell><cell>0.02</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>(0.10)</cell><cell></cell><cell></cell><cell></cell><cell>(0.10)</cell><cell></cell></row><row><cell>Prior x Concordance</cell><cell></cell><cell></cell><cell>-0.01</cell><cell></cell><cell></cell><cell></cell><cell>-0.002</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>(0.03)</cell><cell></cell><cell></cell><cell></cell><cell>(0.03)</cell><cell></cell></row><row><cell>Constant C(𝑆)</cell><cell>0.26  *  *  *</cell><cell>0.70  *  *  *</cell><cell>0.33  *  *  *</cell><cell>0.26  *  *  *</cell><cell>0.07  *  *  *</cell><cell>-0.54  *  *  *</cell><cell>0.07</cell><cell>0.07</cell></row><row><cell></cell><cell>(0.02)</cell><cell>(0.03)</cell><cell>(0.05)</cell><cell>(0.04)</cell><cell>(0.01)</cell><cell>(0.05)</cell><cell>(0.08)</cell><cell>(0.01)</cell></row><row><cell>Story FE</cell><cell>No</cell><cell>No</cell><cell>Yes</cell><cell>No</cell><cell>No</cell><cell>No</cell><cell>Yes</cell><cell>No</cell></row><row><cell>Observations</cell><cell>2,138</cell><cell>2,138</cell><cell>1,049</cell><cell>2,138</cell><cell>2,139</cell><cell>2,139</cell><cell>1,084</cell><cell>2,139</cell></row><row><cell>Adjusted R 2</cell><cell>0.55</cell><cell>0.56</cell><cell>0.52</cell><cell>0.55</cell><cell>0.50</cell><cell>0.52</cell><cell>0.47</cell><cell>0.50</cell></row><row><cell cols="4">Levels of significance are defined as:  *  p&lt;0.05;  *  *  p&lt;0.01;  *  *  *  p&lt;0.001</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>Note:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 :</head><label>8</label><figDesc>Receivers' Updates from Learning Sharer's Decisions vs. BeliefsThe table regresses receivers' posteriors on their priors, after receivers learn that the sharer chose to share the story (𝑠ℎ𝑟 = 1). Sharers' belief is unknown to receivers in this case, unless sharers pass it in a message. Story's favorability is with respect to the receiver's stated political identity. Stories that are "neutral" or "apolitical" are in the omitted category.</figDesc><table><row><cell></cell><cell cols="4">Dependent. var: Receiver's posterior</cell></row><row><cell></cell><cell>(1)</cell><cell>(2)</cell><cell>(3)</cell><cell>(4)</cell></row><row><cell>Receiver's prior</cell><cell cols="4">0.783*** 0.778*** 0.784*** 0.771***</cell></row><row><cell></cell><cell>(0.019)</cell><cell>(0.019)</cell><cell>(0.019)</cell><cell>(0.020)</cell></row><row><cell>Sharer's belief</cell><cell></cell><cell></cell><cell></cell><cell>0.024</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(0.017)</cell></row><row><cell>Sharer's belief &gt; 0.5</cell><cell></cell><cell></cell><cell>0.001</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>(0.012)</cell><cell></cell></row><row><cell>Story selection: representative</cell><cell></cell><cell>0.014</cell><cell>0.005</cell><cell>0.011</cell></row><row><cell></cell><cell></cell><cell>(0.013)</cell><cell>(0.012)</cell><cell>(0.013)</cell></row><row><cell>Story: politically favorable</cell><cell></cell><cell>-0.007</cell><cell>-0.015</cell><cell>-0.009</cell></row><row><cell></cell><cell></cell><cell>(0.012)</cell><cell>(0.012)</cell><cell>(0.012)</cell></row><row><cell>Story: politically unfavorable</cell><cell></cell><cell>-0.008</cell><cell>-0.013</cell><cell>-0.012</cell></row><row><cell></cell><cell></cell><cell>(0.013)</cell><cell>(0.014)</cell><cell>(0.014)</cell></row><row><cell>Location: remote</cell><cell></cell><cell cols="3">-0.045*** -0.040*** -0.041***</cell></row><row><cell></cell><cell></cell><cell>(0.011)</cell><cell>(0.011)</cell><cell>(0.011)</cell></row><row><cell>(Intercept)</cell><cell cols="4">0.164*** 0.197*** 0.197*** 0.186***</cell></row><row><cell></cell><cell>(0.013)</cell><cell>(0.016)</cell><cell>(0.019)</cell><cell>(0.019)</cell></row><row><cell>Num.Obs.</cell><cell>1871</cell><cell>1871</cell><cell>1692</cell><cell>1802</cell></row><row><cell>R2 Adj.</cell><cell>0.649</cell><cell>0.652</cell><cell>0.660</cell><cell>0.650</cell></row><row><cell cols="5">Other omitted categories are: "in-person" (location) and "balanced" (story selection)</cell></row><row><cell cols="5">+𝑝 &lt; 0.1,  * 𝑝 &lt; 0.05,  *   *  𝑝 &lt; 0.01,  *   *   * 𝑝 &lt; 0.001. All std. errors are clustered at the</cell></row><row><cell>participant level.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>Note:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 :</head><label>9</label><figDesc>Estimates of actual vs. perceived value of signals observed by the receiver.</figDesc><table><row><cell>Signal</cell><cell></cell><cell cols="2">Estimated Signal Value</cell></row><row><cell></cell><cell>Actual</cell><cell cols="2">Perceived</cell></row><row><cell></cell><cell></cell><cell>Grether</cell><cell>Quasilinear</cell></row><row><cell></cell><cell>(1)</cell><cell>(2)</cell><cell>(3)</cell></row><row><cell cols="3">Panel A: Prob. the story is true if partner...</cell><cell></cell></row><row><cell>Shared the story (shr = 1)</cell><cell>0.51</cell><cell>0.69</cell><cell>0.69</cell></row><row><cell>95% CI</cell><cell cols="2">[0.50, 0.52] [0.66, 0.72]</cell><cell>[0.67, 0.72]</cell></row><row><cell>N</cell><cell>7410</cell><cell>1871</cell><cell>1871</cell></row><row><cell>Did not share the story (shr = 0)</cell><cell>0.49</cell><cell>0.43</cell><cell>0.38</cell></row><row><cell>95% CI</cell><cell cols="2">[0.48, 0.50] [0.40, 0.43]</cell><cell>[0.36, 0.40]</cell></row><row><cell>N</cell><cell>10783</cell><cell>1548</cell><cell>1548</cell></row><row><cell cols="3">Panel B: Prob. the story is true if ...</cell><cell></cell></row><row><cell>Sharer's belief ∈ [0, 0.25)</cell><cell>0.31</cell><cell>0.12</cell><cell>0.08</cell></row><row><cell>95% CI</cell><cell cols="2">[0.30, 0.33] [0.10, 0.15]</cell><cell>[0.03, 0.13]</cell></row><row><cell>N</cell><cell>3559</cell><cell>640</cell><cell>640</cell></row><row><cell>Sharer's belief ∈ [0.25, 0.5)</cell><cell>0.47</cell><cell>0.43</cell><cell>0.32</cell></row><row><cell>95% CI</cell><cell cols="2">[0.45, 0.49] [0.39, 0.47]</cell><cell>[0.29, 0.35]</cell></row><row><cell>N</cell><cell>2435</cell><cell>773</cell><cell>773</cell></row><row><cell>Sharer's belief ∈ (0.5, 0.75]</cell><cell>0.53</cell><cell>0.64</cell><cell>0.76</cell></row><row><cell>95% CI</cell><cell cols="2">[0.52, 0.55] [0.61, 0.68]</cell><cell>[0.72, 0.79]</cell></row><row><cell>N</cell><cell>4414</cell><cell>908</cell><cell>908</cell></row><row><cell>Sharer's belief ∈ (0.75, 1]</cell><cell>0.59</cell><cell>0.84</cell><cell>0.80</cell></row><row><cell>95% CI</cell><cell cols="2">[0.58, 0.60] [0.81, 0.87]</cell><cell>[0.77, 0.84]</cell></row><row><cell>N</cell><cell>6210</cell><cell>1207</cell><cell>1207</cell></row><row><cell cols="4">Note: This table presents estimates of what sharers' beliefs or actions signal about a story's truth;</cell></row><row><cell cols="4">versus the what receivers perceive those signals to be. Estimates of the actual value of signals (in</cell></row><row><cell cols="4">column 1) are generated from sharer-side data, sharing rates of (and sharers' beliefs about) true</cell></row><row><cell cols="4">versus false stories. The perceived value of these signals is estimated from receivers' updates, using</cell></row><row><cell cols="4">the grether's (col 2) and quasilinear (col 3) models respectively. Confidence intervals for col (1)</cell></row><row><cell cols="4">assume binomial std. errors. Confidence intervals for cols 2 and 3 are generated with a simple</cell></row><row><cell>bootstrap (N</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>This may not the only reason to infer truth from sharing. For instance, if people think their friends are more likely to see true news stories than they are themselves, they would be right to infer that even a randomly forwarded story is more likely to be true. We cannot rule this out in the wild but control for it in our experiment by eliciting receivers' priors about all stories seen by the sharer, before revealing their sharing decisions.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>Built using javascript on Qualtrics. See https://github.com/jimmy-narang/SVB for the code repository</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>There are no special incentives to share; she is simply asked to match her real-life behavior as best as possible. The idea is to mimic the casual, everyday sharing that occurs between friends</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3"><p>And if they did, we pass on any accompanying message from the sharer. This mimics the design of most social media platforms: receivers see who shared a story along with any accompanying message. Unlike most platforms however, receivers in our experiment may also learn that the sharer decided not to share a story with them.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4"><p>Note that identification is comes from temporarily (and randomly) withholding information that their friend chose to share the story, and not from comparing stories that were shared with those that were not.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5"><p>Prior studies<ref type="bibr" target="#b23">(Pennycook and Rand 2019;</ref> Pennycook et al. 2021)  find that sharers' mean belief in true stories is higher than in false stories, but the gap in their sharing intentions are much smaller. We replicate those findings using sharing decisions (rather than intentions) with real friends, and explore the role of doubt and uncertainty in sharing.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_6"><p>Such updating behavior is consistent with a variety of biases previously documented in the literature, such as base-rate-neglect and confirmation bias. See<ref type="bibr" target="#b5">Benjamin (2019)</ref> for an overview.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_7"><p>Receivers do not assume the sharer fully believes every story they share. Rather, receivers update as if the sharer's belief was about 70% across stories, and show little heterogeneity in updates.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_8"><p>For example, when sharers are at least 75% sure a story is true, the story is in fact true about 60% of the time. However, receivers update as if that level of sharer's confidence (≥ 75) indicates an 80-84% chance a story is true. See Table9</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_9"><p>See Altay, Araujo, and Mercier (2022)  for sharing news that is "interesting if true" (but not so terrible if false),<ref type="bibr" target="#b8">Chakrabarti, Stengel, and Solanki (2018)</ref> for sharing claims that reinforce cultural or national pride; and Saha et al. (2023) for fear-related speech.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_10"><p>For convenience, we use "belief" when referring to how likely the sharer thinks a story is true, and "prior" or "posterior" when referring to how likely the receiver thinks a story is true, before and after observing a signal. All three quantities are expressed as probabilities, unless otherwise specified.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16" xml:id="foot_11"><p>In response to the pandemic, nearly every college we contacted had created their own WhatsApp groups to share updates and class announcements</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17" xml:id="foot_12"><p>600,000 INR = approx. 8160 USD in 2022, or USD 25728 adjusting for purchasing power parity (OECD (2023)).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="18" xml:id="foot_13"><p>Choices were elicited before beliefs because we wanted them to be as "natural" as possible. There is consensus in the literature that asking participants about a story's accuracy nudges them to think carefully about subsequent sharing decisions(Pennycook et al. (2021)), reducing their likelihood of sharing false stories.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="19" xml:id="foot_14"><p>Receivers did not know this: they were not told anything about what their partner saw.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="20" xml:id="foot_15"><p>In about 5% of cases, receivers were told their partner did not see the story, and were simply asked to report their belief again. This was to test an identifying assumption, namely, if the noise from eliciting beliefs twice would be mean-zero. (It was the only case of 'deception' in the experiment).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="21" xml:id="foot_16"><p>Supplementary questions such as these were asked on a separate screen, to reduce the likelihood of receivers anchoring on their previous answer.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="22" xml:id="foot_17"><p>Roughly 12 USD adjusting for purchasing power. See https://data.worldbank.org/indicator/PA.NUS.PRVT.PP</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="26" xml:id="foot_18"><p>Symmetrically, for negative signals such as 𝑠ℎ𝑟 = 0 or 𝐵𝑂𝑇 = 𝐹 , the change is largest at high priors. See the appendix for a discussion on receivers' updates on 𝑠ℎ𝑟 = 0.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="29" xml:id="foot_19"><p>This is smaller than the self-reported 4:1 odds receivers reported before the study began, when they were asked "If presented with an equal mix of true vs. false stories, how often would this friend share true vs. false stories in real life?"</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="30" xml:id="foot_20"><p>An advantage of these specifications is that identification is within friend-pair and story: there is no need to assume what one sharer finds interesting is the same as what another would.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="31" xml:id="foot_21"><p>This is a meaningful distinction for two reasons: one, it is the point where the sharer switches to thinking a story is probably true from thinking it is probably false, i.e., it marks a qualitative change in their opinion; and two, since 𝑠 ′ (𝑏𝑒𝑙) &gt; 0 and 𝑠(𝑏𝑒𝑙0.5) = 0.5 in the data, it is also the point where receivers' posteriors should increase vs. decrease from learning the sharer's belief; an empirical difference.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="32" xml:id="foot_22"><p>That is, data from the RSB round are restricted to stories which the sharer chose to share. Note that unlike the previous method, identification here is across-stories but within person.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="34" xml:id="foot_23"><p>To estimate positive signals, we prefer writing an expression in terms of the intercept 𝑐(𝑠) rather than the slope 𝑚(𝑠), because the empirical estimate of the intercept is not susceptible to the noise/censoring issues that occur near (1,1).</p></note>
		</body>
		<back>

			<div type="funding">
<div> *   <p>The project was funded by the <rs type="funder">Crime and Violence Initiative at J-PAL</rs> (<rs type="grantNumber">GR-1192</rs>), The Weiss Fund (Fall <rs type="grantNumber">2020</rs>), and the <rs type="funder">Center for Effective Global Action (CEGA) Development Fund (Fall 2020</rs>). Experiments were approved by the <rs type="institution">IRB at UC Berkeley</rs> (<rs type="grantNumber">2019-10-12673</rs>) and <rs type="institution">IFMR Chennai, India (IRB00007107)</rs>. Preregistration is available at https://doi.org/10.1257/rct.6474-1.0. I would like to thank my advisors <rs type="person">Ted Miguel</rs>, <rs type="person">Ned Augenblick</rs>, and <rs type="person">Supreet Kaur</rs> for their invaluable support. I would also like to thank am also grateful to <rs type="person">Anunay Samantha</rs>, <rs type="person">Satyen Saha</rs>, and <rs type="person">Moloy Sarkar</rs> for their help with conducting the study during the Covid-19 pandemic.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_7Kp9Meq">
					<idno type="grant-number">GR-1192</idno>
				</org>
				<org type="funding" xml:id="_EhEJmA4">
					<idno type="grant-number">2020</idno>
				</org>
				<org type="funding" xml:id="_j5TkupG">
					<idno type="grant-number">2019-10-12673</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this section, we identify a quasilinear function that captures how receivers update their beliefs upon receiving a signal. The idea is to match the almost-linear updating behavior we observe in the study (as seen above in Figure <ref type="figure">7</ref>).</p><p>Let 𝑓(𝑝, 𝑠) denote the receiver's belief updating function, where 𝑝 is the receiver's prior and 𝑠 is the signal they observe (𝑝, 𝑠 ∈ [0, 1]). If the receiver's posterior is linear in their prior (but 9 Supplementary Figures and Tables Each sharer sees 20-24 news stories, exactly half of which are true. Beliefs are elicited on a scale 0 (surely false) -100 (surely true) and translated to probabilities but sharing choices are binary. All standard errors are clustered at the sharer level. 9a plots the series for stories that were manually selected for balance (across topics, political partisanship, etc.). 9b plots the series for stories selected using a random draw: true stories were drawn from the front pages of leading newspapers, and false stories were drawn from webpages of leading fact-checking websites. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1">Study Locations</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">If This Account Is True, It Is Most Enormously Wonderful&apos;: Interestingness-If-True and the Sharing of True and False News</title>
		<author>
			<persName><forename type="first">References</forename><surname>Altay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sacha</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Emma</forename><surname>De Araujo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Mercier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Journalism</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="373" to="394" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Arechar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename><surname>Nancy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rocky</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziv</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiran</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Garimella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackson</forename><forename type="middle">G</forename><surname>Gully</surname></persName>
		</author>
		<author>
			<persName><surname>Lu</surname></persName>
		</author>
		<title level="m">Understanding and Reducing Online Misinformation Across 16 Countries on Six Continents</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Guy</forename><surname>Aridor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Jiménez-Durán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ro'ee</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lena</forename><surname>Song</surname></persName>
		</author>
		<title level="m">The Economics of Social Media</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Educative Interventions to Combat Misinformation: Evidence from a Field Experiment in India</title>
		<author>
			<persName><forename type="first">Sumitra</forename><surname>Badrinathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Political Science Review</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1325" to="1341" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Researching and Countering Misinformation in the Global South</title>
		<author>
			<persName><forename type="first">Sumitra</forename><surname>Badrinathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Chauchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Current Opinion in Psychology</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">101733</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Errors in Probabilistic Reasoning and Judgment Biases</title>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">J</forename><surname>Benjamin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Behavioral Economics: Applications and Foundations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="69" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Income and Wealth Inequality in India, 1922-2023: The Rise of the Billionaire Raj</title>
		<author>
			<persName><forename type="first">Nitin</forename><surname>Bharti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Chancel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anmol</forename><surname>Piketty</surname></persName>
		</author>
		<author>
			<persName><surname>Somanchi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Interventions to Counter Misinformation: Lessons from the Global North and Applications to the Global South</title>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">A</forename><surname>Blair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Gottlieb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Nyhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Paler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Argote</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charlene</forename><forename type="middle">J</forename><surname>Stainfield</surname></persName>
		</author>
		<ptr target="https://www.buzzfeednews.com/article/pranavdixit/whatsapp-destroyed-village-lynchings-rainpada-india" />
	</analytic>
	<monogr>
		<title level="m">Current Opinion in Psychology, 101732. Buzzfeed-News</title>
		<imprint>
			<date type="published" when="2018">2023. 2018</date>
		</imprint>
	</monogr>
	<note>How WhatsApp Destroyed a Village</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Duty, Identity, Credibility:&apos;fake News&apos; and the Ordinary Citizen in India</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Stengel</surname></persName>
		</author>
		<author>
			<persName><surname>Solanki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BBC World Service Audiences Research</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Correlation Neglect in Belief Formation</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Enke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Zimmermann</surname></persName>
		</author>
		<idno type="DOI">10.1093/restud/rdx081</idno>
		<ptr target="https://doi.org/10.1093/restud/rdx081" />
	</analytic>
	<monogr>
		<title level="j">The Review of Economic Studies</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>rdx081</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">WhatApp Doc? A First Look at WhatsApp Public Group Data</title>
		<author>
			<persName><forename type="first">Kiran</forename><surname>Garimella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gareth</forename><surname>Tyson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twelfth International AAAI Conference on Web and Social Media</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">How WhatsApp Leads Mobs to Murder in India</title>
		<author>
			<persName><forename type="first">Vindu</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhasini</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priyadarshini</forename><surname>Ravichandran</surname></persName>
		</author>
		<ptr target="https://www.nytimes.com/interactive/2018/07/18/technology/whatsapp-india-killings.html" />
	</analytic>
	<monogr>
		<title level="j">The New York Times</title>
		<imprint>
			<date type="published" when="2018-07">2018. July</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bayes Rule as a Descriptive Model: The Representativeness Heuristic*</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Grether</surname></persName>
		</author>
		<idno type="DOI">10.2307/1885092</idno>
		<ptr target="https://doi.org/10.2307/1885092" />
	</analytic>
	<monogr>
		<title level="j">The Quarterly Journal of Economics</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="537" to="557" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Reshares on Social Media Amplify Political News but Do Not Detectably Affect Beliefs or Opinions</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Guess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Malhotra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Barberá</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hunt</forename><surname>Allcott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Crespo-Tenorio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">381</biblScope>
			<biblScope unit="issue">6656</biblScope>
			<biblScope unit="page" from="404" to="408" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Labour Incomes in India: A Comparison of Two National Household Surveys</title>
		<author>
			<persName><forename type="first">Mrinalini</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Basole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Indian Journal of Labour Economics</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="181" to="201" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The Strange Case of WhatsApp and the Child-Kidnappers</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Lalwani</surname></persName>
		</author>
		<ptr target="https://www.economist.com/1843/2021/07/30/the-strange-case-of-whatsapp-and-the-child-kidnappers" />
	</analytic>
	<monogr>
		<title level="j">The Economist</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The Science of Fake News</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">Mj</forename><surname>Lazer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">A</forename><surname>Baum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yochai</forename><surname>Benkler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">J</forename><surname>Berinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelly</forename><forename type="middle">M</forename><surname>Greenhill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filippo</forename><surname>Menczer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miriam</forename><forename type="middle">J</forename><surname>Metzger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">359</biblScope>
			<biblScope unit="issue">6380</biblScope>
			<biblScope unit="page" from="1094" to="1096" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">India&apos;s Lynching Epidemic and the Problem with Blaming Tech</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Madrigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Atlantic</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Managing Self-Confidence: Theory and Experimental Evidence</title>
		<author>
			<persName><forename type="first">Markus</forename><forename type="middle">M</forename><surname>Möbius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muriel</forename><surname>Niederle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Niehaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanya</forename><forename type="middle">S</forename><surname>Rosenblat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="7793" to="7817" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Field Experiments on Social Media</title>
		<author>
			<persName><forename type="first">Mohsen</forename><surname>Mosleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gordon</forename><surname>Pennycook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">G</forename><surname>Rand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Directions in Psychological Science</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="75" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Purchasing power parities (PPP) indicator</title>
		<idno type="DOI">10.1787/1290ee5a-en</idno>
		<ptr target="https://doi.org/10.1787/1290ee5a-en" />
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>OECD</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Shifting Attention to Accuracy Can Reduce Misinformation Online</title>
		<author>
			<persName><forename type="first">Gordon</forename><surname>Pennycook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziv</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohsen</forename><surname>Mosleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><forename type="middle">A</forename><surname>Arechar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dean</forename><surname>Eckles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">G</forename><surname>Rand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">592</biblScope>
			<biblScope unit="issue">7855</biblScope>
			<biblScope unit="page" from="590" to="595" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fighting COVID-19 Misinformation on Social Media: Experimental Evidence for a Scalable Accuracy-Nudge Intervention</title>
		<author>
			<persName><forename type="first">Gordon</forename><surname>Pennycook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Mcphetres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackson</forename><forename type="middle">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">G</forename><surname>Rand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="770" to="780" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Lazy, Not Biased: Susceptibility to Partisan Fake News Is Better Explained by Lack of Reasoning Than by Motivated Reasoning</title>
		<author>
			<persName><forename type="first">Gordon</forename><surname>Pennycook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">G</forename><surname>Rand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">188</biblScope>
			<biblScope unit="page" from="39" to="50" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On the Rise of Fear Speech in Online Social Media</title>
		<author>
			<persName><forename type="first">Gordon</forename><surname>Pennycook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">G</forename><surname>Rand</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2021.02.007</idno>
		<ptr target="https://doi.org/https://doi.org/10.1016/j.tics.2021.02.007" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<editor>
			<persName><forename type="first">Punyajoy</forename><surname>Saha</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kiran</forename><surname>Garimella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Narla</forename><surname>Komal Kalyan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Saurabh</forename><surname>Kumar Pandey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Pauras</forename><surname>Mangesh Meher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Binny</forename><surname>Mathew</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Animesh</forename><surname>Mukherjee</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">2212270120</biblScope>
			<date type="published" when="2021">2021. 2023</date>
		</imprint>
	</monogr>
	<note>Trends in Cognitive Sciences</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mistakes, Overconfidence, and the Effect of Sharing on Detecting Lies</title>
		<author>
			<persName><forename type="first">Marta</forename><surname>Serra-Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Gneezy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Economic Review</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3160" to="3183" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">WhatsApp Hits India&apos;s Jio Feature Phones Amidst Fake News Violence</title>
		<author>
			<persName><surname>Techcrunch</surname></persName>
		</author>
		<ptr target="https://techcrunch.com/2018/07/10/whatsapp-forwarded-messages-india/" />
		<imprint>
			<date type="published" when="2018-07">2018. July</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Where Countries Are Tinderboxes and Facebook Is a Match</title>
		<ptr target="https://www.nytimes.com/2018/04/21/world/asia/facebook-sri-lanka-riots.html" />
	</analytic>
	<monogr>
		<title level="j">Times</title>
		<imprint>
			<date type="published" when="2018-04">2018. April</date>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Receivers see sharers&apos; belief in all shared stories (b) Receivers see (i) sharers&apos; beliefs (ii) decisions for all stories (c) Receivers see a BOT-generated clue for all stories (a)</title>
		<author>
			<persName><surname>Whatsapp-Blog</surname></persName>
		</author>
		<ptr target="https://blog.whatsapp.com/labeling-forwarded-messages" />
		<imprint>
			<date type="published" when="2018-07">2018. July</date>
		</imprint>
	</monogr>
	<note>The pre-registration for the Prime Minister&apos;s Berozgar Yojana (Unemployment scheme) is going on. Each unemployed person will be provided Rs. 3500 per month. To register fill the application form below. Zero application fee, ages 18 to 40</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Ukraine-Russia War: ISKCON Temples distribute food among people stranded on the borders&quot; Figure 11: This figure presents screenshots of some stories shown in the experiment</title>
		<imprint/>
	</monogr>
	<note>Gujarat: 13 year old boy beaten, sodomized by two clerics at Madani Madrassa in Ahmedabad. Case registered. All headlines except (c-unemployment. are true</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
