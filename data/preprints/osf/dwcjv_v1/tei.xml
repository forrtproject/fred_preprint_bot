<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hidden Markov Model: Tutorial</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Benyamin</forename><surname>Ghojogh</surname></persName>
							<email>bghojogh@uwaterloo.ca</email>
						</author>
						<author>
							<persName><forename type="first">Fakhri</forename><surname>Karray</surname></persName>
							<email>karray@uwaterloo.ca</email>
						</author>
						<author>
							<persName><forename type="first">Mark</forename><surname>Crowley</surname></persName>
							<email>mcrowley@uwaterloo.ca</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="laboratory">Machine Learning Laboratory</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<settlement>Waterloo</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Centre for Pattern Analysis and Machine Intelligence</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<settlement>Waterloo</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="laboratory">Machine Learning Laboratory</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<settlement>Waterloo</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hidden Markov Model: Tutorial</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4CE8CE327777E8BF4725E56B11552CCB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-10-22T06:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This is a tutorial paper for Hidden Markov Model (HMM). First, we briefly review the background on Expectation Maximization (EM), Lagrange multiplier, factor graph, the sum-product algorithm, the max-product algorithm, and belief propagation by the forward-backward procedure. Then, we introduce probabilistic graphical models including Markov random field and Bayesian network. Markov property and Discrete Time Markov Chain (DTMC) are also introduced. We, then, explain likelihood estimation and EM in HMM in technical details. We explain evaluation in HMM where direct calculation and the forward-backward belief propagation are both explained. Afterwards, estimation in HMM is covered where both the greedy approach and the Viterbi algorithm are detailed. Then, we explain how to train HMM using EM and the Baum-Welch algorithm. We also explain how to use HMM in some applications such as speech and action recognition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Assume we have a time series of hidden random variables. We name the hidden random variables as states <ref type="bibr" target="#b11">(Ghahramani, 2001)</ref>. Every hidden random variable can generate (emit) an observation symbol or output. Therefore, we have a set of states and a set of observation symbols. A Hidden Markov Model (HMM) has some hidden states and some emitted observation symbols <ref type="bibr" target="#b25">(Rabiner, 1989)</ref>. HMM models the the probability density of a sequence of observed symbols <ref type="bibr" target="#b28">(Roweis, 2003)</ref>. This paper is a technical tutorial for evaluation, estimation, and training in HMM. We also explain some applications for HMM. There exist many different applications for HMM. One of the most famous applications is in speech recognition <ref type="bibr" target="#b25">(Rabiner, 1989;</ref><ref type="bibr" target="#b10">Gales et al., 2008)</ref> where an HMM is trained for every word in the speech <ref type="bibr" target="#b26">(Rabiner &amp; Juang, 1986)</ref>. Another application of HMM is in action recognition <ref type="bibr" target="#b33">(Yamato et al., 1992)</ref> because an action can be seen as a sequence or times series of poses <ref type="bibr" target="#b12">(Ghojogh et al., 2017;</ref><ref type="bibr" target="#b20">Mokari et al., 2018</ref>). An interesting application of HMM, which is not trivial in the first glance, is in face recognition <ref type="bibr" target="#b29">(Samaria, 1994)</ref>. The face can be seen as a sequence of organs being modeled by HMM <ref type="bibr" target="#b23">(Nefian &amp; Hayes, 1998)</ref>. Usage of HMM in DNA analysis is another application of HMM <ref type="bibr" target="#b7">(Eddy, 2004)</ref>. The remainder of paper is as follows. In Section 2, we review some necessary background for the paper. Section 3 introduces probabilistic graphical models. Expectation maximization in HMM is explained in Section 4. afterwards, evaluation, estimation, and training in HMM are explain in Sections 5, 6, and 7, respectively. Some applications are introduced in more details in Section 8. Finally, Section 9 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>Some background required for better understanding of the HMM algorithms is mentioned in this Section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Expectation Maximization</head><p>This section is taken from our previous paper <ref type="bibr" target="#b13">(Ghojogh et al., 2019)</ref>. Sometimes, the data are not fully observ-able. For example, the data are known to be whether zero or greater than zero. As an illustration, assume the data are collected for a particular disease but for convenience of the patients participated in the survey, the severity of the disease is not recorded but only the existence or non-existence of the disease is reported. So, the data are not giving us complete information as X i &gt; 0 is not obvious whether is X i = 2 or X i = 1000. In this case, MLE cannot be directly applied as we do not have access to complete information and some data are missing. In this case, Expectation Maximization (EM) <ref type="bibr" target="#b21">(Moon, 1996)</ref> is useful. The main idea of EM can be summarized in this short friendly conversation:</p><p>-What shall we do? The data are missing! The loglikelihood is not known completely so MLE cannot be used.</p><p>-Mmm, probably we can replace the missing data with something... -Aha! Let us replace it with its mean.</p><p>-You are right! We can take the mean of log-likelihood over the possible values of the missing data. Then everything in the log-likelihood will be known, and then... -And then we can do MLE! Assume D (obs) and D (miss) denote the observed data (X i 's = 0 in the above example) and the missing data (X i 's &gt; 0 in the above example). The EM algorithm includes two main steps, i.e., E-step and M-step. Let Θ be the parameter which is the variable for likelihood estimation. In the E-step, the log-likelihood (Θ), is taken expectation with respect to the missing data D (miss) in order to have a mean estimation of it. Let Q(Θ) denote the expectation of the likelihood with respect to D (miss) :</p><formula xml:id="formula_0">Q(Θ) := E D (miss) |D (obs) ,Θ [ (Θ)].</formula><p>(1)</p><p>Note that in the above expectation, the D (obs) and Θ are conditioned on, so they are treated as constants and not random variables.</p><p>In the M-step, the MLE approach is used where the loglikelihood is replaced with its expectation, i.e., Q(Θ); therefore:</p><formula xml:id="formula_1">Θ = arg max Θ Q(Θ).<label>(2)</label></formula><p>These two steps are iteratively repeated until convergence of the estimated parameters Θ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Lagrange Multiplier</head><p>Suppose we have a multi-variate function Q(Θ 1 , . . . , Θ K ) (called "objective function") and we want to maximize (or minimize) it. However, this optimization is constrained and its constraint is equality P (Θ 1 , . . . , Θ K ) = c where c is a constant. So, the constrained optimization problem is: maximize</p><formula xml:id="formula_2">Θ1,...,Θ K Q(Θ 1 , . . . , Θ K ), subject to P (Θ 1 , . . . , Θ K ) = c.</formula><p>(3) For solving this problem, we can introduce a new variable η which is called "Lagrange multiplier". Also, a new function L(Θ 1 , . . . , Θ K , η), called "Lagrangian" is introduced:</p><formula xml:id="formula_3">L(Θ 1 , . . . , Θ K , η) = Q(Θ 1 , . . . , Θ K ) -η P (Θ 1 , . . . , Θ K ) -c .<label>(4)</label></formula><p>Maximizing (or minimizing) this Lagrangian function gives us the solution to the optimization problem <ref type="bibr" target="#b5">(Boyd &amp; Vandenberghe, 2004)</ref>:</p><formula xml:id="formula_4">∇ Θ1,...,Θ K ,η L set = 0,<label>(5)</label></formula><p>which gives us:</p><formula xml:id="formula_5">∇ Θ1,...,Θ K L set = 0 =⇒ ∇ Θ1,...,Θ K Q = η∇ Θ1,...,Θ K P, ∇ η L set = 0 =⇒ P (Θ 1 , . . . , Θ K ) = c.</formula><p>2.3. Factor Graph, The Sum-Product and Max-Product Algorithms, and The Forward-Backward Procedure</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1.">FACTOR GRAPH</head><p>A factor graph is a bipartite graph where the two partitions of graph include functions (or factors) f j , ∀j and the variables x i , ∀i <ref type="bibr" target="#b17">(Kschischang et al., 2001;</ref><ref type="bibr" target="#b19">Loeliger, 2004)</ref>. Every factor is a function of two or more variables. The variables of a factor are connected by edges to it. Hence, we have a bipartite graph. Figure <ref type="figure" target="#fig_0">1</ref> shows an example factor graphs where the factor and variable nodes are represented by circles and squares, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2.">THE SUM-PRODUCT ALGORITHM</head><p>Assume we have a factor graph with some variable nodes and factor nodes. For this, one of the nodes is considered as the root of tree. The result of algorithm does not depend on which node to be taken as the root <ref type="bibr" target="#b2">(Bishop, 2006)</ref>. Usually, the first variable or the last one are considered as the root.</p><p>Then, some messages from the leaf (or leaves) are sent up on the tree to the root <ref type="bibr" target="#b2">(Bishop, 2006)</ref>. The messages can be initialized to any fixed constant values (see Eq. ( <ref type="formula" target="#formula_8">8</ref>)). The message from the variable node x i to its neighbor factor node f j is:</p><formula xml:id="formula_6">m xi→fj = f ∈N (xi)\fj m f →xi ,<label>(6)</label></formula><p>where f ∈ N (x i ) \ f j means all the neighbor factor nodes to the variable x i except f j . Also, m xi→fj and m f →xi denote the message from the variable x i to the factor f j and the message from the factor f to the variable x i , respectively. The message from a the factor node f j to its neighbor variable x i is:</p><formula xml:id="formula_7">m fj →xi = values of x∈N (fj )\xi f j x∈N (fj )\xi m x→fj . (7)</formula><p>The Eq. ( <ref type="formula" target="#formula_6">6</ref>) includes both sum and product. This is why this procedure is named the sum-product algorithm <ref type="bibr" target="#b17">(Kschischang et al., 2001;</ref><ref type="bibr" target="#b2">Bishop, 2006)</ref>. In the factor graph, if the variable x i has degree one, the message is:</p><formula xml:id="formula_8">m xi→fj = [1, 1, . . . , 1] ∈ R k , (<label>8</label></formula><formula xml:id="formula_9">)</formula><p>where k is the number of possible values that the variables can get. Moreover, if the variable x i has degree two connected to f j and f , the message is:</p><formula xml:id="formula_10">m xi→fj = m f →xi .<label>(9)</label></formula><p>A good example for better understanding of Eqs. ( <ref type="formula" target="#formula_6">6</ref>) and ( <ref type="formula">7</ref>) exists in chapter 8 in <ref type="bibr" target="#b2">(Bishop, 2006)</ref>.</p><p>For the exact convergence (and inference) of belief propagation, the graph should be a tree, i.e., should be cyclefree <ref type="bibr" target="#b17">(Kschischang et al., 2001)</ref>. If the factor graph has cycles, the inference is approximate where the algorithm is stopped after a while manually. The belief propagation in such graphs is called loopy belief propagation <ref type="bibr" target="#b22">(Murphy et al., 1999;</ref><ref type="bibr" target="#b2">Bishop, 2006)</ref>. Note that Markov chains are cycle-free so the exact belief propagation can be applied for them.</p><p>It is also noteworthy that as every variable which is connected to only two factor nodes merely passes the message through (see Eq. ( <ref type="formula" target="#formula_10">9</ref>)), a new graphical model, named normal graph <ref type="bibr" target="#b9">(Forney, 2001)</ref>, was proposed which states every variable as an edge rather than a node (vertex). More details about factor graph and sum-product algorithm can be found in references <ref type="bibr" target="#b17">(Kschischang et al., 2001;</ref><ref type="bibr" target="#b19">Loeliger, 2004)</ref> and chapter 8 of <ref type="bibr" target="#b2">(Bishop, 2006)</ref>. Also note that some alternatives to sum-product algorithm are min-product, maxproduct, and min-sum <ref type="bibr" target="#b2">(Bishop, 2006)</ref>. The max-product algorithm <ref type="bibr" target="#b32">(Weiss &amp; Freeman, 2001;</ref><ref type="bibr" target="#b24">Pearl, 2014)</ref> is similar to the sum-product algorithm where the summation operator is replaced by the maximum operator. In this algorithm, the messages from the variable nodes to the factor nodes and vice versa are:</p><formula xml:id="formula_11">m xi→fj = f ∈N (xi)\fj m f →xi ,<label>(10)</label></formula><formula xml:id="formula_12">m fj →xi = max values of x∈N (fj )\xi f j x∈N (fj )\xi m x→fj .<label>(11)</label></formula><p>2.3.4. BELIEF PROPAGATION WITH FORWARD-BACKWARD PROCEDURE In order to learn the beliefs over the variables and factor nodes, belief propagation can be applied using a forwardbackward procedure (see chapter 8 in <ref type="bibr" target="#b2">(Bishop, 2006)</ref>). The forward-backward algorithm using the sum-product sub-algorithm is shown in Algorithm 1. In the forwardbackward procedure, the belief over a random variable x i is the product of the forward and backward messages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Probabilistic Graphical Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Markov and Bayesian Networks</head><p>A Probabilistic Graphical Model (PGM) is a grpah-based representation of a complex distribution in the possibly high dimensional space <ref type="bibr" target="#b16">(Koller &amp; Friedman, 2009)</ref>. In other words, PGM is a combination of graph theory and probability theory. In a PGM, the random variables are represented by nodes or vertices. There exist edges between two variables which have interaction with one another in terms of probability. Different conditional probabilities can be represented by a PGM. There exist two types of PGM which are Markov network (also called Markov random field) and Bayesian network <ref type="bibr" target="#b16">(Koller &amp; Friedman, 2009)</ref>. In the Markov network and Bayesian network, the edges of graph are undirected and directed, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The Markov Property</head><p>Consider a times series of random variables X 1 , X 2 , . . . , X n . In general, the joint probability of these random variables can be written as:</p><formula xml:id="formula_13">P(X 1 ,X 2 , . . . , X n ) = P(X 1 ) P(X 2 | X 1 ) P(X 3 | X 2 , X 1 ) . . . P(X n | X n-1 , . . . , X 2 , X 1 ),<label>(12)</label></formula><p>according to chain (or multiplication) rule in probability. [The first order] Markov property is an assumption which states that in a time series of random variables X 1 , X 2 , . . . , X n , every random variable is merely dependent on the latest previous random variable and not the others. In other words:</p><formula xml:id="formula_14">P(X i | X i-1 , X i-2 , . . . , X 2 , X 1 ) = P(X i | X i-1 ). (13)</formula><p>Hence, with Markov property, the chain rule is simplified to:</p><formula xml:id="formula_15">P(X 1 , X 2 , . . . , X n ) = P(X 1 ) P(X 2 | X 1 ) P(X 3 | X 2 ) . . . P(X n | X n-1 ).<label>(14)</label></formula><p>The Markov property can be of any order. For example, in a second order Markov property, a random variable is dependent on the latest and one-to-latest variables. Usually, the default Markov property is of order one. A stochastic process which has the Markov property is called a Markovian process (or Markov process).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Discrete Time Markov Chain</head><p>A Markov chain is a PGM which has Markov property. The Markov chain can be either directed or undirected. Usually, Markov chain is a Bayesian network where the edges are directed. It is important not to confuse Markov chain with Markov network.</p><p>There are two types of Markov Chain which are Discrete Time Markov Chain (DTMC) <ref type="bibr" target="#b27">(Ross, 2014)</ref> and Continuous Time Markov Chain (CTMC) <ref type="bibr" target="#b18">(Lawler, 2018)</ref>. As it is obvious from their names, in DTMC and CTMC, the time of transitions from a random variable to another one is and is not partitioned into discrete slots, respectively. If the variables in a DTMC are considered as states, the DTMC can be viewed as a Finite-State Machine (FSM) or a Finite-State Automaton (FSA) <ref type="bibr" target="#b4">(Booth, 1967)</ref>. Also, note that the DTMC can be viewed as a Sub-Shifts of Finite Type in modeling dynamic systems <ref type="bibr" target="#b6">(Brin &amp; Stuck, 2002)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Hidden Markov Model (HMM)</head><p>HMM is a DTMC which contains a sequence of hidden variables (named states) in addition to a sequence of emitted observation symbols (outputs).</p><p>We have an observation sequence of length τ which is the number of clock times, t ∈ {1, . . . , τ }. Let n and m denote the number of states and observation symbols, respectively. We show the sets of states and possible observation symbols by S = {s 1 , . . . , s n } and O = {o 1 , . . . , o m }, respectively. We show being in state s i and in observation symbol o i at time t by s i (t) and o i (t), respectively. Let R n×n A = [a i,j ] be the state Transition Probability Matrix (TPM), where:</p><formula xml:id="formula_16">a i,j := P s j (t + 1) | s i (t) .<label>(15)</label></formula><p>We have:</p><formula xml:id="formula_17">n j=1 a i,j = 1. (<label>16</label></formula><formula xml:id="formula_18">)</formula><p>The Emission Probability Matrix (EPM) is denoted by as</p><formula xml:id="formula_19">R n×m B = [b i,j ] where: b i,j := P o j (t) | s i (t) ,<label>(17)</label></formula><p>which is the probability of emission of the observation symbols from the states. We have:</p><formula xml:id="formula_20">n j=1 b i,j = 1.<label>(18)</label></formula><p>Let the initial state distribution be denoted by the vector</p><formula xml:id="formula_21">R n π = [π 1 , . . . , π n ]</formula><p>where:</p><formula xml:id="formula_22">π i := P s i (1) ,<label>(19) and:</label></formula><p>n i=1</p><formula xml:id="formula_23">π i = 1,<label>(20)</label></formula><p>to satisfy the probability properties. An HMM model is denoted by the tuple λ = (π, A, B).</p><p>Assume that a sequence of states is generated by the HMM according to the TPM. We denote this generated sequence of states by S g := s g (1), . . . , s g (τ ) where s g (t) ∈ S, ∀t.</p><p>Likewise, a sequence of outputs (observations) is generated by the HMM according to EPM. We denote this generated sequence of output symbols by</p><formula xml:id="formula_24">O g := o g (1), . . . , o g (τ )</formula><p>where o g (t) ∈ O, ∀t.</p><p>We denote the probability of transition from state s g (t) to s g (t + 1) by a s g (t),s g (t+1) . So:</p><formula xml:id="formula_25">a s g (t),s g (t+1) := P(s g (t + 1) | s g (t)).<label>(21)</label></formula><p>Note that s g (t) ∈ S and s g (t + 1) ∈ S. We also denote:</p><formula xml:id="formula_26">π s g (i) := P(s g (i)).<label>(22)</label></formula><p>Likewise, we denote the probability of state s g (t) emitting the observation o g (t) by b s g (t),o g (t) . So:</p><formula xml:id="formula_27">b s g (t),o g (t) := P(o g (t) | s g (t)).<label>(23)</label></formula><p>Note that s g (t) ∈ S and o g (t) ∈ O.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Likelihood and Expectation Maximization in HMM</head><p>The EM algorithm can be used for analysis and training in HMM <ref type="bibr" target="#b21">(Moon, 1996)</ref>. In the following, we explain the details of EM for HMM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Likelihood</head><p>According to Fig. <ref type="figure" target="#fig_2">2</ref>, the likelihood of occurrence of the state sequence S g and the observation sequence O g is <ref type="bibr" target="#b11">(Ghahramani, 2001)</ref>:</p><formula xml:id="formula_28">L = P(S g , O g ) = P s g (1) P(next states | previous states) P(O g | S g ) = P s g (1) τ -1 t=1 P(s g (t + 1) | s g (t)) τ t=1 P(o g (t) | s g (t)) = π s g (1) τ -1 t=1 a s g (t),s g (t+1) τ t=1 b s g (t),o g (t) .<label>(24)</label></formula><p>The log-likelihood is:</p><formula xml:id="formula_29">= log(L) = log π s g (1) + τ -1 t=1 log(a s g (t),s g (t+1) ) + τ t=1 log(b s g (t),o g (t) ).<label>(25)</label></formula><p>Let 1 i be a vector with entry one at index i, i.e., 1 i := [0, 0, . . . , 0, 1, 0 . . . , 0] . Also, 1 s g (1) means the vector with entry one at the index of the first state in the sequence S g . For example, if there are three possible states and a sequence of length three,</p><formula xml:id="formula_30">s g (1) = 2, s g (2) = 1, s g (3) = 3, we have 1 s g (1) = [0, 1, 0] .</formula><p>The terms in this log-likelihood are:</p><formula xml:id="formula_31">log π s g (1) = 1 s g (1) log π,<label>(26)</label></formula><formula xml:id="formula_32">a s g (t-1),s g (t) = n i=1 n j=1 (a i,j ) 1i[i] 1j [j] , =⇒ log(a s g (t-1),s g (t) ) = n i=1 n j=1 1 i [i] 1 j [j] log(a i,j ) = 1 s g (t-1) (log A)1 s g (t) , (27) b s g (t),o g (t) = n i=1 n j=1 (b i,j ) 1i[i] 1j [j] , =⇒ log(b s g (t),o g (t) ) = n i=1 n j=1 1 i [i] 1 j [j] log(b i,j ) = 1 s g (t) (log B)1 o g (t) ,<label>(28)</label></formula><p>where</p><formula xml:id="formula_33">1 i [i] = 1 j [j] = 1.</formula><p>Hence, we can write the loglikelihood as:</p><formula xml:id="formula_34">= 1 s g (1) log π + τ -1 t=1 1 s g (t-1) (log A)1 s g (t) + τ t=1 1 s g (t) (log B)1 o g (t) .<label>(29)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">E-step in EM</head><p>The missing variables in the log-likelihood are 1 s g (1) , 1 s g (t-1) , 1 s g (t) , and 1 o g (t) . The expectation of the loglikelihood with respect to the missing variables is:</p><formula xml:id="formula_35">Q(π,A, B) = E( ) = E(1 s g (1) log π) + τ -1 t=1 E 1 s g (t) (log A)1 s g (t+1) + τ t=1 E 1 s g (t) (log B)1 o g (t) .<label>(30)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">M-step in EM</head><p>We maximize the Q(π, A, B) with respect to the parameters π i , a i,j , and b i,j :</p><formula xml:id="formula_36">maximize x Q(π, A, B) subject to n i=1 π i = 1, n j=1 a i,j = 1, ∀i ∈ {1, . . . , n}, n j=1 b i,j = 1, ∀i ∈ {1, . . . , n},<label>(31)</label></formula><p>where the constraints ensure that the probabilities in the initial states, the transition matrix, and the emission matrix add to one.</p><p>The Lagrangian <ref type="bibr" target="#b5">(Boyd &amp; Vandenberghe, 2004)</ref> for this optimization problem is:</p><formula xml:id="formula_37">L = E(1 s g (1) log π) + τ -1 t=1 E 1 s g (t) (log A)1 s g (t+1) + τ t=1 E 1 s g (t) (log B)1 o g (t) -η 1 ( n i=1 π i -1) -η 2 ( n j=1 a i,j -1) -η 3 ( n j=1 b i,j -1),<label>(32)</label></formula><p>where η 1 , η 2 , and η 3 are the Lagrange multipliers.</p><p>The first term in the Lagrangian is simplified to</p><formula xml:id="formula_38">E(1 s g (1) [1] log π 1 + • • • + 1 s g (1) [τ ] log π τ ) = E(1 s g (1) [1] log π 1 ) + • • • + E(1 s g (1)</formula><p>[τ ] log π τ ); therefore, we have:</p><formula xml:id="formula_39">∂L ∂π i = E(1 s g (1) [i]) -η 1 π i set = 0 =⇒ π i = 1 η 1 E(1 s g (1) [i]). (<label>33</label></formula><formula xml:id="formula_40">) n i=1 π i = 1 (33) =⇒ 1 η 1 E(1 s g (1) [1])+ + • • • + E(1 s g (1) [i]) + • • • + E(1 s g (1) [n]) = 1 η 1 (0 + • • • + 1 + • • • + 0) set = 1 =⇒ η 1 = 1. (34) ∴ π i = E(1 s g (1) [i]).<label>(35)</label></formula><p>Similarly, we have:</p><formula xml:id="formula_41">∂L ∂a i,j = τ -1 t=1 E(1 s g (t) [i] 1 s g (t+1) [j]) -η 2 a i,j set = 0 =⇒ a i,j = 1 η 2 τ -1 t=1 E(1 s g (t) [i] 1 s g (t+1) [j]). (<label>36</label></formula><formula xml:id="formula_42">) n j=1 a i,j = 1 (36) =⇒ 1 η 2 n j=1 τ -1 t=1 E(1 s g (t) [i] 1 s g (t+1) [j]) = = 1 η 2 τ -1 t=1 E(1 s g (t) [i] × 0) + . . . + E(1 s g (t) [i] × 1) s g (t+1)-th element + • • • + E(1 s g (t) [i] × 0) = 1 η 2 τ -1 t=1 E(1 s g (t) [i]) set = 1 =⇒ η 2 = τ -1 t=1 E(1 s g (t) [i]). (<label>37</label></formula><formula xml:id="formula_43">) ∴ a i,j = τ -1 t=1 E(1 s g (t) [i] 1 s g (t+1) [j]) τ -1 t=1 E(1 s g (t) [i]) . (<label>38</label></formula><formula xml:id="formula_44">)</formula><p>Likewise, we have:</p><formula xml:id="formula_45">∂L ∂b i,j = τ t=1 E(1 s g (t) [i] 1 o g (t) [j]) -η 3 b i,j set = 0 =⇒ b i,j = 1 η 3 τ t=1 E(1 s g (t) [i] 1 o g (t) [j]). (<label>39</label></formula><formula xml:id="formula_46">) n j=1 b i,j = 1 (39) =⇒ 1 η 3 n j=1 τ t=1 E(1 s g (t) [i] 1 o g (t) [j]) = = 1 η 3 τ t=1 E(1 s g (t) [i] × 0) + • • • + E(1 s g (t) [i] × 1) o g (t)-th element + • • • + E(1 s g (t) [i] × 0) = 1 η 3 τ t=1 E(1 s g (t) [i]) set = 1 =⇒ η 3 = τ t=1 E(1 s g (t) [i]). (<label>40</label></formula><formula xml:id="formula_47">) ∴ b i,j = τ t=1 E(1 s g (t) [i] 1 o g (t) [j]) τ t=1 E(1 s g (t) [i]) .<label>(41)</label></formula><p>In Section 7.1, we will simplify the statements for π i , a i,j , and b i,j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Evaluation in HMM</head><p>Evaluation in HMM means the following <ref type="bibr" target="#b26">(Rabiner &amp; Juang, 1986;</ref><ref type="bibr" target="#b25">Rabiner, 1989)</ref>: Given the observation sequence O g = o g (1), . . . , o g (τ ) and the HMM model λ = (π, A, B), we want to compute P(O g | λ), i.e., the probability of the generated observation sequence. In summary:</p><formula xml:id="formula_48">O g , given: λ =⇒ P(O g | λ) = ?<label>(42)</label></formula><p>Note that P(O g | λ) can also be denoted by P(O g ; λ). The P(O g | λ) is sometimes referred to as the likelihood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Direct Calculation</head><p>Assume that the state sequence S g = s g (1), . . . , s g (τ ) has caused the observation sequence O g = o g (1), . . . , o g (τ ). Hence, we have:</p><formula xml:id="formula_49">P(O g | S g , λ) = b s g (1),o g (1) b s g (2),o g (2) . . . b s g (τ ),o g (τ ) .<label>(43)</label></formula><p>On the other hand, the probability of the state sequence S g = s g (1), . . . , s g (τ ) is:</p><formula xml:id="formula_50">P(S g | λ) = π s g (1) a s g (1),s g (2) . . . a s g (τ -1),s g (τ ) . (<label>44</label></formula><formula xml:id="formula_51">)</formula><p>According to chain rule, we have:</p><formula xml:id="formula_52">P(O g , S g | λ) = P(O g | S g , λ) P(S g | λ) = (45) π s g (1) b s g (1),o g (1) a s g (1),s g (2) . . . b s g (τ ),o g (τ ) a s g (τ -1),s g (τ ) ,<label>(46)</label></formula><p>which is the probability of occurrence of both the observation sequence O g and state sequence S g . Any state sequence may have caused the observation sequence O g . Therefore, according to the law of total probability, we have:</p><formula xml:id="formula_53">P(O g | λ) = ∀S g P(O g , S g | λ) (45) = ∀S g P(O g | S g , λ) P(S g | λ) (46) = ∀s g (1) ∀s g (2) • • • ∀s g (τ ) π s g (1) b s g (1),o g (1) a s g (1),s g (2)</formula><p>. . . b s g (τ ),o g (τ ) a s g (τ -1),s g (τ ) , (47</p><formula xml:id="formula_54">)</formula><p>which means that we start with the first state, then output the first observation, and then go to the next state. This procedure is repeated until the last state. The summations are over all possible states in the state sequence.</p><p>The time complexity of this direct calculation of</p><formula xml:id="formula_55">P(O g | λ)</formula><p>is in the order of O(2τ n τ ) because at every time clock t ∈ {1, . . . , τ }, there are n possible states to go through <ref type="bibr" target="#b26">(Rabiner &amp; Juang, 1986)</ref>. Because of n τ , this is very inefficient especially for long sequences (large τ ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">The Forward-Backward Procedure</head><p>A more efficient algorithm for evaluation in HMM is forward-backward procedure <ref type="bibr" target="#b26">(Rabiner &amp; Juang, 1986)</ref>.</p><p>The forward-backward procedure includes two stages, i.e., forward and backward belief propagation stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">THE FORWARD BELIEF PROPAGATION</head><p>Similar to the belief propagation procedure, we define the forward message until time t as:</p><formula xml:id="formula_56">α i (t) := P o g (1), o g (2), . . . , o g (t), s g (t) = s i | λ ,<label>(48)</label></formula><p>which is the probability of partial observation sequence until time t and being in state s i at time t.</p><p>Algorithm 2 shows the forward belief propagation from state one to the state τ . In this algorithm, α i (t) is solved inductively. The initial forward message is:</p><formula xml:id="formula_57">α i (1) = π i b i,o g (1) , ∀i ∈ {1, . . . , n},<label>(49)</label></formula><p>which is the probability of occurrence of the initial state s i and the observation symbol o g (1). The next forward messages are calculated as:</p><formula xml:id="formula_58">α j (t + 1) = n i=1 α i (t) a i,j b j,o g (t+1) , (<label>50</label></formula><formula xml:id="formula_59">)</formula><formula xml:id="formula_60">1 Input: λ = (π, A, B) 2 α i (1) = π i b i,o g (1)</formula><p>, ∀i ∈ {1, . . . , n} 3 for state j from 1 to n do 4 for time t from 1 to (τ -1) do 5</p><formula xml:id="formula_61">α j (t + 1) = n i=1 α i (t) a i,j b j,o g (t+1) 6 P(O g | λ) = n i=1 α i (τ ) 7 Return P(O g | λ), ∀i, ∀t : α i (t)</formula><p>Algorithm 2: The forward belief propagation in the forward-backward procedure which is the probability of occurrence of observation sequence o g (1), . . . , o g (t), being in state s i at time t, going to state j at time t+1, and the observation symbol o g (t+1). The summation is because, at time t, the state s g (t) can be any state so we should use the law of total probability. Proposition 1. The Eq. ( <ref type="formula" target="#formula_58">50</ref>) can be interpreted as the sumproduct algorithm (see Section 2.3.2).</p><p>Proof. The Algorithm 2 has iterations over states indexed by j. Also the Eq. ( <ref type="formula" target="#formula_58">50</ref>) has sum over states index by i. Consider all states indexed by i and a specific state s j (see Fig. <ref type="figure" target="#fig_3">3</ref>). We can consider every two successive states as a factor node in the factor graph. Hence, a state indexed by i and the state s j form a factor node which we denote by f i,j . The observation symbol o g (t + 1) emitted from s j is considered as the variable node in the factor graph. The message α i (t) is the message received to the factor node f i,j so far. Therefore, in the sum-product algorithm (see Eq. ( <ref type="formula" target="#formula_6">6</ref>)), we have:</p><formula xml:id="formula_62">m o g (t)→f i,j = α i (t).<label>(51)</label></formula><p>The message a i,j b j,o g (t+1) is the message received from the factor nodes f i,j , ∀i to the variable node o g (t + 1). Therefore, in the sum-product algorithm (see Eq. ( <ref type="formula">7</ref>)), we have:</p><formula xml:id="formula_63">m f i,j →o g (t+1) = a i,j b j,o g (t+1) .<label>(52)</label></formula><p>Hence:</p><formula xml:id="formula_64">m f →o g (t+1) = n i=1 m f i,j →o g (t+1) m o g (t)→f i,j next (53) = n i=1 a i,j b j,o g (t+1) α i (t) = n i=1 α i (t) a i,j b j,o g (t+1) . (<label>54</label></formula><formula xml:id="formula_65">)</formula><p>where f is the set of all factor nodes, {f i,j } n i=1 , and f i,j next is the factor f i,j in the next time slot. Note that if we consider Fig. <ref type="figure" target="#fig_3">3</ref> for all states indexed by j, a lattice network is formed.</p><p>Finally, using the law of total probability, we have:</p><formula xml:id="formula_66">P(O g | λ) = n i=1 P O g , s g (τ ) = s i | λ = n i=1 α i (τ ),<label>(55)</label></formula><p>which is the desired probability in the evaluation for HMM. Hence, the forward belief propagation suffices for evaluation. In the following, we explain the backward belief propagation which is required for other sections of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">THE BACKWARD BELIEF PROPAGATION</head><p>Again similar to the belief propagation procedure, we define the backward message since time τ to t + 1 as:</p><formula xml:id="formula_67">β i (t) := P o g (t + 1), o g (t + 2), . . . , o g (τ ) | s g (t) = s i , λ ,<label>(56)</label></formula><p>which is the probability of partial observation sequence from t + 1 until the end time τ given being in state s i at time t. Algorithm 3 shows the backward belief propagation. In this algorithm, the initial backward message is:</p><formula xml:id="formula_68">β i (τ ) = 1, ∀i ∈ {1, . . . , n}.<label>(57)</label></formula><p>The next backward messages are calculated as: 6 Return ∀i, ∀t :</p><formula xml:id="formula_69">β i (t) = n j=1 a i,j b j,o g (t+1) ,<label>(58)</label></formula><formula xml:id="formula_70">β i (t)</formula><p>Algorithm 3: The backward belief propagation in the forward-backward procedure which is the probability of being in state s i at time t, going to state j at time t + 1, and the observation symbol o g (t + 1). The summation is because, at time t + 1, the state s g (t + 1) can be any state so we should use the law of total probability. It is noteworthy that for very long sequences, the α i (t) and β(t) become extremely small, recursively (see Algorithms 2 and 3). Hence, some people normalize them at every iteration of algorithm <ref type="bibr" target="#b11">(Ghahramani, 2001)</ref>:</p><formula xml:id="formula_71">α i (t) ← α i (t) τ j=1 α j (t) ,<label>(59)</label></formula><formula xml:id="formula_72">β i (t) ← β i (t) τ j=1 β j (t) ,<label>(60)</label></formula><p>in order to sum to one. However, note that if this normalization is done, we will have P(O g | λ) = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3.">TIME COMPLEXITY</head><p>The time complexity of the forward belief propagation is in the order of O(τ n 2 ) because we have O(nτ ) loops each of which includes summation over n states <ref type="bibr" target="#b26">(Rabiner &amp; Juang, 1986)</ref>. This is much more efficient than the complexity of direct calculation of P(O g | λ) which was O(2τ n τ ). Similarly, the time complexity of the backward belief propagation is in the order of O(τ n 2 ) <ref type="bibr" target="#b26">(Rabiner &amp; Juang, 1986</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Estimation in HMM</head><p>Estimation in HMM means the following <ref type="bibr" target="#b26">(Rabiner &amp; Juang, 1986;</ref><ref type="bibr" target="#b25">Rabiner, 1989)</ref>: Given the observation sequence O g = o g 1 , . . . , o g τ and the HMM model λ = (π, A, B), we want to compute or estimate P(S g | O g , λ), i.e., the probability of a state sequence given an observation sequence. In summary:</p><formula xml:id="formula_73">S g , given: O g , λ =⇒ P(S g | O g , λ) = ?</formula><p>(61)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Greedy Approach</head><p>Let the probability of being in state s i at time t given the observation sequence O g and the HMM model λ be denoted by:</p><formula xml:id="formula_74">γ i (t) := P s g (t) = s i | O g , λ .<label>(62)</label></formula><p>We can say:</p><formula xml:id="formula_75">γ i (t) P(O g | λ) = P s g (t) = s i | O g , λ P(O g | λ) (a) = P s g (t) = s i , O g , λ (b) = α i (t) β i (t) =⇒ γ i (t) = α i (t) β i (t) P(O g | λ) (63) = α i (t) β i (t) n j=1 α j (t) β j (t) ,<label>(64)</label></formula><p>where (a) is because of chain rule in probability and (b) is because:</p><formula xml:id="formula_76">α i (t) β i (t) (48),(56) = = P o g (1), o g (2), . . . , o g (t), s g (t) = s i | λ × P o g (t + 1), o g (t + 2), . . . , o g (τ ) | s g (t) = s i , λ = P o g (1), o g (2), . . . , o g (τ ), s g (t) = s i , λ = P O g , s g (t) = s i , λ .</formula><p>The reason of (b) can also be interpreted in this way: it is because of the forward-backward procedure which states the belief over a variable as product of the forward and backward messages (see Section 2.3.4). Note that we have n i=1 γ i (t) = 1. Also, note that P(O g | λ) in Eq. ( <ref type="formula">63</ref>) can be obtained from either the denominator of Eq. ( <ref type="formula" target="#formula_75">64</ref>) or line 6 in Algorithm 2 (i.e., Eq. ( <ref type="formula" target="#formula_66">55</ref>)). In the greedy approach, at every time t, we select a state with maximum probability of occurrence without considering the other states in the sequence. Therefore, we have:</p><formula xml:id="formula_77">s g (t) = arg max 1≤i≤n γ i (t), ∀t ∈ {1, . . . , τ },<label>(65)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">The Viterbi Algorithm</head><p>The greedy approach does not optimize over the whole path but greedily chooses the best state at every time step. Another approach is to find the best state sequence which has the highest probability of occurrence, i.e., maximizing P(O g , S g | λ) <ref type="bibr" target="#b26">(Rabiner &amp; Juang, 1986)</ref>. The Viterbi algorithm <ref type="bibr" target="#b31">(Viterbi, 1967;</ref><ref type="bibr" target="#b8">Forney, 1973)</ref> can be used to find this path of states <ref type="bibr" target="#b3">(Blunsom, 2004)</ref>. Different works, such as <ref type="bibr" target="#b14">(He, 1988)</ref>, have worked on using Viterbi algorithm for HMM.</p><p>Algorithm 4 shows the Viterbi algorithm for estimation in HMM <ref type="bibr" target="#b26">(Rabiner &amp; Juang, 1986)</ref>. In this algorithm, we have variable δ j (t):</p><formula xml:id="formula_78">δ j (t) = max 1≤i≤n δ i (t -1) a i,j b j,o g (t) ,<label>(66)</label></formula><p>which is similar to α j (t) defined in Eq. ( <ref type="formula" target="#formula_58">50</ref>), except that α j (t) in the forward belief propagation uses sum-product algorithm <ref type="bibr" target="#b17">(Kschischang et al., 2001)</ref> while δ j (t) in the Viterbi algorithm uses max-product algorithm <ref type="bibr" target="#b32">(Weiss &amp; Freeman, 2001;</ref><ref type="bibr" target="#b24">Pearl, 2014)</ref> (see Sections 2.3.2 and 2.3.3).</p><p>1 Input: λ = (π, A, B) 2 // Initialization:  <ref type="formula" target="#formula_78">66</ref>) can be interpreted as the maxproduct algorithm (see Section 2.3.3).</p><formula xml:id="formula_79">3 δ i (1) = π i b i,o g (</formula><p>Proof. The Algorithm 4 has iterations over states indexed by j. Also the Eq. ( <ref type="formula" target="#formula_78">66</ref>) has maximum operator over states index by i. Consider all states indexed by i and a specific state s j (see Fig. <ref type="figure" target="#fig_5">4</ref>). The definitions of factor node and variable node in the factor graph are similar to the proof of Proposition 1.</p><p>The message δ i (t -1) is the message received to the factor node f i,j so far. Therefore, in the max-product algorithm (see Eq. ( <ref type="formula" target="#formula_11">10</ref>)), we have:</p><formula xml:id="formula_80">m o g (t-1)→f i,j = δ i (t -1). (<label>67</label></formula><formula xml:id="formula_81">)</formula><p>The message a i,j b j,o g (t) is the message received from the factor nodes f i,j , ∀i to the variable node o g (t). Therefore, in the max-product algorithm (see Eq. ( <ref type="formula" target="#formula_12">11</ref>)), we have:</p><formula xml:id="formula_82">m f i,j →o g (t) = a i,j b j,o g (t) .<label>(68)</label></formula><p>Hence:</p><formula xml:id="formula_83">m f →o g (t) = max 1≤i≤n m f i,j →o g (t) m o g (t)→f i,j next (69) = max 1≤i≤n a i,j b j,o g (t) δ i (t -1) = max 1≤i≤n δ i (t -1) a i,j b j,o g (t) . (<label>70</label></formula><formula xml:id="formula_84">)</formula><p>where f is the set of all factor nodes, {f i,j } n i=1 , and f i,j next is the factor f i,j in the next time slot. Note that if we consider Fig. <ref type="figure" target="#fig_5">4</ref> for all states indexed by j, a lattice network is formed which is common in Viterbi algorithm (see <ref type="bibr" target="#b26">(Rabiner &amp; Juang, 1986)</ref>). Similar to Eq. ( <ref type="formula" target="#formula_57">49</ref>), the initial δ j (t) is:</p><formula xml:id="formula_85">δ i (1) = π i b i,o g (1) , ∀i ∈ {1, . . . , n}.<label>(71)</label></formula><p>We define the index maximizing in Eq. ( <ref type="formula" target="#formula_78">66</ref>) as:</p><formula xml:id="formula_86">ψ j (t) = arg max 1≤i≤n δ i (t -1) a i,j .<label>(72)</label></formula><p>Then, a backward analysis is done starting from the end of state sequence:</p><formula xml:id="formula_87">p * = max 1≤i≤n δ i (τ ),<label>(73)</label></formula><formula xml:id="formula_88">s * (τ ) = arg max 1≤i≤n δ i (τ ),<label>(74)</label></formula><p>and the other states in the sequence are backtracked as:</p><formula xml:id="formula_89">s * (t) = ψ s * (t+1) (t + 1).<label>(75)</label></formula><p>The states S g = s * (1), s * (2), . . . , s * (τ ) are the desired state sequence in the estimation. Therefore, the states in the state sequence are maximizing the forward belief propagation in a max-product setting. The probability of this path of states with maximum probability of occurrence is:</p><formula xml:id="formula_90">P(O g , S g | λ) = p * .<label>(76)</label></formula><p>Note that the Viterbi algorithm can be visualized using a trellis structure (see Appendix A in <ref type="bibr" target="#b15">(Jurafsky &amp; Martin, 2019)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Training the HMM</head><p>Training HMM means the following <ref type="bibr" target="#b26">(Rabiner &amp; Juang, 1986;</ref><ref type="bibr" target="#b25">Rabiner, 1989)</ref>: Given the observation sequence O g = o g 1 , . . . , o g τ , we want to adjust the HMM model parameters λ = (π, A, B) in order to maximize P(O g | λ). In summary:</p><formula xml:id="formula_91">given: O g , O, S =⇒ λ = arg max λ P(O g | λ). (77)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">The Baum-Welch Algorithm</head><p>We can solve for Eq. ( <ref type="formula">77</ref>) using maximum likelihood estimation using Expectation Maximization (EM). The Baum-Welch algorithm <ref type="bibr" target="#b1">(Baum et al., 1970)</ref> is the most wellknown method for training HMM. It makes use of the EM results. We define the probability of occurrence of a path being in states s i and s j , respectively, at times t and t + 1 by:</p><formula xml:id="formula_92">ξ i,j (t) := P s g (t) = s i , s g (t + 1) = s j | O g , λ . (78)</formula><p>We can say:</p><formula xml:id="formula_93">ξ i,j (t) P(O g | λ) = P s g (t) = s i , s g (t + 1) = s j | O g , λ P(O g | λ) (a) = P s g (t) = s i , s g (t + 1) = s j , O g , λ (b) = α i (t) a i,j b j,o g (t+1) β j (t + 1) =⇒ ξ i,j (t) = α i (t) a i,j b j,o g (t+1) β j (t + 1) P(O g | λ) (79) = α i (t) a i,j b j,o g (t+1) β j (t + 1) n r=1 n =1 α r (t) a r, b ,o g (t+1) β (t + 1) ,<label>(80)</label></formula><p>where (a) is because of chain rule in probability and (b) is because of the following: According to Eqs. ( <ref type="formula" target="#formula_56">48</ref>), ( <ref type="formula" target="#formula_16">15</ref>), (17), and (56), we have:</p><formula xml:id="formula_94">α i (t) = P(o g (1), . . . , o g (t), s g (t) = s i | λ), a i,j = P(s j (t + 1) | s i (t)), b j,o g (t+1) = P(o g (t + 1) | s j (t + 1)), β j (t + 1) = P(o g (t + 2), . . . , o g (τ ) | s g (t + 1) = s j , λ).</formula><p>Therefore, we have:</p><formula xml:id="formula_95">α i (t) a i,j (t) b j,o g (t+1) β j (t + 1) = P s g (t) = s i , s g (t + 1) = s j , O g , λ .</formula><p>Note that we have n i=1 n j=1 ξ i,j (t) = 1. Also, note that P(O g | λ) in Eq. ( <ref type="formula">79</ref>) can be obtained from either the denominator of Eq. ( <ref type="formula" target="#formula_93">80</ref>) or line 6 in Algorithm 2 (i.e., Eq. ( <ref type="formula" target="#formula_66">55</ref>)). In Eq. ( <ref type="formula">79</ref>), the terms α i (t), a i,j (t), b j,o g (t+1) , and β j (t + 1) stand for the probability of the first t observations ending in state s i at time t, the probability of transitioning from state s i (at time t) to state s j (at time t + 1), the probability of observing o g (t + 1) from state s j at time t + 1, and the probability of the remainder of the observation sequence, respectively. Now, recall the Eqs. ( <ref type="formula" target="#formula_40">35</ref>), (38), and (41) from EM algorithm for HMM. We write these equations again for convenience of the reader:</p><formula xml:id="formula_96">π i = E(1 s g (1) [i]), a i,j = τ -1 t=1 E(1 s g (t) [i] 1 s g (t+1) [j]) τ -1 t=1 E(1 s g (t) [i]) , b i,j = τ t=1 E(1 s g (t) [i] 1 o g (t) [j]) τ t=1 E(1 s g (t) [i])</formula><p>.</p><p>On the other hand, recall Eqs. ( <ref type="formula" target="#formula_74">62</ref>) and ( <ref type="formula">78</ref>) which are repeated here:</p><formula xml:id="formula_97">γ i (t) = P s g (t) = s i | O g , λ , ξ i,j (t) = P s g (t) = s i , s g (t + 1) = s j | O g , λ .</formula><p>Therefore, we can say:</p><formula xml:id="formula_98">E(1 s g (1) [i]) = γ i (1),<label>(81)</label></formula><formula xml:id="formula_99">E(1 s g (t) [i]) = γ i (t),<label>(82)</label></formula><formula xml:id="formula_100">E(1 s g (t) [i] 1 s g (t+1) [j]) = ξ i,j (t),<label>(83)</label></formula><formula xml:id="formula_101">E(1 s g (t) [i] 1 o g (t) [j]) = γ i (t), where o g (t) = j in γ i (t).<label>(84)</label></formula><p>Hence:</p><formula xml:id="formula_102">π i = γ i (1) (62) = P s g (1) = s i | O g , λ , ∀i ∈ {1, . . . , n},<label>(85)</label></formula><formula xml:id="formula_103">a i,j = τ -1 t=1 ξ i,j (t) τ -1 t=1 γ i (t)</formula><p>, ∀i, j ∈ {1, . . . , n},</p><formula xml:id="formula_104">b i,j = τ t=1, o g (t)=j γ i (t) τ t=1 γ i (t)<label>(86)</label></formula><p>, ∀i ∈ {1, . . . , n}, ∀j ∈ {1, . . . , m}.</p><p>With change of variable, we have:</p><formula xml:id="formula_105">b j,k = τ t=1, o g (t)=k γ j (t) τ t=1 γ j (t)</formula><p>, ∀j ∈ {1, . . . , n}, (87) ∀k ∈ {1, . . . , m}.</p><p>The algorithm is shown in Algorithm 5 <ref type="bibr" target="#b26">(Rabiner &amp; Juang, 1986)</ref>. In this algorithm the initial probabilities of being in state s i at time t = 1 is according to Eq. ( <ref type="formula" target="#formula_102">85</ref>). Then the a i,j is then calculated using Eq. ( <ref type="formula" target="#formula_104">86</ref>). According to counting in probability, it can also be interpreted as the ratio of the expected number of transitions from state s i to s j over the </p><formula xml:id="formula_106">π i ← π i / n j=1 π j 11 a i,j ← a i,j / n =1 a i, 12 b j,k ← b j,k / m =1 b j, 13 π = [π 1 , . . . , π n ] , A = [a i,j ], B = [b j,k ]</formula><p>, ∀i, j ∈ {1, . . . , n}, ∀k ∈ {1, . . . , m} 14 Return λ = (π, A, B)</p><p>Algorithm 5: The Baum-Welch algorithm for training the HMM expected number of transitions out of state s i . Finally, the b j,k is calculated using Eq. ( <ref type="formula">87</ref>). Likewise, using counting in probability, the b j,k can be interpreted as the ratio of the expected number of times being in state s j and seeing observation o k over the expected number of times being in state s j . Note that in the numerator of Eq. ( <ref type="formula">87</ref>), we have o g (t) = k in γ j (t), i.e., according to Eq. ( <ref type="formula" target="#formula_74">62</ref>), we have γ j (t) = P s g (t) = s j | o g (1), . . . , o g (t) = k, . . . , o g (τ ), λ . This means that according to Eq. ( <ref type="formula">63</ref>), we set o g (t) = k in Eqs. ( <ref type="formula" target="#formula_58">50</ref>) and (58), or in Algorithms 2 and 3, for calculation of γ j (t) in Eq. ( <ref type="formula">87</ref>). On the other hand, in line 8 in Algorithm 5, we are iterating over all the time slots t ∈ {1, . . . , τ }. Hence, in the numerator of Eq. ( <ref type="formula">87</ref>), we will have γ j (t) = P s g (t) = s j | o g (1) = k, . . . , o g (t) = k, . . . , o g (τ ) = k, λ . Therefore:</p><p>• For calculating the numerator of Eq. ( <ref type="formula">87</ref>), we use Eq.</p><p>(64) where:</p><formula xml:id="formula_107">α j (t + 1) = n i=1 α i (t) a i,j b j,k ,<label>(88)</label></formula><formula xml:id="formula_108">β i (t) = n j=1 a i,j b j,k ,<label>(89)</label></formula><p>in line 5 in Algorithm 2 and in line 5 in Algorithm 3, respectively. We use the obtained α i , ∀i, ∀t and β i (t), ∀i, ∀t for calculating the numerator of Eq. ( <ref type="formula" target="#formula_75">64</ref>).</p><p>• For calculating the denominator of Eq. ( <ref type="formula">87</ref>), we use Eq. ( <ref type="formula" target="#formula_75">64</ref>) where Algorithms 2 and 3 are used for calculating α i (t) and β i (t).</p><p>It is noteworthy that because of a possible error caused by computer, it is better to normalize the obtained HMM model parameters (see lines 10 to 12 in Algorithm 5) in order to make sure that Eqs. ( <ref type="formula" target="#formula_23">20</ref>), ( <ref type="formula" target="#formula_17">16</ref>), and (18) are satisfied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Training the HMM</head><p>We know that Algorithm 5 requires γ i (t) and ξ i,j (t). According to Eqs. ( <ref type="formula">63</ref>) and ( <ref type="formula">79</ref>), the γ i (t) and ξ i,j (t) also require α i (t) and β i (t). The α i (t) and β i (t) can be computed using Algorithms (2) and (3), respectively. Moreover, note that in calculating α i (t), β(i), and ξ i,j (t), we need some π i , a i,j , and b i,j from π, A, and B, respectively. Therefore, we need an initial λ = (π, A, B) to compute another λ = (π , A , B ) at the end according to Algorithm 5. Thus, we should fine tune λ = (π, A, B) iteratively. The obtained λ from the previous λ satisfies P(O g | λ ) ≥ P(O g | λ) <ref type="bibr" target="#b26">(Rabiner &amp; Juang, 1986</ref>) (see <ref type="bibr" target="#b0">(Baum &amp; Eagon, 1967)</ref> or <ref type="bibr" target="#b1">(Baum et al., 1970)</ref> for proof) so we have progress in convergence (note that P(O g | λ) is obtained for each iteration in line 6 in Algorithm 6). If we have several training sequences Q, indexed by q ∈ {1, . . . , |Q|}, we use one of the sequences at the first iteration, the second sequence at the second iteration, and so on. We repeat this procedure until convergence which is reached when there is no significant change in λ. Algorithm 6 shows how to train an HMM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Usage of HMM in Applications</head><p>In Section 1, we introduced some application of HMM in speech recognition <ref type="bibr" target="#b26">(Rabiner &amp; Juang, 1986;</ref><ref type="bibr" target="#b25">Rabiner, 1989;</ref><ref type="bibr" target="#b10">Gales et al., 2008)</ref>, action recognition <ref type="bibr" target="#b33">(Yamato et al., 1992;</ref><ref type="bibr" target="#b12">Ghojogh et al., 2017)</ref>, and face recognition <ref type="bibr" target="#b23">(Nefian &amp; Hayes, 1998;</ref><ref type="bibr" target="#b29">Samaria, 1994)</ref>. Here, we explain the applications in speech and action recognition in more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.">Application in Speech Recognition</head><p>Assume we have a dictionary of words consisting of |W| words. For every word indexed by w ∈ {1, . . . , |W|}, we have |Q w | training instances spoken by one or several people. The training instances for a word are indexed by q where q ∈ {1, . . . , |Q w |}. Every training instance is a sequence of observation symbols obtained from formants <ref type="bibr" target="#b30">(Titze &amp; Martin, 1998)</ref>. We consider an HMM model for every word in the dictionary. Training the HMMs are as <ref type="bibr" target="#b26">(Rabiner &amp; Juang, 1986;</ref><ref type="bibr" target="#b25">Rabiner, 1989)</ref>:</p><p>1. For every word w i , consider the training sequences, O g w , indexed by q, i.e., o g 1 , . . . , o g |Qw| .</p><p>2. Train the HMM for the w-th word using Algorithm 6 to obtain λ w .</p><p>For an unknown test word with sequence O g t = o g 1 , . . . , o g |Qt| , we recognize the word as <ref type="bibr" target="#b26">(Rabiner &amp; Juang, 1986;</ref><ref type="bibr" target="#b25">Rabiner, 1989)</ref>:  <ref type="formula" target="#formula_66">55</ref>)).</p><formula xml:id="formula_109">1 Input: {O g = o g 1 , . . . , o g τ } |Q| q=1 2 Initialize λ = (π, A, B) where n i=1 π i = 1, n j=1 a i,j = 1, n j=1 b i,j =</formula><p>2. The test word is recognized as:</p><formula xml:id="formula_110">w * = arg max w P(O g t | λ w ).<label>(90)</label></formula><p>So, the test word is recognized as the w-th word in the dictionary.</p><p>In test phase for speech recognition, usually, the Viterbi algorithm is used <ref type="bibr" target="#b25">(Rabiner, 1989)</ref>. Hence, another approach for recognition of the test word O g t is: 1. Calculate P(O g t , S g t | λ w ) for all w ∈ {1, . . . , |W|} using the Viterbi algorithm, i.e., Algorithm 4 (see Eq. ( <ref type="formula" target="#formula_90">76</ref>)).</p><p>2. The test word is recognized as:</p><formula xml:id="formula_111">w * = arg max w P(O g t , S g t | λ w ).<label>(91)</label></formula><p>So, the test word is recognized as the w-th word in the dictionary.</p><p>Note that the words are pronounced with different lengths (fast or slowly) by different people. As HMM is robust to different repetitions of states, the recognition of words with different pacing is possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.">Application in Action Recognition</head><p>In action recognition, every action can be seen as a sequence of poses where every pose may be repeated for several frames <ref type="bibr" target="#b12">(Ghojogh et al., 2017)</ref>. Hence, HMM can be used for action recognition <ref type="bibr" target="#b33">(Yamato et al., 1992)</ref>. Every training instance is a sequence of observation symbols where the symbols are the poses, e.g., sitting, standing, etc. An HMM is trained for every action <ref type="bibr" target="#b12">(Ghojogh et al., 2017;</ref><ref type="bibr" target="#b20">Mokari et al., 2018)</ref>. Training and testing HMMs for action recognition is the same as training and test phases explained for speech recognition.</p><p>The actions are performed with different sequence lengths (fast or slowly) by different people. As HMM is robust to different repetitions of states, the recognition of actions with different pacing is possible.</p><p>In action recognition, we have a dataset of actions consisting of several defined poses <ref type="bibr" target="#b12">(Ghojogh et al., 2017;</ref><ref type="bibr" target="#b20">Mokari et al., 2018)</ref>. For example, if the dataset includes three actions sit, stand, and turn, the format of actions is as follows:  <ref type="table" target="#tab_4">1</ref>. In some sequences of dataset, there are some noisy poses in the middle of sequences of correct poses for making a difficult instance. An example test dataset is also shown in Table <ref type="table" target="#tab_5">2</ref>. The three test sequences are different from the training sequences to check the generalizability of the HMM models. Three HMM models can be trained for the three actions in this dataset and then, the test action sequences can be fed to the HMM models to be recognized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Conclusion</head><p>In this paper, we explained the theory of HMM for evaluation, estimation, and training. We started with some required background, i.e., EM, factor graphs, sum-product and max-product algorithms, forward-backward propagation, Markov and Bayesian networks, Markov property, and DTMC. We then introduced HMM and detailed EM in HMM. Evaluation in HMM was explained in both direct calculation and forward-backward procedure. We introduced estimation in HMM using the greedy approach and the Viterbi algorithm. Training the HMM was also covered using the Baum-Welch algorithm based on EM algorithm. We also introduced speech and action recognition as two popular applications of HMM.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. (a) An example factor graph, and (b) its representation as a bipartite graph.</figDesc><graphic coords="2,345.24,67.06,158.40,175.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 2 depicts the structure of an HMM model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. A Hidden Markov Model</figDesc><graphic coords="5,64.44,67.06,215.99,129.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Modeling forward belief propagation for HMM as a sum-product algorithm in a factor graph.</figDesc><graphic coords="8,64.44,67.06,216.00,209.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1</head><label></label><figDesc>Input: λ = (π, A, B) 2 β i (τ ) = 1, ∀i ∈ {1, . . . , n} 3 for state i from 1 to n do 4 for time t from (τ -1) to 1 do 5 β i (t) = n j=1 a i,j b j,o g (t+1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Modeling Viterbi algorithm for HMM as a max-product algorithm in a factor graph. This figure is very similar to Fig. 3 where the difference is in the index of time.</figDesc><graphic coords="10,64.44,67.06,216.00,210.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>1) , ∀i ∈ {1, . . . , n} 4 ψ i (1) = 0, ∀i ∈ {1, . . . , n} 5 // Recursion: 6 for state j from 1 to n do</figDesc><table><row><cell>7</cell><cell>for time t from 2 to τ do</cell></row><row><cell>8</cell><cell>δ j (t) = max 1≤i≤n δ i (t-1) a i,j b j,o g (t)</cell></row><row><cell>9</cell><cell>ψ j (t) = arg max 1≤i≤n δ i (t -1) a i,j</cell></row><row><cell cols="2">10 // Termination:</cell></row><row><cell cols="2">11 p  *  = max 1≤i≤n δ i (τ )</cell></row><row><cell cols="2">12 s  *  (τ ) = arg max 1≤i≤n δ i (τ )</cell></row><row><cell cols="2">13 // Backtracking:</cell></row><row><cell cols="2">14 for time t from τ -1 to 1 do</cell></row><row><cell>15</cell><cell>s  *  (t) = ψ s  *  (t+1) (t + 1)</cell></row></table><note><p><p>16 P(O g , S g | λ) = p * 17 Return P(O g , S g | λ), S g = s * (1), . . . , s * (τ )</p>Algorithm 4: The Viterbi algorithm for estimation in HMM Proposition 2. The Eq. (</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>1</head><label></label><figDesc>Input: γ i (t), ξ i,j (t), ∀i, ∀j, ∀t 2 π i = γ i (1), ∀i ∈ {1, . . . , n} 3 for state i from 1 to n do</figDesc><table><row><cell>4</cell><cell cols="2">for state j from 1 to n do</cell><cell></cell></row><row><cell>5</cell><cell>a i,j =</cell><cell>τ -1 t=1 ξ i,j (t)/</cell><cell cols="2">τ -1 t=1 γ i (t)</cell></row><row><cell cols="3">6 for state j from 1 to n do</cell><cell></cell></row><row><cell>7</cell><cell cols="3">for observation k from 1 to m do</cell></row><row><cell>8</cell><cell>b j,k =</cell><cell cols="2">τ t=1, o g (t)=k γ j (t)/</cell><cell>τ t=1 γ j (t)</cell></row><row><cell cols="5">9 // Normalization, for computer error corrections:</cell></row><row><cell>10</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>|W|} using the forward belief propagation, i.e., Algorithm 2 (see Eq. (</figDesc><table><row><cell></cell><cell>1</cell></row><row><cell cols="2">3 while Convergence do</cell></row><row><cell>4</cell><cell>for each q from 1 to |Q| do</cell></row><row><cell>5</cell><cell>Consider the q-th training sequence</cell></row><row><cell>6</cell><cell>P(O g | λ), ∀i, ∀t : α i (t) ← Do</cell></row><row><cell></cell><cell>Algorithm 2 [input: λ]</cell></row><row><cell>7</cell><cell>∀i, ∀t : β i (t) ← Do Algorithm 3 [input:</cell></row><row><cell></cell><cell>λ]</cell></row><row><cell>8</cell><cell>∀i, ∀t : γ i (t) ← Eq. (63) [input: α, β]</cell></row><row><cell>9</cell><cell>∀i, ∀j, ∀t : ξ i,j (t) ← Eq. (79) [input: α,</cell></row><row><cell></cell><cell>β, λ]</cell></row><row><cell>10</cell><cell>λ ← Do Algorithm 5 [input: γ, ξ]</cell></row><row><cell>11</cell><cell>if change of λ is small then</cell></row><row><cell>12</cell><cell>Convergence ← True</cell></row></table><note><p>13 Return λ = (π, A, B) Algorithm 6: Training the HMM 1. Calculate P(O g t | λ w ) for all w ∈ {1, . . . ,</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 .</head><label>1</label><figDesc>Action Sequence t = 1 t = 2 t = 3 t = 4 t = 5 t = 6 t = 7 t = 8 t = 9 t = 10 t = 11 t = 12 An example training dataset for action recognition Action t = 1 t = 2 t = 3 t = 4 t = 5 t = 6 t = 7 t = 8 t = 9 t = 10 t = 11</figDesc><table><row><cell></cell><cell>1</cell><cell cols="3">stand stand stand</cell><cell>sit</cell><cell>sit</cell><cell>sit</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell></row><row><cell></cell><cell>2</cell><cell cols="2">stand stand</cell><cell>sit</cell><cell>sit</cell><cell>sit</cell><cell>sit</cell><cell>sit</cell><cell>sit</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell></row><row><cell>Sit</cell><cell>3</cell><cell cols="5">stand stand stand stand stand</cell><cell>sit</cell><cell>sit</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell></row><row><cell></cell><cell>4</cell><cell cols="5">stand stand stand stand stand</cell><cell>sit</cell><cell>sit</cell><cell>sit</cell><cell>sit</cell><cell>sit</cell><cell>×</cell><cell>×</cell></row><row><cell></cell><cell>5</cell><cell cols="5">stand stand stand stand stand</cell><cell>sit</cell><cell>sit</cell><cell>sit</cell><cell>sit</cell><cell>stand</cell><cell>sit</cell><cell>sit</cell></row><row><cell></cell><cell>1</cell><cell>sit</cell><cell>sit</cell><cell>sit</cell><cell cols="3">stand stand stand</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell></row><row><cell></cell><cell>2</cell><cell>sit</cell><cell>sit</cell><cell>sit</cell><cell>sit</cell><cell>sit</cell><cell>sit</cell><cell cols="2">stand stand</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell></row><row><cell>Stand</cell><cell>3</cell><cell>sit</cell><cell>sit</cell><cell cols="5">stand stand stand stand</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell></row><row><cell></cell><cell>4</cell><cell>sit</cell><cell>sit</cell><cell>sit</cell><cell>sit</cell><cell cols="5">stand stand stand stand stand</cell><cell>×</cell><cell>×</cell><cell>×</cell></row><row><cell></cell><cell>5</cell><cell>sit</cell><cell>sit</cell><cell>sit</cell><cell>sit</cell><cell cols="3">stand stand stand</cell><cell>sit</cell><cell>stand</cell><cell>stand</cell><cell>stand</cell><cell>×</cell></row><row><cell></cell><cell>1</cell><cell cols="3">stand stand stand</cell><cell>tilt</cell><cell>tilt</cell><cell>tilt</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell></row><row><cell></cell><cell>2</cell><cell cols="2">stand stand</cell><cell>tilt</cell><cell>tilt</cell><cell>tilt</cell><cell>tilt</cell><cell>tilt</cell><cell>tilt</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell></row><row><cell>Turn</cell><cell>3</cell><cell cols="5">stand stand stand stand stand</cell><cell>tilt</cell><cell>tilt</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell></row><row><cell></cell><cell>4</cell><cell cols="4">stand stand stand stand</cell><cell>tilt</cell><cell>tilt</cell><cell>tilt</cell><cell>tilt</cell><cell>tilt</cell><cell>tilt</cell><cell>×</cell><cell>×</cell></row><row><cell></cell><cell>5</cell><cell cols="4">stand stand stand stand</cell><cell>tilt</cell><cell>tilt</cell><cell>tilt</cell><cell>stand</cell><cell>tilt</cell><cell>tilt</cell><cell>tilt</cell><cell>×</cell></row><row><cell>Sit</cell><cell></cell><cell cols="3">stand stand stand</cell><cell>sit</cell><cell>sit</cell><cell>sit</cell><cell>sit</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell></row><row><cell cols="2">Stand</cell><cell>sit</cell><cell>sit</cell><cell>sit</cell><cell>sit</cell><cell cols="3">stand stand stand</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell></row><row><cell cols="2">Turn</cell><cell cols="4">stand stand stand stand</cell><cell>tilt</cell><cell>tilt</cell><cell cols="2">stand stand</cell><cell>tilt</cell><cell>tilt</cell><cell>tilt</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 .</head><label>2</label><figDesc>An example test dataset for action recognition Assume we have a set of actions denoted by W where the actions are indexed by w ∈ {1, . . . , |W|}. We have |Q w | training instances for every action where the training instances are indexed by q ∈ {1, . . . , |Q w |}.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>•</head><label></label><figDesc>Action sit: stand, stand . . . , stand    </figDesc><table /><note><p>stand , sit, sit . . . , sit sit • Action stand: sit, sit . . . , sit sit , stand, stand . . . , stand stand • Action turn: stand, stand . . . , stand stand , tilt, tilt . . . , tilt tilt where the actions are modeled as sequences of some poses, i.e., stand, sit, and tilt. The actions can have different lengths or pacing. An example training dataset with its instances is shown in Table</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>The authors hugely thank <rs type="person">Prof. Mehdi Molkaraie</rs>, <rs type="person">Prof. Kevin Granville</rs>, and <rs type="person">Prof. Mu Zhu</rs> whose courses partly covered the materials mentioned in this tutorial paper.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An inequality with applications to statistical estimation for probabilistic functions of Markov processes and to a model for ecology</title>
		<author>
			<persName><forename type="first">Leonard</forename><forename type="middle">E</forename><surname>Baum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Eagon</surname></persName>
		</author>
		<author>
			<persName><surname>Alonzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin of the American Mathematical Society</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="360" to="363" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains. The annals of mathematical statistics</title>
		<author>
			<persName><forename type="first">Leonard</forename><forename type="middle">E</forename><surname>Baum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ted</forename><surname>Petrie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Soules</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norman</forename><surname>Weiss</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1970">1970</date>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="164" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning</title>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Hidden Markov models</title>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Sequential machines and automata theory</title>
		<author>
			<persName><forename type="first">Taylor</forename><forename type="middle">L</forename><surname>Booth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1967">1967</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Convex optimization</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lieven</forename><surname>Vandenberghe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Introduction to dynamical systems</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Garrett</forename><surname>Stuck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">What is a hidden Markov model?</title>
		<author>
			<persName><forename type="first">Sean</forename><forename type="middle">R</forename><surname>Eddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature biotechnology</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">1315</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The Viterbi algorithm</title>
		<author>
			<persName><forename type="first">G</forename><surname>Forney</surname></persName>
		</author>
		<author>
			<persName><surname>David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="268" to="278" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Codes on graphs: Normal realizations</title>
		<author>
			<persName><forename type="first">G</forename><surname>Forney</surname></persName>
		</author>
		<author>
			<persName><surname>David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="520" to="548" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The application of hidden Markov models in speech recognition</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Gales</surname></persName>
		</author>
		<author>
			<persName><surname>Young</surname></persName>
		</author>
		<author>
			<persName><surname>Steve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends R in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="195" to="304" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An introduction to hidden Markov models and Bayesian networks</title>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hidden Markov models: applications in computer vision</title>
		<imprint>
			<publisher>World Scientific</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="9" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fisherposes for human action recognition using kinect sensor data</title>
		<author>
			<persName><surname>Ghojogh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammadzade</forename><surname>Benyamin</surname></persName>
		</author>
		<author>
			<persName><surname>Hoda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mozhgan</forename><surname>Mokari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Sensors Journal</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1612" to="1627" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><surname>Ghojogh</surname></persName>
		</author>
		<author>
			<persName><surname>Benyamin</surname></persName>
		</author>
		<author>
			<persName><surname>Ghojogh</surname></persName>
		</author>
		<author>
			<persName><surname>Aydin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Crowley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fakhri</forename><surname>Karray</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.06708</idno>
		<title level="m">Fitting a mixture distribution to data: tutorial</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Extended Viterbi algorithm for second order hidden markov process</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings] 9th International Conference on Pattern Recognition</title>
		<meeting>] 9th International Conference on Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1988">1988. 1988</date>
			<biblScope unit="page" from="718" to="720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Speech and Language Processing: An introduction to speech recognition, computational linguistics and natural language processing</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">H</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Pearson Prentice Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Probabilistic graphical models: principles and techniques</title>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nir</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Factor graphs and the sum-product algorithm</title>
		<author>
			<persName><forename type="first">Frank</forename><forename type="middle">R</forename><surname>Kschischang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName><surname>Loeliger</surname></persName>
		</author>
		<author>
			<persName><surname>Hans-Andrea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on information theory</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="498" to="519" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Introduction to stochastic processes</title>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">F</forename><surname>Lawler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Chapman and Hall/CRC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An introduction to factor graphs</title>
		<author>
			<persName><forename type="first">H-A</forename><surname>Loeliger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="28" to="41" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recognizing involuntary actions from 3d skeleton data using body states</title>
		<author>
			<persName><forename type="first">Mozhgan</forename><surname>Mokari</surname></persName>
		</author>
		<author>
			<persName><surname>Mohammadzade</surname></persName>
		</author>
		<author>
			<persName><surname>Hoda</surname></persName>
		</author>
		<author>
			<persName><surname>Ghojogh</surname></persName>
		</author>
		<author>
			<persName><surname>Benyamin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientia Iranica</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The expectation-maximization algorithm</title>
		<author>
			<persName><forename type="first">Todd</forename><forename type="middle">K</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal processing magazine</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="47" to="60" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Loopy belief propagation for approximate inference: An empirical study</title>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence</title>
		<meeting>the Fifteenth conference on Uncertainty in artificial intelligence</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="467" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hidden markov models for face recognition</title>
		<author>
			<persName><forename type="first">Ara</forename><forename type="middle">V</forename><surname>Nefian</surname></persName>
		</author>
		<author>
			<persName><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Monson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP&apos;98 (Cat. No. 98CH36181)</title>
		<meeting>the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP&apos;98 (Cat. No. 98CH36181)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="2721" to="2724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<title level="m">Probabilistic reasoning in intelligent systems: networks of plausible inference</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A tutorial on hidden markov models and selected applications in speech recognition. Proceedings of the IEEE</title>
		<author>
			<persName><forename type="first">Lawrence</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="257" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An introduction to hidden Markov models</title>
		<author>
			<persName><forename type="first">Lawrence</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biing-</forename><surname>Juang</surname></persName>
		</author>
		<author>
			<persName><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ASSP Magazine</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="16" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Introduction to probability models</title>
		<author>
			<persName><forename type="first">Sheldon</forename><forename type="middle">M</forename><surname>Ross</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Academic press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Hidden Markov models</title>
		<author>
			<persName><forename type="first">Sam</forename><surname>Roweis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Face recognition using hidden Markov models</title>
		<author>
			<persName><forename type="first">Ferdinando</forename><surname>Samaria</surname></persName>
		</author>
		<author>
			<persName><surname>Silvestro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<pubPlace>Cambridge, UK</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Cambridge</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Principles of voice production</title>
		<author>
			<persName><forename type="first">Ingo</forename><forename type="middle">R</forename><surname>Titze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">W</forename><surname>Martin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Acoustical Society of America</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Error bounds for convolutional codes and an asymptotically optimum decoding algorithm</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Viterbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="260" to="269" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On the optimality of solutions of the max-product belief-propagation algorithm in arbitrary graphs</title>
		<author>
			<persName><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="736" to="744" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Recognizing human action in time-sequential images using hidden Markov model</title>
		<author>
			<persName><forename type="first">Junji</forename><surname>Yamato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Ohya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenichiro</forename><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 1992 IEEE Computer Society conference on computer vision and pattern recognition</title>
		<meeting>1992 IEEE Computer Society conference on computer vision and pattern recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="379" to="385" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
