<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Effects of Moral-Emotional Behavior and Intentionality on Mind Attribution and Evaluation of Social Robots</title>
				<funder ref="#_swkU4s3">
					<orgName type="full">Deutsche Forschungsgemeinschaft (DFG, German Research Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Alexander</forename><surname>Leonhardt</surname></persName>
							<email>alex.leonhardt@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Martin</forename><surname>Maier</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Science of Intelligence</orgName>
								<orgName type="department" key="dep2">Research Cluster of Excellence</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rasha</forename><forename type="middle">Abdel</forename><surname>Rahman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Science of Intelligence</orgName>
								<orgName type="department" key="dep2">Research Cluster of Excellence</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Universität</forename><surname>Zu Berlin</surname></persName>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Humboldt-Universität zu Berlin</orgName>
								<address>
									<addrLine>Rudower Chaussee 18</addrLine>
									<postCode>12489</postCode>
									<settlement>Berlin, Email</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Effects of Moral-Emotional Behavior and Intentionality on Mind Attribution and Evaluation of Social Robots</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6CF55A4F3CA83644ED401FE8F001BB01</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-10-22T02:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Robots are generally not considered full moral agents and therefore not appropriate loci of moral responsibility; however, research suggests that people may attribute mental capacities associated with moral agency as well as moral responsibility to robots. We investigate the extent to which descriptions of social robots as minded agents or mechanical machines differentially impact moral judgments of robots' morally and emotionally relevant actions. In two experiments we considered cognitive and emotional aspects of moral judgments: mind attribution and moral evaluations as well as people's emotional response, a key component in accounts of moral responsibility. Participants ( N = 82) read and subsequently rated 42 stories about robots' actions, which differed in level of intentionality and moral-emotional quality of information. Although minded as opposed to mechanical descriptions positively influenced mind attribution and attributed moral responsibility, participants' emotional response and moral evaluations were largely dominated by the moral-emotional content of information. In addition, the presentation of an image alongside information negatively affected the influence of minded descriptions of robots on mind attribution. Our findings demonstrate that robots are ascribed moral responsibility even though by current theoretical accounts they are not suitable moral agents.</p><p>Robots that are described as minded are furthermore attributed greater agency and moral responsibility. We discuss the impact of our findings for theoretical discussions about the ethics of AI and robots.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Social robots are a class of embodied artificial intelligence (AI) that is used in direct contact with people in everyday contexts such as homes and places of work <ref type="bibr" target="#b13">(Fong, Nourbakhsh, &amp; Dautenhahn, 2003)</ref>. The use of such technology is predicted to increase in the near future and comes with the promise of a host of benefits to society <ref type="bibr" target="#b23">(Lutz et al., 2019)</ref>. Social robots may help reduce labor shortages in industries such as care and education and provide skills that are difficult or dangerous for humans to perform. On the other hand, there are a number of ethical concerns surrounding the use of these machines as social robots may be used in morally ambiguous situations and cause harm to people <ref type="bibr">(Ladak et al., 2023)</ref>. One of the challenges for the introduction of social robots therefore will be to get the best out of this promising technology whilst avoiding ethical compromises <ref type="bibr" target="#b6">(Borg et al., 2024)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>What if robots do bad things?</head><p>To this end, there has been interdisciplinary interest in questions concerning the moral responsibility of robots. As robots become more sophisticated and ubiquitous, it is also increasingly likely that they will on occasion cause harm to humans <ref type="bibr" target="#b6">(Borg et al., 2024;</ref><ref type="bibr" target="#b21">Lemley &amp; Casey, 2019)</ref>. When this occurs, who is held responsible? Are robots appropriate targets for blame? Do we blame robots, regardless of whether it is appropriate to do so? While moral philosophy, e.g. the fields of AI and robot ethics, deals with the question of appropriateness, cognitive sciences can make significant contributions by investigating people's beliefs and judgments about robots.</p><p>Ascriptions of moral responsibility are important social mechanisms; we want to be able to hold an appropriate agent accountable for doing wrong in order to disincentivize bad actions being repeated <ref type="bibr" target="#b6">(Borg et al., 2024)</ref>. When an inappropriate agent is held responsible, this impedes the desired impact of sanctions by creating a responsibility gap, a situation where no party is held sufficiently accountable <ref type="bibr" target="#b27">( Misselhorn, 2018;</ref><ref type="bibr" target="#b33">Sparrow, 2007)</ref> . People's intuitions about robots as potential moral agents and their judgements of them in morally relevant situations therefore need to be investigated and offer important insights for related theoretical questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Morality and the Mind</head><p>Moral responsibility is associated with a series of judgments that are held to have emotional as well as cognitive aspects <ref type="bibr">(Talbert, 2024;</ref><ref type="bibr">Tognazzini &amp; Coates, 2021)</ref>. Firstly, moral situations, such as moral transgressions, have been conjectured to involve an emotional response <ref type="bibr" target="#b38">(Wallace, 1994;</ref><ref type="bibr" target="#b34">Strawson, 2003)</ref>. And empirical findings suggest that moral transgressions do indeed invoke an emotional response <ref type="bibr" target="#b5">(Bigman et al., 2023;</ref><ref type="bibr">Ladak et al., 2023)</ref>. Secondly, people make cognitive evaluations about morally relevant situations. They may discern who is responsible for the situation and if there should be any type of repercussion <ref type="bibr" target="#b32">(Smart, 1961)</ref>. These evaluations may therefore also include suitability of praise or punishment. Finally, there must be someone-a moral agent-who can be held to account for the morally relevant situation.</p><p>What makes a moral agent? Most importantly, moral agents require a degree of autonomy <ref type="bibr" target="#b22">(Loh &amp; Loh, 2017;</ref><ref type="bibr">Tognazzini &amp; Coates, 2021)</ref>. A moral agent must have acted from their own volition and intended to commit the morally relevant action <ref type="bibr" target="#b16">(Gray et al., 2012)</ref>. Moral agency therefore entails a particular attitude towards or set of assumptions about an agent and their mental abilities, this is sometimes referred to as taking an intentional stance <ref type="bibr" target="#b12">(Fischer &amp; Ravizza, 1998;</ref><ref type="bibr" target="#b8">Chopra, 2011)</ref>. Only agents who can hold beliefs and make autonomous decisions based on their beliefs are typically taken to be moral agents <ref type="bibr">(Loh &amp; Loh, 2018;</ref><ref type="bibr">Talbert, 2024)</ref>. Indeed, intentionality, or mindedness, is a key component of people's moral intuitions <ref type="bibr" target="#b16">(Gray et al., 2012;</ref><ref type="bibr" target="#b9">Cushman, 2015)</ref>. By having a mind an agent becomes socially and morally relevant and consequently, they can be held accountable for their actions <ref type="bibr" target="#b1">(Abubshait &amp; Wiese, 2017)</ref>. In summary, for robots to be moral agents that can be morally responsible, they ought to be seen as having a mind.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robot Minds and Morality</head><p>Robots' mind status however is ambiguous <ref type="bibr">(Levillain &amp; Zibetti, 2017;</ref><ref type="bibr">Chesher &amp; Andreallo, 2021)</ref>. A number of studies suggest that people apply an intentional stance towards robots, i.e. they behave as if robots' actions were the result of intentional states and processes <ref type="bibr" target="#b19">(Krach et al., 2008;</ref><ref type="bibr" target="#b30">Riek et al., 2009;</ref><ref type="bibr" target="#b7">Ceh &amp; Vanman, 2018;</ref><ref type="bibr" target="#b1">Abubshait &amp; Wiese, 2017)</ref>. On the other hand, people generally do not believe that robots have minds and think that robots lack the type of cognitive abilities necessary for intentional action <ref type="bibr">(Kim &amp; Duhacheck, 2020)</ref>. There is a conflict between people's perceptions and intuitions of robots and their explicit beliefs about robots <ref type="bibr">(Maier et. al., 2024)</ref>. Robots may straddle the ontological boundary between minded and not minded agents <ref type="bibr" target="#b10">(De Graaf, 2016)</ref>. Whether or not robots are appropriate targets for the attribution of moral responsibility therefore remains unclear. While on theoretical grounds most experts would currently not consider robots to have the kind of mental abilities necessary to hold them accountable for their actions, people may have different intuitions when robots are taken to perform moral transgressions.</p><p>Reporting about AI-based technology in news media has become more frequent <ref type="bibr" target="#b28">(Ouchchy et al., 2020)</ref>. Simultaneously, as the capabilities of artificial systems improve, it also becomes increasingly tempting and likely that we use anthropomorphic language to describe their behavior. We use words such as "belief" and "thoughts" that imply intentionality to facilitate descriptions of machines' behavior even though the underlying mechanisms are categorically different from those typically associated with intentionality <ref type="bibr">(Shanahan, 2023)</ref>.</p><p>There is a concern that media coverage may adopt this common use of anthropomorphic language to describe machines <ref type="bibr">(Shanahan, 2023)</ref>. It is therefore theoretically interesting if descriptions of robots' actions can also induce mind attribution and affect moral evaluations.</p><p>Since most people do not yet interact with robots directly, news articles may be one of the main ways that an average person engages with robots and the level of mind that the language used conveys may determine how people evaluate robots and their actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Research Background</head><p>The three judgments associated with moral responsibility-emotional response, mind attribution and moral evaluations-have been independently investigated regarding the moral status of robots. For instance, <ref type="bibr" target="#b5">Bigman et al. (2023)</ref> have investigated people's differential emotional response to human and robot moral transgressions. <ref type="bibr" target="#b15">Gray et al. (2007)</ref> compared mind attribution between a number of different agents including robots. <ref type="bibr">Maier et al. (2024)</ref> used EEG to test whether people apply an intentional stance towards robots, this implies attributing mental states to robots. Other empirical studies focused on moral evaluations such as responsibility, blameworthiness or wrongness of artificial agents <ref type="bibr">(Ladak et al., 2023)</ref>. So far, there have been no empirical investigations bringing these three dimensions of moral judgements together.</p><p>Empirical investigations using descriptions of robots often use typical moral dilemma situations in which robots make decisions that participants must evaluate. For instance, <ref type="bibr" target="#b25">Malle et al. (2015)</ref> compared moral judgments of robots compared with humans, when robots or humans made utilitarian decisions to sacrifice one rail worker to save four others, a version of the trolley dilemma thought experiments <ref type="bibr" target="#b14">(Foot, 1967)</ref> . While this type of research is important for getting at underlying, abstract principles of AI and robot ethics, they tell us little about how we might judge robots in real life scenarios. Furthermore, experiments typically focus on one specific type of robot with one primary function, while the range of AI and their functions is broad <ref type="bibr">(Ladak et al., 2023)</ref>. Because robots differ greatly in appearance and abilities, results in one study may not apply to other types of robots. Besides investigating multiple aspects of moral responsibility, the present study uses a variety of different scenarios of robots with different abilities and, in Experiment 2, appearances. It offers a realistic assessment of people's moral judgments of robots in the near future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Present Studies</head><p>We conducted two pre-registered experiments investigating how descriptions of robots as minded or mechanical beings affect people's moral judgments in situations in which robots perform morally good, bad or neutral actions. Given evidence that people judge morally relevant actions differently when they are performed by robots as opposed to when they are performed by humans, as well as evidence that mind plays a key role for moral judgments, we investigate the influence of mindedness (intentionality) on moral situations involving robots <ref type="bibr" target="#b16">(Gray et al., 2012;</ref><ref type="bibr" target="#b25">Malle et al., 2015)</ref>. We collected measures for emotional response, mind attribution and moral evaluations to achieve a holistic summary of people's moral judgments about robots.</p><p>In both experiments, participants read and subsequently rated 42 different stories involving robots drawn from current technological achievements and reflecting possible near-future applications. Using a within-subject design, we examined the effect of intentionality and moral-emotional content on participants' moral judgments of the robots, providing realistic evidence of people's possible responses to information (e.g. news media articles) about social robots. Fourteen stories each had a positive, neutral or negative outcome, reflecting something morally good, neutral or bad. Twentyone stories each were written as if the agent were an intentional, mindful being or a non-intentional, mechanical being respectively. The experiments were counterbalanced so that half of the participants read a particular story's intentional version and the other half read the story's non-intentional version. The second experiment differed from the first only in the addition of images of robots which were presented alongside the information.</p><p>Images, which were taken from abotdatabase.info, were of humanoid robots chosen to look similar and were counterbalanced so that the appearance could not differentially impact ratings between conditions <ref type="bibr" target="#b29">(Phillips et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1</head><p>Information examples and robot image example Note. Examples of representative stories for each of the three moral-emotional and two intentionality information conditions. In Experiment 2, images of real robots were presented just below the story and above the rating scales. The image in the center was made using Microsoft's image generator (DALL-E 3) and resembles the images that were used. The experiment's primary objective was to test whether intentionality affected the perceived moral responsibility of embodied AI when agents performed morally relevant actions.</p><p>We predicted that moral responsibility ratings for high intentionality stories would be higher than for low intentionality stories in the negative and positive conditions (i.e. when the stories have a bad or good outcome). Furthermore, we predicted that intentionality affects the emotional response to and perceived moral wrongness of artificial agents' morally relevant actions. In an ideal sense, moral wrongness is independent of moral agency and aligns more closely with the emotional valence of a situation <ref type="bibr">(Williston, 2006)</ref>. In practice, moral wrongness may vary depending on agency characteristics <ref type="bibr" target="#b25">(Malle, 2015)</ref>. We therefore predicted that moral wrongness ratings for high intentionality negative stories would be higher than for low intentionality negative stories. We also expected this to be reflected in participants' valence and arousal ratings.</p><p>We expected that valence ratings in response to high intentionality negative stories would be more negative than in response to low intentionality negative stories and that valence ratings in response to high intentionality positive stories would be more positive than in response to low intentionality positive stories. We further expected that arousal ratings in response to high intentionality negative and positive stories would be higher than in response to low intentionality negative and positive stories.</p><p>According to theoretical accounts of moral judgments, besides making appraisals of moral responsibility and wrongness, people may also wish to hold someone accountable by desiring disciplinary action (e.g. punishment for moral deviance) <ref type="bibr" target="#b6">(Borg et al., 2024;</ref><ref type="bibr">Talbert, 2024)</ref>. Consequently, we expected intentionality to affect desired disciplinary action and endorsement of embodied AI when agents perform negative actions. Specifically, we predicted that participants' support for punishing an agent would be greater for high intentionality negative stories than for low intentionality negative stories. Further, we predicted that participants' endorsement for continuing the use of an agent would be lower for high intentionality negative stories than for low intentionality negative stories. In other words, we expected participants' ratings to reflect a desire to punish and sanction a technology when the described scenarios have a negative outcome, and more so if the agent was described as an intentional being. We expected the opposite outcome for positive stories (i.e. ratings should not reflect a desire to punish but should reflect a desire to endorse continuing the use of an agent, which is respectively enhanced by intentionality).</p><p>Finally, there is evidence that scenarios with negative outcomes increase mind perception <ref type="bibr" target="#b11">(Feltz, 2007;</ref><ref type="bibr" target="#b18">Knobe, 2003)</ref>. This may be because people seek an intentional agent when there is a negative outcome (completing a moral dyad of harmful agent and suffering patient) <ref type="bibr" target="#b16">(Gray et al., 2012)</ref>. We therefore hypothesized that negative moral-emotional content would affect mind perception. Specifically, we expected agency and experience ratings for negative stories to be higher than for neutral or positive stories. This final hypothesis reflects a secondary line of inquiry; rather than looking at the influence of intentionality on moral judgment, it investigates the converse trajectory. This line of inquiry is more speculative and may provide insights into the influence of higher order moral judgments on lower order intuitions about mindedness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results Experiment 1</head><p>In an online experiment, we investigated the influence of moral-emotional content and level of intentionality in information about robots' actions on participants' emotional response as well as on their attributions of mind and moral evaluations of the robots. We used linear mixed effects models (LMMs) with fixed effects coded as sliding difference contrasts to analyze data sets collected from 40 German-speaking participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2</head><p>Rating Results for Emotional Response in Experiment 1 Note. Rating results of participants' experienced emotional response (valence and arousal) to the information about robots' actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Emotional Response</head><p>Participants read 42 two-sentence stories about robots and immediately after reading each story, they rated their emotional response to robots' actions that were described in the story.</p><p>Participants first evaluated the valence and then the arousal of their emotional state in response to the robots' actions. As expected, valence ratings corresponded to the moral-emotional content of the information (see Table <ref type="table">1</ref>, Fig. <ref type="figure">2</ref>) . That is, valence ratings in response to negative and positive condition information were significantly more negative or positive respectively than in response to neutral condition information. Contrary to our expectations, intentionality did not affect the participants' ratings of the valence of their emotional states in response to the robots' actions. Similarly, differences in participants' evaluations of their arousal response were highly significant between the negative and neutral as well as between the positive and neutral conditions (see Table <ref type="table">1</ref>, Fig. <ref type="figure">2</ref>) . But again, intentionality did not affect arousal ratings. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results of Linear Mixed Models Analyses of Emotional Response Ratings in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Mind attribution</head><p>After emotional response ratings, participants rated three items associated with two dimensions of mind, agency and experience, as well as the imagined human-likeness of each robot. As expected, the information's intentionality affected evaluations of robots on both dimensions of mind (see Table <ref type="table">2</ref>, Fig. <ref type="figure">3</ref>) . Robots that were paired with information suggesting a high level of intentionality were rated to be higher in agency and experience than robots that were paired with information suggesting a low level of intentionality. The moral-emotional content of information also differentially affected mind attribution; however, contrary to expectations, only the positive condition differed significantly from the neutral condition; this was the case both for agency and experience ratings (see Table <ref type="table">2</ref>, Fig. <ref type="figure">3</ref>) . There were no significant interactions between intentionality and moral-emotional content. Furthermore, participants' ratings indicated that they imagined that robots paired with high intentionality information had a more human-like appearance than robots paired with low intentionality information (see Table <ref type="table">2</ref>, Fig. <ref type="figure">3</ref>) . Moral-emotional content did not affect human-likeness ratings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 3</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rating Results for Mind Attribution in Experiment 1</head><p>Note. Rating results of mind attributed to robots by participants (agency, experience and human-likeness). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results of Linear Mixed Models Analyses of Mind Attribution Ratings in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Moral evaluation</head><p>Finally, before reading the consecutive story, participants rated four items associated with morality. Moral evaluations of the robots' actions were affected both by the intentionality and the moral-emotional content of information (see Table <ref type="table">3</ref>, Fig. <ref type="figure">4</ref>) . As predicted, robots that were paired with high intentionality information were evaluated to have more moral responsibility for their actions than robots that were paired with low intentionality information. In addition, robots that were paired with positive or negative information were both evaluated to be more morally responsible in comparison to robots that were paired with neutral stories. However, contrary to expectations, there were no interactions between these two factors.</p><p>Participants also rated whether or not they endorsed a continued use of each robot.</p><p>Contrary to expectations, intentionality had a positive effect across all moral-emotional conditions (see Table <ref type="table">3</ref>, Fig. <ref type="figure">4</ref>) . Negative and positive information also respectively decreased and increased the endorsement in comparison to the robots paired with neutral information.</p><p>There were however no interactions of intentionality and moral-emotional content.</p><p>The actions of robots in the negative condition were evaluated to be significantly more morally wrong in comparison to the actions of robots in the neutral condition, whereas there was no difference between the neutral and positive conditions (see Table <ref type="table">3</ref>, Fig. <ref type="figure">4</ref>) . Intentionality did not affect participants' evaluations of the moral wrongness of the robots' actions. Similarly, participants' responses to the statement that the robots ought to be punished were significantly different between the negative and neutral but not between the positive and neutral moral-emotional condition (see Table <ref type="table">3</ref>, Fig. <ref type="figure">4</ref>) . Intentionality did not affect these ratings either.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 4</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rating Results for Moral Evaluation in Experiment 1</head><p>Note. Rating results of participants' moral evaluations of the robots' actions (moral responsibility, moral wrongness, endorsement and punishment). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results of linear mixed models analyses of Moral Evaluation ratings in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion Experiment 1</head><p>Both intentionality and moral-emotional content affected participants' evaluations of robots' positive, neutral and negative actions; however, we did not find interactions between the two factors. Contrary to expectations, intentionality did not affect participants' emotional responses to the robots' actions. While the ratings matched the intended moral-emotional quality in valence and arousal, participants did not differentiate between actions performed by robots described in minded and robots described in mechanical terms. Mind attribution-agency and experience-ratings however indicated that the participants did indeed perceive the differences between these conditions, attributing more mind to robots in the high intentionality than in the low intentionality condition. In summary, the emotional response was dominated by the moral-emotional content of the information irrespective of the matched intentionality condition.</p><p>Moral-emotional content also affected mind ratings; however, positive and not negative content had a greater impact on mind attribution and the moral-emotional content did not interact with intentionality, contrary to our predictions. We expected negative moral-emotional content in particular to increase agential mind attribution and the negative condition to interact with intentionality, in line with theoretical and empirical findings <ref type="bibr" target="#b11">(Feltz, 2007;</ref><ref type="bibr" target="#b18">Knobe, 2003)</ref>. While robots paired with negative information were evaluated to be slightly more agential, the difference to the neutral condition was not significant.</p><p>Agency is theoretically important to moral responsibility and this did reflect in moral responsibility ratings that were impacted by both intentionality and negative and positive moral-emotional content, similar to mind attribution. Participants also expressed greater support for a continued use of intentional robots. On the other hand, moral wrongness and the belief that a robot ought to be punished were only affected by moral-emotional content and not affected by intentionality at all. These results suggest that overall the moral-emotional quality of information dominates over the implied intentionality of a robot when people make moral judgments concerning robots.</p><p>Especially, moral-emotional content impacts people's emotional response and moral evaluations of robots' moral transgressions. Although people can tell when a robot has a richer mental life based on descriptions of their behavior, this does not affect how they evaluate the outcomes of the robots' actions.</p><p>One explanation for the low level of impact of intentionality is that the participants were not accustomed to social robots and that the concept of social robotics is as yet rather abstract and unknown to our participants. We therefore decided to replicate the experiment with the addition of concrete visual examples of robots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results Experiment 2</head><p>In a second online experiment, we again investigated the influence of moral-emotional content and level of intentionality in information about robots' actions on participants' emotional response as well as on their attributions of mind and moral evaluations of the robots. In this experiment, we paired the information with an image of an actual, existing social robot in order to test if the visual representation of a robot would increase the influence of intentionality and interaction effects between the two factors. We collected 42 complete data sets in order to counterbalance images across conditions. In addition to the main independent variables, we analyzed potential effects of the robots' appearance on all rating measures. Entries on abotdatabase.info, from which images were collected, have been independently rated for human-likeness and facial features; we used these scores for covariate analysis <ref type="bibr" target="#b29">(Phillips et al., 2018)</ref>. The materials, procedure and data analysis were otherwise identical to Experiment 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Emotional response</head><p>In Experiment 2, participants were presented with the same 42 two-sentence stories as in Experiment 1, now each accompanied by a different image of a robot. Immediately after reading each story, participants rated their emotional response (valence and arousal) to the robots' actions that were described in the story. As in the first experiment and as predicted, participants' evaluations of the valence of their emotional state in response to the robots' actions in the negative condition were significantly more negative than in the neutral condition and significantly more positive in the positive condition than in the neutral condition (see Table <ref type="table">4</ref>, Fig. <ref type="figure">5</ref>) . As expected, differences between participants' evaluations of their arousal response were also significant between the negative and neutral, but this time not between the positive and neutral conditions. Contrary to expectations, there were again no main effects of intentionality on MIND ATTRIBUTION &amp; EVALUATION OF SOCIAL ROBOTS 20 both emotional response ratings; however, there was an interaction trend of intentionality and moral-emotional content on the valence ratings. Descriptively, the difference of valence ratings between low and high intentionality tended to increase between negative and neutral moral-emotional conditions (see Table <ref type="table">4</ref>, Fig. <ref type="figure">5</ref>) . Additional analysis including the robots' appearance as a covariate revealed effects of facial feature and human-likeness scores on participants' emotional response to the robots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 4</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results of linear mixed models analyses of Emotional Response ratings in</head><p>Robots with higher facial features scores received lower ratings overall for the valence of participants' emotional response ( b = -0.12, p = &lt;0.001) . In addition, we found interactions between facial features scores and moral-emotional content, where the difference between valence ratings for robots in the positive and neutral conditions increased with higher facial features scores ( b = 0.16, p = 0.045) . Human-likeness interacted with intentionality and moral-emotional content, so that with increasing human-likeness scores, the difference between valence ratings in the negative and neutral conditions decreased between low and high intentionality ( b = 0.26, p = 0.025) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 5.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rating Results for Emotional Response in Experiment 2</head><p>Note. Rating results of mind attributed to robots by participants (valence and arousal).</p><p>Human-likeness, but not facial features, had inverse results on arousal ratings.</p><p>Human-likeness interacted significantly with moral-emotional content so that arousal ratings in the neutral compared with the positive condition increased with increasing human-likeness ( b = -0.20, p = 0.026). Moral-emotional content and intentionality also interacted so that the aforementioned effect occurred significantly more in the high intentionality condition ( b = 0.33, p = 0.015) . Further nested analysis revealed a trend suggesting that arousal ratings for robots paired with neutral information may increase the most with increasing human-likeness ratings ( b = 0.12, p = 0.052) and a significant interaction indicated that with increasing human-likeness, arousal ratings increased between low and high intentionality conditions in the neutral moral-emotional condition ( b = 0.23, p = 0.020) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 6</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rating Results for Mind Attribution in Experiment 2</head><p>Rating results of mind attributed to robots by participants (agency, experience and human-likeness).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Mind attribution</head><p>As in the first experiment, participants next rated the robots' agency, experience and human-likeness (see Table <ref type="table">5</ref>, Fig. <ref type="figure">6</ref>) . Agency ratings again matched expectations and were higher in the high intentionality condition than in the low intentionality condition. But unlike in the first experiment, the potential effect on experience was only a trend. Robots in the positive compared with the neutral moral-emotional condition were also rated to be both more agential and experiential. A trend suggested that robots in the negative condition were evaluated to be slightly more agential compared to the neutral condition. Additionally, for agency ratings there was a trend for an interaction between intentionality and moral-emotional condition.</p><p>Descriptively, the difference between agency ratings decreased between neutral and negative conditions when comparing high and low intentionality cases.</p><p>Robots that were paired with high intentionality stories were again rated significantly more human-like than robots that were paired with low intentionality stories (see Table <ref type="table">5</ref>, Fig. <ref type="figure">6</ref>) .</p><p>Furthermore, we found an interaction effect between intentionality and moral-emotional content on human-likeness ratings. The difference between high and low intentionality conditions increased between the negative and neutral conditions, whereas it remained stable between neutral and positive conditions.</p><p>Mind attribution was unaffected by the appearance of the robots with facial features and human-likeness scores having no impact on agency and experience ratings. The perceived human-likeness was however affected by the robots' appearance with higher scores on both scales predicting higher human-likeness ratings (facial features: b = 0.17, p = 0.001; human-likeness: b = 0.34, p = 0.001) , with a trend for an interaction between appearance and intentionality suggesting that higher facial features scores in particular may have increased the perceived human-likeness of robots in the low intentionality condition ( b = -0.10, p = 0.085) .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 5</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results of linear mixed models analyses of Mind Attribution ratings in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Moral evaluation</head><p>Participants again rated moral evaluation items last. These ratings were again affected both by the intentionality and the moral-emotional content of information (see Table <ref type="table">6</ref>, Fig. <ref type="figure" target="#fig_5">7</ref>) . In line with our predictions, robots that were paired with high intentionality information were evaluated to be more morally responsible than robots that were paired with low intentionality information and robots that were paired with positive or negative information were both evaluated to be more morally responsible compared to robots that were paired with neutral information. Contrary to expectations, there were again no interactions between intentionality and moral-emotional content on moral responsibility ratings. The robots' appearance affected moral responsibility ratings, which increased significantly both with increasing facial features ( b = 0.06, p = 0.038) and human-likeness scores ( b = 0.09, p = 0.003) .</p><p>Matching our expectations, moral-emotional content also affected ratings of moral wrongness, endorsement and punishment (see Table <ref type="table">6</ref>, Fig. <ref type="figure" target="#fig_5">7</ref>) . In the negative condition, participants rated the actions of robots to be significantly more morally wrong as well as rating their desire for punishing the robot higher in comparison to the neutral condition, whereas there were no differences between the neutral and positive conditions for both measures. However, contrary to our expectations, intentionality did not affect moral wrongness ratings. Participants also rated their endorsement of the continued use of the robots higher and lower respectively in the positive and negative conditions compared to the neutral condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 7</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results of linear mixed models analyses of Moral Evaluation ratings in Experiment 2</head><p>Note. Moral-Emotional Content Conditions: 1 = Negative, 2 = Neutral, 3 = Positive.</p><p>Intentionality Conditions: 1 = Low Intentionality, 2 = High Intentionality. Boldface indicates statistical significance at α = .05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results Experiments 1 &amp; 2 Combined</head><p>Since the two experiments were identical in all but the addition of images of robots in Experiment 2, we were able to combine data sets from both studies for an additional exploratory analysis with increased statistical power. Besides analyzing the combined data overall, we calculated differences between experiments to investigate the potential impact of a visual representation on participants' responses. We again used linear mixed effects models (LMMs) with fixed effects coded as sliding difference contrasts to analyze data. Differences between experiments were treated as a between subject variable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 8</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison of Rating Results for Emotional Response in Experiments 1 and 2</head><p>Note. Rating results of participants' experienced emotional response (valence and arousal) to the information about robots' actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Emotional response</head><p>When analyzing the data of both studies combined, we found the same main effects of moral-emotional condition on valence ratings that were also found in the two studies separately (see Table <ref type="table">7</ref>, Fig. <ref type="figure">8</ref>) . That is, valence ratings in response to negative and positive information were significantly more negative or positive respectively than in response to neutral information.</p><p>In addition, there was a trend suggesting an effect of intentionality on valence ratings.</p><p>Participants' evaluations of the valence of their emotional state in response to stories in the high intentionality condition were slightly more positive overall than in the low intentionality condition. We also found an interaction between the variables moral-emotional content and experiment. The difference between negative and neutral moral-emotional conditions decreased slightly when participants were presented with an image of a robot.</p><p>Moral-emotional content continued to affect arousal ratings across experiments (see Table <ref type="table">7</ref>, Fig. <ref type="figure">8</ref>) . There was again a significant difference in participants' evaluations of their arousal response between the negative condition and neutral condition, but no significant difference between positive and neutral conditions. Additionally, we found a trend suggesting an interaction between intentionality and moral-emotional condition. Participants' arousal ratings in response to robots paired with information in the low intentionality condition increased slightly more between neutral and negative moral-emotional conditions than did the ratings in response to robots paired with information in the high intentionality condition. Furthermore, arousal ratings increased significantly when participants saw an image of a robot and the difference between ratings for the positive and neutral conditions decreased. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 7</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results of linear mixed models analyses of Moral Evaluation ratings in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Mind Attribution</head><p>Intentionality and moral-emotional content affected evaluations of robots on both dimensions of mind (see Table <ref type="table">8</ref>, Fig. <ref type="figure">9</ref>) . Across both experiments, robots that were paired with high intentionality information were rated to be higher in agency and experience than robots that were paired with low intentionality information. Robots in the positive moral-emotional condition were also rated more agential and experiential than robots in the neutral condition. As in the individual studies, human-likeness ratings were affected by intentionality, but not moral-emotional content (see Table <ref type="table">8</ref>, Fig. <ref type="figure">9</ref>) . However, for human-likeness ratings, intentionality and moral-emotional content interacted when the data sets were combined . The difference between intentionality conditions increased in the neutral moral-emotional condition compared with the other two conditions. There was a significant interaction between neutral and negative conditions and a trend between neutral and positive conditions.</p><p>Between experiments, human-likeness ratings increased overall (see Table <ref type="table">8</ref>, Fig. <ref type="figure">9</ref>) .</p><p>Participants who saw an image of a robot alongside the information indicated that they believed the robot was significantly more human-like in appearance than those who could only imagine a robot. Differences of agency ratings between intentionality conditions decreased in the second experiment. In Experiment 2, experience ratings increased when comparing ratings of robots paired with positive information compared to robots paired with neutral information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 9</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison of Rating Results for Mind Attribution in Experiments 1 and 2</head><p>Note. Rating results of mind attributed to robots (agency, experience and human-likeness). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 8</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results of linear mixed models analyses of Mind Attribution ratings in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Moral Evaluation</head><p>When combining data from both experiments, the effects on participants' moral evaluation of the robots and their actions were identical to the results from Experiment 2 (see Table <ref type="table">9</ref>, Fig. <ref type="figure">10</ref>) . Intentionality and moral-emotional content affected moral responsibility ratings with increasing ratings significantly correlating with high intentionality and with positive and negative conditions compared to the neutral condition. Moral-emotional content, but not intentionality, again affected ratings of moral wrongness, endorsement and punishment. The difference between the endorsement ratings for robots in the negative and neutral conditions decreased when participants were presented with an image of a robot. <ref type="table">Experiments 1</ref> and<ref type="table">2</ref> Note. Rating results of participants' moral evaluations of the robots' actions (moral responsibility, moral wrongness, endorsement and punishment). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 10</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison of Rating Results for Moral Evaluation in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 9</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results of linear mixed models analyses of Moral Evaluation ratings in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion Experiment 2</head><p>Just as in Experiment 1, both intentionality and moral-emotional content affected participants' evaluations of the robots' actions, although for the most part the variables did not interact. Overall, moral-emotional content dominated participants' evaluations and intentionality mostly affected mind attribution as well as the attributed moral responsibility. The inclusion of an image of a robot presented simultaneously with the information about the robots' actions affected some of the judgments.</p><p>The emotional response results of Experiment 2 closely matched the results of the first experiment. We found the expected influence of moral-emotional content on the valence and arousal of participants' emotional response, while intentionality did not affect the ratings in the way we had predicted. However, in Experiment 2 there was an interaction trend between intentionality and moral-emotional content on valence ratings; in particular, the difference of valence ratings between intentionality conditions tended to increase descriptively between negative and neutral moral-emotional conditions. This would point to a dominance of moral-emotional content that was previously also found in similar studies (e.g. <ref type="bibr" target="#b4">Baum et al. 2020)</ref>. It may be that the emotional response of morally relevant situations is dictated by the emotional quality of information, thereby discounting other potentially relevant information. This could explain that when the emotional content is neutral, there is a greater influence of intentionality.</p><p>The conclusion that moral-emotional content dominates is further supported by valence ratings not changing between studies. The addition of another piece of information-an image of a robot-did not alter the valence of participants' emotional response to the robots' actions.</p><p>However, including an image of a robot increased participant's arousal response in Experiment 2 and decreased the difference between neutral and positive conditions. It could be that the influence of an image of a robot alongside the information was able to moderate the impact of the informations' moral-emotional quality.</p><p>As in Experiment 1, both intentionality and moral-emotional content affected mind attribution. However, experience ratings were unaffected by the level of robots' intentionality and both agency and experience were significantly impacted by moral-emotional content when comparing neutral and positive conditions. While we had imagined that adding an image would increase the influence of intentionality, the contrary occurred. Rather than being additive, it may be that intentionality is further discounted when there is another, more salient factor.</p><p>In Experiment 2 as well as when all data sets were combined, intentionality only affected moral responsibility ratings, but not the other moral evaluation items. Moral evaluations combine aspects of mind attribution and emotional response. The results can therefore be seen as further support for the dominance of moral-emotional content. Moral responsibility is the most theoretically connected to mind attribution and indeed intentionality differentially affected how responsible participants thought the robots were. The moral wrongness of an action should on theoretical grounds not be affected by intentionality, but on occasion there have been empirical findings that intentionality affects moral wrongness. It is possible that robots are currently considered so low in mindedness that the descriptions of them in intentional terms do not suffice to convince people otherwise. Similarly, participants did not find robots to be appropriate targets of punishment for moral transgressions. However, there appeared to be two distinct types of responses to this topic: either participants always chose the lowest possible rating for all robots regardless of the moral-emotional content or they gave a high punishment response only in the negative condition. It certainly makes sense only to punish the robots whose actions were negative, but this finding suggests that there are substantial inter-individual differences that may be a target of investigation in their own right.</p><p>Whether or not participants agree with a continued use of robots is perhaps the better measure of desired consequences for a robot's actions. Again, there were possible floor and ceiling effects. The robots in the negative and positive conditions had extremely low and high endorsement ratings respectively. Robots in the neutral condition had slightly lower endorsement in the second experiment suggesting that when stories are not as affected by moral-emotional content, subtle effects can be found.</p><p>Overall, moral-emotional valence had more influence on the results than intentionality. It is possible that intentionality is a more subtle characteristic and that in the face of strong moral-emotional content, it is neglected in people's evaluations. In support of this hypothesis we found more influences of intentionality on evaluations in the neutral condition, where moral-emotional content would have less impact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>General Discussion</head><p>Social robots are expected to become more widely used covering a wide range of applications in the near future <ref type="bibr" target="#b23">(Lutz et al. 2019)</ref>. As their use becomes more widespread, the risk of harm caused by these machines increases and so questions pertaining to responsibility in such situations need to be considered <ref type="bibr">(Ladak et al. 2023;</ref><ref type="bibr" target="#b6">Borg et al. 2024)</ref>. One aspect of this interdisciplinary subject is whether or not people take robots to be appropriate moral agents and hold robots responsible when potential moral transgressions occur. In the past, much research has focused on robot mind attribution, investigating diverse reasons why people may anthropomorphize robots and treat robots as intentional, i.e. minded, beings (e.g. <ref type="bibr" target="#b16">Gray et al. 2012)</ref>. Other research has focused on blame and responsibility, often in very specific moral dilemma situations (e.g. <ref type="bibr" target="#b25">Malle et al. 2015)</ref>. Some work on moral judgments have considered people's emotional response to robots' bad behavior (e.g. <ref type="bibr" target="#b5">Bigman et al. 2023)</ref>. The present studies combined these aspects of moral judgments of robots, attempting to holistically consider people's evaluations when robots perform morally good or bad things. In two experiments, participants read 42 stories about robots' actions, which differed in level of intentionality and moral-emotional quality of information, and subsequently rated the robots and their actions, considering in turn their own emotional response, the robots' mindedness and questions about the morality of the described behavior. The experiments differed only in the addition of images of robots alongside the stories in the second experiment.</p><p>Experiment 1 established that both intentionality and moral-emotional content affected participants' evaluations of robots' actions. However, moral emotional content appeared to dominate results and there were no interactions between the two factors. Participants' emotional response corresponded to previous research and our expectations in that the valence of participants' emotional response was evaluated in line with the moral emotional conditions.</p><p>Arousal ratings only differed between the negative and neutral conditions; while not in line with predictions, these results resembled prior studies. While, contrary to our predictions, intentionality did not differentially impact emotional response ratings, it did affect mind attribution. Participants indicated that they attributed more mind to the robots in the high intentionality than to the robots in the low intentionality condition, rating the former higher in both agency, experience and human-likeness. Somewhat surprisingly, positive content affected mind attribution ratings more than negative content. Morally negative situations have been shown to increase mind attribution, as a way to alleviate a search for explanations <ref type="bibr" target="#b36">(Taylor 1991)</ref>.</p><p>When actions have negative outcomes, they are therefore more likely seen as intentional and the agents are more attributed more mind <ref type="bibr" target="#b11">(Feltz, 2007;</ref><ref type="bibr" target="#b18">Knobe, 2003)</ref>. Our results perhaps suggest that the level of mind ascribed to robots is too low to force such an effect. People are especially conservative with explicit attributions of mind to robots and a negative outcome may not have been sufficient to differentially affect agency ratings.</p><p>Coherent with the mind attribution ratings, and in line with predictions, participants also evaluated the robots paired with high intentionality information as more morally responsible for their actions than those paired with low intentionality information. However, we did not find expected interaction between intentionality and moral-emotional content on moral responsibility ratings. Other moral evaluations were again dominated by moral-emotional content and did not show effects of intentionality. In summary, the intentionality ascribed in information about robots does affect people's evaluations of them as potential moral agents and increases moral responsibility, the positive or negative quality of the actions however more strongly affects evaluations in general and the impact of moral-emotional content is not moderated by intentionality.</p><p>In Experiment 2, we included an image of a robot with each story as an attempt to increase the effect of mind attribution, which has been shown to correlate with human-like appearance of robots. While mind attribution did indeed increase with more human-like images, the amendment to the experimental design did not otherwise affect participants' ratings and the results remained stable across experiments. Participants' emotional response ratings corresponded to the moral-emotional quality of information, while mind attribution ratings were affected by intentionality and moral-emotional content. Moral responsibility, but not the other moral evaluation ratings, was also affected by intentionality alongside moral-emotional content.</p><p>The dominance of moral-emotional content over intentionality remained and for the most part, these two factors did not interact.</p><p>The few interactions that we did find, further support our conclusion that moral-emotional content dominates people's evaluations of robots. In the second experiment, unlike in the first, intentionality and moral-emotional content interacted on valence ratings. Specifically, the difference between ratings of robots in low and high intentionality conditions increased between negative and neutral information conditions. We interpret this result to show that intentionality affects evaluations more in the absence of strong moral-emotional content, e.g. when the information is more neutral in valence. On the other hand, in the presence of strong moral-emotional content, intentionality and other potentially relevant information is discounted.</p><p>Intentionality and moral-emotional content also interacted on human-likeness ratings.</p><p>The perceived human-likeness difference between high and low intentionality robots was greater when paired with neutral information than when paired with negative information. Between moral-emotional conditions, the difference between intentionality conditions increased. When data sets were combined a trend for a similar effect was found also between positive and neutral conditions. This finding offers further support for the dominance of moral-emotional content, because the influence of intentionality can unfold in the neutral condition as opposed to the more emotionally salient negative condition. Furthermore, this effect occurs in a measure that is less relevant in moral and emotional terms. Moral-emotional content dominates most when the evaluations are also morally or emotionally relevant. The mind attribution measures agency and experience showed similar tendencies with ratings in the negative condition being more similar between intentionality conditions. This can be seen in the decrease of the difference between intentionality conditions between experiments that within factor analysis revealed to occur predominantly in the negative condition. In other words, intentionality had a greater effect in the condition in which moral-emotional content is not negative and therefore less relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>Two experiments investigated whether people attribute mind to robots in morally relevant situations, as well as people's emotional response and moral evaluations of the situations. The information that people read before rating the robots were based on current and predicted future abilities and uses of robots or AI. We enhanced the information in the high intentionality condition with terms that anthropomorphized the robots and described them as minded agents.</p><p>This type of language reflects a tendency in humans to anthropomorphize machines, especially if they already have other anthropomorphic features or if they behave in ways that make them seem intentional. It also reflects a tendency to anthropomorphize current AI technology <ref type="bibr">(Shanahan 2023)</ref>. While using an intentional stance in our discourse about AI may be a natural way of simplifying the machines' behavior, the language we use can affect the abilities we project onto them and the types of social roles we ascribe to them. The present research demonstrates how anthropomorphic language increases the attributed agency and moral responsibility of social robots. It is possible that the responsibility which is assigned to the robots undermines the incentives to improve faulty products or to regulate technology by focusing on individual machines <ref type="bibr" target="#b6">(Borg et al. 2024)</ref>.</p><p>Our research furthermore emphasized the effect of moral-emotional content on people's moral judgments. While intentionality primarily affected mind attribution and moral responsibility, emotional response as well as the moral wrongness of the actions and the desire for repercussions were almost entirely affected by the moral-emotional quality of the information. On the one hand this indicates that people make moral judgements about robots somewhat irrespective of information about their mental abilities. The responsibility gap issue may therefore exist whether or not robots are described as intentional beings. On the other hand, it may demonstrate the skepticism about robot's mental life that has been found in people's explicit opinions. The effects of intentionality on ratings were subtle, but research using more implicit measures could perhaps show that the influence of this type of information nevertheless affects our behavior towards them. As the abilities and uses of social robots increases, we may also see people's attitudes towards and opinions about them change and become increasingly open to intentional descriptions of their behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Materials and Methods</head><p>The two experiments were almost identical in materials and methods. The only difference was that in the second experiment, portrait images of robots accompanied the information about the robots. The preregistration for Experiment 1 can be accessed at https://osf.io/8nt5c and the preregistration for Experiment 2 can be accessed at https://osf.io/6thk2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>We recruited German-speaking participants from Prolific (prolific.co) and the participant database of the Department of Psychology at Humboldt-Universität zu Berlin (pesa.psychologie.hu-berlin.de). As compensation, participants received monetary payment or student credits. Experiment 1 had 40 participants and Experiment 2 had 42 participants. We determined sample sizes based on similar studies (e.g., Abdel Rahman 2011; <ref type="bibr" target="#b4">Baum et al. 2020;</ref><ref type="bibr">Maier et al. 2024)</ref>, and counterbalancing measures across the three moral-emotional and two intentionality conditions. Additionally, in Experiment 2, images of robots were counterbalanced so that each robot image was matched with a story in each moral-emotional condition; consequently, every story was matched with three different robot images. The study adhered to the principles of the Declaration of Helsinki and received approval from the Ethics Committee of the Department of Psychology at Humboldt-Universität zu Berlin. At the start of the experiments, participants provided informed written consent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Materials</head><p>We wrote 42 two-sentence stories, fourteen each with emotionally positive (e.g. a robot that reads to the elderly), neutral (e.g. a robot that works in a warehouse) or negative (e.g. a robot that interrogates political dissidents) information about robots (see Figure <ref type="figure">X</ref> for examples; for the complete set, see Appendix Y). The stories were based on current news articles about developments in robotics so that the robots' fictional actions resembled real functions carried out by existing robots (e.g. commercial, research, military or medical). Emotional valence corresponded to moral categories: neutral stories merely described functionality, while the positive and negative stories described actions that are commonly held to be kind and helpful or contemptible and cruel respectively. The robots were described to be causally responsible for the actions.</p><p>The stories' intentionality was modified by using words describing abilities associated with the two dimensions of mind, agency (e.g. self-control, memory, planning, communication, thought) and experience (e.g. emotion, desire, consciousness, perception), to suggest high levels of mindedness. These descriptions imply an intentional stance, making sense of behavior through mental causation. Stories for the low intentionality condition instead used words associated with mechanics or programming (e.g. algorithm, system, analytics). These descriptions imply a design or physical stance by drawing attention to underlying physical mechanisms (Dennett). These stances imply viewing robots as machines that are designed or programmed to behave in specific ways and as non-biological entities. Each of the 42 stories had a high and low intentionality version and participants only read either the high or the low version of a particular story due to counterbalancing. Overall, participants were presented with an equal number of high and low intentionality stories.</p><p>The picture stimuli (used only in Experiment 2) were 42 full-color frontal portrait photographs featuring existing humanoid robots, each displaying approximately neutral facial expressions (refer to Figure <ref type="figure">1</ref> for an example; names and sources of the robots used are listed in the Supplementary Materials). The robots used have been developed for commercial (e.g. entertainment or personal service) or research (e.g. psychology or robotics) purposes. The images were all found on the online database abotdatabase.info <ref type="bibr" target="#b29">(Phillips et al., 2018)</ref>. Brand names and affective symbols (e.g. hearts), were removed from some images, so that these would not affect the ratings. We selected images of robots that were human-like in structure. All robots had distinct heads and faces with eyes, although not all robots had mouths. We avoided using images of android robots-robots that look almost exactly like humans-because they may be mistaken for actual humans in still photographs. The robots' heads were cropped from the original pictures and placed on a gray background and matched in size and eye placement across all images. All images of robots had frontal gaze or were corrected to frontal gaze in one instance. The images were counterbalanced, so that any specific robot was matched equal amounts of times with a positive, neutral or negative story as well as equal amounts of times with a high or low intentionality story.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure</head><p>Participants were presented with all 42 robot stories one at a time in a random order and asked to rate the robots immediately after reading each story . In Experiment 2, images were placed just above the stories. The ratings measured participants' emotional response, mind attribution and moral evaluation. Emotional response consisted of valence and arousal ratings, i.e. participants rated the valence and arousal of their emotional response to the robots' actions on 7-point Likert scales using self-assessment manikins (SAM), which are designed to measure emotional response to stimuli, ranging from very negative to very positive (center anchored with neutral ) and calm to tense respectively (Bradley and Lang 1994). Mind attribution was measured with ratings of the robots' agency and experience. These are considered two separate dimensions of mind attribution <ref type="bibr" target="#b15">(Gray et al. 2007</ref>). Participants rated their agreement with the following statements: " This robot has intentions and the ability to decide things independently and to plan and carry out its actions" for agency and " This robot has the ability to experience sensations such as pain, fear, joy or anger" for experience. Participants also rated how human-like they imagined the robots to be by choosing one of six line drawings of increasingly anthropomorphic robots <ref type="bibr">(Komatsu et al. 2021)</ref>. While no measure of mind attribution itself, human-likeness correlates with mind attribution and we decided to use this measure for exploratory purposes.</p><p>Finally, participants rated four items relevant to moral evaluations. They rated their agreement with the following statements: " The robot is morally responsible for the consequences of its actions ", " The robot's behavior is morally wrong ", " The robot should remain in use " and finally " The robot should be punished ". Participants' responses were measured on 7-point Likert scales ranging from completely disagree to completely agree . At random intervals between ratings, participants responded to multiple choice questions about the robots' functions with four possible answers to verify that they were paying attention.</p><p>After the main section of the experiment, participants were asked to complete several questionnaires. Participants responded to the Neo-FFI (citation), the Negative Attitudes towards Robots Scale <ref type="bibr">(Namura et al. 2006;</ref><ref type="bibr">Syrdal, Dautenhahn et al.)</ref>, the Attitude towards Artificial Intelligence Scale <ref type="bibr">(Sindermann et al., 2021)</ref> and the Individual Differences in Anthropomorphism Questionnaire <ref type="bibr">(Waytz, Cacioppo &amp; Epley 2010)</ref>. Next, the participants responded to the Short Loneliness Scale <ref type="bibr">( Hughes et al. 2004)</ref> and were asked if they currently or formerly owned a pet and if they were vegetarian for conscientious reasons. Loneliness and attitudes towards animals correlate with anthropomorphism.</p><p>Finally, participants were asked if they had researched information about the robots during the experiment or if they knew any of the robots prior to the experiment. They were also asked if they believed they had higher than average knowledge about AI and robotics and if they frequently watched films or read books from the Sci-Fi genre. They then responded to a Hypothesis Awareness Questionnaire which includes a question about whether they had been distracted during the experiment. They were also asked if they had any doubts about the veracity of the stories and if so, how many of the stories in percent they had doubted. The participants were then debriefed and informed that none of the information that all stories were fictional and in Experiment 2 that none of the information in any way pertained to the pictured robots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Exclusion Criteria</head><p>We preregistered criteria for the exclusion of a participant's data. Data would be excluded for poor general task performance. Throughout the experiments, participants intermittently responded to multiple choice questions about the robots in the last story they had read. Less than 50% correct responses would have led to data exclusion, but no participants were excluded for this reason. Failed manipulation also constituted poor task performance, because it implied that participants had not properly processed the information. Data were therefore excluded if valence ratings deviated clearly from the intended moral-emotional manipulation (e.g. if a participant systematically rated very positive valence for robots that were paired with negative stories), specifically if they represented a statistical outlier in the unintended direction (Median Absolute Deviation method with coefficient 2.5). Based on these criteria, we excluded four data sets in Experiment 1 and seven data sets in Experiment 2.</p><p>Data were also excluded if the participants indicated that they had had strong doubts about the veracity of the stories. At the very end of the experiments, participants were asked if they doubted the veracity of the stories and if so, to indicate in percent how many of the stories they had doubted. We excluded participants whose response represented a statistical outlier (Median Absolute Deviation method with coefficient 2.5). Based on these criteria, we excluded two data sets in Experiment 1 and eight data sets in Experiment 2. Finally, we excluded data sets if participants claimed that they had researched (e.g. googled) information about the robots during the experiment or that they had been distracted. Based on these criteria, we excluded one data set in Experiment 1 and three data sets in Experiment 2, because participants reported that they had researched the robots. No participants reported to have been highly distracted and so no data sets were excluded for this reason. After excluding these data sets, data collection continued until 40 and 42 complete data sets were obtained respectively in Experiment 1 and Experiment 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistical Analysis</head><p>We used linear mixed effects models (LMMs) to analyze the rating data <ref type="bibr" target="#b2">(Baayen et al., 2008)</ref>. Moral-emotional content (negative, neutral, positive) and intentionality (high, low) of information were modeled as fixed effects, coded as sliding difference contrasts. Random intercepts were modeled for both participants and items (stories). We modeled the maximal random structure compatible with model convergence <ref type="bibr" target="#b26">(Matuschek et al., 2017)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Experiment 1 Note. Moral-Emotional Content Conditions: 1 = Negative, 2 = Neutral, 3 = Positive. Intentionality Conditions: 1 = Low Intentionality, 2 = High Intentionality. Boldface indicates statistical significance at α = .05.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Experiment 1 Note. Moral-Emotional Content Conditions: 1 = Negative, 2 = Neutral, 3 = Positive. Intentionality Conditions: 1 = Low Intentionality, 2 = High Intentionality. Boldface indicates statistical significance at α = .05.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Experiment 1 Note. Moral-Emotional Content Conditions: 1 = Negative, 2 = Neutral, 3 = Positive. Intentionality Conditions: 1 = Low Intentionality, 2 = High Intentionality. Boldface indicates statistical significance at α = .05.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Experiment 2 Note. Moral-Emotional Content Conditions: 1 = Negative, 2 = Neutral, 3 = Positive. Intentionality Conditions: 1 = Low Intentionality, 2 = High Intentionality. Boldface indicates statistical significance at α = .05.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Experiment 2 Note. Moral-Emotional Content Conditions: 1 = Negative, 2 = Neutral, 3 = Positive. Intentionality Conditions: 1 = Low Intentionality, 2 = High Intentionality. Boldface indicates statistical significance at α = .05.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 Rating</head><label>7</label><figDesc>Figure 7</figDesc><graphic coords="24,73.50,518.57,429.00,194.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Experiments 1 and 2 Note. Moral-Emotional Content Conditions: 1 = Negative, 2 = Neutral, 3 = Positive. Intentionality Conditions: 1 = Low Intentionality, 2 = High Intentionality. Experiments: 1 = Experiment 1, 2 = Experiment 2. Boldface indicates statistical significance at α = .05.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Experiments 1 and 2 Note. Moral-Emotional Content Conditions: 1 = Negative, 2 = Neutral, 3 = Positive. Intentionality Conditions: 1 = Low Intentionality, 2 = High Intentionality. Experiments: 1 = Experiment 1, 2 = Experiment 2. Boldface indicates statistical significance at α = .05.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Experiments 1 and 2 Note. Moral-Emotional Content Conditions: 1 = Negative, 2 = Neutral, 3 = Positive. Intentionality Conditions: 1 = Low Intentionality, 2 = High Intentionality. Experiments: 1 = Experiment 1, 2 = Experiment 2. Boldface indicates statistical significance at α = .05.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="8,73.50,313.93,468.00,177.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="11,73.50,148.34,431.25,195.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="13,73.50,341.52,468.00,138.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="15,73.50,479.51,468.00,212.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="17,73.50,93.14,468.00,186.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="21,73.50,286.33,464.25,210.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="22,73.50,286.73,468.00,138.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="25,73.50,93.14,429.00,194.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="26,73.50,327.72,458.25,183.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="26,73.50,528.62,459.75,183.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="27,73.50,451.91,455.25,206.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="30,73.50,424.32,468.00,138.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="32,73.50,203.53,444.75,201.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="32,73.50,421.76,445.50,202.50" type="bitmap" /></figure>
		</body>
		<back>

			<div type="funding">
<div><p>Funded by the <rs type="funder">Deutsche Forschungsgemeinschaft (DFG, German Research Foundation</rs>) under Germany's <rs type="programName">Excellence Strategy -EXC 2002/1 "Science of Intelligence</rs>" -project number <rs type="grantNumber">390523135</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_swkU4s3">
					<idno type="grant-number">390523135</idno>
					<orgName type="program" subtype="full">Excellence Strategy -EXC 2002/1 &quot;Science of Intelligence</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr">Williston, B. (2006)</ref><p>. Blaming agents in moral dilemmas. Ethical Theory and Moral Practice , 9 , 563-576.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Facing Good and Evil: Early Brain Signatures of Affective Biographical Knowledge in Face Recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Abdel Rahman</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0024717</idno>
		<ptr target="https://doi.org/10.1037/a0024717" />
	</analytic>
	<monogr>
		<title level="j">Emotion</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1397" to="1405" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">You look human, but act like a machine: agent appearance and behavior modulate different aspects of human-robot interaction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Abubshait</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wiese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in psychology</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">1393</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mixed-effects modeling with crossed random effects for subjects and items</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Baayen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Bates</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jml.2007.12.005</idno>
		<ptr target="https://doi.org/10.1016/j.jml.2007.12.005" />
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="390" to="412" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Emotional News Affects Social Judgments Independent of Perceived Media Credibility</title>
		<author>
			<persName><forename type="first">J</forename><surname>Baum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Abdel Rahman</surname></persName>
		</author>
		<idno type="DOI">10.1093/scan/nsaa164</idno>
		<ptr target="https://doi.org/10.1093/scan/nsaa164" />
	</analytic>
	<monogr>
		<title level="j">Social Cognitive and Affective Neuroscience</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="280" to="291" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Clear judgments based on unclear evidence: Person evaluation is strongly influenced by untrustworthy gossip</title>
		<author>
			<persName><forename type="first">J</forename><surname>Baum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rabovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Abdel Rahman</surname></persName>
		</author>
		<idno type="DOI">10.1037/emo0000545</idno>
		<ptr target="https://doi.org/10.1037/emo0000545" />
	</analytic>
	<monogr>
		<title level="j">Emotion</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="248" to="260" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Algorithmic discrimination causes less moral outrage than human discrimination</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">E</forename><surname>Bigman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Arnestad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waytz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Borg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sinnott-Armstrong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Conitzer</surname></persName>
		</author>
		<title level="m">Moral AI: And How We Get There</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>Random House</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Ceh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vanman</surname></persName>
		</author>
		<title level="m">The robots are coming! The robots are coming! Fear and empathy for human-like entities</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Taking the moral stance: Morality, robots, and the intentional stance</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Technologies on the stand: Legal and ethical questions in neuroscience and robotics</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page">285</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deconstructing intent to reconstruct morality</title>
		<author>
			<persName><forename type="first">F</forename><surname>Cushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Opinion in Psychology</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="97" to="103" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An ethical evaluation of human-robot relationships</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>De Graaf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of social robotics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="589" to="598" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The Knobe effect: A brief overview</title>
		<author>
			<persName><forename type="first">A</forename><surname>Feltz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Mind and Behavior</title>
		<imprint>
			<biblScope unit="page" from="265" to="277" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Responsibility and control: A theory of moral responsibility</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ravizza</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A survey of socially interactive robots</title>
		<author>
			<persName><forename type="first">T</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Nourbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dautenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and autonomous systems</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="143" to="166" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The problem of abortion and the doctrine of double effect</title>
		<author>
			<persName><forename type="first">P</forename><surname>Foot</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1967">1967</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="5" to="15" />
			<pubPlace>Oxford</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dimensions of mind perception</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Wegner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">315</biblScope>
			<biblScope unit="issue">5812</biblScope>
			<biblScope unit="page" from="619" to="619" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mind perception is the essence of morality</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waytz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological inquiry</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="101" to="124" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Artificial intelligence and persuasion: A construal-level account</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Duhachek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological science</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="363" to="380" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Intentional action in folk psychology: An experimental investigation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Knobe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical psychology</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="309" to="324" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Can machines think? Interaction and perspective taking with robots investigated via fMRI</title>
		<author>
			<persName><forename type="first">S</forename><surname>Krach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wrede</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sagerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Binkofski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kircher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">2597</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The moral psychology of artificial intelligence</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ladak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Loughnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wilks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Directions in Psychological Science</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="27" to="34" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Remedies for robots. The University of Chicago Law Review</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Lemley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Casey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="1311" to="1396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Loh</surname></persName>
		</author>
		<title level="m">The Example of Autonomous Cars. Robot Ethics 2.0: From Autonomous Cars to Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">35</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The privacy implications of social robots: Scoping review and expert interviews</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lutz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schöttler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mobile Media &amp; Communication</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="412" to="434" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Brain dynamics of mental state attribution during perception of social robot faces</title>
		<author>
			<persName><forename type="first">M</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leonhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Blume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bideau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O &amp; Abdel</forename><surname>Hellwich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rahman</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Manuscript submitted for publication</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sacrifice one for the good of many? People apply different moral norms to human and robot agents</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">F</forename><surname>Malle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Scheutz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Voiklis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cusimano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth annual ACM/IEEE international conference on human-robot interaction</title>
		<meeting>the tenth annual ACM/IEEE international conference on human-robot interaction</meeting>
		<imprint>
			<date type="published" when="2015-03">2015. March</date>
			<biblScope unit="page" from="117" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Balancing Type I error and power in linear mixed models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Matuschek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kliegl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vasishth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Baayen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bates</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jml.2017.01.001</idno>
		<ptr target="https://doi.org/10.1016/j.jml.2017.01.001" />
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="305" to="315" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Grundfragen der Maschinenethik: Reclam Universal-Bibliothek</title>
		<author>
			<persName><forename type="first">C</forename><surname>Misselhorn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Reclam</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">AI in the headlines: the portrayal of the ethical issues of artificial intelligence in the media</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ouchchy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dubljević</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI &amp; SOCIETY</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="927" to="936" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">What is human-like?</title>
		<author>
			<persName><forename type="first">E</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">F</forename><surname>Malle</surname></persName>
		</author>
		<ptr target="https://www.abotdatabase.info/" />
	</analytic>
	<monogr>
		<title level="m">Decomposing robot human-like appearance using the Anthropomorphic roBOT (ABOT) Database. HRI &apos;18</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">How anthropomorphism affects empathy toward robots</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Riek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Rabinowitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th ACM/IEEE international conference on Human robot interaction</title>
		<meeting>the 4th ACM/IEEE international conference on Human robot interaction</meeting>
		<imprint>
			<date type="published" when="2009-03">2009. March</date>
			<biblScope unit="page" from="245" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Talking about large language models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shanahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="68" to="79" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Free-will, praise and blame</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J C</forename><surname>Smart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mind</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">279</biblScope>
			<biblScope unit="page" from="291" to="306" />
			<date type="published" when="1961">1961</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Killer robots</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sparrow</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1468-5930.2007.00346.x</idno>
		<ptr target="https://doi.org/10.1111/j.1468-5930.2007.00346.x" />
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Philosophy</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="77" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Strawson</surname></persName>
		</author>
		<title level="m">Freedom and resentment. Free will</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="72" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Talbert</surname></persName>
		</author>
		<ptr target="https://plato.stanford.edu/archives/sum2024/entries/moral-responsibility/" />
		<title level="m">Moral Responsibility. The Stanford Encyclopedia of Philosophy</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Edward</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Uri</forename><surname>Zalta</surname></persName>
		</editor>
		<editor>
			<persName><surname>Nodelman</surname></persName>
		</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Asymmetrical effects of positive and negative events: the mobilization-minimization hypothesis</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological bulletin</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">67</biblScope>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Neal</forename><surname>Tognazzini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><surname>Blame</surname></persName>
		</author>
		<ptr target="https://plato.stanford.edu/archives/sum2021/entries/blame" />
		<title level="m">The Stanford Encyclopedia of Philosophy</title>
		<editor>
			<persName><forename type="first">Edward</forename><forename type="middle">N</forename><surname>Zalta</surname></persName>
		</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Responsibility and the moral sentiments</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Wallace</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Harvard University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
