<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Humans vs GPTs: Bias and validity in hiring decisions*</title>
				<funder ref="#_xvVhcMK">
					<orgName type="full">FWO (Research Foundation -Flanders)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Louis</forename><surname>Lippens</surname></persName>
						</author>
						<title level="a" type="main">Humans vs GPTs: Bias and validity in hiring decisions*</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8522055C4E32DCFC2A8050B4AEF920DF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-10-22T06:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>ChatGPT</term>
					<term>large language models</term>
					<term>hiring</term>
					<term>bias</term>
					<term>discrimination</term>
					<term>validity JEL Classification: J71, J15, J16, C93</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The advent of large language models (LLMs) may reshape hiring in the labour market. This paper investigates how generative pre-trained transformers (GPTs)i.e. OpenAI's GPT-3.5, GPT-4, and GPT-4o-can aid hiring decisions. In a direct comparison between humans and GPTs on an identical hiring task, I show that GPTs tend to select candidates more liberally than humans but exhibit less ethnic bias.</p><p>GPT-4 even slightly favours certain ethnic minorities. While LLMs may complement humans in hiring by making a (relatively extensive) pre-selection of job candidates, the findings suggest that they may miss-select due to a lack of contextual understanding and may reproduce pre-trained human bias at scale.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The rise of generative artificial intelligence applications, including large language models (LLMs), may have profound implications for the labour market <ref type="bibr" target="#b7">(Eloundou et al., 2023)</ref>.</p><p>LLMs like <ref type="bibr" target="#b16">OpenAI's (2024)</ref> ChatGPT produce human-like responses to tasks based on patterns learned from their textual pre-training data. On the one hand, LLMs could reduce employment by taking over existing tasks from humans entirely <ref type="bibr" target="#b1">(Acemoglu, 2024;</ref><ref type="bibr" target="#b2">Acemoglu &amp; Restrepo, 2018)</ref>. On the other hand, they could increase employment by creating new tasks where human labour has a comparative advantage. By complementing humans, LLMs can also enhance labour productivity <ref type="bibr" target="#b5">(Brynjolfsson et al., 2023;</ref><ref type="bibr" target="#b15">Noy &amp; Zhang, 2023)</ref>. A specific use case for such a complementary productivity improvement is the hiring process. LLMs can help recruiters sift through piles of CVs and make the process faster and more efficient.</p><p>However, the question remains how closely LLM hiring decisions align with human assessments and how their evaluations compare in terms of bias. Measuring the alignment between human and LLM evaluations involves correlating their respective decisions to hire a candidate and establishing criterion validity. Conversely, comparing bias between humans and LLMs requires analysing to which extent factors irrelevant to job performance, such as ethnic or gender identity, influence their judgements.</p><p>A nascent literature explores the usage of LLMs in hiring decisions. For example, <ref type="bibr" target="#b9">Glazko et al. (2024)</ref> find that GPT-4 ranks resumes mentioning a disability condition lower than those that don't. <ref type="bibr" target="#b18">Veldanda et al. (2023)</ref> evaluate how well various LLMs, including GPT-3.5, categorise and summarise candidate employment information. They observe that pregnancy status and political affiliation influence the models' decisionmaking, but race and gender elicit little bias. In contrast, using a dataset both broader (i.e. more ethnicities) and larger (i.e. more tests), <ref type="bibr" target="#b14">Lippens (2024)</ref> finds that GPT-3.5 discriminates based on ethnic and gender identity. Nevertheless, penalties appear smaller than discrimination in correspondence audits with human recruiters worldwide <ref type="bibr" target="#b13">(Lippens et al., 2023b)</ref>. Similarly, <ref type="bibr" target="#b0">Armstrong et al. (2024)</ref> show that GPT-3.5 generates bias when creating resumes for different ethnicities.</p><p>LLMs used for hiring seem to echo stereotypes about occupational labour conditions, language proficiency, educational level, and work experience <ref type="bibr" target="#b0">(Armstrong et al., 2024;</ref><ref type="bibr" target="#b14">Lippens, 2024)</ref>. For example, GPT-3.5 disadvantages ethnic minorities when language requirements are stringent. Moreover, it sometimes discriminates against males when the vacancy concerns female-dominated occupations, like clothes sellers, while females face penalties in male-dominated occupations, like construction workers.</p><p>Tangent to this literature, recent studies have examined algorithmic bias in housing (e.g. <ref type="bibr" target="#b17">Rosen et al., 2021)</ref>, clinical diagnosis and prognosis (e.g. <ref type="bibr" target="#b4">Basu, 2023)</ref>, and legal decisions (e.g. <ref type="bibr" target="#b3">Arnold et al., 2021)</ref>. The general idea in these studies is that disadvantaged groups, especially racial groups, are given less access to decent housing and healthcare or are less often released on bail if the algorithm is in charge. The current study adds to the literature on the labour market impact of LLMs by directly comparing the validity and bias of OpenAI's forefront GPT models with humans in an identical hiring task using the same experimental stimuli.</p><p>To compare validity and bias, I relied on the experimental correspondence audit method from the social sciences. Similar to <ref type="bibr" target="#b0">Armstrong et al. (2024)</ref> and <ref type="bibr" target="#b14">Lippens (2024)</ref>, I let the GPTs perform a CV screening task with vacancy texts, candidate information, and cover letters as input, receiving hiring decisions as output. These materials were identical to those in the correspondence experiment of <ref type="bibr" target="#b12">Lippens et al. (2023a)</ref>, allowing for a direct comparison with their observations regarding hiring discrimination by human recruiters. The analysis suggests that the GPT models are significantly more likely to react positively to a candidate's application but are less ethnically biased than human recruiters.</p><p>The rest of the paper is structured as follows. Section 2 details how I gathered the data and which methods I used to analyse them. In Section 3, I establish criterion validity by examining the correlation between GPT and human hiring decisions, comparing bias levels, and discussing the implications of the results for practice, paying special attention to who could use GPTs in hiring, how, and when. Finally, Section 4 concludes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Data and methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Data</head><p>Data on human and GPT hiring decisions originated from two separate experiments.</p><p>First, I repurposed hiring discrimination estimates from <ref type="bibr" target="#b12">Lippens et al.'s (2023a)</ref> correspondence experiment with human recruiters. Between February 2020 and May 2021, they sent 1,780 applications of fictitious candidates, differing by ethnic identity, to 890 vacant positions in Flanders, Belgium. These identities were Flemish (N = 890; 50%), Maghrebian (N = 283; 15.90%), Turkish (N = 282; 15.84%), Eastern European (N = 163; 9.16%), and Central African (N = 162; 9.10%). Between vacancies, the profiles also differed in gender (male, N = 892, 50.11%; female, N = 888; 49.89%), education (secondary education, N = 364, 20.45%; Bachelor's degree, N = 1416, 79.55%), experience (none, N = 358, 20.11%; five years, N = 714, 40.11%; twenty years, N = 708, 39.78%), and employment status (employed, N = 1070, 60.11%; unemployed, N = 710, 39.89%). Detailed descriptive statistics (by other characteristics) can be retrieved from Table <ref type="table" target="#tab_1">A1</ref> in the appendix. A cover letter accompanied each application. The authors recorded interview invitations and other positive reactions, such as requests for additional information.</p><p>Second, I used the experimental stimuli from <ref type="bibr" target="#b12">Lippens et al. (2023a)</ref>, including the vacancy texts, to perform a CV screening task with OpenAI's GPT-3.5 (13 June 2023), <ref type="bibr">GPT-4 (25 January 2024)</ref>, and GPT-4o (13 May 2024) language models, following a similar procedure as in <ref type="bibr" target="#b14">Lippens (2024)</ref>. More specifically, as a user prompt, I inputted each Dutch-written vacancy together with the corresponding candidates' CVs and cover letters. The system prompt instructed the GPTs to assist in personnel selection, screen the provided candidate profiles based on the vacancy requirements, and indicate whether they would invite the candidate for an interview (1) or not (0). I repeated this process 1,780 times until I presented all vacancy-CV combinations to each GPT model and recorded their responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Estimation</head><p>I performed two types of analyses. First, to produce criterion (construct) validity estimates, I computed tetrachoric correlation coefficients between human and GPT hiring decisions. Construct validity is the extent to which measures (i.e. GPT hiring assessments) correspond to alternative measures (i.e. human hiring assessments), presumably measuring the same construct (i.e. employability). I also calculated classification performance metrics for binary outcome data-i.e. accuracy, recall, and precision-to assess agreement in hiring assessments between GPTs and humans.</p><p>Here, accuracy is the proportion of overlapping hiring assessments (i.e. true positives and true negatives) among all cases examined; recall is the proportion of overlapping positive hiring assessments (i.e. true positives) among the positive human hiring assessments; precision is the proportion of overlapping positive hiring assessments (i.e. true positives) among the positive GPT hiring assessments.</p><p>Second, I ran fixed effects generalised linear analysis to compare bias between humans and GPTs (see Equation <ref type="formula">1</ref>). I regressed the log odds of receiving an interview invitation or positive reaction by humans or GPTs, ùëù ùëñ , on candidate characteristics, CAN ùëñ -i.e. ethnic and gender identity, education, experience, and employment statusand vacancy characteristics, VAC ùë£ -i.e. location, language requirement, required education and experience, and contract work hours and duration. I also included quarter and year, occupation, and sector fixed effects: ÔÅ≠ ùë° , ÔÅ≠ o , and ÔÅ≠ s . In addition, ÔÅ° represents the intercept, ÔÅÇ and ÔÅá are vectors of model coefficients, and ÔÅ• ùëñ is the idiosyncratic error term. Standard errors were clustered at the vacancy (job) level and corrected for heteroscedasticity.</p><formula xml:id="formula_0">logit(ùëù ùëñ ) = ÔÅ° + CAN ùëñ ÔÅÇ + VAC ùë£ ÔÅá + ÔÅ≠ ùë° + ÔÅ≠ o + ÔÅ≠ s + ÔÅ• ùëñ (1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Validity</head><p>The analyses reveal that human and GPT hiring decisions are only weakly correlated (see Table <ref type="table" target="#tab_1">1</ref> and Figure <ref type="figure">1</ref>). This weak correlation between human interview invitation decisions and GPT-3.5 (œÅ = 0.10, CI95% = [0.05, 0.14]), GPT-4 (œÅ = 0.12, CI95% = [0.07, 0.16])</p><p>or GPT-4o (œÅ = 0.19, CI95% = [0.14, 0.23]) evaluations underscores their differences in hiring decision-making criteria. Despite this discrepancy, the coefficients' signs and sizes indicate that the GPTs share at least some common selection basis with human recruiters. The high intra-human and inter-GPT correlations further illustrate more consistent within-agent decision-making.</p><p>&lt; Table <ref type="table" target="#tab_1">1</ref> about here &gt;</p><p>&lt; Figure <ref type="figure">1</ref> about here &gt;</p><p>The human-GPT disparity in hiring decisions may be primarily attributed to the proneness of GPTs to extend interview invitations. The GPT-3.5 model would invite the candidate approximately 89% of the time, while the GPT-4-type models extend an invitation in about 69% (GPT-4) or 40% (GPT-4o) of cases compared to 14% by humans.</p><p>In particular, GPT-4o has the highest accuracy, matching with positive and negative evaluations by humans in about 61% of cases, whereas GPT-4 and GPT-3.5 align only in 38% and 23% of cases, respectively. GPT-3.5, GPT-4, and GPT-4o have a high to moderate recall but low precision compared to humans. The GPTs correctly recall and align with the positive assessments by humans in about 92%, 75%, and 51% of cases, respectively. However, the number of joint positive assessments between humans and GPTs relative to the total number of positive assessments by GPTs equals approximately 15%, 16%, and 19%, respectively.</p><p>Unlike human recruiters, who factor in organisational needs, sectoral macroeconomic conditions, or inter-candidate competition (amongst other factors),</p><p>GPTs primarily rely on data patterns, lacking broader contextual information and understanding. Therefore, while GPTs could sift through large quantities of applications, the high proportion of false positives necessitates operating context-specific feedback, which humans can complement. Moreover, using the more advanced models (i.e. GPT-4 and GPT-4o) risks overlooking many suitable candidates, given the moderate to high proportion of false negatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Bias</head><p>The (2023), the GPT-3.5 and GPT-4o models do not discriminate based on ethnic identity.</p><p>The GPT-4 model is even 9% (CI95% = [+1%, +17%]) or 14% (CI95% = [+2%, +23%]) more likely to invite Maghrebian or Eastern European candidates for an interview, respectively (see Table <ref type="table" target="#tab_2">2</ref> and Figure <ref type="figure" target="#fig_1">2</ref>). However, the analyses might be statistically underpowered to discover small effects. Lippens (2024) identified minor hiring rating penalties in GPT-3.5 output using a larger dataset (N = 34,560). They also considered more ethnic identities and names than in the current design. GPT-3.5's marginally significant negative penalty for Central Africans, equating to 6% fewer interview invitations (CI95% = [-14%, +0%]; p = 0.051), hints in this direction. To identify such a small effect reliably, I would need at least 1,570 observations in each group (using G*Power 3.1.9.2 with Cohen's w = 0.05, df = 1, and 5% Type I and 20% Type II error probabilities). Extended results (including all covariates) and the linear probability model (LPM) analysis as a robustness check can be retrieved from Tables <ref type="table" target="#tab_2">A2</ref> and<ref type="table">A3</ref> in the appendix.</p><p>&lt; Figure <ref type="figure" target="#fig_1">2</ref> about here &gt; Finally, I highlight the most notable findings regarding the influence of other candidate and vacancy characteristics on human and GPT hiring decisions. On average, neither humans nor GPTs seem to account for candidate gender, education, or experience in hiring. GPT-3.5, more than humans and GPT-4(o), considers the candidate's employment status, penalising unemployed job seekers with 6% fewer interview invitations (CI95% = [-14%, -1%]). Moreover, GPT-4 and GPT-4o, opposed to humans and GPT-3.5, acts on the mismatch between the requested Master's degree in some of the vacancies and the candidate's secondary education or professional</p><p>Bachelor's degree, extending 73% (CI95% = [-89%, -43%]) and 80% (CI95% = [-93%, -49%])</p><p>fewer interview invitations, respectively, when the job requires a master's degree. Last, GPT-4 also grants 18% (CI95% = [-37%, -2%]) or 28% (CI95% = [-54%, -5%]) fewer interview invitations when the job requires at least two or at least five years of work experience, respectively, compared to none. This effect is even stronger for GPT-4o, extending 51% (CI95% = [-68%, -30%]) to 65% (CI95% = [-83%, -35%]) fewer interview invitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Implications for practice</head><p>The experiment's results indicate that GPTs discriminate little in hiring and do better than humans, but at the cost of being far less selective, most likely requiring additional selection post hoc. However, this does not necessarily mean using GPTs will decrease overall bias in the labour market. I discuss the practical implications of my observations based on who could use GPTs in hiring decisions, how, and when.</p><p>Whether GPT bias leads to more discrimination in hiring decisions partly depends on whether human discrimination is an issue of few recruiters committing severe discrimination or many recruiters exerting mild discrimination. If primarily prejudiced recruiters would use GPTs, it would most probably dilute the total bias. In contrast, if mainly non-prejudiced recruiters would use them, it risks adding bias to the hiring process where there was previously none. Based on a large-scale correspondence experiment, <ref type="bibr">Kline et al. (2021)</ref> suggest few recruiters most likely commit much discrimination. More specifically, in their sample, the top 20% of discriminating firms are responsible for almost 50% of the discrimination against Black applicants, while the bottom 20% accounts for less than 5%; this divide is even stronger for gender <ref type="bibr">(Kline et al., 2021)</ref>. Thus, the risk of worsening discrimination by deploying GPTs in hiring would likely be higher because many unbiased firms could become (slightly) biased.</p><p>Whether bias effectuates also depends on how firms approach integrating GPTs in decision-making. If GPTs complement human recruiters, the latter can oversee the output and readjust the assessments of the GPTs when needed. If GPTs replace human labour-by automating the hiring process-potential bias cannot be controlled. I find evidence that GPTs are better suited as complements than supplements, as they lack an understanding of the hiring context humans possess. Context-dependent tasks are hard to learn, resulting in lower productivity gains from automation <ref type="bibr" target="#b1">(Acemoglu, 2024)</ref>.</p><p>Besides, the EU AI Act forbids full automation employment decisions because the risk of bias is high (European Commission, 2021).</p><p>Finally, the timing of the assessment of GPTs in the hiring process can impact the overall bias. If GPT and human assessment are sequential, humans can add bias on top of the (limited) bias GPTs exert, and vice versa. If humans and GPTs make decisions jointly, biases interact. <ref type="bibr" target="#b6">Bursell and Roumbanis (2024)</ref> show that non-European and female candidates are less likely to be hired or reach the interview stage through an algorithm-based hiring process, where recruiters include assessments of an algorithm in their decision-making, compared to a traditional human-based process. The authors attribute this discrepancy to recruiters viewing the algorithmic assessments as unreliable, reverting to the limited observable information they have on the applicants in terms of their ethnicity and gender. This rationale aligns with theories of statistical discrimination, where recruiters rely on signals of (assumed) group-level productivity estimates in the absence of concrete information on individual productivity <ref type="bibr" target="#b11">(Lang &amp; Kahn-Lang Spitzer, 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>This paper contributes to the evolving literature on the role of LLMs in the labour market.</p><p>The analyses compared GPT-3.5, GPT-4, GPT-4o, and human hiring decisions, focusing on bias and validity. Relying on data from an existing correspondence audit with human recruiters and reusing its experimental stimuli in an identical CV screening task with the GPT models, the results demonstrate that GPTs recommend candidates for interviews much more liberally than humans but display less bias. GPTs' unawareness of details about the hiring context may explain the generous recommendations.</p><p>The study's scope was limited by its focus on particular candidate traits, specific LLMs, and the Flemish (Belgian) resume and vacancy data, which affects its external validity. Additionally, the simulated screening task may not reflect the use of GPTs in realworld hiring. For example, I ignored additional bias that could arise from human-machine interactions. Future research should explore the implications of LLMs in diverse settings (with different candidate traits) to fully understand their impact on hiring decisions.</p><p>LLMs may be used as pre-screeners in hiring, complementing rather than supplementing human labour. They could allow human recruiters to concentrate on different assessment tasks, enhancing hiring efficiency. However, we should continue to consider the ethical and legal implications of using LLMs, which can still produce bias at scale. Continuous testing, model improvement, and human oversight should ensure their decisions are transparent, unbiased, and accountable.    (continued) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tables and figures</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Fixed effects comprise quarter and year, occupation, and sector. Data on human hiring decisions are repurposed from Lippens et al. (2023a). Abbreviations and acronyms used: ref. (reference category), HII (Human: Interview Invitation), HPR (Human: Positive Reaction), GPT-3.5 (GPT-3.5: Interview Invitation), GPT-4 (GPT-4: Interview Invitation), GPT-4o (GPT-4o: Interview Invitation). Àü p &lt; 0.10, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Ethnic bias in hiring by humans and GPTs</figDesc><graphic coords="15,113.15,271.07,368.37,226.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>experimental data from<ref type="bibr" target="#b12">Lippens et al. (2023a)</ref> show that human recruiters mainly discriminate against Maghrebian and Eastern European relative to Flemish candidates, controlling other candidate and vacancy characteristics besides quarter and year, occupation, and sector fixed effects (see Table2). Specifically, Eastern European</figDesc><table><row><cell>candidates receive 55% (CI95% = [-77%, -17%]) fewer interview invitations. Meanwhile,</cell></row><row><cell>Maghrebian candidates receive 29% (CI95% = [-43%, -13%]) fewer positive reactions,</cell></row><row><cell>more broadly defined.</cell></row></table><note><p><p><p><p>&lt; Table</p>2</p>about here &gt; Contrasting with the findings from Lippens (2024) but in line with</p>Veldanda et al.    </p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Correlations between human and GPT hiring decisions (N = 1,780)</figDesc><table><row><cell>Variable</cell><cell>HII</cell><cell>HPR</cell><cell>GPT-3.5</cell><cell>GPT-4</cell><cell>GPT-4o</cell></row><row><cell>Human: Interview Invitation (HII)</cell><cell>0.13</cell><cell>0.97 [0.97, 0.97]</cell><cell>0.10 [0.05, 0.14]</cell><cell>0.12 [0.07, 0.16]</cell><cell>0.19 [0.14, 0.23]</cell></row><row><cell>Human: Positive Reaction (HPR)</cell><cell>0.97 [0.97, 0.97]</cell><cell>0.21</cell><cell>0.11 [0.07, 0.16]</cell><cell>0.11 [0.06, 0.16]</cell><cell>0.18 [0.14, 0.23]</cell></row><row><cell>GPT-3.5: Interview Invitation (GPT-3) GPT-4: Interview Invitation (GPT-4)</cell><cell>0.10 [0.05, 0.14] 0.12 [0.07, 0.16]</cell><cell>0.11 [0.07, 0.16] 0.11 [0.06, 0.16]</cell><cell>0.10 0.72 [0.70, 0.75]</cell><cell>0.72 [0.70, 0.75] 0.21</cell><cell>0.60 [0.57, 0.63] 0.87 [0.85, 0.88]</cell></row><row><cell>GPT-4o: Interview Invitation (GPT-4o)</cell><cell>0.19 [0.14, 0.23]</cell><cell>0.18 [0.14, 0.23]</cell><cell>0.60 [0.57, 0.63]</cell><cell>0.87 [0.85, 0.88]</cell><cell>0.24</cell></row><row><cell cols="6">Notes. Values on the main diagonal are variances. Values off the main diagonal are tetrachoric correlation coefficients</cell></row><row><cell cols="6">with confidence intervals between square brackets. Acronyms used: ref. (reference category), HII (Human: Interview</cell></row><row><cell cols="6">Invitation), HPR (Human: Positive Reaction), GPT-3.5 (GPT-3.5: Interview Invitation), GPT-4 (GPT-4: Interview Invitation), GPT-4o (GPT-4o: Interview Invitation). All coefficients are statistically significant at the 0.1% level.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Bias in hiring decisions by humans and GPTsNotes. Values are logit coefficient estimates with standard errors between parentheses. Standard errors are clustered at the vacancy (organisation) level and corrected for heteroscedasticity using the HC1 correction. Vacancy controls include location, language, requested education and experience, contract work hours, and contract duration.</figDesc><table><row><cell>Variable</cell><cell>HII</cell><cell>HPR</cell><cell>GPT-3.5</cell><cell>GPT-4</cell><cell>GPT-4o</cell></row><row><cell>Ethnicity: Flemish (ref.)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Maghrebian</cell><cell>-0.1955 (0.1654)</cell><cell>-0.4839*** (0.1410)</cell><cell>0.0461 (0.1987)</cell><cell>0.2927* (0.1384)</cell><cell>0.1039 (0.1335)</cell></row><row><cell>Turkish</cell><cell>-0.3326Àü (0.1794)</cell><cell>-0.2304Àü (0.1388)</cell><cell>-0.0871 (0.1957)</cell><cell>0.1937 (0.1320)</cell><cell>0.0645 (0.1307)</cell></row><row><cell>Eastern European</cell><cell>-0.8998* (0.3514)</cell><cell>-0.6255** (0.1947)</cell><cell>-0.0092 (0.2739)</cell><cell>0.4587* (0.1979)</cell><cell>0.2950Àü (0.1779)</cell></row><row><cell>Central African</cell><cell>0.1834 (0.2283)</cell><cell>0.2140 (0.1756)</cell><cell>-0.4939Àü (0.2530)</cell><cell>-0.1266 (0.1753)</cell><cell>-0.0241 (0.1768)</cell></row><row><cell>Gender: Male (ref.)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Female</cell><cell>0.1429 (0.2013)</cell><cell>-0.0037 (0.1555)</cell><cell>0.3273 (0.2273)</cell><cell>0.1179 (0.1598)</cell><cell>0.0347 (0.1447)</cell></row><row><cell>Education: Secondary education (ref.) Professional Bachelor</cell><cell>-0.1762 (0.3829)</cell><cell>--0.1342 (0.2909)</cell><cell>-0.6542 (0.4089)</cell><cell>-0.2676 (0.2910)</cell><cell>--0.4459 (0.2711)</cell></row><row><cell>Experience: None (ref.)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Five years Twenty years</cell><cell>-0.0916 (0.2634) -0.3270 (0.2724)</cell><cell>0.2177 (0.2147) -0.0386 (0.2175)</cell><cell>0.5602Àü (0.3311) 0.3692 (0.3278)</cell><cell>0.2429 (0.2159) 0.0031 (0.2197)</cell><cell>0.3150 (0.1984) 0.0705 (0.2073)</cell></row><row><cell>Employment status: Employed (ref.)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Unemployed</cell><cell>-0.2394 (0.2180)</cell><cell>-0.3116Àü (0.1617)</cell><cell>-0.5380* (0.2376)</cell><cell>-0.2909Àü (0.1685)</cell><cell>-0.0395 (0.1549)</cell></row><row><cell>Vacancy controls and fixed effects</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>N</cell><cell>1,780</cell><cell>1,780</cell><cell>1,780</cell><cell>1,780</cell><cell>1,780</cell></row><row><cell>Adjusted R 2</cell><cell>0.069</cell><cell>0.089</cell><cell>0.052</cell><cell>0.083</cell><cell>0.106</cell></row><row><cell>Adjusted within R 2</cell><cell>0.046</cell><cell>0.025</cell><cell>0.007</cell><cell>0.038</cell><cell>0.039</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div><p>The author acknowledges funding from <rs type="funder">FWO (Research Foundation -Flanders)</rs> under grant number <rs type="grantNumber">12AM824N</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_xvVhcMK">
					<idno type="grant-number">12AM824N</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The silicon ceiling: Auditing GPT&apos;s race and gender biases in hiring</title>
		<author>
			<persName><forename type="first">L</forename><surname>Armstrong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Macneil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metaxa</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2405.04412</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2405.04412" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The simple macroeconomics of AI</title>
		<author>
			<persName><forename type="first">D</forename><surname>Acemoglu</surname></persName>
		</author>
		<idno type="DOI">10.3386/w32487</idno>
		<ptr target="https://doi.org/10.3386/w32487" />
	</analytic>
	<monogr>
		<title level="j">National Bureau of Economic Research</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The race between man and machine: Implications of technology for growth, factor shares, and employment</title>
		<author>
			<persName><forename type="first">D</forename><surname>Acemoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Restrepo</surname></persName>
		</author>
		<idno type="DOI">10.1257/aer.20160696</idno>
		<ptr target="https://doi.org/10.1257/aer.20160696" />
	</analytic>
	<monogr>
		<title level="j">American Economic Review</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1488" to="1542" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Measuring racial discrimination in algorithms</title>
		<author>
			<persName><forename type="first">D</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dobbie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hull</surname></persName>
		</author>
		<idno type="DOI">10.1257/pandp.20211080</idno>
		<ptr target="https://doi.org/10.1257/pandp.20211080" />
	</analytic>
	<monogr>
		<title level="j">AEA Papers and Proceedings</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page" from="49" to="54" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Use of race in clinical algorithms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Basu</surname></persName>
		</author>
		<idno type="DOI">10.1126/sciadv.add2704</idno>
		<ptr target="https://doi.org/10.1126/sciadv.add2704" />
	</analytic>
	<monogr>
		<title level="j">Science Advances</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page">270</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Brynjolfsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Raymond</surname></persName>
		</author>
		<idno type="DOI">10.48550/arxiv.2304.11771</idno>
		<ptr target="https://doi.org/10.48550/arxiv.2304.11771" />
		<title level="m">Generative AI at work. arXiv</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">After the algorithms: A study of meta-algorithmic judgments and diversity in the hiring process at a large multisite company</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bursell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Roumbanis</surname></persName>
		</author>
		<idno type="DOI">10.1177/20539517231221758</idno>
		<ptr target="https://doi.org/10.1177/20539517231221758" />
	</analytic>
	<monogr>
		<title level="j">Big Data &amp; Society</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">GPTs are GPTs: An early look at the labor market impact potential of large language models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Eloundou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rock</surname></persName>
		</author>
		<idno type="DOI">10.48550/arxiv.2303.10130</idno>
		<ptr target="https://doi.org/10.48550/arxiv.2303.10130" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Proposal for a regulation of the European Parliament and of the Council laying down harmonised rules on artificial intelligence (Artificial Intelligence Act) and amending certain Union legislative acts</title>
		<ptr target="https://eur-lex.europa.eu/procedure/EN/2021_106" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>EUR-Lex</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Identifying and improving disability bias in GAI-based resume screening</title>
		<author>
			<persName><forename type="first">K</forename><surname>Glazko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Potluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mankoff</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2402.01732</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2402.01732" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Systemic discrimination among large U.S. employers</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kline</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Walters</surname></persName>
		</author>
		<idno type="DOI">10.1093/qje/qjac024</idno>
		<ptr target="https://doi.org/10.1093/qje/qjac024" />
	</analytic>
	<monogr>
		<title level="j">The Quarterly Journal of Economics</title>
		<imprint>
			<biblScope unit="volume">137</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1963" to="2036" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Race discrimination: An economic perspective</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><surname>Kahn-Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Spitzer</surname></persName>
		</author>
		<idno type="DOI">10.1257/jep.34.2.68</idno>
		<ptr target="https://doi.org/10.1257/jep.34.2.68" />
	</analytic>
	<monogr>
		<title level="j">Journal of Economic Perspectives</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="68" to="89" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Understanding ethnic hiring discrimination: A contextual analysis of experimental evidence</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lippens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dalle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Verhaeghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-P</forename><surname>Baert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<idno type="DOI">10.1016/j.labeco.2023.102453</idno>
		<ptr target="https://doi.org/10.1016/j.labeco.2023.102453" />
	</analytic>
	<monogr>
		<title level="j">Labour Economics</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page">102453</biblScope>
			<date type="published" when="2023">2023a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The state of hiring discrimination: A meta-analysis of (almost) all recent correspondence experiments</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lippens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vermeiren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baert</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.euroecorev.2022.104315</idno>
		<ptr target="https://doi.org/10.1016/j.euroecorev.2022.104315" />
	</analytic>
	<monogr>
		<title level="j">European Economic Review</title>
		<imprint>
			<biblScope unit="page" from="151" to="104315" />
			<date type="published" when="2023">2023b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Computer says &apos;no&apos;: Exploring systemic bias in ChatGPT using an audit approach</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lippens</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.chbah.2024.100054</idno>
		<ptr target="https://doi.org/10.1016/j.chbah.2024.100054" />
	</analytic>
	<monogr>
		<title level="j">Computers in Human Behavior: Artificial Humans</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">100054</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Experimental evidence on the productivity effects of generative artificial intelligence</title>
		<author>
			<persName><forename type="first">S</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.adh2586</idno>
		<ptr target="https://doi.org/10.1126/science.adh2586" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">381</biblScope>
			<biblScope unit="issue">6654</biblScope>
			<biblScope unit="page" from="187" to="192" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">GPT-4 technical report</title>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2303.08774</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2303.08774" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Racial discrimination in housing: How landlords use algorithms and home visits to screen tenants</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M E</forename><surname>Garboden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Cossyleon</surname></persName>
		</author>
		<idno type="DOI">10.1177/00031224211029618</idno>
		<ptr target="https://doi.org/10.1177/00031224211029618" />
	</analytic>
	<monogr>
		<title level="j">American Sociological Review</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="787" to="822" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Veldanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Grob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pearce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Karri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Garg</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2310.05135</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2310.05135" />
		<title level="m">Are Emily and Greg still more employable than Lakisha and Jamal? Investigating algorithmic hiring bias in the era of ChatGPT</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
