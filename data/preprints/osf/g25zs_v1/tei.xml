<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Curating Content for Course Chatbots: Ethical Considerations for Educators</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Ryan</forename><surname>Watkins</surname></persName>
							<email>rwatkins@gwu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Washington University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Curating Content for Course Chatbots: Ethical Considerations for Educators</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FF2F75228F80B9100FA5702969720893</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-22T20:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As AI-driven chatbots make their way into college courses, the potential to offer 24/7 support for students might be transformative. But there's a catch: when we customize these chatbots by uploading documents, we don't just add knowledge-we introduce biases. Whether intentional or not, the documents we select shape the chatbot's responses, potentially influencing how students engage with course topics and, ultimately, how they think. So, how do we ensure we're promoting critical thinking, and not just using AI (and more specifically Large Language Models or LLMs) to reinforce our own perspectives?</p><p>How Course Chatbots Differ from Administrative Chatbots Chatbot teaching assistants (or tutors) are not new to college courses [1], but with low-cost LLM access and user-friendly tools, it's now easier for instructors to create customized course chatbots. However, there's a key difference between customized LLM-based chatbots for administrative tasks-where they reference a universally accepted "ground truth" such as an organizational policy-and those used in education, where no single "ground truth" exists. Instructors in most disciplines often present specific perspectives or theories, even when multiple viewpoints and evolving debates exist. The materials you choose to customize your chatbot can either reinforce your own perspectives or introduce a diversity of viewpoints that reflect broader academic debates.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The recent rise of LLMs has made it easier for instructors to create customized chatbots to enhance their teaching. Customizing LLMs is now straightforward, often requiring no coding-just uploading documents (e.g., articles, case studies, books) to tools like ChatGPT, Claude, Gemini, or Copilot. These documents provide context for chatbot conversations, allowing the chatbot's responses to be shaped by the supplemental materials, resulting in outputs that differ from those of the original, unmodified LLM.</p><p>In organizational settings, where "ground truth" documents exist (e.g., privacy policies), customized chatbot outputs can be more accurate. However, in education, the documents uploaded to customize a chatbot cannot be assumed to represent a single "ground truth." This introduces new biases-whether intentional or not-into the chatbot's responses, affecting what students learn. This distinction calls for a high standard of ethical responsibility from instructors customizing chatbots for their courses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Curating Content for Classroom Chatbots</head><p>Having a tutor or teaching assistant available for your students 24 hours a day 7 days a week has great potential to complement what you are already doing as an instructor. There are also now many options available for instructors to easily upload course specific documents to supplement an LLM-based chatbot. From building with OpenAI's GPT Assistant to using a service like IBL's AI Mentor and Druid's Conversational AI, you can now curate a collection of documents that will be used to add course specific content to the knowledge-base of a course-specific chatbot.</p><p>Although the technical barriers to customizing LLM-based chatbots have been lowered, important development and ethical considerations remain when creating them for courses. The documents you select will shape, though not fully dictate, the responses students receive. Other factors, such as the LLM's base model, instructions you provide, and the ongoing conversation between the student and the chatbot, will also play a role. However, the curated content can substantially influence the chatbot's output, and the value of that influence will depend on your instructional goals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Curating for Critical Thinking vs. Reinforcing Bias</head><p>The belief that students should be taught "how to think, not what to think" [2, 3] is central to this discussion, though it's not universally held. Some may, consciously or unconsciously, lean toward the opposite-prioritizing content that subtly dictates what to think, for instance when selecting course readings. These choices are often shaped by various biases. When curating documents for LLM-based chatbots, it's crucial to be aware of the biases that can influence the selection process.</p><p>The distinction between teaching critical thinking versus reinforcing specific viewpoints is closely tied to the biases that can be introduced when curating content for course chatbots. Whether intentional or not, these biases influence the information students receive and how they engage with course material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Biases to Consider</head><p>When selecting documents to customize an LLM-based chatbot, it's easy to introduce (consciously or not) biases that shape the responses and interactions students have with the chatbot. These biases can influence not only the content but also how students engage with and understand the material. Below are some common biases to be aware of when curating content [4]:</p><p>Personal Research Bias: Overemphasis on topics aligned with the faculty's research interests and writings, and underrepresentation of areas outside their expertise. Temporal Bias: Favoring older, well-established theories and concepts and neglecting recent developments or cutting-edge research, or the reverse. Subdiscipline Bias: Imbalanced representation of subdisciplines within a field with overemphasis on "core" topics at the expense of interdisciplinary or emerging areas. Theoretical Framework Bias: Faculty's preference of certain theories or models and potential underrepresentation of alternative or competing frameworks. Methodological Bias: Preference for specific research and assessment methodologies or approaches. Cultural Bias: Overrepresentation of dominant cultural perspectives (e.g., politically liberal perspectives at a largely politically liberal college) and potential lack of culturally diverse examples or applications. Ethical Stance Bias: Reflection of the faculty's ethical positions on controversial topics and potential lack of balanced presentation of ethical debates. Journal/Publisher Bias: Selecting publications from a limited number of publications/publishers might bias the chatbot outputs to nudge students towards certain publications and/or publishers. Technological Bias: Favoring research or studies that align with specific technologies (e.g., certain programming languages, software tools) or computational tools that the faculty or institution uses or teaches, while neglecting others. Institutional Bias: Preference for research from high-prestige institutions, potentially neglecting high-quality work from lesser-known or emerging research centers. Disciplinary Conservatism Bias: Overemphasizing the "accepted" or "safe" scientific paradigms and failing to include emerging or controversial research that challenges the status quo. Experimental Bias: Emphasizing studies with positive, significant results while overlooking null or negative findings. Open Access Bias: Overrepresentation of open access resources that might not provide the full spectrum of knowledge available. This list is not exhaustive, and there may be additional biases specific to your discipline that warrant attention. However, these examples should provide a solid starting point for understanding the complexities involved in curating content for your course chatbot. It's important to remember that any data you add will shift-rather than eliminate-biases in the chatbot's responses, as few disciplines offer a clear "ground truth."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tips for Ethical and Thoughtful Chatbot Curation</head><p>This blog will focus on the human decisions-your decisions-that significantly impact your students when using a course chatbot, particularly the intentional and unintentional biases that may be introduced. Here are ten initial tips to get started, though other ethical considerations (e.g., legal considerations, student agency) also deserve attention.</p><p>1. Be Aware of Biases: Foundation models like ChatGPT or Gemini already carry inherent biases from the vast datasets they were trained on. When you customize these models with your own documents, you introduce additional bias-both intentional and unintentional. For example, selecting materials that emphasize one theory over another (e.g., "I want students to learn theory X instead of theory Y") is an intentional bias. However, unintended biases may also emerge from subtle patterns in your data, such as favoring certain journals or perspectives over others. Being aware of these layers of bias is critical to ensuring your chatbot offers balanced content. 2. Curate for Diversity of Perspectives: When selecting documents to supplement the LLM, intentionally choose diverse and reputable sources. Include materials that represent a broad range of perspectives, such as readings from international journals or sources with differing ideological viewpoints (e.g., left-leaning, right-leaning, centrist). Establish clear guidelines that prioritize diversity in your course materials (see the list of potential biases above). Even if you don't personally agree with certain perspectives, including them in the chatbot's content is crucial for fostering critical thinking skills and encouraging students to explore a range of viewpoints. 3. Encourage Exploration: Instruct the chatbot to offer responses that present multiple viewpoints when relevant. For example, instead of giving a single answer to a question like "What is the impact of climate change?", the chatbot can be instructed to provide a range of scientific, economic, and political perspectives. You can also consider using multiple chatbots (i.e., a multi-agent system) to further diversify the perspectives presented (e.g., STORM). 4. Frame Responses: Instruct the chatbot to frame its answers in a way that highlights diverse perspectives. For example, it could respond with: "There are different views on this issue. According to X, ... but Y suggests a different approach, arguing that..." 5. Document the Selection Process: Keep a clear record of how and why you chose specific materials for the chatbot. This transparency not only allows for self-reflection but also provides a reference for students or colleagues who may ask about the rationale behind the selected content. 6. Share Curation Lists with Students: Provide students with access to the list of curated documents and resources, encouraging them to explore the original materials if they wish to dive deeper into specific viewpoints. To further support their critical thinking, include explanations of why certain documents were selected and, when helpful, note any materials considered but not included. 7. Capture Student Feedback: Establish ways for students to report when they feel a viewpoint is missing or underrepresented. Regularly review this feedback to ensure the chatbot maintains balanced and diverse content. 8. Plan to Keep Updating: Review the curated materials each semester to ensure they remain current and represent diverse viewpoints. This prevents the reinforcement of outdated or irrelevant perspectives. Regularly incorporate new, peer-reviewed research or perspectives to keep the chatbot's responses dynamic and aligned with ongoing debates in the field. 9. Consider Institutional and Ethical Responsibility: Ensure the chatbot aligns with institutional policies on academic freedom and bias. Faculty should be mindful of legal obligations, particularly in publicly funded institutions where viewpoint neutrality might be mandated. Likewise, understand your ethical responsibility in promoting critical thinking and diverse viewpoints, ensuring that the chatbot does not become a tool for propagating a singular perspective (even if it is your perspective) on complex issues. 10. Compare Outputs: Systematically compare the responses of the original LLM with those of the supplemented chatbot. Evaluate whether the supplemented chatbot aligns more closely with the curated data and if any new biases or shifts in perspectives-whether obvious or subtle, intended or unintended-have emerged, particularly in response to complex or ambiguous questions.</p><p>This list is not exhaustive; it doesn't address issues like student data privacy or the technical challenges of ensuring the LLM prioritizes specific content from the curated documents. It's also crucial to remember that the LLM doesn't "read" or "learn" the content the way a human would. Instead, it breaks the material into chunks (typically a few hundred tokens/words) and uses techniques like cosine similarity to find the best matches to the student's question. These matched chunks, along with any provided instructions and conversation history, shape the chatbot's response by influencing the next word it generates.</p><p>If the chatbot is only given supplemental documents with a single perspective, that bias will inevitably shape its responses, even if it's not immediately apparent. The extent to which this additional data influences the output is difficult to measure, but the volume and type of content you provide will have an impact. For instance, if most of the curated documents are empirical studies, the chatbot is more likely to reflect that empirical bias in subtle yet important ways when responding to student questions. If aligning with a particular perspective is part of your course objectives, this should be made explicit and transparent.</p><p>Bias in curation can cause students to accept certain viewpoints without question, limiting their critical thinking. That's why it's essential not only to include diverse resources but also to be clear about why specific perspectives were selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethical Obligations</head><p>As educators, we have ethical responsibilities to our students. Students are especially vulnerable to losing their agency by outsourcing critical thinking to LLMs (e.g., relying on ChatGPT to write essays). Societal and economic pressures further encourage this tendency, as LLMs become increasingly adept at mimicking high levels of intelligence.</p><p>In this context, transparency about the materials used to customize an AI-driven chatbot is the minimum obligation. To further meet our ethical responsibilities, we should also explain why specific documents were included over others. Even if certain documents reflect what you consider "ground truth" (e.g., on climate change or animal rights), it's essential to present alternative viewpoints and equip students with the critical thinking skills to evaluate the chatbot's outputs. Failing to do so undermines their agency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>Ultimately, customizing chatbots for course use presents a powerful tool for education, but it requires careful consideration of the biases we introduce. By curating diverse perspectives, being transparent with our choices, and continuously refining our chatbot content, we can foster critical thinking and help students engage more fully with the material.</p><p>Footnotes 1. Described in this article, but also see: Goel, A. K., &amp; Polepeddi, L. (2018). Jill Watson: A virtual teaching assistant for online education. In Learning engineering for online education (pp. 120-143). Routledge. 2. The quote is often attributed to Margaret Mead, though it appears to be more of a description of her philosophy than a direct quote from her writings. 3. Also see: <ref type="bibr">Velez, G., &amp; Power, S. A. (2020)</ref>. Teaching students how to think, not what to think: Pedagogy and political psychology. Journal of Social and Political Psychology, 8(1), 388-403.  4. A special thanks to Ani Meliksetyan for her recommendations on the potential biases.</p></div>		</body>
		<back>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
