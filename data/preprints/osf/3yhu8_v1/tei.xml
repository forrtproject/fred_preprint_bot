<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Computational text analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Marko</forename><surname>Bachl</surname></persName>
							<email>marko.bachl@fu-berlin.de</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Freie Universität Berlin</orgName>
								<orgName type="institution" key="instit2">Johannes Gutenberg-Universität Mainz</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Computational text analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F07C424C2F8F9CAAE3386600524A9BAB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-22T20:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction 1</head><p>Computational text analysis (CTA) comprises techniques for measuring the content of texts with the help of computer algorithms. The methods are discussed under various labels, such as text-as-data, automated content analysis, natural language processing, or text mining. The defining characteristic of a CTA technique is that once it is initially configured, the computational system performs the measurements independently without requiring any manual intervention or effort. The strength of CTA lies in its scalability, enabling the measurement of characteristics across vast amounts of text. As a result, CTA has seen widespread application in communication, related social sciences, and the digital humanities, with the increasing availability of digital or digitized, machine-readable texts.</p><p>We start this chapter with an overview of the historical development of CTA. We then systematize CTA along two dimensions: the representations of texts for the computational analysis and the supervision of the measurement process. While doing so, we provide some examples of popular techniques. The chapter ends with an outlook into the near future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Historical development</head><p>The history and development of CTA can be understood through three central themes: (1) the conceptual complexity of content analytical measures and the potential for measuring them with computer algorithms, (2) the development of hardware and software resources for CTA, and (3) the availability of digital, machine-readable texts. The first phase of CTA development in the late 1950s was characterized mainly by experiments using the computer as a new tool for the social sciences. These initial experiments were almost exclusively limited to producing text statistics, such as word counts, a technique that had been applied in disciplines including political science and literature studies since the 1920s <ref type="bibr" target="#b27">(Holsti, 1969;</ref><ref type="bibr" target="#b71">Stone et al., 1966)</ref>. Conceptually, these simple CTA methods were much less complex than contemporary manual content analysis approaches <ref type="bibr" target="#b36">(Lasswell et al., 1952;</ref><ref type="bibr" target="#b52">Osgood, 1959)</ref>. However, early content analysts were increasingly troubled by the costs of manual classification. They had high expectations for CTA, viewing its development as central to the success of content analysis as a method for the social sciences <ref type="bibr" target="#b70">(Stone, 1997)</ref>. However, CTA was plagued with numerous problems. Hardware and software limitations allowed only small amounts of text to be analyzed and supported only a few simple measures <ref type="bibr" target="#b28">(Iker &amp; Harway, 1965)</ref>. Additionally, all texts had to be digitized through a complex and error-prone process using punch cards. Thus, in the early 1960s, CTA was neither cost-effective-with half an hour of computing time costing as much as a secretary's monthly salary <ref type="bibr">(Stone, 1997, p. 42</ref>)-nor less laborious than traditional content analysis.</p><p>The introduction of the General Inquirer <ref type="bibr" target="#b71">(Stone et al., 1966)</ref> and Words <ref type="bibr" target="#b28">(Iker &amp; Harway, 1965)</ref> programs in the 1960s marked a milestone in the development of CTA. These software packages enabled CTA with relative ease, representing the prototypes of two techniquesdictionary-based classification and co-occurrence analysis-that would dominate the discipline for several decades. Despite continued technological advancements in the 1970s that made computers more accessible to social scientists, conceptual and methodological development of CTA slowed, and interest in these techniques waned, especially in the United States. However, many German scholars continued the research program of the General Inquirer by developing dictionaries and software such as Textpack, which were primarily designed for classifying openended survey questions but also used for text analysis <ref type="bibr" target="#b84">(Züll &amp; Mohler, 2001)</ref>.</p><p>The lack of machine-readable documents remained a limitation for many studies until the late 1970s, restricting researchers to archive material or non-news media. In 1973, DeWeese pioneered the automatic collection of daily media content by leveraging the increasingly common electronic typesetting machines used in newspapers <ref type="bibr" target="#b15">(DeWeese, 1977)</ref>. A few years later, LexisNexis began offering digital editions of American newspapers that were accessible electronically through remote access terminals. The 1980s saw a resurgence of interest in content analysis, particularly in communication and political science, driven by the development of personal computers and the growing availability of digital and digitized media content. Simultaneously, with advances in artificial intelligence research, approaches beyond dictionary classification and co-occurrence analysis were rediscovered. However, the initial enthusiasm (e.g., <ref type="bibr">Weber, 1984, p. 142)</ref> was tempered by the realization that computers could not understand texts in the foreseeable future (van Cuilenburg, <ref type="bibr" target="#b78">Kleinnijenhuis, &amp; Ridder, 1988)</ref>. Nevertheless, social scientists, most notably Dutch researchers from the CETA project, began to investigate the possibilities of syntactic-semantic content analysis with computer assistance, aiming to move beyond word counts and purely statistical approaches to text analysis.</p><p>The proliferation of the Internet since the 1990s has resolved the issue of limited access to machine-readable text content. Instead, the current challenge lies in managing the increasing volume and diversity of textual (and audio-visual) content produced and shared online. The 2000s and 2010s saw increased computational power of personal computers and the introduction of more user-friendly software packages. These developments made CTA accessible to a broader range of applied social scientists from communication and related disciplines <ref type="bibr" target="#b7">(Boumans &amp; Trilling, 2016;</ref><ref type="bibr" target="#b22">Grimmer &amp; Stewart, 2013;</ref><ref type="bibr" target="#b24">Hase et al., 2023;</ref><ref type="bibr" target="#b46">Macanovic, 2022)</ref>. While dictionaries and simple word co-occurrence analyses remained popular, they were supplemented by more advanced techniques such as supervised machine learning or topic models. However, these techniques still relied on a bag-of-words representation of texts, which largely ignored the syntactic structure of texts. This began to change in the 2010s and 2020s when deep learning approaches revolutionized many areas of natural language processing and computational text analysis. Neural network architectures, including word embeddings, recurrent neural networks, and transformers, enabled the learning of rich text representations that capture semantic and syntactic relationships. Embedding models gained popularity for representing words, sentences, and documents as dense vectors that encode their meaning and context. The availability of large pre-trained language models through open-source platforms and closed-source but user-friendly programming interfaces facilitated powerful transfer learning approaches. These approaches involve fine-tuning the models on specific downstream tasks using smaller labeled datasets or even zero-shot learning (i.e., new tasks without training data) <ref type="bibr">(Laurer et al., 2024a;</ref><ref type="bibr" target="#b74">Törnberg, 2023</ref><ref type="bibr" target="#b75">Törnberg, , 2024))</ref>. Social scientists increasingly adopted analysis techniques built with such models while critically examining bias, fairness, and interpretability <ref type="bibr" target="#b35">(Kroon et al., 2024)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Representing text as data</head><p>While obtaining and processing machine-readable texts was historically challenging, today, it is primarily a matter of technical access, ethical considerations, and legal restrictions. The critical methodological issue is structuring and condensing the texts to make them amenable to CTA. This section introduces three common text representations: bag of words, semantic networks, and embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Texts as bags of words</head><p>The bag-of-words (BoW) representation is a simple yet effective technique for representing text data in natural language processing tasks. In the BoW approach, a text (such as a sentence or a document) is represented as an unordered collection, or "bag," of its constituent tokens, disregarding grammar and word order but keeping track of token frequencies <ref type="bibr" target="#b22">(Grimmer &amp; Stewart, 2013)</ref>.</p><p>The BoW representation requires several text preprocessing steps to clean and normalize the text data <ref type="bibr" target="#b49">(Manning &amp; Schütze, 1999)</ref>. Commonly used techniques include tokenization, lowercasing, removing punctuation and numbers, stemming, lemmatization, and removing stop words. While text preprocessing is essential for model performance when analyzing BoW representations, different preprocessing choices can substantially impact the analysis results. This opens up researchers' degrees of freedom and calls reproducibility into question <ref type="bibr" target="#b13">(Denny &amp; Spirling, 2018;</ref><ref type="bibr" target="#b48">Maier et al., 2020;</ref><ref type="bibr" target="#b53">Pipal et al., 2023)</ref>. After tokenization and the other preprocessing steps, each unique token in the corpus becomes a feature in the resulting document-term matrix, where each row represents a document, each column corresponds to a token, and each cell contains the frequency or occurrence of a token in a document.</p><p>While using individual words as tokens is most common, the BoW model can be extended to incorporate n-grams -contiguous sequences of n items from a text sample. N-grams allow capturing some local word order and context. For example, with bigrams (n=2), the phrases "not good" and "very good" would be treated as distinct features, while the token "good" would be indistinguishable under a unigram (n=1) model. However, using higher-order n-grams can quickly lead to high-dimensional and sparse representations. Besides using n-grams, the BoW model makes the simplifying assumption that the order of tokens in a document can be neglected and that the content of documents is represented as a multiset of their tokens. While this representation loses the original word order, syntactic structures, and word-level context, it still preserves essential information about the content and topics of a document.</p><p>The main advantages of the BoW approach are its simplicity, efficiency, and ability to extract numeric features from text that can be used as input to statistical machine learning models. BoW models are straightforward to understand and implement, computationally efficient, and can handle large amounts of text data. However, the approach also has several limitations. One major disadvantage is that BoW is unsuited for short texts like social media messages or headlines. Such texts often lack sufficient word co-occurrence data and context for BoW to capture meaningful patterns. Social media content tends to be noisier, using more abbreviations, slang, and irregular structures that confuse BoW models. The BoW representation of short texts is typically very sparse, as most words in the vocabulary are absent. This sparsity can hurt the performance of machine learning models trained on BoW features. Other general limitations of BoW include losing word order information, ignoring word meaning and semantics, and struggling with frequent or rare words. Despite its drawbacks, due to its simplicity and efficiency, the BoW model remains a popular baseline and starting point for many CTA applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Texts as semantic networks</head><p>Semantic network analysis is a method for representing and analyzing the meaning and relationships between concepts in a text corpus <ref type="bibr" target="#b76">(van Atteveldt, 2008;</ref><ref type="bibr" target="#b78">van Cuilenburg et al., 1988)</ref>. In semantic networks, nodes represent semantic concepts such as words, phrases, or topics, while edges represent associations or relationships between those concepts, including cooccurrence, similarity, or causality.</p><p>A common approach to constructing semantic networks from text is identifying the cooccurrence of terms within a specific context window, such as a sentence, paragraph, or document. The more frequently two terms appear together, the stronger their association in the resulting network. Automatic extraction of key concepts and relationships from text to build the semantic network can be achieved using statistical and natural language processing techniques, such as parsing linguistic dependencies. Compared to the BoW representation, semantic networks retain more information beyond single tokens. The relational structure of the data necessitates more complex data representations than the rectangular document-feature matrix, often using nested tree structures.</p><p>Representing text as a semantic network can uncover implicit patterns and structures of meaning that may not be apparent from directly reading the text. Visual analysis of the network structure, such as identifying densely interconnected clusters of related terms, can reveal the text's main themes, frames, and associations. Additionally, quantitative network analysis measures can be applied to compare the centrality of different concepts and the strength of their relationships.</p><p>The network representation enables visual and quantitative analysis of semantic patterns in text, providing insights complementary to other text analysis methods. However, its adoption in applied social science research has remained limited. Initially, the much less complex BoW representations could increasingly rely on larger data and more powerful statistical models, compensating for some of their deficits compared to semantic networks. Subsequently, the more user-friendly embedding approaches, which share some of the advantages of semantic networks without requiring a deeper linguistic understanding of the texts, superseded the use of semantic networks in applications that required contextual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Texts as dense vectors (embeddings)</head><p>Word embeddings have revolutionized computational text analysis by enabling the representation of words as dense vectors in a high-dimensional space. These vector representations capture semantic and syntactic relationships between words, allowing machines to understand and process natural language more effectively <ref type="bibr" target="#b35">(Kroon et al., 2024)</ref>. Early word embedding models, such as word2vec <ref type="bibr" target="#b50">(Mikolov et al., 2013)</ref> and GloVe <ref type="bibr" target="#b54">(Pennington et al., 2014)</ref>, learn word vectors based on the distributional hypothesis, which states that words occurring in similar contexts tend to have similar meanings. For example, word2vec uses a neural network to predict a target word given its surrounding context words or vice versa. GloVe, on the other hand, learns word vectors by factorizing a word co-occurrence matrix. These models have been widely adopted because they capture meaningful word relationships.</p><p>While word embeddings have proven highly effective, they have limitations. Each word is represented by a single vector, regardless of its context, which can be problematic for polysemous words with multiple meanings. Moreover, word embeddings do not capture higherlevel semantic information in sentences or documents. Recent advances in transformer-based models, such as BERT <ref type="bibr" target="#b14">(Devlin et al., 2019)</ref> and GPT <ref type="bibr">(Radford et al., n.d.)</ref>, have introduced contextualized word embeddings to address these issues. These models use an attention mechanism to learn dynamic word representations that adapt to the surrounding context, enabling them to disambiguate word senses and capture more nuanced semantic relationships.</p><p>Building upon the success of contextualized word embeddings, researchers have developed methods to generate sentence and document embeddings. These embeddings aim to encapsulate the overall meaning of a piece of text into a single vector representation. Approaches such as doc2vec <ref type="bibr" target="#b45">(Le &amp; Mikolov, 2014)</ref> extend the word2vec framework to learn document embeddings, while others leverage pre-trained transformer models to generate sentence embeddings by pooling or averaging the contextualized word embeddings. These higher-level embeddings enable document classification, clustering, and semantic similarity analysis tasks.</p><p>One key advantage of embedding representations is that they often require only minimal preprocessing of the texts. For example, stemming and lemmatization become unnecessary when words are projected to the vector space where different word forms are very close but not identical. This retains the information that they are very similar features, compared to a BoW document-term matrix where two word forms are treated as entirely different, unrelated features. Another advantage is the availability of multilingual and even multimodal embeddings. The former allows for analyses across language boundaries without the intermediate translation step <ref type="bibr" target="#b42">(Licht, 2023)</ref>. While still in earlier stages of development, the latter promises to enable an integrated content analysis of texts, visuals, and audio signals <ref type="bibr" target="#b41">(Li et al., 2019)</ref>. Importantly, embedding models are well-suited for shorter texts like social media posts. Traditional bag-ofwords approaches struggle with the sparsity and lack of context in short texts, while word and sentence embeddings can effectively capture semantic similarities even with limited information <ref type="bibr" target="#b35">(Kroon et al., 2024)</ref>. However, these benefits come at the potential cost of using pre-trained models, which might contain biases present in the training data that can propagate to downstream applications <ref type="bibr" target="#b4">(Bender et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">A typology of CTA approaches</head><p>Similar to general statistical learning methods, approaches to CTA can be divided into unsupervised and supervised methods. Unsupervised methods aim to develop a function to classify messages into a priori unknown categories or along unknown dimensions. In contrast, supervised approaches assign messages to pre-defined categories or dimensions <ref type="bibr" target="#b22">(Grimmer &amp; Stewart, 2013)</ref>. A typical example of an unsupervised statistical method is cluster analysis, in which similar objects are grouped automatically. Unsupervised approaches have the advantage of being fully automatic. A researcher can define the relevant features or clustering rules but cannot directly influence the outcome of the analysis. This property makes unsupervised methods attractive for descriptive or explorative studies but unsuitable for hypothesis-driven content analyses. While a fully automated analysis can provide results quickly and without much prior effort, making sense of the results can be complex and leaves much room for subjective interpretations.</p><p>A typical example of a supervised statistical method is logistic regression, in which cases are predicted to be in one of two groups based on a function of predictor variables. Supervised approaches to CTA require human intervention in the classification process. Researchers must provide either pre-defined classification rules or classified example texts from which classification rules are derived. Because the researcher can define the outcomes of supervised approaches, they produce results that are straightforward to interpret. However, since they require human judgment, they are more costly in terms of time and effort and are subject to human errors in the classification or definition of rules and instructions. The errors can, in turn, bias the results of an analysis based on the classification <ref type="bibr" target="#b2">(Bachl &amp; Scharkow, 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Unsupervised approaches</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Text statistics</head><p>CTA based on text statistics typically builds on the BoW representation of texts. The techniques assume that inferences can be made about the context of messages, their authors, and their reception from the frequency of words and n-grams. Because computers are demonstrably faster and more reliable in counting words, they have been in use since the introduction of CTA. Text statistics are frequently employed in stylometry and authorship research, as an author's relatively clear fingerprint can be generated by examining the frequency distributions of certain words <ref type="bibr" target="#b51">(Mosteller &amp; Wallace, 1964)</ref>. Technically, this can be accomplished by simply summarizing the columns of one or more document-term matrices.</p><p>Similarly, selecting key terms from documents by comparing their frequency within a document and in a larger corpus is possible. The ratio of the term frequency and the inverse document frequency (TF/IDF, see <ref type="bibr" target="#b49">Manning &amp; Schütze, 1999)</ref> is commonly used to measure the relative importance of a term and used as a feature weight in subsequent analyses. Word clouds, a figure that displays the most essential words in a collection of texts sized by their relevance, are popular but controversial visualizations of a simple text statistic.</p><p>Text statistics is also used to determine the formal readability and comprehensibility of texts, which assumes that specific text indicators can predict the content's complexity and comprehensibility. Typical indicators are average word and sentence length, vocabulary diversity, and the frequency of punctuation marks. These text statistics can be used as properties of message origins or production, for example, to distinguish between content from broadsheet and tabloid newspapers, tailor content to the expected recipients, e.g., children or adults, or check the comprehensibility of a message. For example, <ref type="bibr" target="#b31">Kleinnijenhuis (1991)</ref> used statistical readability measures of newspaper coverage to indicate their complexity and linked these measures to the readers' knowledge. Thoms and colleagues (2020) investigated whether corporate reports differed in readability depending on whether they communicated good or bad news.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Co-occurrence analysis</head><p>In unsupervised analyses of texts, both the frequency of individual words and their associations-the co-occurrence of specific features within messages-are of interest. Cooccurrence analysis is the bivariate extension of the word frequency analysis. It is based on the assumption that cognitively or semantically related constructs are also spatially close to each other. By looking at words within a pre-specified unit of text, such as documents, paragraphs, or sentences, the association (collocation) of specific terms can be summarized in a contingency table or a similarity matrix. For example, a document-term matrix can be multiplied with its transposed form to yield a term-term matrix. This, in turn, can be subjected to cluster analysis or multidimensional scaling, which can be used to condense and visualize word associations. An alternative approach, pioneered by <ref type="bibr" target="#b28">Iker and Harway (1965)</ref> with their Words program, uses exploratory factor analysis directly on the columns of a DTM. Co-occurrence analysis has been used extensively in communication research to understand semantic relations between concepts and how they change over time. It has also been used in the meta-analysis of communication research: <ref type="bibr" target="#b18">Doerfel and Barnett (1999)</ref> analyzed ICA conference paper titles using the popular CATPAC software <ref type="bibr" target="#b17">(Doerfel &amp; Barnett, 1996)</ref> to reveal divisional and topical associations.</p><p>Co-occurrence analysis is frequently used as a dimensionality reduction technique. Instead of working with individual columns of a document-term matrix, researchers can use latent semantic indexing to reduce the number of variables by summarizing co-occurring terms in larger components or indices, which are then used in subsequent analyses <ref type="bibr" target="#b12">(Deerwester et al., 1990)</ref>. While simpler forms of co-occurrence analysis have fallen out of fashion with the availability of more powerful models, it is noteworthy that the concept of co-occurrence remains integral to embedding-based models. The embedding vectors represent co-occurrence patterns in the training material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Text clustering, text scaling, and topic models</head><p>Grouping texts by their content and, thereby, exploring the content of a large collection of texts is one of the most popular applications of CTA. The most straightforward approach is document clustering based on the document-term matrix <ref type="bibr" target="#b22">(Grimmer &amp; Stewart, 2013)</ref>. Similar to co-occurrence analyses, document clustering assumes that messages containing the same features are semantically or thematically similar. To determine the similarity between two documents, one can compute the similarity or distance measure based on the feature vector of each document. The cosine or the Jaccard coefficient is often used since these are relatively independent of the text length, i.e., the number of relevant features <ref type="bibr" target="#b49">(Manning &amp; Schütze, 1999)</ref>. The resulting document-document distance matrix is used as a starting point for various cluster-based analytical methods. As with co-occurrence analysis, only the number of clusters can be determined before the analysis (e.g., using k-means clustering) or retrospectively (for hierarchical agglomerative methods). Document scaling methods build on a similar idea. Instead of assigning a text to one discrete group, these techniques analyze and position documents along a latent dimension, often representing political ideology or sentiment. Wordscore <ref type="bibr" target="#b39">(Laver et al., 2003)</ref> and Wordfish <ref type="bibr" target="#b67">(Slapin &amp; Proksch, 2008)</ref> are popular text scaling methods. Wordfish is a completely unsupervised scaling model that uses word frequencies to estimate the positions of documents on a single dimension, assuming the frequencies follow a Poisson distribution. It does not require reference texts and simultaneously learns document positions and word weights. In contrast, Wordscore requires some supervision, namely a set of two reference texts with known positions relative to each other, to estimate the positions of virgin texts. Both methods assume that word usage reflects the underlying positions of documents. A similar approach that goes beyond the estimation of political positions is latent semantic scaling <ref type="bibr" target="#b81">(Watanabe, 2021)</ref>. It aims to place texts on a latent semantic dimension by analyzing semantic proximity between some seed words provided by the user and other words in the corpus. These methods identify latent traits that characterize and differentiate the documents.</p><p>Topic models are a conceptual extension of the document clustering technique. The different varieties of topic models are arguably among the most popular CTA methods in communication (see <ref type="bibr" target="#b11">Chen et al., 2023</ref>, for a recent review). In contrast to traditional clustering techniques, topic models are mixed-membership models. They assume that texts belong to several latent topics at once and that terms and their co-occurrences have different probabilities conditional on the topic. Therefore, topic models can be considered a combination of term and document clustering. Latent Dirichlet allocation <ref type="bibr" target="#b6">(Blei, 2012)</ref> was the first type of topic model widely used in communication <ref type="bibr" target="#b11">(Chen et al., 2023)</ref>. Another popular choice are structural topic models, which can estimate the relationships of topic frequency with covariates, making it easy to analyze trends over time or differences between communicators or media outlets <ref type="bibr" target="#b60">(Roberts et al., 2019)</ref>.</p><p>There is an active methodological research stream in communication on applying and improving topic models. <ref type="bibr">Maier and colleagues (2018)</ref> presented an early guideline on how to apply topic modeling and report the results. A recent review by Chen and colleagues (2023) discusses the use of the technique in communication. They critically conclude that, while topic models contributed substantially to exploratory research, many applications remained undertheorized and would benefit from more systematic validation (see also <ref type="bibr" target="#b5">Bernhard et al., 2023)</ref>. Several studies have investigated topic modeling of multilingual text collections <ref type="bibr" target="#b10">(Chan et al., 2020;</ref><ref type="bibr" target="#b43">Lind et al., 2022;</ref><ref type="bibr" target="#b47">Maier et al., 2022)</ref>. Other papers made recommendations on how topic modeling can be integrated into a larger content-analytical framework that includes both CTA and human input <ref type="bibr" target="#b3">(Baden et al., 2020;</ref><ref type="bibr">Drohr &amp; Ophir, 2019;</ref><ref type="bibr" target="#b59">Rinke et al., 2022)</ref>.</p><p>While most of the published applications of topic modeling and related techniques were built on BoW representations, modern implementations take word or text embedding vectors as their input data. Text scaling methods can use word embeddings to estimate political ideology <ref type="bibr" target="#b58">(Rheault &amp; Cochrane, 2020)</ref>. The BERTopic implementation <ref type="bibr" target="#b23">(Grootendorst, 2022)</ref> starts with embedding the texts in a high-dimensional vector space. The dimensionality is reduced, and text clusters are identified in the low-dimensional space. Only after the clustering are the tokens of all texts in one cluster merged and analyzed in a bag-of-word logic. Embedding-based topic models have great promise for communication research. Many studies aim to explore the content of social media messages, which are typically too short for BoW-based clustering <ref type="bibr" target="#b11">(Chen et al., 2023)</ref>. <ref type="bibr">Simon and colleagues (2023)</ref> showcased an early adoption of BERTopic in their largescale analysis of information flows within the Dutch Telegramsphere.</p><p>Topic models and other text clustering or scaling techniques are expected to remain highly popular in communication research. These methods enable researchers to efficiently explore, describe, and, to a certain extent, understand previously unknown properties of large text collections. From a methodological standpoint, novel models that move beyond the BoW representation will address many limitations identified in previous applications. However, to enhance the contribution of such studies to communication research, a more robust reflection on the results from a theoretical and conceptual perspective is still necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Supervised approaches</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Rule-based CTA: Dictionaries</head><p>For decades, dictionaries have often been used synonymously with CTA. The basic principles underlying this approach have barely changed since <ref type="bibr" target="#b71">Stone et al. (1966)</ref> introduced the General Inquirer. Researchers develop a category system in which individual words (or other features) are defined to serve as indicators for the category of interest. The word list, or dictionary, must be constructed to be both exhaustive (all relevant features are scored) and specific (only relevant features are scored so that the risk of false positive classifications is minimized). This simplifies the analysis of term-document matrix problems. In every row, the number of relevant features from the dictionary is counted, and the documents are classified according to a threshold criterion. This approach enables researchers to quickly and reliably assign many texts to pre-defined categories. Because of the fully deterministic matching process, dictionary-based classification is perfectly reliable (in the sense that all classifications are fully reproducible); however, there is no room for fuzzy categories, double meanings, or contextual factors inherent to natural language. Accordingly, most of the earlier research on dictionary classification focused on text and term pre-processing to reduce language ambiguity and develop valid dictionaries for various research questions.</p><p>Dictionary-based classifications were considered attractive because of the quick and reliable classification process and because they promised reusable dictionaries that could foster collaboration and replication. However, this promise was only partially fulfilled. While several general-purpose dictionaries such as the Harvard IV Dictionary, the Lasswell Values Dictionary, or the Linguistic Inquiry and Word Count have been frequently applied, most dictionary-based studies used ad-hoc instruments that were rarely shared or reused. In addition, scholars have argued against the validity of dictionary classification because many theoretically relevant concepts that are relatively easy for humans to grasp cannot help build a reliable and valid dictionary despite extensive work. For example, Chan and colleagues (2021) demonstrated how using different dictionaries developed to measure the same construct can lead to completely different results. Moreover, when manual pre-processing is necessary to deal with spelling errors, homonyms, and other sources of measurement errors, dictionary-based content analysis requires more resources and effort than a traditional manual approach unless enormous quantities of messages have to be classified. Despite these limitations, dictionaries are still actively used and developed, not least because they are so straightforward to understand and apply. This sets the method apart from machine learning methods, which require more technical skills from the researchers and perform the classification with algorithms that are harder to understand than matches with predefined indicators. There is ongoing methodological research into improving the application and development of dictionaries. A critical issue for comparative research is the creation of multilingual dictionaries (e.g., <ref type="bibr" target="#b44">Lind et al., 2019)</ref>. Researchers have recommended combining dictionaries with machine-learning approaches to improve the quality of the dictionaries <ref type="bibr" target="#b29">(King et al., 2017)</ref> and to transfer dictionaries to new domains <ref type="bibr" target="#b16">(Dobbrick et al., 2022)</ref>.</p><p>Embedding-based models have also impacted dictionary-related work in several ways. For one, embeddings can be used to construct dictionaries or extend them beyond some seed terms <ref type="bibr" target="#b1">(Amsler, 2020;</ref><ref type="bibr" target="#b40">Liang et al., 2023;</ref><ref type="bibr" target="#b69">Stoll et al., 2023;</ref><ref type="bibr" target="#b83">Widmann &amp; Wich, 2023)</ref>. The researchers compile an initial set of terms from the literature to measure the construct. They then expand the dictionary by the nearest neighbors of the seed terms in an embedding space. A more complete representation of the construct of interest is achieved by reiterating this procedure and validating the expanded dictionaries against standard data. The new dictionary is then matched against the texts as usual for classification. The advantage of this approach is that the result is still a dictionary that can be easily understood, modified, and applied. The disadvantage is that, as a dictionary, it still relies on exact string matching for classification with all its shortcomings and requirements on preprocessing.</p><p>Another approach is based on training a word embedding model on the texts of interest and then computing the distances of target words and dictionary entries in the embedding space. For example, Andrich and colleagues (2023) trained embedding models on US American news articles. They then computed the distances between politicians and a dictionary of adjectives indicative of ten trait groups to measure gender-stereotypical representations. Similarly, Kroon and colleagues (2021) used the distances between ethnic categories and low-and high-thread terms in embedding representations of Dutch news to study stereotypical depictions of in-and outgroup minorities. The primary advantage of measuring distances in custom embedding spaces is that continuous distances do not require exact matches and can accommodate different word forms or misspellings. Furthermore, these distances carry more information than a binary match/mismatch indicator. However, a significant disadvantage is that this approach is only feasible with large collections of texts and considerable computational resources necessary for training custom embedding models. Additionally, the distance metric is less intuitive than the straightforward "number of matches" measure.</p><p>"Distributed Dictionary Representations" <ref type="bibr" target="#b19">(Garten et al., 2018)</ref> is a related approach that can rely on publicly available embedding models trained on extensive text collections or custom embedding models. Like traditional dictionary analysis, a list of terms is compiled to represent a construct. However, instead of matching the dictionary with BoW representations of the texts, the similarity between the construct and a text is measured by the distance in the embedding space. Each text receives a score from -1 (entirely dissimilar) to +1 (identical). For example, Thiele (2024) applied this method to measure populism in comments to COVID-related social media posts by news organizations. Like measuring distances between target words and dictionary terms in the space of custom embedding models, this approach has the advantages of not requiring discrete string matching and providing more granular indicators, but the distance measure is less intuitive. When used with a pre-trained model, distributed dictionary representations can be applied to smaller text samples using user-grade compute resources, making it feasible for a broader range of applications. However, the choice of embedding model can substantially impact classification performance <ref type="bibr" target="#b19">(Garten et al., 2018)</ref>, and unknown biases in the training data can have downstream consequences. When used with a custom model, the previously described requirements of large samples and computing resources also apply.</p><p>Dictionary methods remain among the most frequently used CTA techniques despiteand because of-their simplicity. However, transitioning from string matching to measuring distances in embedding spaces has the potential to address some of the weaknesses associated with traditional dictionaries. These approaches share certain characteristics with few-shot and zero-shot machine learning models, described in the section after the next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Supervised machine learning</head><p>A fundamental disadvantage of the dictionary method is that operationalization and classification differ significantly from how one would describe the construct of interest to another human, e.g., the annotators in traditional content analysis. Consequently, transferring expertise, previous results, or even instruments from manual content analyses to dictionary approaches is rarely possible. Supervised machine learning promises to make this possible by using messages classified by humans as training material for statistical learning algorithms, which in turn are used to classify large amounts of documents <ref type="bibr" target="#b22">(Grimmer &amp; Stewart, 2013)</ref>. In supervised learning, training the computer is done by example: Instead of providing an extensive and exhaustive list of dictionary terms, the researcher provides example documents that belong to one category or another. The learning algorithms derive the classification rules through repeated examples and feedback.</p><p>The general workflow of CTA using supervised machine learning remains consistent regardless of the materials or topic being investigated. First, researchers define the constructs under investigation for human annotators, who then classify the training and test texts. Next, the texts are transformed into one or more data representations (see section "Representing texts as data"). One or more models are then trained on the training texts and validated on the test texts. These models can be relatively simple and computationally efficient, such as Naive Bayes, logistic regression, support vector machines, or more advanced, such as different shallow or deep neural network implementations. Based on their performance, one or a set of data representation and model combinations are selected if their quality is deemed sufficient. If none of the modeldata combinations perform satisfactorily, parts of the process are adapted, and the subsequent steps repeated. Additional tests are conducted on new test data. The process is repeated until the desired model performance is achieved.</p><p>Supervised machine-learning approaches to CTA based on BoW models and simpler algorithms have shown mixed results in early evaluations in the social sciences <ref type="bibr" target="#b22">(Grimmer &amp; Stewart, 2013)</ref>. <ref type="bibr" target="#b62">Scharkow (2013)</ref> demonstrated that machine learning can reliably detect sports or politics in news articles. Similarly, Burscher and colleagues (2014) successfully classified the manifest indicators of relatively simple, holistic frames. In the political domain, <ref type="bibr">Hillard and colleagues (2008)</ref> developed models that accurately predicted the topics of congressional bills. However, <ref type="bibr" target="#b62">Scharkow (2013)</ref> found that more complex categories, such as the news factor controversy, were considerably more challenging to classify automatically. Although these findings were methodologically promising, the use of supervised machine-learning approaches to CTA in applied communication research remained limited. One likely reason was that theoretically more interesting concepts were still beyond the capabilities of these earlier implementations. Another reason seemed to be the complexities of the pre-processing pipelines of the BoW approach, which had the potential to bias the results in unexpected directions.</p><p>Embedding representations of texts and more advanced machine-learning algorithms have helped to address these challenges. <ref type="bibr">Kroon and colleagues (2022)</ref> compared the impact of bag-ofwords (BoW) and word-embedding representations of Dutch news articles. Still using relatively simple statistical algorithms, such as support vector machines and stochastic gradient descent, they demonstrated that models trained on word embeddings required less training data and achieved better performance in classifying policy issues and frames compared to the same models trained on BoW data and dictionaries. <ref type="bibr">Rudkowsky and colleagues (2018)</ref> showed the superior performance of a neural network classifier based on an embedding representation for sentiment analysis of sentences from parliamentary speeches compared to a multinomial Naive Bayes classifier based on a BoW representation. Van Atteveldt and colleagues (2021) reported similar results for measuring economic sentiment in news headlines, with a convolutional neural network based on word embeddings outperforming BoW-based supervised machine learning and dictionaries.</p><p>Large language models, deep-learning models based on the transformer architecture, have defined the next wave of innovation. These models have two main advantages compared to earlier methods. First, they can account for word order and context, allowing them to distinguish between identical terms that have different meanings depending on the context. Second, their multilayer architecture makes them ideal for fine-tuning. Pre-trained models can be adapted for new classification tasks instead of requiring completely new training. For example, Viehmann and colleagues (2023) showed that domain-and language-adapted BERT-based models performed best in detecting the stance of Twitter messages towards an issue. Using data from van Atteveldt and colleagues (2021), they also demonstrated the superior performance of adapted transformer models in classifying the sentiment of economic news. Similar results hold for classifying discrete emotions in political texts, with fine-tuned transformers considerably outperforming previous supervised machine-learning approaches based on word embeddings and dictionaries <ref type="bibr" target="#b83">(Widmann &amp; Wich, 2023)</ref>. Laurer and colleagues (2024a) added natural language inference capabilities to transformer models by pre-training on classification tasks from various domains, making fine-tuning to new classification tasks more efficient. Their headline result in the political context shows that training their model on 500 labeled texts is about as effective as training older models on approximately 5,000 texts.</p><p>The modern transformer models are more performant and much easier to use than any previous machine learning setup for CTA. Advanced embeddings and the availability of adapted models and tokenizers for many domains and languages reduce the formerly tedious preprocessing tasks to a few reasonable default choices. Platforms like Hugging Face (<ref type="url" target="https://huggingface.co/">https://huggingface.co/</ref>) are increasingly used to share pre-trained models for many tasks, which can be fine-tuned or used out of the box. Easy-to-use software makes the techniques available to the average social scientist by directly connecting to the platforms and using the models in established analysis pipelines (e.g., <ref type="bibr" target="#b8">Chan, 2023;</ref><ref type="bibr" target="#b30">Kjell et al., 2023;</ref><ref type="bibr" target="#b57">Rajapakse, 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Few-shot and zero-shot classification</head><p>Large language models have recently introduced few-shot and zero-shot approaches to classification tasks, bringing significant innovations to CTA. In few-shot learning, a model is trained on a minimal number of samples before classifying new texts, sometimes using as few as one example per class. Zero-shot approaches take this further by performing classification without any training data. Laurer and colleagues (2024a) presented natural language inference models capable of zero-shot classification on new tasks due to their training on diverse classification tasks. These models assign probabilities to each class based on a text and a short hypothesis about its classification (e.g., "This text is about {class}," where {class} is a list of classes). While their model can still be improved by task-relevant training data, the zero-shot classification beat the random and majority baselines in many of their test cases.</p><p>The public introduction of ChatGPT (<ref type="url" target="https://chat.openai.com/">https://chat.openai.com/</ref>) in late 2022 popularized generative language models, prompting many researchers to experiment with using these models for few-shot or zero-shot classification. In this approach, researchers provide a generative language model with a prompt containing a category, its classes, definitions, possibly example text, and additional instructions. The model then classifies the provided text based on this information. It can also generate text to explain its "reasoning" for choosing the respective class <ref type="bibr" target="#b74">(Törnberg, 2023)</ref>. For example, <ref type="bibr">Gilardi and colleagues (2023)</ref> tested the zero-shot performance of GPT-3.5, the model behind the first public version of ChatGPT, in several classification tasks typical for communication research, such as classifying frames, topics, and sentiment in tweets and newspaper articles. They concluded that the model outperformed crowd workers at a fraction of the cost and that model performance was sufficient compared to expert annotators for most tasks. <ref type="bibr" target="#b25">Heseltine and Clemm von Hohenberg (2024)</ref> found that GPT-4, the successor to GPT-3.5, could reliably classify whether a tweet was political, negative, its sentiment, and its political ideology (left-wing, centrist, or right-wing). Moreover, the results were almost as good for tweets from Chile, Germany, and Italy and longer texts (news articles from U.S. news outlets) as for English tweets from U.S. politicians. Many similar results have been published as preprints over the past two years.</p><p>Few-shot and zero-shot approaches share similarities with dictionaries that distinguish them from supervised machine-learning approaches. The model that performs the classification task remains fixed during the study, akin to the term list and scoring rules of a dictionary approach. Rather than learning from training data by adapting model weights, these approaches rely solely on information from the instruction and texts for classification. As a result, the main focus for researchers becomes constructing and improving these inputs through an iterative process known as "prompt engineering" <ref type="bibr" target="#b75">(Törnberg, 2024)</ref>. Human-classified texts are only necessary for model validation when developing a new instruction or transferring an instruction to a new model or texts from a different population. By reducing the need for time-and costintensive human classifications, few-shot and zero-shot classification techniques can increase the accessibility and flexibility of large-scale CTA. However, the computing resources required for model inference with state-of-the-art generative language models exceed what is available to most social scientists, limiting individual researchers' flexibility. Additionally, commercial companies offer many of the most performant models as closed-source cloud services, posing challenges to open science principles such as reproducibility and raising concerns about biases encoded in the models <ref type="bibr" target="#b4">(Bender et al., 2021;</ref><ref type="bibr" target="#b68">Spirling, 2023)</ref>. Natural language inference models offer a computationally efficient alternative for classification tasks, as they do not generate text output and can run on modern end-user hardware. Moreover, most of these models are available as open-source software <ref type="bibr" target="#b38">(Laurer et al., 2024b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Outlook</head><p>Methods based on embeddings, transformers, and large language models are poised to replace traditional bag-of-words approaches sooner rather than later. This shift represents a positive development for the field, as these advanced techniques offer superior performance and robustness compared to their predecessors. By reducing the impact of researchers' choices in text preprocessing, these methods can help to standardize analyses and improve the reproducibility of findings. However, as we embrace this change, it is crucial to remain aware of the potential drawbacks associated with these cutting-edge approaches. One concern is the presence of unclear biases in the underlying models, which may stem from the training data used. Additionally, these methods often require more computational resources than traditional techniques, which could limit their accessibility to some researchers. Furthermore, reliance on closed-source models from commercial organizations raises questions about transparency and reproducibility.</p><p>To address these challenges, the communication research community should prioritize building upon open-source models and fostering interdisciplinary cooperation. By collaborating with experts in computer science, linguistics, and other relevant fields, we can develop robust, transparent, and accessible tools for CTA. This collaborative approach will enable us to leverage the strengths of CTA techniques while mitigating their potential weaknesses. Ultimately, by embracing the potential of embeddings, transformers, and large language models while also addressing their limitations, communication researchers can focus on their core expertise: improving measures in relation to theories and validating the performance of these measures. As we move forward, it is essential to balance harnessing the potential of cutting-edge computational methods and maintaining a strong connection to the fundamental principles of social science research.</p></div>			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>This chapter is a completely revised and extended version of<ref type="bibr" target="#b63">Scharkow (2017)</ref>.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Goodbye, gender stereotypes? Trait attributions to politicians in 11 years of news coverage</title>
		<author>
			<persName><forename type="first">A</forename><surname>Andrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bachl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Domahidi</surname></persName>
		</author>
		<ptr target="https://doi.org/10/gsxh3w" />
	</analytic>
	<monogr>
		<title level="j">Journalism &amp; Mass Communication Quarterly</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="473" to="497" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Using lexical-semantic concepts for fine-grained classification in the embedding space</title>
		<author>
			<persName><forename type="first">M</forename><surname>Amsler</surname></persName>
		</author>
		<idno type="DOI">10.5167/uzh-189884</idno>
		<ptr target="https://doi.org/10.5167/uzh-189884" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
		<respStmt>
			<orgName>University of Zurich</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Dissertation</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Correcting measurement error in content analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bachl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Scharkow</surname></persName>
		</author>
		<ptr target="https://doi.org/10/ghhzbn" />
	</analytic>
	<monogr>
		<title level="j">Communication Methods and Measures</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="87" to="104" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hybrid content analysis: Toward a strategy for the theory-driven, computer-assisted classification of large text corpora</title>
		<author>
			<persName><forename type="first">C</forename><surname>Baden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kligler-Vilenchik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yarchi</surname></persName>
		</author>
		<ptr target="https://doi.org/10/ghzn2b" />
	</analytic>
	<monogr>
		<title level="j">Communication Methods and Measures</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="165" to="183" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the dangers of stochastic parrots: Can language models be too big? 🦜</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mcmillan-Major</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shmitchell</surname></persName>
		</author>
		<ptr target="https://doi.org/10/gh677h" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the 2021 ACM Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="610" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Topic model validation methods and their impact on model selection and evaluation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bernhard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Teuffenbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Boomgaarden</surname></persName>
		</author>
		<ptr target="https://doi.org/10/gt2x55" />
	</analytic>
	<monogr>
		<title level="j">Computational Communication Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Probabilistic topic models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<ptr target="https://doi.org/10/b39c" />
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="77" to="84" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Taking stock of the toolkit: An overview of relevant automated content analysis approaches and techniques for digital journalism scholars</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Boumans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Trilling</surname></persName>
		</author>
		<idno type="DOI">10.1080/21670811.2015.1096598</idno>
		<ptr target="https://doi.org/10.1080/21670811.2015.1096598" />
	</analytic>
	<monogr>
		<title level="j">Digital Journalism</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="8" to="23" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">grafzahl: Fine-tuning Transformers for text data from within R</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chan</surname></persName>
		</author>
		<idno type="DOI">10.5117/CCR2023.1.003.CHAN</idno>
		<ptr target="https://doi.org/10.5117/CCR2023.1.003" />
	</analytic>
	<monogr>
		<title level="j">Computational Communication Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Four best practices for measuring news sentiment using &apos;off-the-shelf&apos; dictionaries: A large-scale p-hacking experiment</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chan Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bajjalieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Auvil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Althaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Welbers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Atteveldt</surname></persName>
		</author>
		<author>
			<persName><surname>Van</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jungblut</surname></persName>
		</author>
		<idno type="DOI">10.5117/CCR2023.1.003.CHAN</idno>
		<ptr target="https://doi.org/10/gt3d7x" />
	</analytic>
	<monogr>
		<title level="j">Computational Communication Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reproducible extraction of cross-lingual topics (rectr)</title>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jungblut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Welbers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Bajjalieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Van Atteveldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Althaus</surname></persName>
		</author>
		<ptr target="https://doi.org/10/gms9jj" />
	</analytic>
	<monogr>
		<title level="j">Communication Methods and Measures</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="285" to="305" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">What we can do and cannot do with topic modeling: A systematic review</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Choi</surname></persName>
		</author>
		<ptr target="https://doi.org/10/gr4345" />
	</analytic>
	<monogr>
		<title level="j">Communication Methods and Measures</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="111" to="130" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Harshman</surname></persName>
		</author>
		<ptr target="https://doi.org/10/db4ft5" />
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Text preprocessing for unsupervised learning: Why it matters, when it misleads, and what to do about it</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Denny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Spirling</surname></persName>
		</author>
		<ptr target="https://doi.org/10/gdjsqk" />
	</analytic>
	<monogr>
		<title level="j">Political Analysis</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="168" to="189" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1810.04805</idno>
		<ptr target="https://doi.org/10.48550/arXiv.1810.04805" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Computer content analysis of &quot;day-old&quot; newspapers: A feasibility study</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deweese</surname></persName>
		</author>
		<idno type="DOI">10.1086/268357</idno>
		<ptr target="https://doi.org/10.1086/268357" />
	</analytic>
	<monogr>
		<title level="j">Public Opinion Quarterly</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="94" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Enhancing theory-informed dictionary approaches with &quot;glass-box&quot; machine learning: The case of integrative complexity in social media comments</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dobbrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wessler</surname></persName>
		</author>
		<ptr target="https://doi.org/10/gt28wz" />
	</analytic>
	<monogr>
		<title level="j">Communication Methods and Measures</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="303" to="320" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The use of CATPAC for text analysis</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Doerfel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Barnett</surname></persName>
		</author>
		<ptr target="https://doi.org/10/dc8mgt" />
	</analytic>
	<monogr>
		<title level="j">CAM Journal</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="4" to="7" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A semantic network analysis of the international communication association</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Doerfel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Barnett</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1468-2958.1999.tb00463.x</idno>
		<ptr target="https://doi.org/10.1111/j.1468-2958.1999.tb00463.x" />
	</analytic>
	<monogr>
		<title level="j">Human Communication Research</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="589" to="603" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dictionaries and distributions: Combining expert knowledge and large scale textual data content analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Garten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Boghrati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Iskiwitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<ptr target="https://doi.org/10/gc3z28" />
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="344" to="361" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The analysis of communication content</title>
		<author>
			<persName><forename type="first">G</forename><surname>Gerbner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Holsti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Krippendorff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Paisley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1969">1969</date>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ChatGPT outperforms crowd workers for textannotation tasks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Gilardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kubli</surname></persName>
		</author>
		<ptr target="https://doi.org/10/gsqx5m" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">30</biblScope>
			<biblScope unit="page">2305016120</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Text as data: The promise and pitfalls of automatic content analysis methods for political texts</title>
		<author>
			<persName><forename type="first">J</forename><surname>Grimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Stewart</surname></persName>
		</author>
		<idno type="DOI">10.1093/pan/mps028</idno>
		<ptr target="https://doi.org/10.1093/pan/mps028" />
	</analytic>
	<monogr>
		<title level="j">Political Analysis</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="267" to="297" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">BERTopic: Neural topic modeling with a class-based TF-IDF procedure</title>
		<author>
			<persName><forename type="first">M</forename><surname>Grootendorst</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2203.05794</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2203.05794" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The &quot;computational turn&quot;: An &quot;interdisciplinary turn&quot;? A systematic review of text as data approaches in journalism studies</title>
		<author>
			<persName><forename type="first">V</forename><surname>Hase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Schäfer</surname></persName>
		</author>
		<ptr target="https://doi.org/10/gt2kx9" />
	</analytic>
	<monogr>
		<title level="j">Online Media and Global Communication</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="122" to="143" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Large language models as a substitute for human experts in annotating political text</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heseltine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Clemm Von Hohenberg</surname></persName>
		</author>
		<ptr target="https://doi.org/10/gtkhqr" />
	</analytic>
	<monogr>
		<title level="j">Research &amp; Politics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">20531680241236239</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Computer-assisted topic classification for mixed-methods social science research</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Purpura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wilkerson</surname></persName>
		</author>
		<idno type="DOI">10.1080/19331680801975367</idno>
		<ptr target="https://doi.org/10.1080/19331680801975367" />
	</analytic>
	<monogr>
		<title level="j">Journal of Information Technology &amp; Politics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="31" to="46" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Holsti</surname></persName>
		</author>
		<title level="m">Content analysis for the social sciences and humanities</title>
		<imprint>
			<publisher>Addison-Wesley</publisher>
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A computer approach towards the analysis of content</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Iker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">I</forename><surname>Harway</surname></persName>
		</author>
		<idno type="DOI">10.1002/bs.3830100209</idno>
		<ptr target="https://doi.org/10.1002/bs.3830100209" />
	</analytic>
	<monogr>
		<title level="j">Behavioral Science</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="173" to="182" />
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Computer-assisted keyword and document set discovery from unstructured text</title>
		<author>
			<persName><forename type="first">G</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Roberts</surname></persName>
		</author>
		<ptr target="https://doi.org/10/gcgmwn" />
	</analytic>
	<monogr>
		<title level="j">American Journal of Political Science</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="971" to="988" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The text-package: An R-package for analyzing and visualizing human language using natural language processing and transformers</title>
		<author>
			<persName><forename type="first">O</forename><surname>Kjell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Giorgi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Schwartz</surname></persName>
		</author>
		<ptr target="https://doi.org/10/gsmcq8" />
	</analytic>
	<monogr>
		<title level="j">Psychological Methods</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1478" to="1498" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Newspaper complexity and the knowledge gap</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kleinnijenhuis</surname></persName>
		</author>
		<ptr target="https://doi.org/10/cmfpdq" />
	</analytic>
	<monogr>
		<title level="j">European Journal of Communication</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="499" to="522" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Content analysis: An introduction to its methodology</title>
		<author>
			<persName><forename type="first">K</forename><surname>Krippendorff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Sage</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Beyond counting words: Assessing performance of dictionaries, supervised machine learning, and embeddings in topic and frame classification</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kroon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G L A V</forename><surname>Meer</surname></persName>
		</author>
		<author>
			<persName><surname>Der</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vliegenthart</surname></persName>
		</author>
		<ptr target="https://doi.org/10/gtw82b" />
	</analytic>
	<monogr>
		<title level="j">Computational Communication Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Guilty by association: Using word embeddings to measure ethnic stereotypes in news coverage</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kroon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Trilling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Raats</surname></persName>
		</author>
		<ptr target="https://doi.org/10/gt2xk5" />
	</analytic>
	<monogr>
		<title level="j">Journalism &amp; Mass Communication Quarterly</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="451" to="477" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Advancing automated content analysis for a new era of media effects research: The key role of transfer learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kroon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Welbers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Trilling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Van Atteveldt</surname></persName>
		</author>
		<ptr target="https://doi.org/10/gsv44t" />
	</analytic>
	<monogr>
		<title level="j">Communication Methods and Measures</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="142" to="162" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Lasswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lerner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sola Pool</surname></persName>
		</author>
		<author>
			<persName><surname>De</surname></persName>
		</author>
		<title level="m">The comparative study of symbols: An introduction</title>
		<meeting><address><addrLine>Stanford</addrLine></address></meeting>
		<imprint>
			<publisher>Stanford University Press</publisher>
			<date type="published" when="1952">1952</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Less annotating, more classifying: Addressing the data scarcity issue of supervised machine learning with deep transfer learning and BERT-NLI</title>
		<author>
			<persName><forename type="first">M</forename><surname>Laurer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Atteveldt</surname></persName>
		</author>
		<author>
			<persName><surname>Van</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Welbers</surname></persName>
		</author>
		<ptr target="https://doi.org/10/gsgptm" />
	</analytic>
	<monogr>
		<title level="j">Political Analysis</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="84" to="100" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Building efficient universal classifiers with natural language inference</title>
		<author>
			<persName><forename type="first">M</forename><surname>Laurer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Van Atteveldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Welbers</surname></persName>
		</author>
		<ptr target="https://doi.org/10/m7cn" />
		<imprint>
			<date type="published" when="2024">2024b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Extracting policy positions from political texts using words as data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Laver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Benoit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Garry</surname></persName>
		</author>
		<ptr target="https://doi.org/10/ctpzr7" />
	</analytic>
	<monogr>
		<title level="j">American Political Science Review</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="311" to="331" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Word embedding enrichment for dictionary construction: An example of incivility in Cantonese</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L T</forename><surname>Tsang</surname></persName>
		</author>
		<ptr target="https://doi.org/10/gt3d7s" />
	</analytic>
	<monogr>
		<title level="j">Computational Communication Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">VisualBERT: A simple and performant baseline for vision and language</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1908.03557</idno>
		<ptr target="https://doi.org/10.48550/arXiv.1908.03557" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cross-lingual classification of political texts using multilingual sentence embeddings</title>
		<author>
			<persName><forename type="first">H</forename><surname>Licht</surname></persName>
		</author>
		<ptr target="https://doi.org/10/gt2xk7" />
	</analytic>
	<monogr>
		<title level="j">Political Analysis</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="366" to="379" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Building the bridge: Topic modeling for comparative research</title>
		<author>
			<persName><forename type="first">F</forename><surname>Lind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Eberl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Eisele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Heidenreich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Galyga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Boomgaarden</surname></persName>
		</author>
		<ptr target="https://doi.org/10/gmxm6c" />
	</analytic>
	<monogr>
		<title level="j">Communication Methods and Measures</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="96" to="114" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">When the journey is as important as the goal: A roadmap to multilingual dictionary construction</title>
		<author>
			<persName><forename type="first">F</forename><surname>Lind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Eberl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Heidenreich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Boomgaarden</surname></persName>
		</author>
		<ptr target="https://ijoc.org/index.php/ijoc/article/view/10578" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Communication</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v32/le14.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Text mining for social science -The state and the future of computational text analysis in sociology</title>
		<author>
			<persName><forename type="first">A</forename><surname>Macanovic</surname></persName>
		</author>
		<ptr target="https://doi.org/10/grhwwc" />
	</analytic>
	<monogr>
		<title level="j">Social Science Research</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page">102784</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Machine translation vs. multilingual dictionaries: Assessing two strategies for the topic modeling of multilingual text collections</title>
		<author>
			<persName><forename type="first">D</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Baden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stoltenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Vries-Kedem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waldherr</surname></persName>
		</author>
		<ptr target="https://doi.org/10/gms9mp" />
	</analytic>
	<monogr>
		<title level="j">Communication Methods and Measures</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="38" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">How document sampling and vocabulary pruning affect the results of topic models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Niekler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wiedemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stoltenberg</surname></persName>
		</author>
		<ptr target="https://doi.org/10/gt2x6g" />
	</analytic>
	<monogr>
		<title level="j">Computational Communication Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Foundations of statistical natural language processing</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper_files/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html" />
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Inference and disputed authorship: The Federalist</title>
		<author>
			<persName><forename type="first">F</forename><surname>Mosteller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wallace</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">The representational model and relevant research methods</title>
		<author>
			<persName><forename type="first">C</forename><surname>Osgood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Trends in content analysis</title>
		<editor>
			<persName><forename type="first">I</forename><surname>De</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Pool</surname></persName>
		</editor>
		<imprint>
			<publisher>University of Illinois Press</publisher>
			<date type="published" when="1959">1959</date>
			<biblScope unit="page" from="33" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">If you have choices, why not choose (and share) all of them? A multiverse approach to understanding news engagement on social media</title>
		<author>
			<persName><forename type="first">C</forename><surname>Pipal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Boomgaarden</surname></persName>
		</author>
		<ptr target="https://doi.org/10/gt2w9r" />
	</analytic>
	<monogr>
		<title level="j">Digital Journalism</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="255" to="275" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="https://doi.org/10/gfshwg" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<title level="s">Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Moschitti</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Pang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">&amp;</forename><forename type="middle">W</forename><surname>Daelemans</surname></persName>
		</editor>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Computer-assisted text analysis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Popping</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Sage</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>(n.D</surname></persName>
		</author>
		<ptr target="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" />
		<title level="m">Improving language understanding by generative pre-training</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Simple Transformers</title>
		<author>
			<persName><forename type="first">T</forename><surname>Rajapakse</surname></persName>
		</author>
		<ptr target="https://simpletransformers.ai/" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Software package</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Word embeddings for the analysis of ideological placement in parliamentary corpora</title>
		<author>
			<persName><forename type="first">L</forename><surname>Rheault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cochrane</surname></persName>
		</author>
		<ptr target="https://doi.org/10/ghfjw4" />
	</analytic>
	<monogr>
		<title level="j">Political Analysis</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="112" to="133" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Expert-informed topic models for document set discovery</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Rinke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dobbrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Löb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zirn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wessler</surname></persName>
		</author>
		<ptr target="https://doi.org/10/gmbkwd" />
	</analytic>
	<monogr>
		<title level="j">Communication Methods and Measures</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="58" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">stm: An R Package for Structural Topic Models</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tingley</surname></persName>
		</author>
		<ptr target="https://doi.org/10/ggc8cz" />
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="40" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">More than bags of words: Sentiment analysis with word embeddings</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rudkowsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Haselmayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wastian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jenny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Š</forename><surname>Emrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sedlmair</surname></persName>
		</author>
		<ptr target="https://doi.org/10/ghhzgh" />
	</analytic>
	<monogr>
		<title level="j">Communication Methods and Measures</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="140" to="157" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Thematic content analysis using supervised machine learning: An empirical evaluation using German online news</title>
		<author>
			<persName><forename type="first">M</forename><surname>Scharkow</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11135-011-9545-7</idno>
		<ptr target="https://doi.org/10.1007/s11135-011-9545-7" />
	</analytic>
	<monogr>
		<title level="j">Quality &amp; Quantity</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="761" to="773" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Content analysis, automatic</title>
		<author>
			<persName><forename type="first">M</forename><surname>Scharkow</surname></persName>
		</author>
		<idno type="DOI">10.1002/9781118901731.iecrm0043</idno>
		<ptr target="https://doi.org/10.1002/9781118901731" />
	</analytic>
	<monogr>
		<title level="m">The International Encyclopedia of Communication Research Methods</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Matthes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Potter</surname></persName>
		</editor>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title/>
		<idno type="DOI">10.1002/9781118901731.iecrm0043</idno>
	</analytic>
	<monogr>
		<title level="j">iecrm</title>
		<imprint>
			<date>0043</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">When scale Meets depth: Integrating natural language processing and textual analysis for studying digital corpora</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shahin</surname></persName>
		</author>
		<idno type="DOI">10.1080/19312458.2015.1118447</idno>
		<ptr target="https://doi.org/10.1080/19312458.2015.1118447" />
	</analytic>
	<monogr>
		<title level="j">Communication Methods and Measures</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="28" to="50" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Linked in the dark: A network approach to understanding information flows within the Dutch Telegramsphere</title>
		<author>
			<persName><forename type="first">M</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Welbers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kroon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Trilling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
		<ptr target="https://doi.org/10/grhj9f" />
	</analytic>
	<monogr>
		<title level="j">Information, Communication &amp; Society</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="3054" to="3078" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">A scaling model for estimating time-series party positions from texts</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Slapin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-O</forename><surname>Proksch</surname></persName>
		</author>
		<ptr target="https://doi.org/10/brh9q7" />
	</analytic>
	<monogr>
		<title level="j">American Journal of Political Science</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="705" to="722" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Why open-source generative AI models are an ethical way forward for science</title>
		<author>
			<persName><forename type="first">A</forename><surname>Spirling</surname></persName>
		</author>
		<ptr target="https://doi.org/10/gsqx6v" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">616</biblScope>
			<biblScope unit="page" from="413" to="413" />
			<date type="published" when="2023">2023. 7957</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Developing an incivility dictionary for German online discussions -a semi-automated approach combining human and artificial knowledge</title>
		<author>
			<persName><forename type="first">A</forename><surname>Stoll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wilms</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ziegele</surname></persName>
		</author>
		<ptr target="https://doi.org/10/gsnfdn" />
	</analytic>
	<monogr>
		<title level="j">Communication Methods and Measures</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="131" to="149" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">statistical inferences from texts and transcripts Text analysis for the social sciences: Methods for drawing statistical inferences from texts and transcripts</title>
		<author>
			<persName><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text analysis for the social sciences: Methods for drawing</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Roberts</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="35" to="54" />
		</imprint>
	</monogr>
	<note>Thematic text analysis: New agendas for analyzing text content</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">The General Inquirer: A computer approach to content analysis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dunphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ogilvie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1966">1966</date>
			<publisher>The MIT Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">How COVID-19 and the news shaped populism in Facebook comments in seven European countries: A computational analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Thiele</surname></persName>
		</author>
		<ptr target="https://doi.org/10/gt3dhm" />
	</analytic>
	<monogr>
		<title level="j">Computational Communication Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Is bad news difficult to read? A readability analysis of differently connoted passages in the annual reports of the 30 DAX companies</title>
		<author>
			<persName><forename type="first">C</forename><surname>Thoms</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Degenhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wohlgemuth</surname></persName>
		</author>
		<ptr target="https://doi.org/10/gt3vpb" />
	</analytic>
	<monogr>
		<title level="j">Journal of Business and Technical Communication</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="187" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Törnberg</surname></persName>
		</author>
		<ptr target="https://doi.org/mqx9" />
		<title level="m">How to use LLMs for text analysis</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Best practices for text annotation with large language models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Törnberg</surname></persName>
		</author>
		<ptr target="https://doi.org/gtn9qf" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Semantic network analysis: Techniques for extracting, representing, and querying media content</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Van Atteveldt</surname></persName>
		</author>
		<ptr target="https://research.vu.nl/files/75843774/complete%20dissertation.pdf" />
		<imprint>
			<date type="published" when="2008">2008</date>
			<pubPlace>VU Amsterdam</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Dissertation</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">The validity of sentiment analysis: Comparing manual annotation, crowd-coding, dictionary approaches, and machine learning algorithms</title>
		<author>
			<persName><forename type="first">W</forename><surname>Van Atteveldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A C G</forename><surname>Van Der Velden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Boukes</surname></persName>
		</author>
		<ptr target="https://doi.org/10/gh8dk3" />
	</analytic>
	<monogr>
		<title level="j">Communication Methods and Measures</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="121" to="140" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Artificial intelligence and content analysis</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Van Cuilenburg</surname></persName>
		</author>
		<author>
			<persName><surname>Van</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kleinnijenhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Ridder</surname></persName>
		</author>
		<author>
			<persName><surname>De</surname></persName>
		</author>
		<idno type="DOI">10.1007/bf00430638</idno>
		<ptr target="https://doi.org/10.1007/bf00430638" />
	</analytic>
	<monogr>
		<title level="j">Quality and Quantity</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="97" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Investigating opinions on public policies in digital media: Setting up a supervised machine learning tool for stance classification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Viehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maurer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Quiring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<ptr target="https://doi.org/10/gsr7sv" />
	</analytic>
	<monogr>
		<title level="j">Communication Methods and Measures</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="150" to="184" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">News frame analysis: An inductive mixed-method computational approach</title>
		<author>
			<persName><forename type="first">D</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ophir</surname></persName>
		</author>
		<ptr target="https://doi.org/10/ggjvnz" />
	</analytic>
	<monogr>
		<title level="j">Communication Methods and Measures</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="248" to="266" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Latent semantic scaling: A semisupervised text analysis technique for new domains and languages</title>
		<author>
			<persName><forename type="first">K</forename><surname>Watanabe</surname></persName>
		</author>
		<ptr target="https://doi.org/10/gmbkwh" />
	</analytic>
	<monogr>
		<title level="j">Communication Methods and Measures</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="81" to="102" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Computer-aided content analysis: A short primer</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Weber</surname></persName>
		</author>
		<idno type="DOI">10.1007/bf00987112</idno>
	</analytic>
	<monogr>
		<title level="j">Qualitative sociology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="126" to="147" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Creating and comparing dictionary, word embedding, and transformer-based models to measure discrete emotions in German political text</title>
		<author>
			<persName><forename type="first">T</forename><surname>Widmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wich</surname></persName>
		</author>
		<ptr target="https://doi.org/10/gr9dpq" />
	</analytic>
	<monogr>
		<title level="j">Political Analysis</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="626" to="641" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Computerunterstützte Inhaltsanalyse: Codierung und Analyse von Antworten auf offene Fragen [Computer-aided content analysis: classification and analysis of responses to open-ended survey questions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Züll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mohler</surname></persName>
		</author>
		<author>
			<persName><surname>Ph</surname></persName>
		</author>
		<ptr target="https://nbn-resolving.org/urn:nbn:de:0168-ssoar-201405" />
	</analytic>
	<monogr>
		<title level="s">Zentrum für Umfragen, Methoden und Analysen</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
