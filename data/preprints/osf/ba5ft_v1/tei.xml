<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Perspective Explanation of Data Bias in AI: A Case Study</title>
				<funder ref="#_GMrPSGe">
					<orgName type="full">European Commission</orgName>
				</funder>
				<funder>
					<orgName type="full">Understanding Us, Our Society</orgName>
				</funder>
				<funder>
					<orgName type="full">World Around Us</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Tosin</forename><surname>Adewumi</surname></persName>
							<email>tosin.adewumi@ltu.se</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Machine Learning Group</orgName>
								<orgName type="institution">Luleå University of Technology</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Perspective Explanation of Data Bias in AI: A Case Study</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F64B5E730940FF95E6CC2FD118890C09</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-10-22T02:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Explanations are largely lacking in some Machine Learning (ML) systems but having explanations is very helpful because they clarify events. In this case study, we use 7 bias metrics in the AI Fairness 360 (AIF360) library to explain bias in the German Credit Dataset (GCD) in a credit scoring scenario from multiple perspective. Some of the metrics applied are applicable in Natural Language Processing (NLP). Investigations reveal that bias exists in the dataset for the Sensitive Attribute (SA) age. As a contribution, we show that having multiple perspectives (through multiple metrics) of bias gives a clearer assessment compared to a single one. We highlight some of the mitigation algorithms that are available for handling the bias. We publicly release our source code 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Explanability is the ability to use information (i.e. explanans) to clarify the causes or reasoning for an event (i.e. explanandum), contributing a better understanding of the event, as a result <ref type="bibr" target="#b21">(Strevens, 2011;</ref><ref type="bibr" target="#b6">Burkart and Huber, 2021)</ref>. It is a very useful concept in Machine Learning (ML) systems, just as it is in other human endeavors. However, it is lacking to a great extent in many deep learning systems <ref type="bibr" target="#b13">(Heuillet et al., 2021;</ref><ref type="bibr" target="#b9">Dhar et al., 2023)</ref>. In critical application domains such as healthcare, forensics, criminal justice, and credit scoring, among others, this becomes all the more important, especially with the observed challenge of bias <ref type="bibr" target="#b12">(Hassani, 2021)</ref>. One important thing to note about explanations is that they should comply with ethical standards or regulations, such as the EU's General Data Protection Regulation (GDPR), and for explanations to be useful, they should be easy to understand by the stakeholders.</p><p>Bias may be defined as systematic error arising from prejudices, which may be based on a certain 1 colab.research.google.com/drive/1ID5xji4Q9YwyKH jfoQ9jFXpLBHHsL8iv#scrollTo=ghzJIojSvAdk Sensitive Attribute (SA), such as age <ref type="bibr" target="#b3">(Antoniak and Mimno, 2021;</ref><ref type="bibr" target="#b19">Mehrabi et al., 2021;</ref><ref type="bibr" target="#b2">Alkhaled et al., 2023)</ref>. It has strong relation to unfairness. A couple of examples of metrics for estimating bias are Mean Difference (MD) <ref type="bibr" target="#b22">(Thissen et al., 1986)</ref> and Disparate Impact Ratio (DIR) <ref type="bibr" target="#b10">(Feldman et al., 2015)</ref>. These and more are discussed in the next section.</p><p>In this case study, we research what effect the sensitive attribute age has on data. We explain bias in the popular German Credit Dataset (GCD) <ref type="bibr" target="#b14">(Hofmann, 1994)</ref> using the tool AI Fairness 360 (AIF360) <ref type="bibr" target="#b4">(Bellamy et al., 2019)</ref>. We provide multiple (7) perspectives with the following bias metrics: Consistency, DIR, MD, Number of Positives (NoP), Number of Negatives (NoN), Smoothed Empirical Differential Fairness (SEDF) <ref type="bibr" target="#b11">(Foulds et al., 2020)</ref>, and Base rate. Our contribution is that we show having multiple perspectives of bias gives a clearer and more insightful assessment compared to a single metric.</p><p>The rest of this paper is structured as follows. In Section 2, we discuss the method used in this work. We present and discuss the results in Section 3, including the explanations. In Section 4, we discuss some of the related work. We conclude this work in Sedction 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>We used the GCD and the AIF360 library. These are discussed briefly in the following subsections. We split the dataset in the ratio 7:3 for training and test sets. For SA, we use only age and distribute the age brackets according to <ref type="bibr" target="#b15">Kamiran and Calders (2009)</ref>, where the privileged group are people equal to or older than 25 and the unprivileged group are those younger than 25. For evaluation, we use only the training set, since this is what models are typically trained on and we use the seven bias metrics identified in the following subsection. They are based on the BinaryLabelDatasetMetric class and we provide explanations through MetricJSONExplainer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">German Credit Dataset (GCD)</head><p>The dataset categorizes people by a set of attributes as bad or good credit risks. It has 1,000 samples and 20 features. The age distribution in the dataset for the privileged and unprivileged groups (for the cut-off age of 25) is given in Figure <ref type="figure" target="#fig_0">1</ref>. It shows the privileged group has more than 800 people.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">AI Fairness 360 (AIF360)</head><p>The tool is a library of bias mitigation algorithms and over 70 bias metrics. 2 It can be used to examine and mitigate bias and discrimination in ML models. Some of the state-of-the-art (SotA) mitigation algorithms include Fair Data Adaptation (FDA) <ref type="bibr" target="#b20">(Plečko et al., 2021)</ref>, Grid Search Reduction (GSR) <ref type="bibr" target="#b1">(Agarwal et al., 2019)</ref>, and Rich Subgroup Fairness (RSF) <ref type="bibr" target="#b16">(Kearns et al., 2018)</ref>, among others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Metrics</head><p>The following bias metrics are used in this study.</p><p>Consistency. It compares the class of prediction of a given data point to its k-nearest neighbors <ref type="bibr" target="#b23">(Zemel et al., 2013)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Disparate Impact Ratio (DIR). The ratio of</head><p>predicted favorable outcomes for the group that is unprivileged to the privileged one <ref type="bibr" target="#b10">(Feldman et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Mean Difference (MD)</head><p>. It compares the privileged and unprivileged groups by subtracting the former's percentage of favorable results from the latter <ref type="bibr" target="#b22">(Thissen et al., 1986)</ref>. It may also be called Statistical Parity.</p><p>4. Number of Positives. This computes the number of positives in the total instances of the data 2 .</p><p>5. Number of Negatives. This also computes the number of negatives in the total instances of the data 2 .</p><p>6. Smoothed Empirical Differential Fairness (SEDF). It is the estimation of the posterior predictive distribution of a Dirichletmultinomial model, where Differential Fairness (DF) extends the 80% rule in the U.S.</p><p>2 ai-fairness-360.org</p><p>Code of Federal Regulations <ref type="bibr">(Commission et al., 1970)</ref>. It does so to protect multidimensional intersectional categories <ref type="bibr" target="#b11">(Foulds et al., 2020)</ref>.</p><p>7. Base rate. This is conceptualized from stereotypes <ref type="bibr" target="#b7">(Cao and Banaji, 2016;</ref><ref type="bibr" target="#b17">Locksley et al., 1980</ref><ref type="bibr" target="#b18">Locksley et al., , 1982))</ref>. Neglect of it will result in predictions that deviate from what is statistically likely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results and Discussion</head><p>The results of the seven metrics show there's bias in the original dataset for the sensitive attribute age when the privileged and unprivileged groups are compared.</p><p>Consistency The Consistency result of 0.681143 is not equal to 1, the preferred and fair value, hence, this implies that similar inputs are not treated similarly. The following Listing shows the JSONformatted explanation for the prediction.</p><p>{ "metric": "Consistency", "message": "Consistency <ref type="bibr" target="#b23">(Zemel, et al. 2013</ref>): [0.68114286]", "description": "Individual fairness metric from Zemel, Rich, et al.</p><p>\"Learning fair representations.\", ICML 2013. Measures how similar the labels are for similar instances.", "ideal": "The ideal value of this metric is 1.0" } DIR The DIR = 0.8338. This means the privileged group has favourable outcomes. A value of 1 would have indicated no disparate impact while a value above 1 would have indicated the unprivileged group has favourable outcomes. The following Listing shows the JSON-formatted explanation for the prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{</head><p>"metric": "Disparate Impact", "message": "Disparate impact (probability of favorable outcome for unprivileged instances / probability of favorable outcome for privileged instances): 0.8338481338481339", "numPositivePredictionsUnprivileged": 66.0, "numUnprivileged": 111.0, "numPositivePredictionsPrivileged": 420.0, "numPrivileged": 589.0, of rate of favorable outcome for the unprivileged group to that of the privileged group.", "ideal": "The ideal value of this metric is 1.0 A value &lt; 1 implies higher benefit for the privileged group and a value &gt;1 implies a higher benefit for the unprivileged group." }</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mean Difference</head><p>The MD result = -0.1185. This means the unprivileged group has less favourable outcomes compared to the privileged group. The score implies the privileged group was getting 11.85% more positive outcomes. A value of 0 would mean there's no difference in the outcomes for the groups. The Listing below shows the JSONformatted explanation for the prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{</head><p>"metric": "Mean Difference", "message": "Mean difference (mean label value on unprivileged instances -mean label value on privileged instances): -0.11847841049878394", "numPositivesUnprivileged": 66.0, "numInstancesUnprivileged": 111.0, "numPositivesPrivileged": 420.0, "numInstancesPrivileged": 589.0, "description": "Computed as the difference of the rate of favorable outcomes received by the unprivileged group to the privileged group.", "ideal": "The ideal value of this metric is 0.0" }</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Positives</head><p>The Number of Positives identified in the data is 486. The Listing below shows the JSON-formatted explanation for the prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{</head><p>"metric": "Number Of Positives", "message": "Number of positive-outcome instances: 486.0", "numPositives": 486.0, "description": "Computed as the number of positive instances for the given (privileged or unprivileged) group.", "ideal": "The ideal value of this metric lies in the total number of positive instances made available" } Number of Negatives The Number of Negatives identified in the data is 214. The Listing below shows the JSON-formatted explanation for the prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{</head><p>"metric": "Number Of Negatives", "message": "Number of negative-outcome instances: 214.0", "numNegatives": 214.0, "description": "Computed as the number of negative instances for the given (privileged or unprivileged) group.", "ideal": "The ideal value of this metric lies in the total number of negative instances made available" }</p><p>The other results for SEDF and Base rate are 0.346483 and 0.694286, respectively, which both indicate bias. We observe that combining the explanation of the various metrics, for which are shown, give us better appreciation of the bias in the data. This is a preferred first step in order to mitigate the bias in the data before utilizing it for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>The challenge of bias in data is one that has persisted for years and cuts across many fields of AI or research, including Natural Language Processing (NLP) <ref type="bibr" target="#b5">(Blodgett et al., 2020;</ref><ref type="bibr" target="#b0">Adewumi et al., 2024)</ref>. As a result, there have been efforts at detecting and mitigating bias in Machine Learning (ML) for many years, which has led to the introduction of metrics for quantifying bias in data or models <ref type="bibr" target="#b17">(Locksley et al., 1980;</ref><ref type="bibr" target="#b7">Cao and Banaji, 2016)</ref>. Such efforts include the work of <ref type="bibr" target="#b23">Zemel et al. (2013)</ref>, who introduced the Consistency metric, <ref type="bibr" target="#b10">Feldman et al. (2015)</ref> with DIR, and <ref type="bibr" target="#b22">(Thissen et al., 1986)</ref> with MD.</p><p>Although many metrics for quantifying bias and fairness exist, each one on its own may not be sufficient to paint the true picture of affairs, hence the reason why some researchers create new metrics that address the short coming of an older one. This is the case with the SEDF, which builds on the original Differential Fairness <ref type="bibr" target="#b11">(Foulds et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we presented a case study of explaining bias using the AIF360 in the original GCD. Seven metrics were experimented with, some of which are applicable in NLP, such as DIR. We observed that bias exists in the data for the SA age. Being able to measure bias, as a first step, gives the opportunity to mitigate such bias. The AIF360 library provides multiple bias mitigation algorithms, such as FDA, GSR, and RSF. Future work may include exploring more bias metrics and the available mitigation strategies to ascertain which is most effective in a given scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Limitations</head><p>Being a case study, this work is limited to the specific case of the GCD dataset. The library is also limited to the AIF360.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Age distribution in the GCD</figDesc><graphic coords="3,116.22,70.87,362.83,279.59" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work is supported by the <rs type="funder">European Commission</rs>-funded project "<rs type="projectName">Humane AI: Toward AI Systems That Augment</rs> and <rs type="person">Empower Humans</rs> by <rs type="funder">Understanding Us, Our Society</rs> and the <rs type="funder">World Around Us</rs>." The author wishes to thank <rs type="person">Thilakarathne Dilhan</rs> and our colleagues at the <rs type="institution">Responsible Artificial Intelligence Group at Umeå University</rs> for their contributions.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_GMrPSGe">
					<orgName type="project" subtype="full">Humane AI: Toward AI Systems That Augment</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Tosin</forename><surname>Adewumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lama</forename><surname>Alkhaled</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Namrata</forename><surname>Gurung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goya</forename><surname>Van Boven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irene</forename><surname>Pagliai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.19097</idno>
		<title level="m">Fairness and bias in multimodal ai: A survey</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fair regression: Quantitative definitions and reduction-based algorithms</title>
		<author>
			<persName><forename type="first">Alekh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miroslav</forename><surname>Dudík</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><forename type="middle">Steven</forename><surname>Wu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="120" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bipol: A novel multi-axes bias evaluation metric with explainability for nlp</title>
		<author>
			<persName><forename type="first">Lama</forename><surname>Alkhaled</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tosin</forename><surname>Adewumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sana</forename><forename type="middle">Sabah</forename><surname>Sabry</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.nlp.2023.100030</idno>
	</analytic>
	<monogr>
		<title level="j">Natural Language Processing Journal</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">100030</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bad seeds: Evaluating lexical methods for bias measurement</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Antoniak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1889" to="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ai fairness 360: An extensible toolkit for detecting and mitigating algorithmic bias</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K E</forename><surname>Bellamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Houde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lohia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mojsilović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nagar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Natesan Ramamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Richards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sattigeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1147/JRD.2019.2942287</idno>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">4/5</biblScope>
			<biblScope unit="page" from="1" to="4" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Language (technology) is power: A critical survey of &quot;bias&quot; in NLP</title>
		<author>
			<persName><forename type="first">Lin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Solon</forename><surname>Blodgett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hal</forename><surname>Barocas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.485</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5454" to="5476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A survey on the explainability of supervised machine learning</title>
		<author>
			<persName><forename type="first">Nadia</forename><surname>Burkart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><forename type="middle">F</forename><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="245" to="317" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The base rate principle and the fairness principle in social judgment</title>
		<author>
			<persName><forename type="first">Jack</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mahzarin</surname></persName>
		</author>
		<author>
			<persName><surname>Banaji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">27</biblScope>
			<biblScope unit="page" from="7475" to="7480" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Guidelines on employee selection procedures</title>
		<author>
			<orgName type="collaboration">Equal Employment Opportunity Commission et</orgName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Federal Register</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">149</biblScope>
			<biblScope unit="page" from="12333" to="12336" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Challenges of deep learning in medical image analysis-improving explainability and trust</title>
		<author>
			<persName><forename type="first">Tribikram</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nilanjan</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surekha</forename><surname>Borra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Sherratt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Technology and Society</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="68" to="75" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Certifying and removing disparate impact</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sorelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Friedler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Moeller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suresh</forename><surname>Scheidegger</surname></persName>
		</author>
		<author>
			<persName><surname>Venkatasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 21th ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="259" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An intersectional definition of fairness</title>
		<author>
			<persName><forename type="first">Rashidul</forename><surname>James R Foulds</surname></persName>
		</author>
		<author>
			<persName><surname>Islam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 36th International Conference on Data Engineering (ICDE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1918" to="1921" />
		</imprint>
	</monogr>
	<note>Kamrun Naher Keya, and Shimei Pan</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Societal bias reinforcement through machine learning: a credit scoring perspective</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bertrand</surname></persName>
		</author>
		<author>
			<persName><surname>Hassani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI and Ethics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="239" to="247" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Explainability in deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Heuillet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabien</forename><surname>Couthouis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Díaz-Rodríguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">214</biblScope>
			<biblScope unit="page">106685</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Hans</forename><surname>Hofmann</surname></persName>
		</author>
		<idno type="DOI">10.24432/C5NC77</idno>
		<ptr target="https://doi.org/10.24432/C5NC77" />
		<title level="m">Statlog (German Credit Data). UCI Machine Learning Repository</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Classifying without discriminating</title>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Kamiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toon</forename><surname>Calders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 2nd international conference on computer, control and communication</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Preventing fairness gerrymandering: Auditing and learning for subgroup fairness</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><surname>Neel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><forename type="middle">Steven</forename><surname>Wu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2564" to="2572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sex stereotypes and social judgment</title>
		<author>
			<persName><forename type="first">Anne</forename><surname>Locksley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Borgida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nancy</forename><surname>Brekke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><surname>Hepburn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social psychology</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">821</biblScope>
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Social stereotypes and judgments of individuals: An instance of the base-rate fallacy</title>
		<author>
			<persName><forename type="first">Anne</forename><surname>Locksley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><surname>Hepburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vilma</forename><surname>Ortiz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of experimental social psychology</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="42" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A survey on bias and fairness in machine learning</title>
		<author>
			<persName><forename type="first">Ninareh</forename><surname>Mehrabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><surname>Morstatter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nripsuta</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Drago</forename><surname>Plečko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolai</forename><surname>Meinshausen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.10200</idno>
		<title level="m">Causal reasoning for fair data pre-processing</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Depth: An account of scientific explanation</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Strevens</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Harvard University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Beyond group-mean differences: The concept of item bias</title>
		<author>
			<persName><forename type="first">David</forename><surname>Thissen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lynne</forename><surname>Steinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meg</forename><surname>Gerrard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological bulletin</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">118</biblScope>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning fair representations</title>
		<author>
			<persName><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toni</forename><surname>Pitassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Dwork</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="325" to="333" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
