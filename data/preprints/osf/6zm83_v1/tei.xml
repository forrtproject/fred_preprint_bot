<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AI or Human? Evaluating Student Feedback Perceptions in Higher Education</title>
				<funder>
					<orgName type="full">Swiss State Secretariat for Education, Research and Innovation</orgName>
					<orgName type="abbreviated">SERI</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tanya</forename><surname>Nazaretsky</surname></persName>
							<email>tanya.nazaretsky@epfl.ch</email>
							<idno type="ORCID">0000-0003-1343-0627</idno>
							<affiliation key="aff0">
								<orgName type="institution">EPFL</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Paola</forename><surname>Mejia-Domenzain</surname></persName>
							<idno type="ORCID">0000-0003-1242-3134</idno>
							<affiliation key="aff0">
								<orgName type="institution">EPFL</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vinitra</forename><surname>Swamy</surname></persName>
							<email>vinitra.swamy@epfl.ch</email>
							<idno type="ORCID">0000-0002-6840-5923</idno>
							<affiliation key="aff0">
								<orgName type="institution">EPFL</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jibril</forename><surname>Frej</surname></persName>
							<email>jibril.frej@epfl.ch</email>
							<idno type="ORCID">0009-0009-0631-0636</idno>
							<affiliation key="aff0">
								<orgName type="institution">EPFL</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tanja</forename><surname>KÃ¤ser</surname></persName>
							<email>tanja.kaeser@epfl.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">EPFL</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AI or Human? Evaluating Student Feedback Perceptions in Higher Education</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D107A2C8EE6565D3E1A9D68A983E4F35</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-10-22T06:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Generative AI</term>
					<term>Formative Feedback</term>
					<term>Human Factors</term>
					<term>Higher Education</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Feedback plays a crucial role in learning by helping individuals understand and improve their performance. Yet, providing timely, personalized feedback in higher education presents a challenge due to the large and diverse student population, often resulting in delayed and generic feedback. Recent advances in generative Artificial Intelligence (AI) offer a solution for delivering timely and scalable feedback. However, little is known about students' perceptions of AI feedback. In this paper, we investigate how the identity of the feedback provider affects students' perception, focusing on the comparison between AI-generated and human-created feedback. Our approach involves students evaluating feedback in authentic educational settings both before and after disclosing the feedback provider's identity, aiming to assess the influence of this knowledge on their perception. Our study with 457 students across diverse academic programs and levels reveals that students' ability to differentiate between AI and human feedback depends on the task at hand. Disclosing the identity of the feedback provider affects students' preferences, leading to a greater preference for human-created feedback and a decreased evaluation of AI-generated feedback. Moreover, students who failed to identify the feedback provider correctly tended to rate AI feedback higher, whereas those who succeeded preferred human feedback. These tendencies are similar across academic levels, genders, and fields of study. Our results highlight the complexity of integrating AI into educational feedback systems and underline the importance of considering student perceptions in AI-generated feedback adoption in higher education.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Feedback has always been a crucial part of the learning process. Properly formulated and timely feedback helps individuals understand and improve their performance <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b38">40]</ref>. In 2015, Hattie conducted a thorough synthesis of 1200 meta-analyses related to the influences of teaching activities and methods (e.g., traditional lectures, flipped classrooms, problem-based learning, using online and Internet-based solutions, etc.) on student learning and concluded that most likely "the method of teaching is less critical than the attributes of the teaching within the methods" <ref type="bibr" target="#b12">[13]</ref>. In particular, this study strongly emphasized evidence-based teaching practices and the importance of feedback to enhance student learning outcomes. However, managing the feedback process is the most challenging part of teaching <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" target="#b37">39]</ref>. According to the 2022 Student Experience Survey <ref type="bibr" target="#b0">[1]</ref>, among all "Teaching quality items," the item aimed to evaluate the quality of feedback (formulated as "Teachers commented on your work in ways that help you learn") got the lowest positive rates (&lt; 63% of postgraduates and &lt; 56% of undergraduate students). Similarly, according to the Annual Report and Accounts 2022- <ref type="bibr">23 [34]</ref>, the ratings for "Assessment and Feedback" items were the lowest overall "Teaching and Assessment" items (&lt; 69% of students). Indeed, providing meaningful, personalized feedback requires significant time and effort. This is especially true in higher education contexts, where personal attention from the instructors is limited <ref type="bibr" target="#b2">[3]</ref>. At the same time, students often come from diverse backgrounds and have varying levels of prior skills and knowledge. As a result, the feedback delivery is often delayed, reducing its effectiveness, and the provided feedback is not specific to the concrete student performance <ref type="bibr" target="#b11">[12]</ref>.</p><p>Automated feedback produced by AI can help speed up and scale the feedback delivery process <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b16">17]</ref>. Generative AI, especially Large Language Models (LLM), is rapidly gaining momentum for automating feedback provision in educational contexts. A recent systematic scoping review by <ref type="bibr" target="#b39">[41]</ref> examined 118 peerreviewed papers since 2017 on applying LLMs in educational settings. Among these, 54 focused explicitly on student-oriented tasks such as automated feedback provision and resource recommendation. Notable examples include GPT-based models that provide feedback on student performance in data science open-ended tasks <ref type="bibr" target="#b7">[8]</ref>, source based argument essays in history <ref type="bibr" target="#b32">[33]</ref>, inquiry-based learning <ref type="bibr" target="#b1">[2]</ref>, algebra <ref type="bibr" target="#b26">[27]</ref> and programming tasks <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b9">10]</ref>, achieving high performance in some cases even comparable to well-trained human evaluators. However, most existing studies primarily target technological aspects like model accuracy and often miss the social-emotional aspects of AI's acceptance. As shown in other fields, the preference for human over AI advice, known as "algorithm aversion" <ref type="bibr" target="#b8">[9]</ref>, persists despite AI's proven efficiency. For example, in medical radiology, where AI outperforms humans <ref type="bibr" target="#b28">[29]</ref>, previous research demonstrated that expert opinions were less favorable towards recommendations framed as AI-generated compared to the same recommendations framed as humans' <ref type="bibr" target="#b10">[11]</ref>, highlighting the importance of addressing algorithm aversion in AI-based system implementation. Although recent studies in education have started considering this issue, they mainly focus on educators' attitudes <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25]</ref>, leaving student perspectives on AI feedback provision largely unexplored <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b34">36]</ref>.</p><p>In this study, we aim to examine how students' awareness of the feedback provider's identity might influence their evaluation of feedback content, partic-ularly in the context of algorithm aversion and preference for human expertise. With these goals in mind, the study seeks to address the following research questions: First, can students distinguish between AI-generated and human-created feedback (simplified Turing Test), and what factors influence their ability to make this distinction (RQ1)? Second, how do students' perceptions of the same feedback content change after revealing the feedback provider's identity (RQ2)? And third, do students hold a negative bias towards AI as a feedback provider (RQ3)?</p><p>To achieve our goals, we conduct an extensive user study by analyzing the responses of 457 higher education students from various courses, fields of study, and academic levels (Bachelor's and Master's). Specifically, we ask students to evaluate two feedback options (human-created and AI-generated) in their authentic learning settings twice: initially without (blind condition) and then with (informed condition) knowledge of the feedback provider's identity. This deliberate priming and reveal is designed to uncover changes in students' evaluative judgments, thus helping to evaluate the impact of the provider's identity on educational feedback perception. Our results reveal that students' ability to distinguish between AI-generated and human-created feedback depends on the underlying course and task. We also detect a change in feedback perception upon disclosure of the provider's identity, with a notable trend of increased favorability towards feedback identified as human-created and a decline in the evaluation of AI-generated feedback. Moreover, Turing Test outcomes are significantly correlated with feedback perception. Students failing the test favor AI-generated feedback, while those passing prefer human feedback. In addition, there is a negative correlation between participants' age and their perceptions of provider credibility for AI and human feedback providers. We, therefore, can confirm a bias against AI as a feedback provider.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Motivational Aspects of Effective Formative Feedback</head><p>A considerable body of research exists in the area of design and implementation of effective formative human and AI feedback in the context of task-level performance <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b31">32]</ref>. This research advocates for a multi-dimensional approach to feedback, including feedback content and provider characteristics <ref type="bibr" target="#b22">[23]</ref>, and recommends a learner-centric approach to the feedback considering the learners as active participants of the feedback process rather then passive receivers of the information <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b38">40]</ref>. To ensure the uptake of the feedback for effective enhancing learning process, feedback should be task-focused <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>, provide objective performance evaluations <ref type="bibr" target="#b2">[3]</ref>, offer actionable advice on how to proceed <ref type="bibr" target="#b30">[31]</ref>, and motivate learners to adopt it to improve their response <ref type="bibr" target="#b31">[32]</ref>.</p><p>Based on the previous research the following motivational factors influence learners' willingness to act upon feedback: learners' perceptions of feedback content objectivity <ref type="bibr" target="#b29">[30]</ref>, usefulness <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b30">31]</ref>, genuineness <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b31">32]</ref> and feedback provider credibility <ref type="bibr" target="#b29">[30]</ref>. With respect to objectivity, according to the theories of procedural and informational justice, the feedback should be perceived by learners as fair and factual alignment between its content provided and the actual performance of the recipient <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b29">30]</ref> and should be precise and specific to the response <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b35">37]</ref>. To act upon receiving feedback, it should be useful <ref type="bibr" target="#b30">[31]</ref>, meaning informative, relevant, and applicable <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b31">32]</ref>. Applicability can be achieved by error-flagging and providing cues and hints on how to improve an answer while avoiding giving the correct solution itself <ref type="bibr" target="#b22">[23]</ref>. With respect to feedback genuineness, it should use a sincere, positive tone <ref type="bibr" target="#b13">[14]</ref> and be authentic, meaning honest and earned, not repeated often <ref type="bibr" target="#b15">[16]</ref> and focused on the learning process and not student abilities <ref type="bibr" target="#b31">[32]</ref>. With respect to feedback provider credibility, the feedback source should be considered by the learners as safe <ref type="bibr" target="#b33">[35]</ref> and ethical <ref type="bibr" target="#b39">[41]</ref>. Safety refers to trustworthy, well-intentioned, and reliable feedback providers fostering confidence <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b33">35]</ref>, while ethics relates to qualities such as accountability, commitment to privacy, equality, and absence of biases <ref type="bibr" target="#b39">[41]</ref>.</p><p>In this research, we explored how students perceive both AI and human feedback quality in terms of objectivity, usefulness, and genuineness, as well as their views on the competence of the feedback provider. These elements, informed by prior research, are shown to predict a student's inclination to apply the feedback constructively to improve their responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>To evaluate how students' feedback perceptions depend on their knowledge of the feedback provider, we developed two perception instruments and incorporated them into the experimental procedure illustrated in Fig. <ref type="figure">1</ref>. Our study with 457 students employed a within-subject experimental design with priming effect aimed to blur the information about the feedback identity <ref type="bibr" target="#b27">[28]</ref>. The study consisted of three steps (Fig. <ref type="figure">1</ref>). During Step 1, we presented students with a routine assessment task from their course, their authentic answer, and two variants of feedback generated by human experts and the OpenAI GPT4 Interface (Section 3.2) and asked them to evaluate the feedback content. In Step 2, we revealed the existence of primer manipulation and asked students to guess the origin of each feedback (a simplified Turing Test based on one attempt only). In Step 3, we revealed the actual identity of each feedback provider and asked students if they wanted to change their initial opinions regarding the feedback content. In addition, we asked participants to evaluate the feedback providers. During the study, the students had no access to the correct solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Participants</head><p>We conducted an online survey with 457 students from EPFL who were above the legal age and had voluntarily agreed to participate. These students were enrolled in five different courses: Discrete Mathematics (ICC), Information Theory (AICC), Applied Software Engineering (ASE), Applied Data Analysis (ADA), and Mathematical Analysis (MA). Civil, Mechanical, Life Sciences, Electrical and Electronic, and Financial Engineering, and more. The data collection for this study occurred in fall 2023, and detailed socio-demographic information of the participants is presented in Table <ref type="table">1</ref>. In further analyses, we analyzed differences between two genders: males vs. minorities, while minorities referred to students who identify as women and non-binary. This categorization was necessary since a separate statistical analysis for non-binary students alone was not feasible due to their small representation in the sample (only two students). The study was conducted with the approval of the Human Research Ethics Committee at EPFL (HREC 017-2023).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Feedback Generation</head><p>For each course in the study, we selected a pre-existing assignment and gathered genuine student responses. We adopted a positive formative feedback strategy, focusing on encouragement and support, recognizing effort accurately, and highlighting improvement areas. We created two variants of feedback for each student, one from the teaching team and the other from AI. An illustration of human and</p><formula xml:id="formula_0">1. CONTRAPOSITION 1) x is rationnal -&gt; x^2 is rationnal [Premise]</formula><p>2) x = p/q and p,q are integers and q != 0 [Definition] 3) x^2 = p^2/q^2 [Algebra] 4) Therefore, assuming that the square of an integers is an integer, x^2 is rationnal [Definition] 2. CONTRADICTION 1) Let's assume that x^2 is rational and x irrational [Premise]</p><p>2) x^2 = p/q and p,q are integers and q!=0 [Definition]</p><p>3)x = r/s such as r^2 = p and s^2 = q 4) There are r and s which are integers and verifiy the equation: for example q = 4 and p = 4 [Counterexample] 5) The demonstration is false so for x irrational, x^2 must be irrationnal Let x be a real number. Prove that if x 2 is irrational, then x is irrational using: a) a proof by contraposition b) a proof by contradiction Detail the two proofs in a way that shows that you understand the difference between a proof by contraposition and a proof by contradiction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Assignment</head><p>You've done a good job on the contraposition proof. However, for the contradiction part, you misunderstood the premise. You should assume x^2 is irrational and x is rational, then derive a contradiction from there. Also, step 4 is not clear and seems unnecessary; it's important to focus on deriving contradictions rather than counterexamples in this instance. Keep practicing your proof writing skills!</p><p>The contraposition is true but you have to review the contradiction.</p><p>You have to know that proofing p-&gt;q by contradiction: we show that assuming p and not q leads to a contradiction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feedback Feedback</head><p>Student Answer Fig. <ref type="figure">2</ref>. An AICC course assignment, a corresponding student answer, and two variants of feedback: human-created and AI-generated. AI-generated feedback, taken from a logical reasoning task in the AICC course, can be found in Fig. <ref type="figure">2</ref>.</p><p>Human-Generated Feedback For each course, human feedback was provided by the original teaching teams. For MA, ASE, and ADA, existing authentic feedback was used as is, while in ICC and AICC, formative feedback was not provided to the students as part of the routine. For these courses teaching teams were trained through a workshop on creating positive, formative feedback, emphasizing feedback consistency and quality. The workshop included guidelines, feedback examples, and a communication channel for graders to ensure uniformity of feedback. More details about the types of assignments and number of human graders per course are presented in Table <ref type="table" target="#tab_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AI-Generated Feedback</head><p>We asked OpenAI GPT interface<ref type="foot" target="#foot_0">1</ref> to provide elaborated formative feedback using the prompt illustrated in Fig. <ref type="figure">3</ref>. While a grading rubric existed for all the courses, we decided not to provide it in the prompt, as we wanted GPT to focus less on the grades and more on formative feedback. The correct solution was provided for the AICC, MA, and ADA courses only. We did not provide solutions for the coding projects in the ASE and ICC courses, as there is a wide range of perfectly correct answers. Our goal was to have GPT focus on enhancing the student's response instead of making comparisons to a single correct solution provided in the prompt. The GPT-generated feedback was provided to the students as is, except for removing the "Dear student" greeting at the beginning and "Your instructor" at the end (in some cases, GPT added them to the feedback text). Feedback Quality This study did not aim at quantitatively assessing the objective quality of AI-generated feedback or its comparison to human-created feedback. Instead, our focus was on exploring how the knowledge of the feedback provider's identity affects students' perceptions of feedback. As a result, we did not conduct a statistical analysis of the quality of AI feedback as evaluated by experts. However, the study involved a qualitative assessment through a quality control test with 5-10 randomly chosen feedback samples presented to course professors and their teaching teams, who deemed the quality of feedback as high before agreeing to present it to their students. Moreover, a comprehensive review of all generated feedback by the authors revealed no instances of GPT hallucinations or contextually irrelevant content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Perception Instrument</head><p>Based on the previous research presented in Section 2, we identified three dimensions and eight terms to evaluate the feedback content. More specifically, we defined the Objectivity dimension using the terms Precise, Fair, Factual ; the Usefulness dimension consisting of the three terms Relevant, Informative, Applicable; and the Genuineness dimension with the terms Authentic, and Sincere <ref type="bibr" target="#b15">[16]</ref>. In the context of the feedback provider, we identified one dimension, Credibility, measured using the following three terms: Trustworthy, Reliable, and Ethical.</p><p>Following the methodology proposed in <ref type="bibr" target="#b33">[35]</ref>, we evaluated students' feedback and feedback provider perceptions with one question per instrument. The questions were formulated as follows: "To what extent do you associate this feedback/feedback provider with the following term?" The students were asked to choose a Likert scale answer for each term (represented by aforementioned adjectives). The Likert items were evaluated on a scale of five (Strongly disagree = 0, Disagree = 1, Neutral = 2, Agree = 3, Strongly agree = 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>We analyzed the students' answers to the blind evaluation, the TT, and their informed evaluations to investigate whether students can distinguish between human and AI-feedback (RQ1), how their perception changes when the feedback provider identity is revealed (RQ2), and whether they hold biases against AI as a feedback provider (RQ3).</p><p>We first evaluated the reliability of the proposed instruments by calculating Cronbach's Î± measure for each dimension separately. The internal consistency was good (Î± â¥ 0.80) for the Objectivity, Usefulness, and Credibility dimensions and excellent (Î± â¥ 0.90) for the Genuineness dimension. Next, we calculated the scores of the factors by averaging over all items associated with the factor and evaluated the responses of the 457 students.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Turing Test for AI and Human Feedback (RQ1)</head><p>In total, 274 out of 457 students could guess the feedback provider correctly.</p><p>To study the possible correlation of demographics (age, gender, course) with students' ability to guess the feedback provider identity correctly, we fit a Logistic Regression Model<ref type="foot" target="#foot_1">2</ref> . The correlation was not significant for age and gender. However, there was a significant correlation with course topics. Therefore, we used Chi-Square Tests of Independence to evaluate the ability of participants of different courses in guessing the feedback provider identity. For the ASE and ICC courses, the results were significant (Ï 2 ASE = 25.2, p &lt; .001; Ï 2 ICC = 21.9, p &lt; .001), indicating that participants could effectively differentiate between the two types of feedback. In contrast, the AICC, MA and ADA courses did not show significant differences (Ï 2 AICC = 1.20, p = .273; Ï 2 M A = 3.57, p = .059; Ï 2 ADA = 1.85, p = .174), suggesting participants in these courses were not able to identify the feedback provider identity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Impact of Identity Disclosure on Perceptions (RQ2)</head><p>To analyze how the knowledge of the feedback provider identity influences students perceptions towards feedback content quality, we utilized a Mixed Linear Model (MLM) 3 approach. More specifically, we fit the MLM separately for each content dimension (Objectivity, Usefulness, and Genuineness) to analyze how the interaction between feedback provider (Human/AI), timing (Blind/Informed) and Turing Test results (TT Passed/TT Failed), and possible covariates (course, age, gender) influence the dimension scores. The individual variation between students was modeled as a random effect. The analysis of the models detected no effect of course, age and gender on the students' perceptions across all the dimensions. Interestingly, in the follow-up analysis of the means<ref type="foot" target="#foot_2">4</ref> , distinct patterns emerged between perceptions towards AI-generated and human feedback. Students tended to increase their scores for the feedback provided by human evaluators after being informed about the feedback provider's identity. However, the change was not statistically significant in all the content evaluation dimensions (Fig. <ref type="figure" target="#fig_0">4(a)</ref>, Human condition). At the same time, for AI-generated feedback, the students tended to decrease their scores in all the categories evaluating feedback content after being informed that AI-generated it. The change was statistically significant for the Genuineness dimension (p &lt; 0.01), but not for the Objectivity and Usefulness dimensions (Fig. <ref type="figure" target="#fig_0">4</ref>(a), AI condition). These results did not correlate with the Turing Test results.</p><p>Subsequently, we analyzed the differences between content evaluation of the human-created and AI-generated feedback for both timing conditions (Blind/ Informed) separately. The MLM analysis indicated a correlation with the Turing Test results in this case. Based on the follow-up analysis of differences in the means, we found distinct patterns that emerged between perceptions towards AI-generated feedback and human feedback for students who succeeded and failed to guess the feedback provider identity correctly (TT passed/TT failed). More specifically, in blind conditions, students who failed the TT evaluated AI feedback significantly higher than humans among all dimensions. However, after being informed about the provider identity, they reduced their scores for AI and increased their scores for human feedback, such that the difference was not significant anymore for the Genuineness and Objectivity dimensions (Fig. <ref type="figure" target="#fig_0">4</ref>(b), TT failed condition). Conversely, in blind conditions, students who passed the TT rated AI feedback lower than human's. However, the difference was significant for the Genuineness dimension only. After being informed about the provider identity, they reduced their scores for AI and increased their scores for human feedback, such that the difference became significant in all dimensions (Fig. <ref type="figure" target="#fig_0">4</ref>(b), TT passed condition).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Bias towards AI as a feedback provider (RQ3)</head><p>To explore how the feedback provider identity influences students' perceptions of the provider Credibility, we fit a mixed linear model 3 . More specifically, we modeled an interaction between feedback provider Credibility and feedback provider identity (Human/AI), along with possible covariates (course, age, gender, and TT results), while representing the individual variation between students as a random effect. The results revealed a significant positive correlation of human identity with feedback provider Credibility (p &lt; .001) and a negative correlation with participants' age (p &lt; .01). We found no significant correlation with the course, gender, and ability to guess the feedback provider identity. The results are presented in Fig. <ref type="figure" target="#fig_1">5</ref>. The follow-up analysis indicated a substantial effect (Cohen's d = 0.88) of the feedback provider identity on feedback provider Credibility, indicating that feedback from human providers (Âµ = 3.29, Ï = 0.77) is perceived as significantly more credible than feedback generated by AI (Âµ = 2.25, Ï = 0.86).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Implications</head><p>AI has the potential to generate personalized and timely feedback at scale. Consequently, there has been an increase in research on this topic, mainly focusing on assessing feedback quality, missing the social-emotional aspects of AI adoption.</p><p>In our study, we investigated students' perceptions of AI-generated feedback, focusing on discovering potential biases against AI as a feedback provider. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quality of AI-generated feedback</head><p>There is no doubt that feedback quality is a significant factor in students' perceptions of AI-generated feedback. Our results, however, emphasize that some factors influencing students' perceptions of AI-generated feedback are not directly related to the feedback content quality (based on RQ2 results). Indeed, students chose to decrease their evaluation of the same feedback content after being informed it was AI-generated. The decrease was significant for the Genuineness dimension, measured by Authentic and Sincere terms, and can be explained by evaluating qualities traditionally attributed to humans. However, the trend of decreased evaluations in the Objectivity and Usefulness dimensions is more complex to interpret. One possible explanation is that students struggled to assess the feedback's correctness. In such situations of uncertainty and vulnerability, human evaluation of objectivity and usefulness of the information depends mainly on their trust in the information provider <ref type="bibr" target="#b23">[24]</ref>. In our study, the students were initially manipulated to believe that the teaching team they trusted was a feedback provider, so they linked their high trust in educators (evidenced by the results of RQ3) to their evaluation of the feedback content. Once the true AI identity of the feedback was disclosed, their lower trust in AI as a feedback provider influenced their feedback assessment, leading to reduced scores in these dimensions. The observed correlation between high evaluations of AI-generated feedback and students failing the Turing Test might be attributable to the quality of the feedback itself. In this scenario, the superior quality of the AI-generated feedback might have influenced both the students' inability to correctly identify the feedback provider and their positive evaluation of the feedback content. Alternatively, the capability of models like GPT to produce human-like text could have impacted the students' Turing Test outcomes, reinforcing their belief that the feedback was generated by the teaching team, and leading to higher feedback content evaluations independent of actual feedback quality.</p><p>Our future research aims to explore the agreement between human and AIgenerated feedback and how each feedback type expert evaluation correlates with students' perceptions of feedback content. To this end, our next step is labeling the feedback corpus we collected by educational experts with respect to similarities between AI-generated and human feedback and their compliance with elaborated feedback components <ref type="bibr" target="#b13">[14]</ref>.</p><p>Transparency and Ethics Echoing the findings of <ref type="bibr" target="#b24">[25]</ref>, our study also reveals a strong preference for human guidance over AI-generated suggestions (based on RQ3 findings), indicating a fundamental human inclination for personal interaction and judgment. In this light, AI-powered system designers might be tempted to conceal or blur the AI nature of feedback providers, justifying this by the system's ability to deliver high-quality feedback. However, we contend that such practices are unethical. Transparency regarding AI involvement in feedback generation is crucial. One reason is the variability in feedback quality between controlled test environments and actual educational settings <ref type="bibr" target="#b40">[42]</ref>. Another reason is the educational value of teaching students to recognize and handle potential AI errors. Educating students to critically assess feedback from accessible but imperfect AI technologies like ChatGPT prepares them to continue leveraging such tools for lifelong learning and their future professional environments, enhancing their critical thinking and self-regulated learning skills (see AI literacy <ref type="bibr" target="#b20">[21]</ref> and student feedback literacy <ref type="bibr" target="#b4">[5]</ref>).</p><p>Human-Mediated Automated Feedback (Human-in-the-Loop) A promising approach to addressing algorithm aversion and potential AI inaccuracies involves integrating human educators' input into AI-based systems. This method entails AI systems initially generating feedback, which human experts review and validate before releasing it to students. Such a hybrid model could efficiently combine AI's speed and scalability with human expertise and ability to consider the broader learning environment (e.g., social, emotional, and psychological aspects of the learning process) and ensure error-free feedback, while saving teachers time and effort. However, this strategy necessitates new competencies for instructors, broadly categorized under the umbrella of "teacher feedback literacy" <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref>. The evolving landscape of educational technology, particularly with freely available tools, demands educators develop skills to effectively integrate automated feedback into their pedagogical practices <ref type="bibr" target="#b3">[4]</ref>. This includes critically assessing and tailoring AI-generated feedback to meet the specific needs of individual students or student groups and comprehending the potential and limitations of AI tools, especially LLM, e.g., ChatGPT. <ref type="bibr" target="#b41">[43,</ref><ref type="bibr" target="#b42">44]</ref>. This comprehension includes critically evaluating the reliability and accuracy of the information provided by AI, being aware of possible model biases, and understanding how these factors might impact educational outcomes. Moreover, recent studies have highlighted the im-portance of enhancing professional development programs for teachers, focusing on increasing their epistemic understanding of AI <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b18">19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>To summarize, our study analyzing 457 student responses in actual learning contexts gave us a detailed and accurate understanding of student responses to human and AI-generated feedback, a depth that synthetic scenarios might not achieve. We found that students' feedback evaluations were influenced by their knowledge of the feedback provider's identity. Students tended to rate human feedback slightly higher after being informed about the provider, whereas AI-generated feedback was rated lower, especially regarding Genuineness where the decrease was significant. Furthermore, the results of the Turing Test had a notable correlation with feedback perception. Students who failed the Turing Test rated AI-generated feedback higher than human feedback, while those who passed the test preferred human-generated feedback. A significant finding of the study was the influence of feedback provider identity on the perceived credibility of the feedback. Humans as feedback providers were consistently rated as more credible compared to AI. This underscores the prevailing preference for human feedback in educational settings and highlights the complexities of integrating AI tools into educational environments. Overall, our study provides valuable insights into how students perceive and evaluate AI versus human feedback, which is crucial for the effective integration of Generative AI in education.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Student perceptions towards feedback content per dimension (Genuineness, Objectivity, Usefulness) by interactions with Feedback provider (Human/AI), Timing (Blind/Informed) and Turing Test results (TT failed/TT passed). Scores: Strongly disagree = 0, Disagree = 1, Neutral = 2, Agree = 3, Strongly agree = 4. Significance codes: *** &lt; 0.001, ** &lt; 0.01, * &lt; 0.05.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Student perceptions towards feedback provider Credibility by interactions with provider identity (Human/AI) and age. (a) Older students exhibited lower perceived Credibility for both feedback providers, yet human providers consistently got significantly higher evaluations. (b) Students evaluated humans as significantly more credible feedback providers than AI. Scores: Strongly disagree = 0, Disagree = 1, Neutral = 2, Agree = 3, Strongly agree = 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The courses spanned various academic programs, including Computer Science, Communication Systems, Data Science,</figDesc><table><row><cell></cell><cell></cell><cell>The Big</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Your Assignment</cell><cell>Reveal</cell><cell></cell><cell>Feedback 1</cell><cell>Feedback 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Content</cell><cell>Content</cell></row><row><cell cols="2">Your answer</cell><cell>Feedback 1</cell><cell></cell><cell>Reevaluation</cell><cell>Reevaluation</cell></row><row><cell>Feedback 1 Content Evaluation</cell><cell>Feedback 2 Content Evaluation</cell><cell>?</cell><cell></cell><cell>Factual Relevant Fair Precise Informative</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Feedback 2</cell><cell></cell><cell>Applicable Authentic</cell><cell></cell></row><row><cell></cell><cell>Fair</cell><cell></cell><cell></cell><cell>Sincere</cell><cell></cell></row><row><cell></cell><cell>Relevant</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Factual</cell><cell></cell><cell></cell><cell>Provider Evaluation</cell><cell>Provider Evaluation</cell></row><row><cell></cell><cell>Precise</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Informative</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Applicable Authentic Sincere</cell><cell></cell><cell></cell><cell>Trustworthy Reliable</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Ethical</cell><cell></cell></row><row><cell></cell><cell>Step 1</cell><cell>Step 2</cell><cell></cell><cell>Step 3</cell><cell></cell></row><row><cell cols="2">Blind evaluation</cell><cell cols="2">Simplified Turing Test</cell><cell cols="2">Informed evaluation</cell></row><row><cell cols="2">Course Field of Study</cell><cell cols="3">Degree Year Participants</cell><cell>Age</cell></row><row><cell>ICC</cell><cell>Mathematics, Physics</cell><cell>BA</cell><cell>1st</cell><cell>66 (53/11/0/2)</cell><cell>19.1 (3.0)</cell></row><row><cell>AICC</cell><cell>Computer Science</cell><cell>BA</cell><cell>1st</cell><cell cols="2">187 (139/37/1/10) 19.3 (2.0)</cell></row><row><cell>MA</cell><cell>Mechanical Engineering</cell><cell>BA</cell><cell>1st</cell><cell>28 (22/5/0/1)</cell><cell>18.9 (1.2)</cell></row><row><cell>ASE</cell><cell cols="2">Life Sciences Engineering BA</cell><cell>2nd</cell><cell>70 (31/37/0/2)</cell><cell>20.3 (1.7)</cell></row><row><cell>ADA</cell><cell>Multiple</cell><cell>MA</cell><cell>1st</cell><cell>106 (72/31/1/2)</cell><cell>23.1 (2.9)</cell></row></table><note><p><p><p><p><p><p>Fig.</p>1</p>. Experimental design consisting of three steps: 1) blind feedback content evaluation, 2) Turing test, and 3) informed feedback and feedback provider evaluation.</p>Table</p>1</p>. Demographics of participants by course and field of study, including degree, year, gender distribution (male/female/non-binary/other), and mean age (standard deviation).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Overview of course assignments, human assessment routines, and additional feedback provision for various courses, detailing the number of human graders involved in the study.</figDesc><table><row><cell cols="2">Course Assignment</cell><cell>Routine human</cell><cell cols="2">Additional human Number of</cell></row><row><cell></cell><cell></cell><cell>assessment</cell><cell>feedback</cell><cell>human graders</cell></row><row><cell>ICC</cell><cell>C++ project</cell><cell>Grade</cell><cell>Code review</cell><cell>8</cell></row><row><cell>AICC</cell><cell>Logical reasoning proof</cell><cell>Grade</cell><cell>Feedback</cell><cell>9</cell></row><row><cell>MA</cell><cell>Logical reasoning proof</cell><cell>Feedback</cell><cell>-</cell><cell>1</cell></row><row><cell>ASE</cell><cell>Python project</cell><cell>Code review</cell><cell>-</cell><cell>17</cell></row><row><cell>ADA</cell><cell cols="3">Statistical inference task Grade &amp; Feedback -</cell><cell>3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>You are an excellent instructor teaching a course called [COURSE NAME]. You gave the students the following assignment: [ASSIGNMENT]. The student submission was [STUDENT ANSWER]. The correct solution is [SOLUTION]. Please evaluate the student's answer and provide elaborated formative feedback. Please follow the following instructions: The feedback should be addressed directly to the student as is. It should be no more than [NUMBER] lines. Please provide one sentence of the overall evaluation at the end.Fig. 3. Standardized prompt template used to create AI-feedback for student assignments, detailing the necessary components and instructions.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Parameters: model = gpt-4, top_p = 1.0, temperature = 0.8, frequency_penalty = 0.8, presence_penalty = 0.5</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>stats R package</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>emmeans R package. Multiple comparisons were adjusted using the Tukey method.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This project was funded by the <rs type="funder">Swiss State Secretariat for Education, Research and Innovation (SERI)</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Australian Government Department of Education: Student experience survey</title>
		<ptr target="https://www.qilt.edu.au/surveys/student-experience-survey-(ses" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Assessing student errors in experimentation using artificial intelligence and large language models: A comparative study with human raters</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bewersdorff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>SeÃler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Baur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nerdel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and Education: Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">100177</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rethinking models of feedback for learning: the challenge of design</title>
		<author>
			<persName><forename type="first">D</forename><surname>Boud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Molloy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Assessment &amp; Evaluation in Higher Education</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="698" to="712" />
			<date type="published" when="2013-09">9 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A comparative analysis of the skilled use of automated feedback tools through the lens of teacher feedback literacy</title>
		<author>
			<persName><forename type="first">S</forename><surname>Buckingham Shum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Boud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bearman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dawson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Educational Technology in Higher Education</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<date type="published" when="2023">7 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Teacher feedback literacy and its interplay with student feedback literacy</title>
		<author>
			<persName><forename type="first">D</forename><surname>Carless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Winstone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Teaching in Higher Education</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="150" to="163" />
			<date type="published" when="2023">1 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automatic feedback in online learning environments: A systematic literature review</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Cavalcanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barbosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>GaÅ¡eviÄ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Mello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and Education: Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">100027</biblScope>
			<date type="published" when="2021">1 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Impact of an Artificial Intelligence Research Frame on the Perceived Credibility of Educational Research Evidence</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cukurova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Luckin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Artificial Intelligence in Education</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="205" to="235" />
			<date type="published" when="2020">6 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Can Large Language Models Provide Feedback to Students? A Case Study on ChatGPT</title>
		<author>
			<persName><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>GaÅ¡eviÄ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2023 IEEE International Conference on Advanced Learning Technologies (ICALT)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="323" to="325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Algorithm aversion: People erroneously avoid algorithms after seeing them err</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Dietvorst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Simmons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Massey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="114" to="126" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Combining LLM-Generated and Test-Based Feedback in a MOOC for Programming</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gabbay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the Eleventh ACM Conference on Learning @ Scale</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>to be published</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Do as AI say: susceptibility in deployment of clinical decision-aids</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gaube</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Merritt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Berkowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lermer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Coughlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Guttag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Colak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghassemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NPJ digital medicine</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Conditions Under Which Assessment Supports Students</title>
		<author>
			<persName><forename type="first">G</forename><surname>Gibbs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Simpson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Learning. Learning and teaching in higher education</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3" to="31" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The applicability of Visible Learning to higher education</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hattie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scholarship of Teaching and Learning in Psychology</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="79" to="91" />
			<date type="published" when="2015">3 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The Power of Feedback</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hattie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Timperley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Review of Educational Research</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="81" to="112" />
			<date type="published" when="2007">3 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The usefulness of feedback</title>
		<author>
			<persName><forename type="first">M</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Boud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dawson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Molloy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Active Learning in Higher Education</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="229" to="243" />
			<date type="published" when="2021">11 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Hirunyasiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Koedinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Aleven</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.02018</idno>
		<title level="m">Comparative Analysis of GPT-4 and Human Graders in Evaluating Praise Given to Students in Synthetic Dialogues</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A Systematic Literature Review of Automated Feedback Generation for Programming Exercises</title>
		<author>
			<persName><forename type="first">H</forename><surname>Keuning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jeuring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Heeren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computing Education</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1" to="43" />
			<date type="published" when="2019">3 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">My Teacher Is a Machine: Understanding Students&apos; Perceptions of AI Teaching Assistants in Online Education</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Merrill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Sellnow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="1902" to="1911" />
			<date type="published" when="2020">12 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">To advance ai use in education, focus on understanding educators</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Kizilcec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Artificial Intelligence in Education</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="12" to="19" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learner-centred Analytics of Feedback Content in Higher Education</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Mello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gasevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LAK23: 13th International Learning Analytics and Knowledge Conference</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="100" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">What is AI Literacy? Competencies and Design Considerations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Magerko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 CHI conference on human factors in computing systems</title>
		<meeting>the 2020 CHI conference on human factors in computing systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">4 2020</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Goodwill: A reexamination of the construct and its measurement</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Mccroskey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Teven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications Monographs</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="90" to="103" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Designing and Evaluating Tutoring Feedback Strategies for digital learning environments on the basis of the Interactive Tutoring Feedback Model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Narciss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Education Review</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="7" to="26" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Teachers&apos; Trust in AIpowered Educational Technology and a Professional Development Program to Improve It</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nazaretsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ariely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cukurova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Alexandron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British Journal of Educational Technology</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="914" to="931" />
			<date type="published" when="2022">7 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An Instrument for Measuring Teachers&apos; Trust in AI-Based Educational Technology</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nazaretsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cukurova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Alexandron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LAK22: 12th International Learning Analytics and Knowledge Conference</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2022">3 2022</date>
			<biblScope unit="page" from="56" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Pankiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Baker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.00150</idno>
		<title level="m">Large Language Models (GPT) for automating feedback on programming assignments</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning gain differences between ChatGPT and human tutor generated algebra hints</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">A</forename><surname>Pardos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhandari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.06871</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">AI-generated vs. Human Artworks. A Perception Bias Towards Artificial Intelligence?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ragot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cojean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Irvin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bagul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Langlotz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shpanskaya</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05225</idno>
		<title level="m">CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Understanding the motivational effects of procedural and informational justice in feedback processes</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">M</forename><surname>Roberson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Stewart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British Journal of Psychology</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="281" to="298" />
			<date type="published" when="2006">8 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Feedback modes matter: Comparing student perceptions of digital and non-digital feedback modes in higher education</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Phillips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British Journal of Educational Technology</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1507" to="1523" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J</forename><surname>Shute</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Focus on Formative Feedback</title>
		<imprint>
			<date type="published" when="2008">3 2008</date>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="153" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Comparing the quality of human and ChatGPT feedback of students&apos; writing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Steiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Warschauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Olson</surname></persName>
		</author>
		<ptr target="https://www.officeforstudents.org.uk/publications/annual-report-and-accounts-2022-23/" />
	</analytic>
	<monogr>
		<title level="m">The Office for Students: Annual report and accounts</title>
		<imprint>
			<date type="published" when="2023">2024. 2023</date>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page" from="2022" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rethinking student-teacher relationships in higher education: a multidimensional approach</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tormey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Higher Education</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="993" to="1011" />
			<date type="published" when="2021">11 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning analytics in European higher education-Trends and barriers</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Moreno-Marcos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>MuÃ±oz-Merino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Jivet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Scheffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Drachsler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Kloos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>GaÅ¡eviÄ</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Education</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="page">103933</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">What is feedback in clinical education?</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Van De Ridder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Stokking</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Mcgaghie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">T J</forename><surname>Ten Cate</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Education</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="189" to="197" />
			<date type="published" when="2008">1 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Designing Effective Feedback Processes in Higher Education: A Learning-Focused Approach</title>
		<author>
			<persName><forename type="first">N</forename><surname>Winstone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Carless</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Routledge</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">It&apos;d be useful, but I wouldn&apos;t use it&apos;: barriers to university students&apos; feedback seeking and recipience</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>Winstone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Nash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rowntree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Parker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Studies in Higher Education</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="2026" to="2041" />
			<date type="published" when="2017">11 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The Power of Feedback Revisited: A Meta-Analysis of Educational Feedback Research</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wisniewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zierer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hattie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2020">1 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Practical and ethical challenges of large language models in education: A systematic scoping review</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Martinez-Maldonado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>GaÅ¡eviÄ</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British Journal of Educational Technology</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="90" to="112" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Understanding the Effect of Accuracy on Trust in Machine Learning Models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2019 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">ChatGPT for Next Generation Science Learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">XRDS: Crossroads, The ACM Magazine for Students</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="42" to="46" />
			<date type="published" when="2023">3 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">AI and formative assessment: The train has left the station</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Nehm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Research in Science Teaching</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="1390" to="1398" />
			<date type="published" when="2023">8 2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
