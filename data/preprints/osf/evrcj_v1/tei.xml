<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning from Noise: Applying Sample Complexity for Political Science Research</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2024-08-06">August 6, 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Perry</forename><surname>Carter</surname></persName>
							<email>pjcarter@princeton.edu</email>
						</author>
						<author>
							<persName><forename type="first">Dahyun</forename><surname>Choi</surname></persName>
							<email>dahyunc@princeton.edu</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Ph.D. Candidate</orgName>
								<orgName type="department" key="dep2">Department of Politics</orgName>
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ph</forename><forename type="middle">D</forename><surname>Candidate</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Politics</orgName>
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning from Noise: Applying Sample Complexity for Political Science Research</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-08-06">August 6, 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">AB19D22726FD8D61947970A7F10ADF15</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-10-22T06:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Probably Approximately Correct Model</term>
					<term>Sample Complexity Bounds</term>
					<term>Vapnik-Chervonenkis Dimension</term>
					<term>Measurement</term>
					<term>Sample Size Word Count: 7615</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While statistical learning bridges the gap between theoretical concepts and complex empirical realities, the question of what constitutes "good enough" data for social scientists remains understudied. In this article, we introduce the Probably Approximately Correct model and present sample complexity bounds, which take advantage of researchersspecified estimates of labeling error to guarantee the sample size required for a minimum level of accuracy. We develop a simulation-based approach to demonstrate its feasibility and provide the scR R package, offering a computationally efficient way to implement the proposed methods. We aim to improve standard practice by providing a general-purpose tool to validate the quality of measures when fuzzy measurement boundaries make generating data with ground-truth labels infeasible.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Sample Size Calculations in Applied Research</head><p>What constitutes "good enough" data? The majority of concepts of interest in political science are not directly observable; consequently, many scholars design and construct their own measurements from messy or unstructured sources of data. There has been extensive work on the use of hand-coding or crowdsourced annotations (e.g., <ref type="bibr" target="#b44">Tian and Zhu 2015;</ref><ref type="bibr" target="#b6">Benoit et al. 2016;</ref><ref type="bibr" target="#b8">Carlson and Montgomery 2017;</ref><ref type="bibr" target="#b50">Ying, Montgomery, and Stewart 2022;</ref><ref type="bibr" target="#b36">Miller, Linder, and Mebane 2018)</ref>, and researchers have attempted to improve the efficiency of measurement by using machine learning to extrapolate from a relatively small training set to a larger set of unlabeled or unseen data (e.g., <ref type="bibr" target="#b19">Grimmer and Stewart 2013;</ref><ref type="bibr" target="#b5">Barberá et al. 2021;</ref><ref type="bibr" target="#b26">Jerzak, King, and Strezhnev 2023)</ref>.</p><p>However, despite the explosion in the use of machine learning, the issue of how data affect measurement quality and inference has received relatively little attention in the political science literature. While measurement models are typically optimized for accuracy on observed labeled instances, the ability to make generalizable claims beyond the training data is the ultimate goal of statistical learning. Yet, if the training data is not representative of the population of interest, the model may not generalize well to other samples or populations, often due to biased sampling methods. Similarly, high variability within the data can make it challenging for researchers to quantify the resources needed to "learn" from given samples or to determine whether learning from such noisy data is always possible.</p><p>Relatedly, one typical implicit assumption in most applications is that learning algorithms have access to noise-free ground-truth labels 1 for training examples of the target concept 2 . However, this assumption can further expose empirical analysis in 1 That is, a mechanism generating accurately labeled examples from a known probability distribution. 2 We use the term concept in this paper to refer to a latent binary class. We prefer this term over the equivalent "class" or "latent variable" in order to highlight the close relationship to concept formation problems in social science. A technically precise definition is provided in Section 3. political science to the fragility of statistical inference, given that many concepts in the discipline such as transparency (e.g., <ref type="bibr" target="#b24">Hollyer, Rosendorff, and Vreeland 2014)</ref>, electoral competitiveness (e.g., <ref type="bibr" target="#b27">Kayser and Lindstädt 2015;</ref><ref type="bibr" target="#b11">Cox, Fiva, and Smith 2020)</ref>, or conflict initiation (e.g., <ref type="bibr" target="#b15">Esarey and Pierce 2012)</ref> are subject to conceptual ambiguity or "stretching" <ref type="bibr" target="#b9">(Collier and Mahon 1993)</ref>. Such latent concepts are typically both high-dimensional and have ambiguous boundaries, making it difficult to specify explicit conditions for inclusion a priori. However, despite the inherent complexity of measurement tasks in social science, "learning" from the noisiness inherent in quantitative political science research has rarely been addressed.</p><p>To address this lacuna, we propose the application of an approach based on the Probably Approximately Correct (PAC) model, which uses the training set to learn a concept that has a small true error on the test distribution. <ref type="foot" target="#foot_0">3</ref> The premise of the PAC model is that examples are drawn from a fixed, but unknown, distribution over the instance space. This assumption provides researchers with hope that what they learn from the training data will generalize to new and unseen test data. Based on this setup, researchers can then determine the sample size necessary to guarantee a probably approximately correct solution.</p><p>The smallest sample size necessary to learn a concept is referred to as sample complexity <ref type="bibr" target="#b7">(Blumer et al. 1989)</ref>. Simply put, the sample complexity bound guarantees the sample size required to achieve a minimum level of accuracy with a precise level of confidence, for all distributions and all target concepts. Therefore, it allows researchers to infer the generalizability of the concepts of interest beyond the training set they use. To determine the sample complexity bound, we incorporate the estimation of the Vapnik-Chervonenkis (VC) dimension, or "capacity," of the target concept. The method then allows researchers to theoretically evaluate how much data is needed for their design before making costly investments in data collection. In addition, we provide an illustrative example of how the method can be extended to hypothesis classes with infinite VC dimensions by applying the union bound to countably infinite unions of concepts.</p><p>After introducing the notion of the PAC model and sample complexity, we provide a novel simulation-based approach to compare the empirical performance of sample complexity bounds for researcher-specified confidence, accuracy, and misclassification parameters. We further demonstrate the feasibility of this approach, implemented in a companion R package. A simulation exercise using a foundational concept in political science, polyarchy, verifies that the sample complexity bound we produce is not overly conservative. To further validate the usefulness of the method, we provide a replication analysis of <ref type="bibr" target="#b13">Dressel and Farid (2018)</ref> and show how the proposed method can be used for a priori analysis before researchers engage in data collection. Given that the collection of human-labeled data tends to be expensive and time-consuming, we anticipate that the application of sample complexity will offer significant time and cost-saving advantages to researchers. This paper aims to provide a general-purpose tool for assessing the sample size needed to achieve adequate performance from statistical learning algorithms in applied social science. This provides a foundation for the supervised learning tasks prevalent in political science using a principled approach analogous to the hypothesis testing framework for statistical inference. Moreover, when researchers intend to estimate causal effects of or on latent concepts as a downstream application, as in the persistent debate over the causal effects of democracy <ref type="bibr" target="#b0">(Acemoglu et al. 2019)</ref>, measurement error has direct implications for the power of the corresponding hypothesis test <ref type="bibr" target="#b29">(Knox, Lucas, and Cho 2022)</ref>. By providing a straightforward way to assess the consequences of sample size for measurement, we therefore offer a means to improve the accuracy of power analysis for causal inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Principled and Unprincipled Measurement-Inference in Social Science</head><p>Significant progress has been made in the application of machine learning within political science. According to <ref type="bibr" target="#b1">Arnold et al. (2023)</ref>, 64 manuscripts relevant to machine learning were published in three leading political science journals, including American Political Science Review , Political Analysis, and Political Science Research and Methods, between 2016 and 2021. Scholars have improved machine learning models through regularization (e.g., <ref type="bibr" target="#b22">Hainmueller and Hazlett 2014;</ref><ref type="bibr" target="#b16">Fariss and Jones 2018)</ref> or parameter tuning (e.g., <ref type="bibr" target="#b38">Neunhoeffer and Sternberg 2019)</ref> to produce a good indicator of prediction in new data. These advances in machine learning techniques have naturally been accompanied by an increase in inferential tasks (e.g., <ref type="bibr" target="#b20">Grimmer, Westwood, and Messing 2014)</ref>.</p><p>However, despite the abundant interest given to improved performance in machine learning models, there has been little attention to downstream questions concerning "measurement-inference" <ref type="bibr" target="#b18">(Grimmer 2015)</ref>. The question of what constitutes sufficient data for feasible statistical inference is a principled one that has been recognized by most scholars using machine learning techniques (e.g., <ref type="bibr" target="#b25">Hopkins and King 2010)</ref>. Yet the answer to this question has been unprincipled, as researchers have designed their studies within the arbitrary scopes allowed by their resources and efficiency. <ref type="foot" target="#foot_1">4</ref> For decades, there has been a lack of formalized justification for determining the sufficient sample size needed for generalizable measurement, taking into account the degree of accuracy a researcher can expect from such samples. Our review of publications in Political Analysis for 2023 that employ applied machine learning methods reveals that none of them have attempted to justify whether "learning" has been successful given the sample sizes chosen in their quantitative empirical studies. While some have discussed train-test splits within the scope of accessible data, the consideration of measuring complex concepts in political science and the feasibility of learning amidst noisy data has not been addressed.</p><p>Principled accounts of what constitutes "good enough" data are crucial not only for helping researchers identify the conditions under which meaningful "learning" is possible but also for better testing of causal theories involving unobservable variables.</p><p>Researchers often map messy and high-dimensional data to low-dimensional measures to capture latent concepts in political science and to empirically test causal theories.</p><p>However, these imperfect and noisy measures can produce biased point estimates and standard errors <ref type="bibr" target="#b29">(Knox, Lucas, and Cho 2022)</ref>, as standard practices implicitly ignore that what is "learned" from the data is also estimated from the subset of data available to researchers, instead simply assuming perfectly observed latent constructs. An active body of work aimed at addressing these issues, arising from measurement uncertainties, is through a design-based approach (e.g., <ref type="bibr" target="#b49">Wang, McCormick, and Leek 2020;</ref><ref type="bibr" target="#b17">Fong and Tyler 2021;</ref><ref type="bibr" target="#b14">Egami et al. 2024)</ref>. This approach seeks to recover valid standard errors and confidence intervals in consideration of unknown non-random prediction errors.</p><p>While design-based approaches provide powerful tools for the researcher to mitigate the bias induced by noise in measurements, the model still relies on the accuracy of predictions on sampled data, which may not provide an accurate picture of the models' predictive generalizability. We directly address this issue and consider the issue of "generalizable" inference from downstream data when there is a probability that the training examples are non-representative or incorrectly labeled. Our proposed approach of sample complexity bounds offers performance guarantees for a model based on a given number of samples, providing more rigorous insights concerning the measurement challenges in social science than empirical prediction error estimates. In the following section, we present a formal framework for statistical learning before discussing the proposed approach.</p><p>An important point of clarification regards our framing of the problem as one of identifying what constitutes good enough data. In political methodology -particularly from the Bayesian perspective -it is standard to take the available data as given and to focus instead on designing an estimator that has desirable properties for a given research context. In this paper, we take the opposite perspective, assuming that a valid estimation strategy corresponding to the concept being measured has been selected but that the researcher is at the design stage of a research project, prior to collecting data.</p><p>Hence, the question faced by practitioners is conceptually equivalent to that of power analysis at the pre-analysis stage of experimental research: given access to infinite data with ground-truth labels, the algorithm is known (or at least assumed) to be able to perform arbitrarily well. Due to real-world resource constraints, however, the researcher seeks to identify how much data and what degree of labeling accuracy is sufficient to guarantee a desired level of performance before investing in data collection.</p><p>As in power analysis, where the target power level is determined by the researcher's individual risk tolerance for Type II error, the accuracy and confidence parameters in our framework our chosen based on the ultimate goal, whether downstream inference or pure description.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">A Framework for Statistical Learning</head><p>Before discussing the PAC model and sample complexity bounds, we describe a formal framework designed to capture statistical learning tasks. To link the model more closely to its practical applications, we consider a stylized version of the well-known model of "polyarchy" proposed in <ref type="bibr" target="#b12">Dahl (2008)</ref>. Here, the goal is to learn where the rectangular cutoff between polyarchies and non-polyarchies lies in a region defined by two latent dimensions -"participation" and "contestation". While we thus abstract away from much of the discussion of measurement that has dominated the recent literature on this topic (e.g., <ref type="bibr" target="#b34">Little and Meng 2023)</ref>, the basic issue of determining the cut-off point between democracies and nondemocracies remains highly salient both theoretically and empirically <ref type="bibr" target="#b3">(Baltz, Vasselai, and Hicken 2022)</ref> and is complicated by small sample size. We therefore employ this as a running example to illustrate the approach. In the basic statistical learning setting, researchers have access to the following <ref type="bibr" target="#b31">(Laird 2012)</ref>:</p><p>Domain set: This refers to an arbitrary set, X. This is the set of all observations that researchers might hope to label. For example, regarding the polyarchy problem just outlined, the domain set would be the set of all political regimes. These domain points are represented by a vector of features. In our case, this is a two-dimensional vector of contestation and participation. In the following discussion, we interchangeably refer to domain points as instances and χ as instance space.</p><p>Label set: For our discussion, we restrict the label set to be a binary classification task, with Y denoting the set of possible labels. For instance, let Y = {0, 1}, where 1 represents polyarchies and 0 stands for non-polyarchies.</p><p>Training data: S = {(x 1 , y 1 ), . . . , (x m , y m )} indicates a finite sequence of pairs in X×Y: that is, a sequence of labeled domain points. This is the input that researchers have access to, such as a set of countries and their observable attributes. We interchangeably </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The learner's rule:</head><p>The learner is expected to produce a prediction rule, which is the concept researchers would like to learn. It is a function h: X → Y. In computational learning theory, this function is also described as a predictor, a hypothesis, or a classifier.</p><p>The classifier can be used to predict the label of new domain points. In our polyarchy example, it is a rule that our learner will use to predict whether future countries it examines are polyarchies or not. We use the notation h to denote the hypothesis that a learning algorithm produces when given the training set.</p><p>Data-generating Process: We now discuss how the training set is generated. It is worthwhile to note that the instances -the countries that researchers could access -are generated by some arbitrary fixed probability distribution. We denote the probability distribution over X by D. To clarify, we do not assume that the learner has any priors about this distribution, or even that is restricted to a particular class. Each pair in the training sets S is generated by first sampling a point x i according to D and then labeling it by f : X → Y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Measures of errors:</head><p>The error of a classifier represents the probability that it does not predict the correct label on a random data point generated by the same underlying distribution as the training data. In other words, the error of h is the probability of drawing a random instance x according to the distribution D such that h(x) ̸ = f (x).</p><p>Given that there is an unknown concept c which determines the true label of instances, the set of labeled instances S = {(x 1 , y 1 ), . . . , (x m , y m )} is generated by taking x i ∼ D i.i.d and observing the corresponding y i = f (x i ). Then the true error and empirical error can be defined as follows.</p><p>DEFINITION 1. True Error: Consider a data-generating distribution D and the true labeling concept c. The true error of a concept h with respect to D is the probability that h makes a mistake.</p><p>(1)</p><formula xml:id="formula_0">R(h) = Pr x∼D [h(x) ̸ = y]</formula><p>DEFINITION 2. Empirical Error: Given a sample set S, the empirical error of a concept h with respect to S is the fraction of instances in S that are incorrectly labeled by h.</p><p>(2)</p><formula xml:id="formula_1">Rm (h) = 1 m m i=1 1(h(x i ) ̸ = y i )</formula><p>The true error of h is also referred to as generalization error or risk, which are used interchangeably throughout this article. Likewise, the terms empirical error and empirical risk are often used interchangeably. We again emphasize that the learner is blind to the underlying distribution D over the world and to the labeling function f . The learner interacts with the environment solely by observing the training set. Therefore, learning is "distribution-free" in the sense that the probability distribution generating the samples can be any distribution on the underlying measurable space <ref type="bibr" target="#b48">(Vidyasagar 2013</ref>). As such, there is a complete absence of prior knowledge about the underlying distribution and the bounds are intended to function even in this setting. While this assumption might seem extreme, the fact that statistical learning can operate without reliance on specific distributions enables us to establish universal conditions needed for a concept to be learned.</p><p>The advantage of this general learning framework marks a significant departure from traditional methods. Typically, empirical analysis in political science relies on specific assumptions or predefined functional forms, such as linear relationships among covariates or data generated from normal distributions. In contrast, PAC learning takes a distribution-free framework whereby researchers assume that samples are created by drawing points independently at random according to an unknown fixed probability distribution and classified based on a target concept. It therefore seeks to identify models that most accurately represent the data-generating process while minimizing assumptions about the distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Probably Approximately Correct (PAC) Learning</head><p>We begin this section with an intuitive explanation of PAC learning, followed by its formal definition. In the plot on the left-hand side of Figure <ref type="figure" target="#fig_1">1</ref>, both hypotheses 1 and 2 are consistent; however, hypothesis 2 learns the concept better, implying that it has a smaller true error.</p><p>Therefore, we hope that our error based on the training samples is smaller than some number ϵ, which we choose to define our desired level of accuracy. At this stage, our approach is considered to be "approximately correct".</p><p>However, since the training set researchers use is randomly drawn, there is always a nonnull probability that the drawn sample contains misleading instances, as shown in the right-hand plot of Figure <ref type="figure" target="#fig_1">1</ref>. Therefore, we can never be 100% certain that the true error of our classifier will not exceed ϵ. Instead, we bound the probability of this occurring with δ, our confidence parameter. Thus, our model is now "probably approximately correct".</p><p>In sum, the accuracy parameter ϵ determines how close the output can be to the optimum and the confidence parameter δ indicates the likelihood that the classifier will meet the accuracy requirement. Under empirical settings, these approximations are inevitable. There is always a small chance that the examples that are drawn will happen to be noninformative. For example, there is always some chance that the training set will contain only one domain point, sampled over and over again as the training set is randomly generated. Furthermore, even if we are fortunate enough to obtain a training sample that closely approximates the true underlying distribution, it may still miss some fine details of the target class. In this regard, the confidence parameter addresses the error-prone nature of classification tasks.</p><p>The formal intuition behind "learning" is quite straightforward. Given a hypothesis class, H, the learner evaluates the risk, |R(h) -RS (h)|, of each h in H on the given sample and outputs a member of H that minimizes the empirical risk. The goal is that an h that minimizes the empirical risk on the sample S also minimizes the risk or has a risk close to the minimum, for the true data probability distribution. To achieve this, it's enough to ensure that the empirical risks of all members of H closely approximate their true risks. In other words, we need the empirical risk to be uniformly close to the true risk across all hypotheses in the hypothesis class. </p><formula xml:id="formula_2">(3) ∀h ∈ H, |R(h) -RS (h)| &lt; ϵ</formula><p>Lemma C1 further states that whenever the sample is ϵ 2 -representative, the learning rule is guaranteed to return a good hypothesis. The details and proofs can be found in the Appendix. This lemma indicates that, to ensure that the minimizing empirical risks rule functions as an agnostic PAC learner, it is enough to prove that with a probability of at least 1 -δ, the randomly chosen training set will be an ϵ-representative training set. In the study of machine learning theory, such property has been described as a uniform convergence property. The term "uniform" here refers to having a fixed sample size that works for all members of H and over all possible probability distributions over the domain. A formal definition is provided in Section C in the Appendix.</p><p>Based on the intuition of the PAC model, the next section further offers formal explanations the sample complexity which is a function of accuracy, ϵ, and confidence, δ, parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">A Sample Complexity Bound For Applied Research</head><p>For a concept to qualify as PAC-learnable, it must satisfy guarantees for all possible target concepts and across all distributions. In this section, we examine the number of training samples required to ensure the feasibility of PAC learning, referred to as sample complexity. To reflect the reality where a set of labeled examples provided to the learning algorithm are often "noisy," we consider the scenario where each example received by the learner is mislabeled randomly and independently with a fixed probability, η &lt; 1 2 .</p><p>Sections A and B in Appendix shows the contrasts between noise-free and noisy learning in further detail.</p><p>We therefore seek to determine the minimum sample size that produces a hypothesis within a specified error tolerance of the true concept with high probability when the data used for learning is corrupted by noise. To achieve this, the learning algorithm should be provided with a number N of i.i.d. training examples along with corresponding correct classifications. To be more precise, we define the sample complexity as the minimal integer that satisfies the requirements of PAC Learning for a given ϵ and δ.</p><p>This concept can be thought of as analogous to the widely used approach of power analysis in experimental research, as both provide a measure of the sample size needed to guarantee a certain level of accuracy. As we will show, the sample complexity also has direct implications for determining power when researchers aim to identify the causal effects of a latent concept. However, sample complexity differs from power analysis in the experimental or quasi-experimental setting. Power analysis assumes certain distributional features -typically encoded as an effect size -and yields an explicit likelihood that researchers will correctly reject a hypothesis with N observations.</p><p>Sample complexity, on the other hand, provides a bound on the minimum number of observations needed to achieve a target accuracy with given confidence across all distributions and target concepts.</p><p>Formally, the sample complexity is the minimum value of N for which the equation ( <ref type="formula">3</ref>) holds. Based on the discussion above, we consider a probability distribution D over X. We assume that the instances we observe are independent and identically distributed (i. For simple and geometrically regular concepts, it is often possible to exploit known features of the boundary's shape to recover bounds that are quite tight. In the polyarchy example, the classification boundary is known to take the form of a rectangle with one vertex located at the point (1, 1). Suppose that our learning algorithm then draws the tightest possible rectangle that encloses all positive examples seen in the training data.</p><p>We take the regime that displays the lowest levels of contestation and participation while still being labeled (for now, let us assume correctly) as a polyarchy and denote it as the lower-left vertex of the bounding rectangle.</p><p>By design, this algorithm will then correctly classify all non-polyarchies as such, since it will never observe a polyarchy that falls outside of the true boundary C. In other words, it produces only Type-II error. Now fix a desired error rate ϵ and consider a hypothesized boundary h with a true error rate R(h) of at least ϵ. Then it must be the case that the lower-left-most observed polyarchy was sufficiently far from the boundary as to leave at least ϵ probability of observing polyarchies with less contestation or participation. In other words, we can draw rectangles r 1 and r 2 from the left and bottom boundaries of C such that the probability of drawing an observation x ∈ r 1 or x ∈ r 2 is at least ϵ/2 and h must disagree entirely with at least one of them. Hence we can conclude that</p><formula xml:id="formula_3">P(R(h) &gt; ϵ) ≤ i=1,2 P(x / ∈ r i ) ≤ 2 1 - ϵ 2 m ≤ 2e -mϵ 2</formula><p>Now recall that we are interested in bounding the probability of this occurring -that is, the probability that we do not draw such observations -by δ. Hence we have that</p><formula xml:id="formula_4">δ ≥ 2e -mϵ 2 or, equivalently, (4) m ≥ 2 ϵ ln 2 δ</formula><p>so that in order to have an error rate below 1% with 95% confidence, we would need at least 738 observations. This bound relies on the particular rectangular shape of our toy example and cannot be applied more generally. However, the logic used to derive it highlights an important feature of the more general bound we state below: it is entirely distribution-free. The distribution-free property of bounds is a double-edged sword for researchers. The primary advantage is that these bounds are generally applicable since they do not assume a specific form for the data distribution, extending their use to a wide range of problems and datasets. Moreover, they guarantee validity regardless of the distribution, making them more reliable in practice. This frees researchers from concerns about misspecification, which can lead to suboptimal performance if the assumed distribution does not match reality.</p><p>However, these bounds can often be conservative, accounting for all possible distri- We would like to note that determining sample complexity is an active research area, and numerous variants of these bounds have been introduced. <ref type="foot" target="#foot_3">5</ref> Therefore, we use the most widely accepted and standard bound acknowledged in the field of statistical learning. As provided and <ref type="bibr" target="#b43">Simon (1993)</ref> with <ref type="bibr" target="#b2">Aslam and Decatur (1996)</ref>, a general lower bound on sample complexity (SCB) is given by ( <ref type="formula">5</ref>) min (N :</p><formula xml:id="formula_5">P(e N &gt; ϵ) &lt; δ) = Ω VC(c) ϵ(1 -2η) 2 + l og(1/δ) ϵ(1 -2η) 2</formula><p>where VC(F) indicates the Vapnik-Chervonenkis (VC) dimension, which is a measure of the capacity -that is, the underlying complexity -of the target concept<ref type="foot" target="#foot_4">6</ref> . Note that the Ω notation is used here in the sense introduced by <ref type="bibr" target="#b30">Knuth (1976)</ref> to imply the inverse of big-O; that is, f = Ω(g) ⇔ g = O( f ). This common variation on big-Oh asymptotic notation is used in much of the literature on learning theory, and represents a lower bound in the same sense that big-Oh represents an upper bound. Returning to our running example, a researcher planning to classify regimes into polyarchies would first need to determine the misclassification rate η. In this context, this could be thought of as the residual error of existing democracy measures, which is assumed to be i.i.d. across countries. Next, she would determine a target accuracy 1 -ϵ with which they want to be able to classify polyarchies not in the training set (for instance, from new historical data), and a confidence level 1 -δ with which she wishes to achieve this accuracy. We note that the residual δ probability of misclassifying more than ϵ proportion of polyarchies need not necessarily imply accuracy much worse than ϵ, but simply that it exceeds it. Finally, they would need an accurate estimate of the VC dimension of the concept of polyarchy, and an estimation algorithm capable of learning the logarithmic factor can be removed for a particular set of classifiers (e.g., <ref type="bibr" target="#b23">Hanneke 2016</ref>). While we stay away from this discussion, the bound we present is more conservative than the bounds where logarithmic factors are removed. Therefore, the bound provided in Equation 5 is sufficient for PAC Learning in any case. it.</p><p>In this instance, since we assume in our stylized example that the classification boundary for polyarchies is simply a rectangle in contestation-participation space, it has a known VC dimension of 4<ref type="foot" target="#foot_5">7</ref> . Suppose then that the researcher has determined values of η = 0.05, δ = 0.05, ϵ = 0.01, indicating that she wishes to achieve 99% accuracy with 95% confidence, and believes there to be 5% of regimes in the available training data that are misclassified. Then these values can be substituted into (5), leading to the conclusion that a minimum of 864 examples would be needed to learn the concept of polyarchy -significantly exceeding the currently extant number of regimes. Hence, the researcher would discover that she needed to either find an additional source of data or to revise her accuracy target.</p><p>It is worthwhile to note a few interesting properties of sample complexity bound before examining its validity using simulation-based approaches. The conventional statistical approach used by social scientists often focuses on asymptotic properties, such as the convergence of sample-based statistical estimates as the sample size increases.</p><p>In contrast, the core idea of sample complexity bounds is concerned with finite-sample bounds. This means that when researchers are given a specific sample size, we aim to capture the expected degree of accuracy based on samples of fixed size. For example, if a political scientist hypothesizes that a certain type of political parties or elections might correlate with democratic backsliding, they would check a sample of countries and test the validity of their guess through hypothesis testing, invoking asymptotic convergence to generate confidence intervals. However, when measuring the concept of democratic backsliding, we use sample data to identify meaningful patterns that may not have been detected by traditional observers. Sample complexity provides the necessary sample size for researchers to determine whether these patterns are significant in the sense of yielding high predictive accuracy.</p><p>The application of sample complexity requires that for a large enough sample size, we get a hypothesis with arbitrarily small error with arbitrarily high probability, no matter what concept in H we are trying to learn or what distribution D it is drawn from. Therefore, the bounds on the sample size must be independent of the underlying distribution, the fixed probability distribution P. A class of concepts with a learning function that satisfies this condition is called uniformly learnable. It can be formalized by requiring that the hypothesis has an error greater than ϵ with probability at most δ for small ϵ and δ, uniformly for all concepts. The smallest size that achieves this for all distributions and all target concepts is called the complexity of the learning function.</p><p>In other words, for any distribution and target concepts, with probability at least 1 -δ, learning algorithms produce with error at most ϵ.</p><p>This general definition of uniform learnability implies that sample sizes are uniform with respect to labeling rules and the underlying distribution. Moreover, it is also worth highlighting that the bounds also hold in the case where distributions over the positive and negative examples are distinguished (e.g., <ref type="bibr" target="#b45">Valiant 1984</ref>).<ref type="foot" target="#foot_6">8</ref> While one crucial assumption necessary for PAC learnability is that of a finite VC dimension, there is a way to relax this assumption by allowing the sample size to be nonuniform for the different hypotheses placing unequal weight over the hypothesis classes. Technical details can be found in Section F of the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Estimating VC dimension</head><p>The bound given by Equation 5 depends on the VC dimension of the target concept, which can generally be calculated analytically only for the most straightforward classifiers. To address this challenge, we calculate the sample complexity bound using an estimate that is consistent in simulation parameters rather than the true VC dimension. To do so, we estimate it empirically based on the known relationship between the worst-case generalization error of a classifier and its VC dimension, following <ref type="bibr" target="#b35">McDonald, Shalizi, and Schervish (2011)</ref>. Formally, the VC dimension of a hypothesis space H indicates the cardinality of the largest set S that can be shattered<ref type="foot" target="#foot_7">9</ref> by H. Further details can be found in Appendix D.</p><p>In Figure <ref type="figure" target="#fig_4">2</ref>, we present the results of this estimation procedure for a k-dimensional linear discriminant classifier, which is known to have a VC dimension of k + 1. The y-axis gives an estimated bound on the relationship between empirical risk and sample size for the given classifier, while the x-axis gives the sample size. Since the functional form of this relationship is known up to a constant given the true VC dimension, we can then estimate the VC dimension of any classifier through non-linear regression <ref type="bibr" target="#b46">(Vapnik, Levin, and Le Cun 1994)</ref>. <ref type="bibr" target="#b35">McDonald, Shalizi, and Schervish (2011)</ref> demonstrates that this estimate is consistent in the number of simulations so that the estimate converges to the true VC dimension given sufficient computational resources. To return to the example of polyarchies, the key idea is that the worst-case performance of any learning algorithm is when all of the examples available to it are wrongly labeled. Hence, it is sufficient to generate training data according to any data-generating process and then flip the labels. In this case, we would randomly assign a square region in the feature space to correspond to the classification boundaries of polyarchies and non-polyarchies, and then draw examples from it with the labels swapped. A classifier trained on this data then approximates the worst-case generalization error, since it is learning the opposite of what is needed. It is then sufficient to repeat this procedure for samples of increasing size in order to recover the classifier's VC dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Simulation Studies</head><p>In order to verify that the sample complexity bounds perform reasonably well for realistic configurations of the parameters, we provide simulation-based analysis similar to <ref type="bibr" target="#b17">Fong and Tyler (2021)</ref> and compare the empirical performance with theoretical bounds given the following inputs: (1) a confidence parameter (δ), (2) the accuracy parameter (ϵ) and (3) the misclassification rate (η). The simulations are generated by the following process: Algorithm Simulation-based Analysis 1. Decide on desired accuracy parameters and concept 2. Calculate the VCD of the chosen model using the above estimation procedure 3. Generate a fine grid of points over the k-dimensional feature space 4. Classify these points according to the pre-defined concept 5. Generate observed labels by adding fixed independent random noise with probability η 6. Calculate sample complexity bounds empirically for a range of acceptable error rates 7. Repeat the process according to a range of values of "optimism" parameter (analytic bound corresponds to worst-case sampling)</p><p>We revisit the example of classifying polyarchy and apply our proposed approach as given by the Algorithm above. Values are calculated by fixing η = 0.05 and either ϵ = 0.06 or δ = 0.01. In this setting, Bound 5 gives a theoretical bound of 178 observations, which is borne out by the simulation. Notably, this bound is significantly higher than the sample size needed to achieve performance of over 95% on conventional out-ofsample performance metrics, since it involves the more stringent requirement that the probability of high error rates in any sample be controlled, and not simply the average misclassification rate. Although this may lead to more conservative conclusions regarding the target sample size, a key advantage is that researchers must explicitly specify the confidence δ with which they hope to achieve the desired out-of-sample accuracy, improving transparency and replicability. Morevoer, the theoretical bound of 178 cases closely corresponds to the simulation results under simple random sampling, shown in Figure <ref type="figure" target="#fig_2">3</ref>, indicating that it is not unduly pessimistic even in this simple application. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Limitations</head><p>Conventional classification tasks using machine learning have dealt with challenges associated with possibly incorrect functional form as well as estimation and measurement errors. However, the inference of our proposed methods is free from these possible concerns over the a priori beliefs on labeled data. In other words, our method does not make any strong functional form or distributional assumptions about true labels beyond the existence of i.i.d. measurement error and is therefore not subject to the usual inference challenges arising from potential sampling bias.</p><p>At the same time, our simulation approach is built on the assumption that the true data-generating process is perfectly learnable with the selected method given sufficient data without noise. In practice, much of the error in real-world applications comes not from measurement error, but from either misspecification error or irreducible noise.</p><p>That is, the problem may not be a lack of data, but rather a fundamental mismatch between the target concept and the chosen algorithm, or even an inherent fuzziness in the concept itself that is not simply a problem of measurement error.</p><p>Since we assume away both of these very real issues, our method cannot speak directly to either of them, and the bounds we estimate may therefore be significant</p><p>The horizontal axis reflects variation in the size of the training set only.</p><p>underestimates when they are present. Worse, the pre-specified target accuracy may simply be unattainable in the presence of a large number of unmeasured variables or poorly selected models. The only advice we can offer to researchers in this regard is therefore to do their utmost to minimize the impact of such factors before performing our calculations, and to incorporate any residual uncertainty into their estimate of η.</p><p>Another important consideration is that our application of PAC learning focuses on binary-valued measures. While researchers can collapse continuous measures to binary by setting thresholds, a valuable extension would be to apply the framework to continuous and categorical concepts. Our analysis of learning can be readily extended to many other scenarios by allowing a variety of loss functions. For example, section I in the Appendix provides further intuition of sample complexity for tree-based models.</p><p>The notion of sample complexity can undoubtedly accommodate various extensions.</p><p>Next, although our validation exercises suggest that the theoretical bounds hold well in practice, one might still be concerned that the bounds may be too pessimistic. Related to this issue, one of our future tasks would be to update our simulation-based approach to allow researchers to parametrically pre-specify their "optimism" regarding the sampling procedure, generating an interval of progressively looser bounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Application to Predicting Recidivism (Dressel and Farid 2018)</head><p>We additionally offer an application of the method to actual noisy data, based on the study implemented by <ref type="bibr" target="#b13">Dressel and Farid (2018)</ref>. The original authors studied how predictions made by people with little or no criminal justice expertise can be comparable to machine-based commercial risk assessment software. The application of our method allows us to precisely assess the additional benefit provided by big data in this context, demonstrating how it could be applied by researchers prior to data collection. Our findings highlight the concept formation problem, as big data provides minimal additional benefit due to imprecise specification of the target concept. <ref type="bibr" target="#b13">Dressel and Farid (2018)</ref> compare the overall accuracy and bias in human assessment with the algorithmic assessment of COMPAS, a widely used algorithm for predicting recidivism. The authors hired 20 human coders recruited through Amazon's Mechanical Turk and used seven features (e.g., age, sex, number of juvenile misdemeanors, number of juvenile felonies, number of prior crimes, crime degree, and crime charge) for the analysis. We access the dataset used by <ref type="bibr" target="#b4">Bansak (2019)</ref> to replicate <ref type="bibr" target="#b13">Dressel and Farid (2018)</ref> and implement linear discriminant analysis as in the original paper. The model is trained on a random 80% of training and 20% testing split, with a VC dimension of 8.</p><p>Note that in this case, we do not need to simulate the VCD dimension since it is known theoretically; however, it is straightforward to verify that our procedure produces the same value with a sufficiently large number of simulations. Figure <ref type="figure" target="#fig_6">4</ref> shows the results of simulations on the observed data using δ, η = 0.05, and values of 0.05 (left panel) and 0.35 (right panel) for ϵ. Note that in this case η, or the prevalence of labeling error, essentially corresponds to the false conviction rate since the data are produced directly from the criminal justice system. We use a value of 5%, since available evidence suggests a false conviction rate of between four and six percent <ref type="bibr" target="#b21">(Gross et al. 2014)</ref>.</p><p>The theoretical bound gives a target sample size of 272 observations for these parameters, which is again born out by the simulation results in the left panel. In practice, however, the results show that the best achievable accuracy with high confidence is approximately 35%, but the additional benefit of sample size above 500 is minimal. This application therefore highlights the central role played by the concept formation problem: the advantages of big data depend on precise specification of the target concept and selection of a corresponding learning algorithm.</p><p>As such, this application clearly demonstrates the advantage of the proposed method when used prior to data collection. While the original paper employs a dataset of almost 8000 observations -quite costly for many social science applications -Figure <ref type="figure" target="#fig_6">4</ref> demonstrates that the additional value of this large sample size is negligible given the methods employed. Although an alternative learning algorithm better suited to the target concept may have been able to take advantage of the additional observations, the combination of conceptual mismatch and algorithm choice ensured that most of the sample was essentially wasted. As with power analysis, the calculation of sample complexity bounds prior to undertaking research would prevent such situations from arising, ensuring that scarce research resources are allocated appropriately. We include supplementary exercises in Appendix G using data from <ref type="bibr" target="#b33">Lewis et al. (2019)</ref>. Since the second exercise yields results that are not significantly different from the key findings in <ref type="bibr" target="#b13">Dressel and Farid (2018)</ref>, we opt not to address it in the main text to economize space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">A Guide for Practitioners</head><p>As previously discussed, a priori analysis is used to calculate the necessary sample size N for PAC learning, given the error parameter ϵ, confidence parameter δ, and noisy rate η. While we emphasize the advantage of using the proposed methods prior to data collection, we do not exclude the possibility that researchers might select the "design" of sample complexity bounds, as shown in Table <ref type="table" target="#tab_1">2</ref>. In addition to a priori analysis, sample complexity can bring benefits to researchers even for post-hoc analysis. While this approach is relatively less ideal as it controls only the confidence parameter rather than the accuracy parameter, it can still provide valuable insights into the accuracy of the measures produced by researchers using the given training sets. For instance, post-hoc sample complexity analysis can help determine whether the collected data was sufficient to support reliable inferences or if additional data is needed to achieve a given accuracy level with desired confidence out of sample.</p><p>As more researchers design and construct their own measures, understanding sample complexity can help them identify which quantities are more sensitive to researcherspecific parameter choices. We provide practical advice to practitioners below:</p><p>1. Clearly define your concept: The most important assumption in our framework is that there is a perfect match between the target concept and the classification algorithm employed by the researcher in the sense that the true classification rule lies within the hypothesis class H over which the algorithm is searching and that the two share the same VC dimension. In addition, the bounds presented in this paper require that the concept be PAC-learnable to begin with, which is equivalent to assuming that it has a finite VC dimension. We provide guidelines for common cases where this assumption is violated -such as tree-based models and countable unions of finite classes -in the Appendix and incorporate them into the accompanying R package.</p><p>While this assumption is fundamentally untestable, it requires careful theoretical justification prior to algorithm selection in order to generate useful results. In other words, a given classification method should be selected not simply because it has been shown to be accurate in other research contexts, but because it is thought to be a good fit for the concept at hand. In particular, this requires careful theorization of concepts and reference to previous empirical work in order to ensure that the problem is well-defined: researchers should know how many independent dimensions there are to a concept, and how these tend to interact.</p><p>For instance, in our running polyarchy example, a Dahlian view of regime type suggests that it can be well-represented as a two-dimensional linear interaction.</p><p>It is precisely this kind of careful theorization that will yield precise results when assessing sample complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Justify your choice of research-specific parameters:</head><p>As in the choice of effect size for power analysis <ref type="bibr" target="#b10">(Correll et al. 2020)</ref>, we encourage practitioners to justify their choice of parameters using their domain expertise or drawing on existing studies in the past or empirical evaluation of the pilot data. This is particularly true of the misclassification rate, η, which has a significant impact on sample complexity.</p><p>For instance, researchers studying democracy might begin by comparing the disagreement rates across existing measures in order to arrive at an approximate estimate of the rate of labeling error.</p><p>3. Run simulation plots for pilot studies: While drawing in domain knowledge and prior research can help to select appropriate parameter values, we reiterate the importance of conducting pilot studies where feasible. Using our companion R package,scR, researchers can run simulation analysis using their own pilot data.</p><p>By visualizing the relationship between error rate and sample size, practitioners will be better positioned to make informed decisions on how to allocate resources.</p><p>If, after taking these steps, the resultant sample complexity of a classification task falls within the parameters of a proposed data collection effort, the analysis then lends weight to a research plan. This is particularly valuable to researchers at the grant application stage, as it can help to assure funders of its robustness. Conversely, when the analysis indicates that the sample complexity exceeds the budget or data availability, it provides an early warning to researchers prior to making costly investments. Such a discovery that the data is likely not to be good enough need not mean an injunction to abandon the project, however. Instead, the following courses of action are generally available to practitioners in this scenario. While the preferred choice dependent on the particular research context, we present them in order of decreasing order of desirability:</p><p>1. Researchers can simply gather more data, for example by applying for additional funding or locating new data sources. While this allows for pursuing the original research plan without compromise, it is frequently not practical due to scarce resources.</p><p>2. Researchers can adjust their target accuracy or confidence parameters or invest in reducing labeling errors in order to reduce the sample complexity to an acceptable level. In practice, this is essentially what widely-used post hoc approaches entail:</p><p>estimation is optimized based on the best (estimated) out-of-sample error rate, which may fall short of the ex-ante target. However, there is a considerable advan-tage to performing this step at the design stage, as it allows adjusting expectations based on what level of accuracy is likely to be achievable.</p><p>3. The third option is to revisit the choice of algorithm. As noted above, a key assumption underlying our method is the match between concept and classification algorithm. In practice, however, this is often not the case: many common classification tasks in political science are quite low-dimensional <ref type="bibr" target="#b37">(Morucci and Spirling 2024)</ref>, leading to a risk of selecting algorithms that are unnecessarily complex in terms of VC dimension. In other words, while the idea of applying a multi-billion parameter neural network capable of adjusting to even the most subtle concepts may be appealing, it is unnecessary if the true relationship can be well-represented by a linear discriminant and will result in unrealistically high sample complexity.</p><p>Perhaps the most important decision facing applied researchers involves the selection of confidence and accuracy parameters, ϵ and δ. As is evident from the simulation studies we provide (see, for example, Figure <ref type="figure" target="#fig_2">3</ref>), reducing the confidence parameter δ typically imposes significantly more stringent requirements on the data than does ϵ.</p><p>To see why this is the case, suppose that the target accuracy is 95%, corresponding to ϵ = 0.05. Then, even as the average accuracy approaches this level, datasets will still be drawn with a high probability that result in marginally lower accuracy, say 94%. Setting δ close to 0 then requires that we control the probability of this occurring, which may result in hitting the point of diminishing returns with regards to average accuracy. For this reason, we would generally recommend that researchers choose a higher δ than ϵ;</p><p>for example, we find δ = 0.2, ϵ = 0.1, or an 80% confidence of 90% accuracy to strike a reasonable balance in many cases.</p><p>Nevertheless, we strongly discourage researchers from simply applying these default values without further consideration. In practice, the appropriate values will largely be determined by the ultimate goal of the classification task. If the goal is purely descriptive -for instance, to estimate the number of polyarchies in the world, or the proportion of progressive Democrats -then achieving high accuracy and confidence may not be particularly important. Conversely, when the classification has policy consequences, as in our recidivism outcome, tight control over error rates is desirable.</p><p>In addition, there are two important cases in which our method overlaps with power analysis for causal inference. The first of these is when the researcher seeks to identify the impact of a randomized treatment on a latent outcome; for instance, the impact of campaign donations on a politician's spoken ideology. The second instance arises in observational settings when the outcome is observed but the causal variable is latent, as in the impact of exposure to media ideology on survey respondents' voting intentions.</p><p>In both of these cases, the accuracy with which the latent construct can be measured has direct consequences for the power of the causal estimator, since misclassification introduces noise, even if it is completely random. In these cases, we would therefore recommend a two-stage approach: first, researchers should identify the impact of classification error on the power of their causal estimator. Second, they should conduct a sample complexity analysis with accuracy and confidence parameters determined by the first stage in order to identify the additional demands on sample size introduced by the latent classification step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Discussion</head><p>In this paper, we consider the question of what constitutes "good enough" data, both in terms of sample size and labeling accuracy, for statistical inference. To address this persistent question in applied research, we propose a novel application of PAC learning.</p><p>We present the theoretical sample complexity bound that makes PAC learning feasible and provide a simulation-based approach to demonstrate its applicability, incorporating the VC dimension. The proposed method is a general-purpose tool for quantitative research in political science that allows researchers to make rigorous predictions about what can be "learned" from data before investing in costly data-collection efforts.</p><p>A key advantage of our approach is that it provides a more precise alternative to the assumption that the sample size is "large enough" for asymptotic approximations to hold. Furthermore, we directly consider the role played by labeling error and concept definition on model performance, a factor that has generally been overlooked in applied work, with hand-labeled data assumed to represent ground truth. Our approach thus provides a practical toolkit for researchers to guarantee adequate performance at the design stage, improving the replicability and external validity of studies reliant on machine learning classifiers.</p><p>Relatedly, we expect that sample complexity bounds will increase efficiency for researchers in terms of time and expenses. Traditional measurement approaches, such as hand-coding, tend to incur high startup costs (e.g., <ref type="bibr" target="#b41">Quinn et al. 2010)</ref>. Without tools to approximate the necessary sample size, researchers have often made decisions based on intuition within the scope allowed by their time and available resources. We hope that the application of sample complexity will add scientific rigor by providing statistical justification for the choice of sample size when applying machine learning to measurement tasks. While these bounds may not always be tight, they currently represent the best tool available to empirical researchers at the data collection stage.</p><p>Although it is certainly possible for classifiers trained on fewer data to perform well, the aim of our paper is to provide theoretically informed guidelines to optimize the allocation of scarce resources.</p><p>Our goal is not to provide the definitive answer to the methodological question of what constitutes "good enough" data. Instead, we aim to initiate a dialogue. Social scientists have made significant strides in developing rigorous statistical models. However, the need to assess the quality of input has often been overlooked when performing downstream statistical inferences. We hope this paper contributes to rekindling atten-that is, p will be arbitrarily close to p for sufficiently many tests of an event whose probability of occurring is p, for all ϵ &gt; 0. If a correct hypothesis fails on average at the rate η, then with enough examples m, we will measure a failure rate closest to η with high probability. Similarly, an ϵbad rule will fail at nearly its expected rate, η + s, where s ≥ ϵ(1 -2η) &gt; 0.</p><p>LEMMA 1 (Hoeffding's inequality). Consider a Bernoulli random variable with probability p of having the value 1 and 1 -p of having value 0. Let GE( p, m, r) be the probability of at least ⌈rm⌉ successes in m independent trials, and LE( p, m, r) be the probability of at most This lemma bounds the probability that r, the empirical rate of success, is very different from p.</p><formula xml:id="formula_6">⌊rm⌋ successes. If 0 ≤ p ≤ 1, 0 ≤ s ≤ 1,</formula><p>Suppose ϵ &gt; 0, δ ≤ 1 2 and 0 ≤ η ≤ η b &lt; 1 2 . Let success for a rule e i refer to the event of disagreeing with a random sample. In m examples, F i is the number of successes, and</p><formula xml:id="formula_7">F i m is the empirical rate of disagreement. THEOREM 1. When m ≥ 2 ϵ 2 (1 -2η b ) 2 l n 2N δ</formula><p>, the algorithm pac-identifies the concept H.</p><p>Or a tighter bound: THEOREM 2. Let η &lt; 1/2 be the rate of classification noise and N the number of rules in the class E. Assume 0 &lt; ϵ, δ &lt; 1 2 . Then the number m of examples required is at least</p><formula xml:id="formula_8">(8) m ≥ max ln(1/2δ) ln[1 -ϵ(1 -2η)] -1 , log 2 N(1 -2ϵ(1 -δ) + 2δ)</formula><p>and at most</p><formula xml:id="formula_9">(9) m ≤ ln(N/δ) ϵ 1 -exp[-1 2 (1 -2η) 2 ]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proofs</head><p>The probability that a good rule fails on an example is η, while the probability that an ϵ-bad rule fails is at least η + ϵ(1 -ϵ). The difference between these two rates is at least</p><formula xml:id="formula_10">ϵ(1 -η) ≥ ϵ(1 -2η b ) = s.</formula><p>The algorithm is poor if some ϵ-bad rule happens to fail less often than all acceptable rules. Consider such an ϵ-bad rule, e i , and let e t be a correct rule. Let F i and F t be their respective failure statistics. By Hoeffding's inequality above, the probability of the first of these is at most,</p><formula xml:id="formula_11">LE(η + s, m, η + s - s 2 ) ≤ e -2(s/2) 2 m ≤ δ 2N</formula><p>Similarly, the latter is</p><formula xml:id="formula_12">GE(η, m, η + s 2 ) ≤ δ 2N</formula><p>Thus, the probability that any ϵ-bad rule e i fails less than e t is at most δ N . Since there are fewer than N bad rules, the probability that one of them minimizes the number of failures by the algorithm is less than δ. The theorem below addresses a more generalizable learning model when instances are not noise-free. For further details, see  </p><formula xml:id="formula_13">10) R(h S ) ≤ min h∈H R(h)| + ϵ</formula><p>The proof is as follows. For every h ∈ H,</p><formula xml:id="formula_14">(11) R(h S ) ≤ R(h s ) + ϵ 2 ≤ R(h) + ϵ 2 ≤ R(h) + ϵ 2 + ϵ 2 = R(h) + ϵ</formula><p>The second equality holds because h s is a predictor that reduces empirical risk. And the assumption that the S is ϵ 2 representatives ensures that the third and first equalities hold.</p><p>The formal definition of the uniform convergence property is as follows. A hypothesis class H has the uniform convergence property regarding its domain and loss function if there exists a function m H : (0, 1) 2 → N such that for every ϵ, δ ∈ (0, 1) and for every probability distribution D over domain X, if S is a sample of m &gt; m H examples drawn according to D, then with probability of at least 1 -δ, S is ϵrepresentative. The function m H is equivalent to the sample complexity, the minimal necessary sample size of obtaining the uniform convergence property. Again, the term uniform refers to having a fixed sample size that works for all members of H and over all possible probability distributions over the domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Estimated VC dimension for risk bounds</head><p>McDonald, Shalizi, and Schervish (2011) propose a simulation-based method to estimate the VC dimension, which measures the generalization capacity of learning algorithms.</p><p>They prove two main results: first, that the estimated VC dimension will concentrate around the true dimension with high probability, and second, that using the estimated VC dimension allows for the recovery of accurate bounds on generalization error.</p><p>Building on <ref type="bibr" target="#b46">Vapnik, Levin, and Le Cun (1994)</ref>, which shows that the expected maximum deviation between the empirical risks of a classifier on two datasets can be bounded by a function that depends only on the VC dimension of the classifier, the authors provide the following function of n and parametrized by H:</p><formula xml:id="formula_15">VC(F) =          1 n &lt; h 2 a l og 2n h +1 h h -a ′′ ( 1 + a ′ ( n h -a ′′ ) l og 2n h +1</formula><p>+ 1) else Following <ref type="bibr" target="#b46">Vapnik, Levin, and Le Cun (1994)</ref> ,the constants were chosen as follows: a = 0.16, a ′ = 1.2 and a ′′ = 0.14927 so that ϕ (.5) = 1. These values are tuned to be optimal for linear discriminant classifiers, for which the VCD is known theoretically. A key assumption is therefore that the same values can be used for the chosen algorithm without introducing bias.</p><p>As we have imperfect knowledge, we generate many observations</p><formula xml:id="formula_16">ξ(n) = Φ h (n) + ϵ(n)</formula><p>along a fine grid of design points n. Here ϵ is centered on mean zero as the bound is tight, having an unknown distribution with support on [0,1]. We then estimate the true VC dimension h * using nonlinear least squares. Of course, generating ξ(n l ) is nontrivial. <ref type="bibr" target="#b46">Vapnik, Levin, and Le Cun (1994)</ref> provides an algorithm for generating the appropriate observations. At each fixed design point n l : l ∈ {1, ..., k}, we simulate m data points for i = 1,....,m, so as to approximate ξ(n l ) as defined. <ref type="bibr" target="#b46">Vapnik, Levin, and Le Cun (1994)</ref> shows that this approach works well in practice, recovering the known VC dimension of linear classifiers and demonstrates that the method for generating the dataset does not affect the algorithm's performance, since for any data structure it is sufficient to flip labels to ensure the most inaccurate possible algorithm is trained.</p><p>Below is the procedure for generating ξ(n l ), discussed in <ref type="bibr" target="#b46">Vapnik, Levin, and Le Cun (1994)</ref>, which we apply here. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm</head><formula xml:id="formula_17">6. Set ξ(n l ) = | Rn l ( f , W ) -Rn l ( f , W ′ )|. 7. Set ξ(n l ) = 1 m m i=1 ξ(n l )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Proofs of Distribution-free Property and Learnability</head><p>This section presents proofs that establish the necessary and sufficient conditions underpinning the distribution-free assumption of the PAC model <ref type="bibr" target="#b46">(Vapnik, Levin, and Le Cun 1994;</ref><ref type="bibr" target="#b47">Vapnik and Chervonenkis 2015)</ref>. We use the mathematical notations in learning theory and proofs as in ?.</p><p>We define the concept class as a nonempty set C ⊆ 2 X . X is a fixed set, either finite or countably infinite, of binary indicators which can be mapped into Euclidean n-dimensional space for n ≥ 1. We also assume that each c ∈ C is a Borel set. The sample space of C, denoted by S C is the set of all m-samples over all c ∈ C and all x ∈ X m for all m ≥ 1.</p><p>A C,H indicates the set of all functions A that maps S C into H, where H is a set of Borel Sets on X. H is the hypothesis space. Elements in H are named hypotheses. We allow A to approximate concepts in C using hypotheses from a different class H. A is consistent if its hypothesis always agrees with the sample, whenever h = A then for all i, 1 ≤ i ≤ m, i = I h (x i ). For any A, probability distribution P on X, c ∈ C, and x ∈ X, the error of A for concept C on x is given by P(c △ h) DEFINITION 4. Given a nonempty concepts class C ⊆ 2 X and a set of points S ⊆ X, Π denotes the set of all subsets of S that can be obtained by intersecting S with a concept in C. If Π = 2 s , then we say S is shattered by C. The VC dimension of C is the cardinality of the largest finite set of points S ⊆ X that is shattered by C. If arbitrarily large finite sets are shattered, the VC dimension of C is infinite.</p><p>The empty set is always shattered, therefore, we also derive the following definition. DEFINITION 5. For any integer m ≥ 0, Π indicates the max over all S ⊆ X of cardinality m.</p><p>Based on this definition, the VC dimension of C can be defined as the largest integer d such that Π(d) = 2 d or infinity. We now present the summary of theories and provide the relevant proofs from <ref type="bibr" target="#b7">Blumer et al. (1989)</ref>. THEOREM 4. Let C be a concept class, then C is uniformly learnable if and only if the VC dimension of C is finite.</p><p>Proof: <ref type="bibr" target="#b7">Blumer et al. (1989)</ref> shows that the "if" part of the theorem follows as we always produce a consistent function that maps from S C to C by simply well ordering the concept in C and choosing for each sample in S C the first concept that is consistent with the sample. Likewise, the "only if" part of the theorem above holds as the lower bound grows arbitrarily large with d for the appropriate choice of ϵ and δ. Detailed proofs are provided on page 936 of <ref type="bibr" target="#b7">Blumer et al. (1989)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Nonuniform learnability</head><p>We start with the definition of uniform convergence, as provided by Lemma C1. It naturally follows that for every sample complexity and δ, with a probability of at least 1 -δ, we have the following equation. Equally weighting hypothesis classes is equivalent to empirical risk manipulation over a weighted superclass of hypothesis classes. If researchers have prior knowledge that a particular hypothesis is more likely to contain correct target concepts, then they should assign a larger weighting. However, uniform weighting does not work if H is an infinite union of hypothesis classes.</p><p>Therefore, researchers should rely on another framework to minimize the structural risks. This is a bound minimization approach, meaning that the goal of the framework is to figure out the hypothesis that minimizes a certain upper bound on the true risk.</p><p>The bound that this framework hopes to minimize is provided in the following theorem. Then, we can derive the following theorem.</p><p>THEOREM 5. For every δ ∈ (0, 1), and distribution D, with a probability of at least 1 -δ, the following bound holds for every n ∈ M and h ∈ H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>|R(h) -RS (h)| &lt; ϵ(w(n), δ)</head><p>Therefore, for every δ ∈ (0, 1) and distribution D, with a probability of least 1 -δ, the following equation holds.</p><p>(14) ∀h ∈ H, R(h) ≤ RS (h) + ϵ(w(n), δ)</p><p>Proof: For each n, we define δ n being equal to w(n)δ. Relying on the assumption that uniform convergence holds for all n, we obtain that if we fix n in advance, then with a probability of at least 1 -δ n over researchers' choices. Then, applying the union bound over n = 1, 2, ...., the preceding holds for all n, with a probability of at least 1n δ n = 1 -δ n w(n) ≥ 1 -δ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Application to Predicting Ideology (Lewis et al. 2024)</head><p>We further examine the validity of the proposed methods using the data provided by <ref type="bibr" target="#b33">Lewis et al. (2024)</ref>. <ref type="bibr" target="#b40">Poole and Daniels (1985)</ref> and <ref type="bibr" target="#b39">Poole (2005)</ref> developed basic data and measurements related to the United States Congress, using roll-call voting data. In this exercise, we examine the generalizable inference of roll-call votes on NOMINATE scores.</p><p>To reduce simulation time, we focus on roll call votes since the 114th Congress.The model is trained with a VC dimension of 3. Since the VC dimension can be analytically calculated, we do not need to simulate the VCD. to rates of roll-call votes that might not match party ideology. While it could be as high as approximately 40% according to <ref type="bibr" target="#b32">Lesniewski and Kelly (2024)</ref>, we use 0.05 for δ for easy comparison with our first application from <ref type="bibr" target="#b13">Dressel and Farid (2018)</ref>. The simulation shows that the best achievable accuracy is 1 when ϵ is 0.01, and the additional benefits of a sample size above 500 appear trivial. This application also conveys similar insights from the previous application, indicating that the advantages of big data might diminish for generalization problems using simple models. Intuitively, we prefer smaller trees over larger trees; to formalize this intuition, we first need to define a description language for decision trees, which is prefix-free and requires fewer bits for smaller decision trees. In principle, this could simply be plain English descriptions of the trees. One possible approach we can take that provides greater precision is as follows: A tree with n nodes will be described in n+1 blocks, each of size l og 2 (d + 3) bits. The first n blocks encode the nodes of the tree, in a depth-first order, and the last block marks the end of the code. Each block indicates whether the current node is:</p><p>• An internal node of the form 1 x i =1 for some i ∈ [d]</p><p>• A leaf whose value is 1</p><p>• A leaf whose value is 0</p><p>• End of the node Overall, there are d+3 options, hence we need l og 2 (d + 3) bits to describe each block. Assuming each node has two children, without loss of generality,<ref type="foot" target="#foot_10">3</ref> we can show that this is a prefix-free encoding of the tree, and the length of a tree with n nodes is (n+1)l og 2 (d+3).</p><p>By Theorem 6, we have that, with a probability of at least 1 -δ over a sample of size m, for every n and every decision tree h ∈ H with n nodes it holds that (16) L D (h) -L S (h) ≤ (n + 1)l og 2 (d + 3) + l og(2/δ) 2m</p><p>where the left-hand side again indicates the gap between true and empirical risks.</p><p>This bound reflects the key tradeoff: we expect more complex and larger decision trees to have a smaller training risk, L s (h), but the respective value of n will be larger.</p><p>However, smaller decision trees will have a smaller value of n, but L s (h) might be larger.</p><p>Researchers are hoping to find a decision tree with both low empirical risk, L s (h) and a number of nodes n not too high. The bounds suggests that such a tree will have low true risk, L D (h), allowing us to overcome the problem of non-finite VC dimension due to overfitting.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>describe such labeled examples S as training examples or training sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FIGURE 1 .</head><label>1</label><figDesc>FIGURE 1. Schematic Illustration of Learning Polyarchies from PAC Learning</figDesc><graphic coords="11,79.20,211.33,453.60,185.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>DEFINITION 3 .</head><label>3</label><figDesc>ϵ-representative: A training set S is ϵ-representative for domain X, hypothesis class H, and distribution D if</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>i.d) according to an unknown D. Given that there is an unknown concept c which determines the true label of instances, the set of labeled instances S = {(x 1 , y 1 ), . . . , (x m , y m )} is generated by taking x i ∼ D i.i.d and observing the corresponding y i = c(x i ). Suppose we have a model that produces a hypothesis h ∈ H, given a sample of N training examples. The algorithm is called consistent if for every ϵ and δ, there exists a positive number of training examples N ∈ N such that for any distribution p * , it holds that |R(h) -RS (h)| ≤ ϵ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>FIGURE 2 .</head><label>2</label><figDesc>FIGURE 2. Simulation Study for Estimating the VC dimension of a linear discriminant model, k = 7</figDesc><graphic coords="21,192.60,442.67,226.80,185.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>FIGURE 3 .</head><label>3</label><figDesc>FIGURE 3. Learning to Classify Polyarchies</figDesc><graphic coords="23,101.88,394.86,408.23,185.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>FIGURE 4 .</head><label>4</label><figDesc>FIGURE 4. Simulation Analysis When ϵ = 0.05 (Left) &amp; ϵ = 0.35 (Right)</figDesc><graphic coords="26,79.20,371.07,238.97,248.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>and m is any positive integer then LE( p, m, ps) ≤ e -2s 2 m and GE( p, m, p + s) ≤ e -2s 2 m</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc><ref type="bibr" target="#b43">Simon (1993)</ref>'s formalized proofs, particularly Corollary 3.13. THEOREM 3. Let C be a class of concepts and VC(C) ≥ 2 its Vapnik-Chveronenkis dimension. Any algorithm that learns C with regard to classification noise rate η needs Ω(VC(C)  ϵ(1-2η) 2 ) observations.Proof The sample for f is defined as the sample of a p-concept f η , wheref η (x) = η if f(x)=0, and f η (x) = 1 -η otherwise. Let C η = { f η | f ∈ C}. Let d = d(C)and S be the sequence of size d which is shattered by C. S is γ-shattered by C η for 2γ = 1 -2η. Let A be an algorithm which learns C under classification noise rate η. The domain distribution D can be chosen as in the proof of Theorem 3.1. If A's output h is a hypothesis for target concept f whose error is bounded by ϵ, then H is an (ϵ, 0)-good model for f η on S. C. Uniform Convergence and ϵrepresentative sample LEMMA C1. Assume that a training set S is ϵ 2 -representative for the domain, loss function, and distribution D. Then, any hypothesis h s ∈ argmin h∈H : L s (h) satisfies the following condition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>(</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>generating ξ(n l ) Given a collection of possible classifier F and a grid of design points, repeat the procedure at each design point, n l , m times 1. Generate a dataset from the same space y × X as the training sample that is independent of the training sample. The generated set should be of size 2n l . 2. Split the data set into two equal sets W and W ′ 3. Flip the labels (y values) of W ′ 4. Merge the two sets and train the classifier simultaneously on the entire set: W with the "correct" labels and W ′ with the "wrong" labels. 5. Calculate the training error of the estimated classifier f on W with the correct labels and on W ′ with the wrong labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>(12) ∀h ∈ H, |R(h) -RS (h)| &lt; ϵ(δ) Let w be a function such that ∞ n=1 w(n) ≤ 1. w indicates a weight function over the hypothesis classes and it reflects the importance of each hypothesis class or the relative complexity of different hypothesis classes. If H is a finite union of N hypotheses classes, researchers can simply assign the same weight of 1 N to all hypothesis classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Formally, let there</head><label></label><figDesc>be a weighting function, such that ∞ n=1 w(n) ≤ 1. Let H be a hypothesis class which is an infinite union of n hypotheses where each hypothesis satisfies the uniform convergence property with a sample complexity function. Let ϵ n indicate the lowest possible upper bound on the gap between empirical and true risks achievable by using a sample of m examples. (13) ϵ n = min ϵ ∈ (0, 1) : m(ϵ, δ) &lt; m</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure G. 1</head><label>1</label><figDesc>Figure G.1 shows the results of the simulation on the observed data using δ, η = 0.05, and values of 0.01 (left panel) and 0.4 (right panel) for ϵ. In this case, η could correspond</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>FIGURE G. 1 .</head><label>1</label><figDesc>FIGURE G.1. Simulation Analysis When ϵ = 0.01 (Left) &amp; ϵ = 0.4 (Right)</figDesc><graphic coords="54,85.49,79.20,213.20,131.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2 .</head><label>2</label><figDesc>Designs for Applying Sample Complexity Bounds</figDesc><table><row><cell>Type of Analysis</cell><cell>Input Parameters</cell><cell>Output</cell></row><row><cell>1. A priori</cell><cell>Accuracy (ϵ), Confidence (δ), Noisy rate (η)</cell><cell>N</cell></row><row><cell>2. Post-hoc</cell><cell>Confidence, Noisy rate, N</cell><cell>Accuracy</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>PAC learning assumes that labeled instances are coming from a fixed but unknown distribution and that there is an unknown concept belonging to a known class that truly labels the instances. We will provide further details in the section on PAC learning.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>In fact,<ref type="bibr" target="#b25">Hopkins and King (2010)</ref> presents the average root mean square error by varying sample sizes, concluding that "more than about</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="500" xml:id="foot_2"><p>documents to estimate a specific quantity of interest is probably unnecessary," due to inefficiency.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>For example, there have been discussions among scholars in machine learning theory about whether</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>Although mismatch between measurement and concept is a potentially serious concern in applied research, we will abstract away from this issue by assuming that the algorithm being used is capable of perfectly learning the target concept given infinite correctly-labeled data, so that we can substitute the unknown VC(F) with the known VC(A) without loss.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>In the preceding discussion, we applied the prior knowledge that the rectangle is anchored at one corner, reducing the VC dimension to 2. Here, we apply the more general bound from the perspective of a researcher who knows the class to which the true concept belongs, but has no further information about its location in feature space.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6"><p>See<ref type="bibr" target="#b28">Kearns (1990)</ref> for further discussion and proofs.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7"><p>"Shattering" is a key notion in machine learning that refers to a classifier's capacity to accurately distinguish any arbitrary labeling of a group of points.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_8"><p>Specifically, we hold the sample size of the test set, on which the experiment is conducted, constant.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_9"><p>, all of the information in the example is obliterated. When it is above one half, the algorithm would perform better by swapping all observed labels.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_10"><p>If a decision node has only one child, we can still replace the node by its child without affecting the prediction of the decision tree.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>The sample complexity is the minimum value of N for which the equation ( 6) holds true.</p><p>We therefore seek to determine the minimum sample size that produces a hypothesis within a specified error tolerance of the true concept with high probability. This is conceptually similar to the widely used approach of power analysis in experimental research and, as we show, has direct implications for power when researchers seek to identify the causal effect of a latent concept.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>tion to the importance of both data quantity and quality in applying statistical learning to social sciences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Conflict of Interest</head><p>The authors are not aware of any conflicts of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supporting Information for</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning from Noise: Applying Sample Complexity for</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Political Science Research</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Perry Carter Dahyun Choi</head><p>July 2024 * Ph.D. Candidate, Department of Politics, Princeton University.Email: pjcarter@princeton.edu † Ph.D. Candidate, Department of Politics, Princeton University. Email: dahyunc@princeton.edu</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. When Learning is Noise-Free</head><p>While measurement models are typically optimized for accuracy on observed labeled instances, the ability to make generalizable claims beyond the training data is the ultimate goal of statistical learning. Although sample splitting is now widely used to address concerns of overfitting, it cannot substitute for having a sufficiently large dataset to begin with.</p><p>We formalize the problem as follows <ref type="bibr" target="#b31">(Laird 2012</ref>): denote domain X, label sets Y, and concept classes C. X includes all possible instances that the researcher may want to label and the set Y includes all possible labels or predictions for a single instance. An instance-label pair (x, y) ∈ X × Y is called a labeled instance and a concept is a function c : X → Y. We assume that there is an unknown concept 1 C that determines the true labels of instances.</p><p>Following the notation in the paper, we consider a probability distribution D over X.</p><p>We assume that the instances we observe are independent and identically distributed (i.i.d) according to an unknown D. Given that there is an unknown concept C which determines the true label of instances, the set of labeled instances S = {(x 1 , y 1 ), . . . , (x m , y m )} is generated by taking x i ∼ D i.i.d and observing the corresponding y i = c(x i ). Then the true error and empirical error can be defined as follows </p><p>1 A concept in this sense is precisely understood as a binary classification rule. This is not as divergent from common social science usage as it may initially appear: for instance, by the concept of "democracy", we mean a set of explicit rules that allow an observer to determine whether a given political system is or is not a democracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. When Learning is Not Noise-Free</head><p>Here we provide further elaboration on Section 4.1 concerning the sample complexity bounds for applied research. This section provides a more detailed overview of the setup in <ref type="bibr" target="#b31">Laird (2012)</ref> and <ref type="bibr" target="#b43">Simon (1993)</ref> on which our approach is based. Suppose we have a rule e with error p. For example, the rule fails or disagrees with an example on average pm times in m examples. With the addition of noise, it may fail more often or less. Then the expected failure rate p η is as follows:</p><p>(7)</p><p>The first indicates that if no classification errors occurs with probability 1 -η, the probability of failure is p. The second term suggests that if a classification error does not strike with probability η, the rule will fail only if it would not have failed without the error, with probability 1 -p. Note the cases below.</p><p>• When p =0 (zero error), its failure rate increases to η with noise.</p><p>• When p ≥ ϵ, its failure rate is at least η + ϵ(1 -2η); and since (1 -2η) &gt; 0, this failure rate is greater than that of any correct one with zero error. 2</p><p>We refer to rules with error greater than ϵ as ϵ-bad. Rules that are not ϵ-bad are ϵ-good. Rules with zero error are described simply as good. On average, ϵ-bad rules have a failure rate that is greater than that of good rules by at least ϵ(1 -2η).</p><p>By the Law of Large Numbers, as m → ∞</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Sample Complexity for Decision Trees</head><p>A decision tree predicts the label associated with an instance x by traveling from the root node of a tree to a leaf. While decision trees can be applied in a variety of settings, including multiclass problems, we focus on the binary classification setting, namely y ∈ {0, 1}, for simplicity. At each node on the root-to-leaf path, the successor is decided by the basis of splitting the input space using either one of the features or a predefined set of splitting rules.</p><p>In this case, we focus on one of the most widely used rules: thresholding the value of a single feature. We move between the right or left child of the node on the basis of 1 χ i &lt;θ , where χ i is an indicator for relevant features and θ ∈ R indicates the threshold.</p><p>Then intuitively, a tree with k leaves can shatter a set of k instances. Therefore, if we assume decision trees of arbitrary size, we obtain a class of infinite VC dimension which can easily lead to overfitting and -most concerningly for our application -an infinite sample complexity bound.</p><p>To avoid this, we can rely on the minimum description length (MDL) principle where hypotheses with shorter descriptions are preferred, following the principle of Occam's razor. While this section does not go over the details of how MDL principle is constructed, the intuition is quite straightforward. We aim to learn a decision tree that fits the data while not being excessively complex.</p><p>We focus on the following scenario to explore how nonuniform learnability can be applied to tree models. For simplicity, we assume that χ ∈ {0, 1} d . In that case, thresholding the value of a single feature corresponds to a splitting rule. This notational simplification is without loss of generality and the following analysis can be applied to general cases.</p><p>With the simplifying assumption above, the classifier's dimension becomes finite, though it can still be very large. For example, any classifier from {0, 1} d to {0, 1} can be</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Democracy does cause growth</title>
		<author>
			<persName><forename type="first">Daron</forename><surname>Acemoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suresh</forename><surname>Naidu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascual</forename><surname>Restrepo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">A</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of political economy</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="100" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The role of hyperparameters in machine learning models and how to tune them</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luka</forename><surname>Biedebach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Küpfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcel</forename><surname>Neunhoeffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political Science Research and Methods</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the sample complexity of noise-tolerant learning</title>
		<author>
			<persName><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Decatur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing Letters</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="189" to="195" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An unexpected consensus among diverse ways to measure democracy</title>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Baltz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabricio</forename><surname>Vasselai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allen</forename><surname>Hicken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Democratization</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="814" to="837" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Can nonexperts really emulate statistical learning methods? a comment on &quot;the accuracy, fairness, and limits of predicting recidivism</title>
		<author>
			<persName><forename type="first">Kirk</forename><surname>Bansak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political Analysis</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="370" to="380" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automated text classification of news articles: a practical guide</title>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Barberá</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amber</forename><forename type="middle">E</forename><surname>Boydstun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suzanna</forename><surname>Linn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Mcmahon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Nagler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political Analysis</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="42" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Crowd-sourced text analysis: reproducible and agile production of political data</title>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Benoit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Drew</forename><surname>Conway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><forename type="middle">E</forename><surname>Lauderdale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Laver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slava</forename><surname>Mikhaylov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Political Science Review</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="278" to="295" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learnability and the vapnik-chervonenkis dimension</title>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Blumer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrzej</forename><surname>Ehrenfeucht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Haussler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manfred</forename><forename type="middle">K</forename><surname>Warmuth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="929" to="965" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A pairwise comparison framework for fast, flexible, and reliable human coding of political texts</title>
		<author>
			<persName><forename type="first">David</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><forename type="middle">M</forename><surname>Montgomery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Political Science Review</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="835" to="843" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Conceptual &quot;stretching&quot; revisited: adapting categories in analysis</title>
		<author>
			<persName><forename type="first">David</forename><surname>Collier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">E</forename><surname>Mahon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Political Science Review</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="845" to="855" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Avoid cohen&apos;s &apos;small&apos;,&apos;medium&apos;, and &apos;large&apos;for power analysis</title>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Correll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Mellinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><forename type="middle">H</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">M</forename><surname>Judd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="200" to="207" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Measuring the competitiveness of elections</title>
		<author>
			<persName><forename type="first">Gary</forename><forename type="middle">W</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><forename type="middle">H</forename><surname>Fiva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political Analysis</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="168" to="185" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Polyarchy: participation and opposition</title>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">A</forename><surname>Dahl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Yale university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The accuracy, fairness, and limits of predicting recidivism</title>
		<author>
			<persName><forename type="first">Julia</forename><surname>Dressel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hany</forename><surname>Farid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science advances</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5580</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Using imperfect surrogates for downstream inference: design-based supervised learning for social science applications of large language models</title>
		<author>
			<persName><forename type="first">Naoki</forename><surname>Egami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Musashi</forename><surname>Hinck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanying</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Assessing fit quality and testing for misspecification in binary-dependent variable models</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Esarey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Pierce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political Analysis</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="480" to="500" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Enhancing validity in observational settings when replication is not possible</title>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Fariss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political Science Research and Methods</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="365" to="380" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Machine learning predictions as regression covariates</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Tyler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political Analysis</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="484" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">We are all social scientists now: how big data, machine learning, and causal inference work together</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Grimmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PS: Political Science &amp; Politics</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="80" to="83" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Text as data: the promise and pitfalls of automatic content analysis methods for political texts</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Grimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><forename type="middle">M</forename><surname>Stewart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political analysis</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="267" to="297" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The impression of influence: legislator communication, representation, and democratic accountability</title>
		<author>
			<persName><forename type="first">Sean</forename><forename type="middle">J</forename><surname>Grimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Solomon</forename><surname>Westwood</surname></persName>
		</author>
		<author>
			<persName><surname>Messing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Princeton University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rate of false conviction of criminal defendants who are sentenced to death</title>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barbara</forename><forename type="middle">O</forename><surname>'brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">H</forename><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="7230" to="7235" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Kernel regularized least squares: reducing misspecification bias with a flexible and interpretable machine learning approach</title>
		<author>
			<persName><forename type="first">Jens</forename><surname>Hainmueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chad</forename><surname>Hazlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political Analysis</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="143" to="168" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The optimal sample complexity of pac learning</title>
		<author>
			<persName><forename type="first">Steve</forename><surname>Hanneke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">38</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Measuring transparency</title>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">R</forename><surname>Hollyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Peter Rosendorff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">Raymond</forename><surname>Vreeland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political analysis</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="413" to="434" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A method of automated nonparametric content analysis for social science</title>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">J</forename><surname>Hopkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Political Science</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="229" to="247" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An improved method of automated nonparametric content analysis for social science</title>
		<author>
			<persName><forename type="first">Connor</forename><forename type="middle">T</forename><surname>Jerzak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Strezhnev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political Analysis</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="42" to="58" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A cross-national measure of electoral competitiveness</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Kayser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">René</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName><surname>Lindstädt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political Analysis</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="242" to="253" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">The computational complexity of machine learning</title>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Kearns</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Testing causal theories with learned proxies</title>
		<author>
			<persName><forename type="first">Dean</forename><surname>Knox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wendy K Tam</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Political Science</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="419" to="441" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Big omicron and big omega and big theta</title>
		<author>
			<persName><forename type="first">Donald</forename><forename type="middle">E</forename><surname>Knuth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Sigact News</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="18" to="24" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning from good and data</title>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">D</forename><surname>Laird</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">47</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">House gop had lowest win rate on &quot;party unity&quot; votes since 1982</title>
		<author>
			<persName><forename type="first">Niels</forename><surname>Lesniewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kelly</surname></persName>
		</author>
		<ptr target="https://rollcall.com/2024/02/08/house-gop-had-lowest-win-rate-on-party-unity-votes-since-1982/" />
		<imprint>
			<date type="published" when="2024-02">2024. February</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Voteview: congressional roll-call votes database</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Howard</forename><surname>Rosenthal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Boche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Rudkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Sonnet</surname></persName>
		</author>
		<ptr target="https://voteview.com/" />
	</analytic>
	<monogr>
		<title level="m">Voteview: congressional roll-call votes database</title>
		<imprint>
			<date type="published" when="2018">2019. 27 July 2018. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Subjective and objective measurement of democratic backsliding</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anne</forename><surname>Meng</surname></persName>
		</author>
		<idno>SSRN 4327307</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">J</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cosma</forename><surname>Rohilla Shalizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Schervish</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1111.3404</idno>
		<title level="m">Estimated vc dimension for risk bounds</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Active learning approaches for labeling text</title>
		<author>
			<persName><forename type="first">Blake</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fridolin</forename><surname>Linder</surname></persName>
		</author>
		<author>
			<persName><surname>Walter R Mebane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political Analysis</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Model complexity for supervised learning: why simple models almost always work best, and why it matters for applied research</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Morucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Spirling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">Working Paper</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">How cross-validation can go wrong and what to do about it</title>
		<author>
			<persName><forename type="first">Marcel</forename><surname>Neunhoeffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Sternberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political Analysis</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="101" to="106" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Spatial models of parliamentary voting</title>
		<author>
			<persName><forename type="first">Keith</forename><forename type="middle">T</forename><surname>Poole</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Ideology, party, and voting in the us congress, 1959-1980</title>
		<author>
			<persName><forename type="first">Keith</forename><forename type="middle">T</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Daniels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Political Science Review</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="373" to="399" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">How to analyze political attention with minimal assumptions and costs</title>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">M</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Burt L Monroe</surname></persName>
		</author>
		<author>
			<persName><surname>Colaresi</surname></persName>
		</author>
		<author>
			<persName><surname>Michael H Crespin</surname></persName>
		</author>
		<author>
			<persName><surname>Dragomir R Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Political Science</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="209" to="228" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Understanding machine learning: from theory to algorithms</title>
		<author>
			<persName><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Shai</surname></persName>
		</author>
		<author>
			<persName><surname>Ben-David</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">General bounds on the number of examples needed for learning probabilistic concepts</title>
		<author>
			<persName><forename type="first">Hans</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><surname>Ulrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixth annual conference on computational learning theory</title>
		<meeting>the sixth annual conference on computational learning theory</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="402" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Max-margin majority voting for learning from crowds</title>
		<author>
			<persName><forename type="first">Tian</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A theory of the learnable</title>
		<author>
			<persName><forename type="first">Leslie</forename><forename type="middle">G</forename><surname>Valiant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1134" to="1142" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Measuring the vc-dimension of a learning machine</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esther</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Le Cun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="851" to="876" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">On the uniform convergence of relative frequencies of events to their probabilities</title>
		<author>
			<persName><forename type="first">Vladimir</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chervonenkis</forename><surname>Ya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Measures of complexity: festschrift for alexey chervonenkis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="11" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Learning and generalisation: with applications to neural networks</title>
		<author>
			<persName><forename type="first">Mathukumalli</forename><surname>Vidyasagar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Methods for correcting inference based on outcomes predicted by machine learning</title>
		<author>
			<persName><forename type="first">Siruo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><forename type="middle">H</forename><surname>Mccormick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">T</forename><surname>Leek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">48</biblScope>
			<biblScope unit="page" from="30266" to="30275" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Topics, concepts, and measurement: a crowdsourced procedure for validating topics as measures</title>
		<author>
			<persName><forename type="first">Luwei</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><forename type="middle">M</forename><surname>Montgomery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><forename type="middle">M</forename><surname>Stewart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political Analysis</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="570" to="589" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
