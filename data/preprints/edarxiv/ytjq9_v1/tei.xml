<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Evaluation of Methods for Assessing Model Fit for Bayesian Diagnostic Classification Models</title>
				<funder ref="#_qeSXA9P">
					<orgName type="full">Institute of Education Sciences, U.S. Department of Education</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">W</forename><forename type="middle">Jake</forename><surname>Thompson</surname></persName>
							<email>jakethompson@ku.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Accessible Teaching</orgName>
								<orgName type="laboratory" key="lab2">Learning, and Assessment Systems (ATLAS)</orgName>
								<orgName type="laboratory" key="lab3">Accessible Teaching</orgName>
								<orgName type="laboratory" key="lab4">Learning, and Assessment Systems (ATLAS)</orgName>
								<orgName type="institution" key="instit1">University of Kansas</orgName>
								<orgName type="institution" key="instit2">University of Kansas</orgName>
								<address>
									<addrLine>1122 West Campus Road</addrLine>
									<postCode>66045-3101</postCode>
									<settlement>Lawrence</settlement>
									<region>KS</region>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Accessible Teaching</orgName>
								<orgName type="laboratory" key="lab2">Learning, and Assessment Systems (ATLAS)</orgName>
								<orgName type="laboratory" key="lab3">Accessible Teaching</orgName>
								<orgName type="laboratory" key="lab4">Learning, and Assessment Systems (ATLAS)</orgName>
								<orgName type="institution" key="instit1">University of Kansas</orgName>
								<orgName type="institution" key="instit2">University of Kansas</orgName>
								<address>
									<addrLine>1122 West Campus Road</addrLine>
									<postCode>66045-3101</postCode>
									<settlement>Lawrence</settlement>
									<region>KS</region>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Accessible Teaching</orgName>
								<orgName type="laboratory" key="lab2">Learning, and Assessment Systems (ATLAS)</orgName>
								<orgName type="laboratory" key="lab3">Accessible Teaching</orgName>
								<orgName type="laboratory" key="lab4">Learning, and Assessment Systems (ATLAS)</orgName>
								<orgName type="institution" key="instit1">University of Kansas</orgName>
								<orgName type="institution" key="instit2">University of Kansas</orgName>
								<address>
									<addrLine>1122 West Campus Road</addrLine>
									<postCode>66045-3101</postCode>
									<settlement>Lawrence</settlement>
									<region>KS</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Evaluation of Methods for Assessing Model Fit for Bayesian Diagnostic Classification Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A4E172E89F426ADE9B3882808DA504C9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-23T06:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>diagnostic assessment</term>
					<term>model fit</term>
					<term>classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Diagnostic classification models (DCMs) are psychometric models that can be used to estimate the presence or absence of psychological traits, or proficiency on fine-grained skills.</p><p>Critical the use of any psychometric model in practice, including DCMs, is an evaluation of model fit. Traditionally, DCMs have been estimated with maximum likelihood methods and then evaluated with limited-information fit indices. However, recently, methodological and technological advancements have made Bayesian methods for estimating DCMs more accessible. When using a Bayesian estimation process, new methods for model evaluation are available to assess model fit. In the current study, we conduct a simulation study to compare the performance of the traditional measures of model fit to Bayesian methods. The results indicate that Bayesian measures of model fit generally outperform the more traditional limitedinformation indices. Notably, flags for model misfit were more likely to be true positives when using Bayesian methods. Additionally, Bayesian methods for model comparisons also showed better performance than has been reported for methods traditionally in conjunction with a maximum likelihood estimation. In summary, the findings suggest that Bayesian methods offer a better evaluation of model fit than more commonly used metrics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>An Evaluation of Methods for Assessing Model Fit for Bayesian Diagnostic Classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>Diagnostic classification models (DCMs; also known as cognitive diagnostic models <ref type="bibr">[CDMs]</ref>) are a class of psychometric models where the latent traits are a set of fine-grained, discrete variables <ref type="bibr" target="#b4">(Bradshaw, 2016;</ref><ref type="bibr" target="#b8">de la Torre &amp; Sorrel, 2023;</ref><ref type="bibr" target="#b31">Rupp et al., 2010)</ref>. That is, rather than modeling a continuous scale score, DCMs estimate a respondent's proficiency or nonproficiency on a set of predefined skills. These estimates result in a profile of skills on which each respondent is proficient. These profiles offer stakeholders fine-grained results that are often more useful than an overall scale score. For example, in educational assessment, DCMs provide results that are more instructionally relevant <ref type="bibr" target="#b22">(Lim et al., 2024;</ref><ref type="bibr" target="#b46">Thompson &amp; Clark, 2024</ref>; S. <ref type="bibr" target="#b51">Zhang et al., 2023)</ref>. Similarly, applying DCMs to psychological assessment allows researchers to directly model the presence or absence of a trait, rather than attempting to apply a cut point to a continuous scale (R. <ref type="bibr" target="#b23">Liu &amp; Shi, 2020;</ref><ref type="bibr" target="#b50">J. Zhang et al., 2024)</ref>.</p><p>As with all psychometric models, an evaluation of model fit is critical for users to have confidence in the inferences drawn from a DCM. Although many methods have been proposed for evaluating model fit for DCMs, most are constrained by the way DCMs are typically estimated. In practice, DCMs are most often estimated using a maximum likelihood procedure (e.g., <ref type="bibr" target="#b42">Templin &amp; Hoffman, 2013)</ref>, as this is the only process historically offered by available software <ref type="bibr" target="#b11">(George et al., 2016;</ref><ref type="bibr" target="#b25">Ma &amp; de la Torre, 2020)</ref>. The lack of accessible alternatives to maximum likelihood estimation has in turn limited the ways in which model fit can be evaluated. However, recent software advances have made Bayesian estimation of DCMs more accessible to practitioners (e.g., <ref type="bibr" target="#b39">Templin, 2023;</ref><ref type="bibr">Thompson, 2023a)</ref>, widening the scope of possible model-fit evaluations.</p><p>In this study, we examine the efficacy of model-fit indices that are available when a Bayesian estimation process is implemented. We first provide a high-level overview of DCMs and existing model-fit indices. We then conduct a simulation study to evaluate the performance of Bayesian model-fit indices and discuss the implications for the practice of diagnostic modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Diagnostic Classification Models</head><p>DCMs are confirmatory latent class models, in which each class represents a particular profile of skill proficiency. Although the latent skills, known as attributes, can be polytomous, they are most often binary. For this discussion, we limit ourselves to binary attributes modeled with dichotomous item responses. Using binary attributes, the number of classes is 2 A , where A is the number of attributes measured by the assessment. For example, an assessment measuring three attributes would have 2 3 = 8 classes:</p><formula xml:id="formula_0">[0,0,0], [1,0,0], [0,1,0], [0,0,1], [1,1,0], [1,0,1], [0,1,1],</formula><p>and [1,1,1], where 1 indicates the presence of or proficiency on the given attribute and 0 indicates absence or nonproficiency.</p><p>In addition to the class definitions, we must also provide a Q-matrix <ref type="bibr" target="#b38">(Tatsuoka, 1983)</ref> that defines which attributes are measured by each item. The Q-matrix has one row per item and one column per attribute. Each cell of the Q-matrix is either a 0 (the item does not measure the attribute) or 1 (the item does measure the attribute). Given the class definitions, the probability of respondent r providing a correct response is defined as</p><formula xml:id="formula_1">𝑃(X 𝑟 = 𝑥 𝑟 ) = ∑ 𝜈 𝑐 𝐶 𝑐=1 ∏ 𝜋 𝑖𝑐 𝑥 𝑖𝑟 𝐼 𝑖=1 (1 -𝜋 𝑖𝑐 ) 1-𝑥 𝑖𝑟 (1)</formula><p>where C is the number of classes, I is the number of items, and xir is the observed item response from respondent r on item i. For the parameters, πic is the probability of a respondent in class c providing a correct response to item i and νc is a mixing parameter that defines the base rate of membership in each class.</p><p>The definition of the π parameter is determined by the particular DCM subtype that is estimated. The choice of a DCM model defines certain assumptions about how attributes interact with each other on items that measure multiple attributes. For example, the deterministic-input, noisy "and" gate (DINA) model assumes that respondents should be proficient on all attributes measured by an item in order to provide a correct response (de la <ref type="bibr" target="#b7">Torre &amp; Douglas, 2004;</ref><ref type="bibr" target="#b20">Junker &amp; Sijtsma, 2001)</ref>. In contrast, the deterministic-input, noisy "or" gate (DINO) model assumes that respondents should provide a correct response if they are proficient on any of the attributes measured by the item <ref type="bibr" target="#b41">(Templin &amp; Henson, 2006)</ref>.</p><p>In addition to models like the DINA and DINO that make strict assumptions about attribute interactions, there are general models that make fewer assumptions and subsume the more-restrictive models. One popular general DCM is the log-linear cognitive diagnostic model (LCDM), which parameterizes π similar to log-linear models <ref type="bibr" target="#b14">(Henson et al., 2009;</ref><ref type="bibr" target="#b13">Henson &amp; Templin, 2019)</ref>. Consider an item measuring two attributes. Conditional on the attribute profile for class c, αc = [α1, α2], the LCDM would define</p><formula xml:id="formula_2">logit(𝑃(𝑋 𝑖𝑐 = 1|𝛼 𝑐 )) = 𝜆 𝑖,0 + 𝜆 𝑖,1,(1) 𝛼 1 + 𝜆 𝑖,1,(2) 𝛼 2 + 𝜆 𝑖,2,(1,2) 𝛼 1 𝛼 2 (2)</formula><p>where λi,0 is an intercept and represents the log odds of providing a correct response to item i when neither attribute is present. We then have two main effects, λi,1,(1) and λi,1,(2), which represent the increase in the log odds when the first or second attribute is present, respectively.</p><p>Finally, an interaction term, λi,2,(1,2), represents the change in the log odds when both attributes are present.</p><p>By constraining the λ parameters in Equation 2, we can achieve models statistically equivalent to more-restrictive models such as the DINA and DINO models <ref type="bibr" target="#b31">(Rupp et al., 2010)</ref>. If both main effects are constrained to 0, the model is equivalent to the DINA model. That is, a respondent is proficient on either none (λi,0) or both (λi,2,(1,2)) attributes, and there is no increase in probability for being proficient on a subset of the required attributes. The DINO model can be achieved by constraining the two main effects to be equal (i.e., λi,1) and constraining the interaction to be -1 × λi,1. With these constraints, the increase in log odds will be equal to λi,1 regardless of whether one or both attributes is present (i.e., the presence of any attribute is sufficient to provide a correct response).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Fit for DCMs</head><p>After choosing and estimating a DCM, one must evaluate the model's performance. In general, model fit can be evaluated in two ways. Measures of absolute fit describe how well an estimated model represents the observed data. Measures of relative fit directly compare the fit of two or more competing models. We will discuss different methods assessing absolute and relative fit for DCMs in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Absolute Fit</head><p>For DCMs estimated with a maximum likelihood process, the most common model-fit indices are so called limited-information indices <ref type="bibr" target="#b26">(Maydeu-Olivares &amp; Joe, 2005</ref><ref type="bibr">, 2006)</ref>. The most widely used of these indices is the M2 statistic, which was originally developed for multidimensional item response theory models <ref type="bibr" target="#b26">(Maydeu-Olivares &amp; Joe, 2005)</ref> and later adapted for DCMs <ref type="bibr" target="#b12">(Hansen et al., 2016;</ref><ref type="bibr" target="#b24">Y. Liu et al., 2016)</ref>. Limited-information indices are required because of the sparse data tables created by categorical response data. For example, an assessment with 10 dichotomous items has 2 10 = 1,024 possible response patterns. With any reasonable sample size, it is unlikely we would observe enough respondents at each response pattern to accurately and reliably compare the number of respondents the model expects at each response pattern to the observed number, and this problem become more pronounced as the number of items increases. We are therefore limited to lower-order summaries of the contingency tables. For example, the aforementioned M2 statistic uses the first-and second-order marginal probabilities. Thus, these limited-information indices cannot capture higher-order aspects of the data.</p><p>When a Bayesian estimation process is used, we have options beyond sparse tables that necessitate the use limited-information indices. Rather, we can utilize posterior predictive model checks (PPMCs). Whereas maximum likelihood methods result in a point estimate for each parameter, Bayesian methods provide a posterior distribution of plausible values for each parameter. When using PPMCs, we simulate new data sets from the joint posterior distribution and then compare the simulated data sets to our observed data <ref type="bibr" target="#b32">(Schad et al., 2021)</ref>. The key decision, then, is to choose the features of the data in the simulated and observed data sets that we want to compare. For example, <ref type="bibr" target="#b36">Sinharay et al. (2006)</ref> and <ref type="bibr" target="#b35">Sinharay and Almond (2007)</ref> have used PPMCs to evaluate item-level fit for item response theory models and DCMs, respectively.</p><p>In the present study, we are interested in overall evaluations of model fit. Both <ref type="bibr" target="#b28">Park et al. (2015)</ref> and <ref type="bibr" target="#b43">Thompson (2019)</ref> describe a PPMC for the raw-score distribution of an assessment.</p><p>Using the simulated data sets, we can calculate the expected number of respondents at each raw score point. We then can compare the counts from each individual data set to the expected count, creating a posterior distribution of a χ 2 -like statistic. Finally, we can compare the counts of respondents at each score point in our observed data, calculate the χ 2 -like statistic, and compare our observed value to the posterior distribution. The posterior distribution represents the plausible values of the statistic if our estimated model were correct. If our observed value is outside of the posterior distribution (e.g., outside the middle 95% of the distribution), this indicates model misfit. This comparison is often summarized as the posterior predictive p-value (ppp), which is the proportion of the posterior distribution that is more extreme than our observed value.</p><p>The raw-score PPMC offers several theoretical advantages over limited-information methods. First, the raw-score distribution accounts for item dependencies that are excluded when looking only at first-and second-order probabilities, as in the M2. Second, the joint posterior used to simulate the replicated data sets includes the estimated uncertainty in each of the parameters. Therefore, the summary statistics calculated for the PPMC reflect the plausible values given the uncertainty in the parameter estimates, rather than relying on point estimates from a maximum likelihood estimation. Third, because we are calculating an empirical distribution for the PPMC and comparing the observed value to prespecified quantiles of the distribution, we do not have to depend on asymptotic assumptions that may or may not be met.</p><p>Thus, the raw-score PPMC χ 2 offers many potential benefits over methods that are more widely used. However, it should be noted that a Bayesian estimation does not preclude the calculation of limited-information indices. That is, when using a maximum likelihood estimation, we cannot calculate PPMCs, but when using a Bayesian estimation, we can calculate both PPMCs and limited-information indices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relative Fit</head><p>Relative-fit indices are used to compare competing models. These indices do not provide information on whether or not a model adequately fits the observed data. Rather, they compare how well one model fits relative to another model. Therefore, relative-fit indices primarily useful after evaluating absolute fit <ref type="bibr" target="#b34">(Sen &amp; Bradshaw, 2017)</ref>.</p><p>When using a maximum likelihood estimation, there are well documented methods for comparing competing models, such as the Akaike Information Criterion (AIC; <ref type="bibr" target="#b0">Akaike, 1973)</ref> and the Bayesian Information Criterion (BIC; <ref type="bibr" target="#b33">Schwarz, 1978)</ref>. These indices purport to estimate the predictive accuracy of models and can thus be used for comparing models to determine which model would offer better predictions, with some penalty for model complexity. Although commonly used, both the AIC and BIC have significant drawbacks when using a Bayesian estimation, making their use inappropriate. As noted by <ref type="bibr" target="#b15">Hollenbach and Montgomery (2020)</ref>, the AIC assumes that we have not placed priors on the parameters, which is common practice for Bayesian models. The AIC also assumes that the posterior is multivariate normal, which is not the case when using categorical classes. Additionally, the AIC has been shown to be unreliable when the data have a nested structure, such as items within respondents <ref type="bibr" target="#b9">(Gelman et al., 2014)</ref>.</p><p>The BIC has similar weaknesses. The BIC has been shown to be inaccurate when using nonuniform prior distributions <ref type="bibr" target="#b3">(Berger et al., 2003)</ref>. <ref type="bibr" target="#b15">Hollenbach and Montgomery (2020)</ref> and <ref type="bibr" target="#b10">Gelman and Rubin (1995)</ref> went so far as to recommend avoiding the BIC altogether for Bayesian models, as, despite its name, the BIC cannot actually approximate any exact Bayesian solution for predictive accuracy and is undefined if specific prior distributions are not selected. Therefore, when using Bayesian estimation, we must turn to other information criteria for comparing models, namely, leave-one-out cross validation (LOO), as described by <ref type="bibr" target="#b48">Vehtari et al. (2017)</ref>. A complete description of the LOO is beyond the scope of this paper, and we direct readers to <ref type="bibr" target="#b48">Vehtari et al. (2017)</ref> for details. In short, the LOO uses the posterior density to estimate out-of-sample predictive fit for a model, known as the expected log predictive density (ELPD). Then, just as with other information criteria such as the AIC and BIC, we can compare the ELPD for competing models. The model with the largest value is the preferred model (i.e., expected to have the highest predictive accuracy). As implemented in the loo package <ref type="bibr" target="#b49">(Vehtari et al., 2023)</ref>, we can also estimate the standard error of the difference. A difference between two models that is much larger than the standard error of the difference (e.g., 2.5; <ref type="bibr" target="#b2">Bengio &amp; Grandvalet, 2004)</ref> indicates a meaningful difference in the LOO estimates between the models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Current Study</head><p>Previous work has compared the efficacy of absolute <ref type="bibr" target="#b16">(Hu et al., 2016)</ref> and relative <ref type="bibr" target="#b21">(Lei &amp; Li, 2016;</ref><ref type="bibr" target="#b34">Sen &amp; Bradshaw, 2017)</ref> fit measures for DCMs. However, these studies were limited to model-fit indices that are possible when using maximum likelihood estimation. No research to date has compared the performance of Bayesian measures of absolute model fit to the maximum likelihood-based methods. Further, no study has yet examined the use of the LOO for DCMs, as all studies on relative fit have focused on the AIC, BIC, and other similar metrics. In this study, we conducted a simulation to evaluate how well Bayesian methods of model-fit performance compared to their maximum likelihood-based counterparts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>To evaluate the performance of Bayesian absolute-and relative-fit indices for DCMs, we conducted a simulation study. In this study, we manipulated the number of assessed attributes (two or three), the minimum number of items measuring each attribute (five or seven), the sample size (500 or 1,000). These factors were chosen to represent test designs that are commonly seen in applied research (e.g., <ref type="bibr" target="#b5">Bradshaw et al., 2014;</ref><ref type="bibr" target="#b42">Templin &amp; Hoffman, 2013)</ref>.</p><p>Using a full factorial design, these factors resulted in a total of eight test-design conditions.</p><p>Within each test-design condition, we also manipulated the data-generating model (LCDM or DINA) and the estimated model (LCDM or DINA) to evaluate the performance of model-fit metrics when the estimated model should and should not fit the data. With fully crossed datagenerating and estimating models, there are four modeling conditions within each condition, resulting in 32 total conditions across all test designs. We conducted 50 replications per condition.</p><p>The simulation and subsequent analyses were conducted in R version 4.3.3 (R Core Team, 2024). All DCMs were estimated using Stan (version 2.32.2; <ref type="bibr" target="#b6">Carpenter et al., 2017)</ref> via the measr package <ref type="bibr">(Thompson, 2023a</ref><ref type="bibr" target="#b45">(Thompson, , 2023b))</ref>, and replications were conducted on AWS EC2 instances using the portableParallelSeeds package (P. E. <ref type="bibr" target="#b18">Johnson, 2024)</ref>. All R code for the simulation and subsequent analyses is available in a public OSF project repository.<ref type="foot" target="#foot_0">foot_0</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Generation</head><p>The data generation followed the design of the data simulations used by M. S. <ref type="bibr" target="#b17">Johnson and Sinharay (2018)</ref> and <ref type="bibr" target="#b47">Thompson et al. (2023)</ref> in their evaluations of reliability indices for DCMs. In this study, the number of respondents was determined by the simulation condition.</p><p>The true attribute profile for each respondent was determined by a random draw from all possible profiles. Additionally, each simulated assessment measured two or three attributes, with each attribute measured by at least five or seven items. The total number of items for each simulated assessment is therefore the product of the number of attributes and the minimum number of items for each attribute. In the simulation, the Q-matrix for each simulated assessment was specified so that the first three items measuring each attribute were single-attribute items.</p><p>The remaining two or four items for each attribute (for the five-item and seven-item conditions, respectively) had a 50% chance of also measuring a second attribute.</p><p>Item-parameter generation depended on the data-generating model. In conditions where data were generated from the LCDM, item parameters included item intercepts, main effects, and interactions, all of which are on the log-odds scale. Item intercepts were drawn from a uniform distribution ranging from -3.0 to 0.6, and main effects were drawn from a uniform distribution ranging from 1.0 to 5.0. In the LCDM, interaction terms are constrained to be greater than -1 times the smallest main effect to ensure monotonicity of the model. Thus, the interaction parameters were drawn from a uniform distribution ranging from the calculated lower bound to 2.0. In conditions where data were generated from the DINA model, item parameters include the slipping and guessing parameters, which are both on the probability scale. For consistency with the simulation of LCDM data, parameters were generated on the log-odds scale and converted to probability values. Guessing parameters were drawn from a uniform distribution ranging from -3.0 to 0.6, consistent with the LCDM intercepts. The final guessing parameters were then calculated as the inverse logit of the generated parameter. Slipping parameters were generated from a uniform distribution ranging from 1.0 to 5.0, consistent with the main effects in the LCDM. Because the slipping parameter represents the probability of not providing a correct response when a respondent is proficient on the measured attributes, the final slipping parameter was calculated as 1 minus the inverse logit of the generated parameter value.</p><p>The generated attribute profiles, Q-matrix, and item parameters were then used to simulate a data set for each replication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Simulation Process and Analysis</head><p>After the data were generated, both an LCDM and a DINA model were estimated on the simulated data set. We then calculated indices of absolute fit (i.e., M2 and raw-score PPMC χ 2 ) and relative fit (i.e., LOO) for each model. All of these indices were calculated for both estimated models to evaluate how they performed under different conditions of known model specifications. When data were generated from the LCDM, we expected the LCDM to show adequate model fit and be the preferred model, as the DINA model was underspecified. On the other hand, when data were generated from the DINA model, we expected both models to show adequate absolute fit, as the LCDM subsumes the DINA model. However, because the LCDM was overspecified in this condition, we expected the DINA model to be preferred by the relativefit indices.</p><p>For each absolute-fit index, an estimated model was flagged for misfit if the p-value or the ppp was less than .05 for the M2 and PPMC χ 2 , respectively. We then used the flags to calculate the positive and negative predictive values <ref type="bibr" target="#b1">(Altman &amp; Bland, 1994;</ref><ref type="bibr" target="#b37">Smith, 2012)</ref>. The positive predictive value (PPV) is the proportion of positive results that are true positives, that is, the proportion of models where the fit index indicated poor model fit where we expected it.</p><p>Similarly, the negative predictive value (NPV) is the proportion of models in which the fit index indicated adequate model fit where we expected it.</p><p>Finally, for relative fit, we determined the preferred model by calculating the difference between the LOOs for each of the estimated models, as well as the standard error of the difference. Using the criteria suggested by <ref type="bibr" target="#b2">Bengio and Grandvalet (2004)</ref>, when the difference between the criterion for the LCDM and the DINA model was greater than 2.5 times the standard error of the difference, we determined the preferred model to be the model with the lowest index value. When the difference was less than 2.5 times the standard error, we determined the models to be equally fitting and therefore selected the more parsimonious model (i.e., the DINA model) as the preferred model. We then calculated the proportion of replications within each condition in which the LOO selected the correct model (i.e., the model that was used to generate the data).</p><p>The expected model-fit results for each combination of generating and estimating models are shown in Table <ref type="table" target="#tab_0">1</ref>. Note. LCDM = log-linear cognitive diagnostic model; DINA = diagnostic input, noisy "and" gate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Absolute Model Fit</head><p>Across all conditions, the M2 statistic had a PPV of .753 and an NPV of .964. In contrast, the PPMC χ 2 had a PPV of .919 and an NPV of .952. The NPVs indicate that negative test values for both metrics (i.e., a nonsignificant result) were usually true negatives. On the other hand, the PPVs indicate that positive test results (i.e., a significant result that indicates model misfit) were a true positive only 75% of the time for the M2 statistic, compared to 92% of the time for the PPMC χ 2 statistic.</p><p>When looking at the PPVs and NPVs by test-design condition, as shown in Figure <ref type="figure" target="#fig_0">1</ref>, we see that the NPVs for each metric are similar for all test designs. The condition-specific results are consistent with the overall results, which showed similar NPVs for the M2 and the PPMC χ 2 .</p><p>However, the PPVs for the M2 are consistently lower than the PPVs for the PPMC χ 2 . This difference becomes more pronounced as the data set becomes larger (i.e., larger samples, more attributes). Thus, as the sample gets larger and the test design gets more complex, the M2 becomes more likely to result in a false positive, indicating model misfit when there is in fact none. On the other hand, the PPMC χ 2 demonstrated consistently high PPVs across all simulation conditions. Positive and Negative Predictive Values, by Test-Design Condition Note. PPMC = Posterior predictive model check.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relative Model Fit</head><p>As previously discussed, evaluations of relative fit are only meaningful only when the competing models have been found to have adequate absolute model fit. Accordingly, for the MODEL FIT FOR BAYESIAN DCMS 17 relative-fit results, we filtered the simulation output to include only include replications in which both the LCDM and the DINA model showed adequate absolute model fit. Using the absolute-fit findings in the previous section, we used the PPMC χ 2 statistic to determine absolute fit. Table <ref type="table" target="#tab_1">2</ref> shows the number of replications by test-design condition and data-generating model in which both estimated models demonstrated adequate absolute fit. As expected, both the estimated DINA model and the LCDM often had adequate absolute fit when data were generated from the DINA model, as the LCDM subsumes the DINA model. In contrast, it was much less likely that both models would show adequate absolute fit when data were generated from the LCDM. Note. LCDM = log-linear cognitive diagnostic model; DINA = diagnostic input, noisy "and"</p><p>gate.</p><p>Across all conditions, the LOO determined the correct model in 82% of replications.</p><p>Figure <ref type="figure" target="#fig_1">2</ref> shows the percentage of replications in which the LOO selected the correct model by test-design condition and data-generating model. Across all test-design conditions, when the LCDM was used to generate the data and both the LCDM and the DINA model showed adequate absolute fit, the LOO always selected the correct model (i.e., the LCDM). On the other hand, when the DINA model was used to generate the data, the LOO selected the LCDM as the preferred model in up to 34% of replications (the three-attribute, seven-item, 1,000-sample-size condition). Thus, even when the DINA model was used to generate the data and the estimated DINA model showed adequate model fit, the LOO still preferred the more-complex model in some situations. However, even with a slight preference for the more-complex model, the LOO still identified the correct model in more than 65% of replications in all conditions. Correct Model Selections, by Test-Design Condition Note. LCDM = log-linear cognitive diagnostic model; DINA = diagnostic input, noisy "and"</p><p>gate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>In this study, we examined the performance absolute and relative model-fit indices for Bayesian DCMs. Overall, the findings support the use of Bayesian estimation for DCMs to facilitate the use of Bayesian methods for model evaluation.</p><p>For evaluating absolute model fit, we examined the M2 and PPMC χ 2 statistics. Across all conditions, the M2 statistic performed well, with results consistent with previous research evaluating the efficacy of the method (e.g., Y. <ref type="bibr" target="#b24">Liu et al., 2016)</ref>. However, the PPMC χ 2 statistic showed comparable or improved performance in all conditions. This improvement was particularly true when examining the PPVs for each statistic (i.e., the probability that a flag actually indicates model misfit). Although both the M2 and PPMC χ 2 had similar negative predictive values, the PPMC χ 2 had consistently higher positive predictive values. Thus, when using the PPMC χ 2 , practitioners can be more confident that a positive test result truly indicates model misfit.</p><p>Bayesian methods offer different methods for evaluating relative model fit than are appropriate when using a maximum likelihood estimation. Whereas indices such as the AIC and BIC can be used when models are estimated using maximum likelihood estimation, the LOO is used for Bayesian models. In this study, the LOO showed good performance, selecting the correct model in 82% of replications. In contrast, the AIC and BIC have been found to identify the correct model in as few as 30% of replications <ref type="bibr" target="#b34">(Sen &amp; Bradshaw, 2017)</ref>. The performance of the LOO in this study compared to reported performance of the AIC and BIC means that using a Bayesian estimation process to access the LOO for model comparisons offers a marked improvement over methods that are used with a maximum likelihood estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations and Future Directions</head><p>There are some limitations to the present study. For example, we investigated a relatively limited set of test designs. Future research should confirm the efficacy of the PPMC χ 2 and LOO under more-complex designs (e.g., more attributes, more-complex item structures). Additionally, the present study assumed that a DCM was always desired, and therefore model comparisons focused on choosing between different DCM subtypes. Future research could also consider how the LOO behaves when practitioners are comparing different psychometric models. For example, we may compare a Bayesian DCM to an item response theory model that was also estimated with a Bayesian procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>Model fit is a crucial evaluation of model performance for any psychometric model, including DCMs. It is important not only to evaluate model fit, but also to ensure that we are using the best methods possible for the evaluation. The present study offers evidence that the PPMC χ 2 and LOO, which can only be utilized when a Bayesian estimation is used, offer improvements over existing maximum likelihood measures of model fit. By using improved methods for model evaluation, we can have greater confidence that the inferences of respondent proficiency we draw from DCMs are valid indications of the respondents' knowledge and skills.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1</head><label>1</label><figDesc>Figure 1</figDesc><graphic coords="16,72.00,152.39,468.00,468.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2</head><label>2</label><figDesc>Figure 2</figDesc><graphic coords="19,72.00,152.39,467.96,289.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Expected Model-Fit Results</figDesc><table><row><cell>Generating model</cell><cell cols="2">Estimated model Absolute-fit flag</cell><cell>Relative-fit preference</cell></row><row><cell></cell><cell>DINA</cell><cell>No</cell></row><row><cell>DINA</cell><cell></cell><cell></cell><cell>DINA</cell></row><row><cell></cell><cell>LCDM</cell><cell>No</cell></row><row><cell></cell><cell>DINA</cell><cell>Yes</cell></row><row><cell>LCDM</cell><cell></cell><cell></cell><cell>LCDM</cell></row><row><cell></cell><cell>LCDM</cell><cell>No</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Number of Replications in Which Both Models Demonstrated Absolute Fit</figDesc><table><row><cell></cell><cell>Test designs</cell><cell></cell><cell cols="2">Data-generating model</cell></row><row><cell>Attributes</cell><cell>Items</cell><cell>Sample size</cell><cell>DINA</cell><cell>LCDM</cell></row><row><cell>2</cell><cell>5</cell><cell>500</cell><cell>49</cell><cell>17</cell></row><row><cell>2</cell><cell>5</cell><cell>1,000</cell><cell>43</cell><cell>10</cell></row><row><cell>2</cell><cell>7</cell><cell>500</cell><cell>48</cell><cell>9</cell></row><row><cell>2</cell><cell>7</cell><cell>1,000</cell><cell>47</cell><cell>1</cell></row><row><cell>3</cell><cell>5</cell><cell>500</cell><cell>47</cell><cell>0</cell></row><row><cell>3</cell><cell>5</cell><cell>1,000</cell><cell>43</cell><cell>0</cell></row><row><cell>3</cell><cell>7</cell><cell>500</cell><cell>49</cell><cell>21</cell></row><row><cell>3</cell><cell>7</cell><cell>1,000</cell><cell>47</cell><cell>1</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The project repository can be found at https://osf.io/t5v96/.</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>supported by the <rs type="funder">Institute of Education Sciences, U.S. Department of Education</rs>, through Grant <rs type="grantNumber">R305D210045</rs> to the <rs type="institution">University of Kansas</rs>. The opinions expressed are those of the authors and do not represent the views of the Institute or the <rs type="institution">U.S. Department of Education</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_qeSXA9P">
					<idno type="grant-number">R305D210045</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Information theory and an extension of the maximum likelihood principle</title>
		<author>
			<persName><forename type="first">H</forename><surname>Akaike</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Symposium on Information Theory</title>
		<editor>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Petrov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Csáki</surname></persName>
		</editor>
		<meeting>the Second International Symposium on Information Theory</meeting>
		<imprint>
			<publisher>Akadémiai Kiadó</publisher>
			<date type="published" when="1973">1973</date>
			<biblScope unit="page" from="267" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Diagnostic tests 2: Predictive values</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Bland</surname></persName>
		</author>
		<idno type="DOI">10.1136/bmj.309.6947.102</idno>
		<ptr target="https://doi.org/10.1136/bmj.309.6947.102" />
	</analytic>
	<monogr>
		<title level="j">British Medical Journal</title>
		<imprint>
			<biblScope unit="volume">309</biblScope>
			<biblScope unit="page">102</biblScope>
			<date type="published" when="1994">1994. 6947</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">No unbiased estimator of the variance of k-fold crossvalidation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Grandvalet</surname></persName>
		</author>
		<ptr target="http://www.jmlr.org/papers/v5/grandvalet04a.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1089" to="1105" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Approximations and consistency of Bayes factors as model dimension grows</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0378-3758(02)00336-1</idno>
		<ptr target="https://doi.org/10.1016/S0378-3758(02)00336-1" />
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Planning and Inference</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="241" to="258" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Diagnostic classification models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bradshaw</surname></persName>
		</author>
		<idno type="DOI">10.1002/9781118956588.ch13</idno>
		<ptr target="https://doi.org/10.1002/9781118956588.ch13" />
	</analytic>
	<monogr>
		<title level="m">The handbook of cognition and assessment: Frameworks, methodologies, and applications</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Rupp</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Leighton</surname></persName>
		</editor>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="297" to="327" />
		</imprint>
	</monogr>
	<note>1st ed.</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Diagnosing teachers&apos; understandings of rational numbers: Building a multidimensional test within the diagnostic classification framework</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bradshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Izsák</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Templin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Jacobson</surname></persName>
		</author>
		<idno type="DOI">10.1111/emip.12020</idno>
		<ptr target="https://doi.org/10.1111/emip.12020" />
	</analytic>
	<monogr>
		<title level="j">Educational Measurement: Issues and Practice</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2" to="14" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Stan: A probabilistic programming language</title>
		<author>
			<persName><forename type="first">B</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Goodrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Betancourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brubaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Riddell</surname></persName>
		</author>
		<idno type="DOI">10.18637/jss.v076.i01</idno>
		<ptr target="https://doi.org/10.18637/jss.v076.i01" />
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="32" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Higher-order latent trait models for cognitive diagnosis</title>
		<author>
			<persName><forename type="first">J</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Douglas</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF02295640</idno>
		<ptr target="https://doi.org/10.1007/BF02295640" />
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="333" to="353" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cognitive diagnosis models</title>
		<author>
			<persName><forename type="first">J</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Sorrel</surname></persName>
		</author>
		<idno type="DOI">10.1017/9781108902724.010</idno>
		<ptr target="https://doi.org/10.1017/9781108902724.010" />
	</analytic>
	<monogr>
		<title level="m">New handbook of mathematical psychology</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">N</forename><surname>Dzhafarov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><forename type="middle">G</forename><surname>Ashby</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Colonius</surname></persName>
		</editor>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="385" to="420" />
		</imprint>
	</monogr>
	<note>Perceptual and cognitive processes</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Understanding predictive information criteria for Bayesian models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vehtari</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11222-013-9416-2</idno>
		<ptr target="https://doi.org/10.1007/s11222-013-9416-2" />
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="997" to="1016" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Avoiding model selection in Bayesian social research</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
		<idno type="DOI">10.2307/271064</idno>
		<ptr target="https://doi.org/10.2307/271064" />
	</analytic>
	<monogr>
		<title level="j">Sociological Methodology</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="165" to="173" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The R package CDM for cognitive diagnosis models</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Robitzsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kiefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Groß</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ünlü</surname></persName>
		</author>
		<idno type="DOI">10.18637/jss.v074.i02</idno>
		<ptr target="https://doi.org/10.18637/jss.v074.i02" />
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Limited-information goodness-of-fit testing of diagnostic classification item response models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1111/bmsp.12074</idno>
		<ptr target="https://doi.org/10.1111/bmsp.12074" />
	</analytic>
	<monogr>
		<title level="j">British Journal of Mathematical and Statistical Psychology</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="225" to="252" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Loglinear cognitive diagnostic model (LCDM)</title>
		<author>
			<persName><forename type="first">R</forename><surname>Henson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Templin</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-05584-4_8</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-05584-4_8" />
	</analytic>
	<monogr>
		<title level="m">Handbook of diagnostic classification models</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Davier</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">&amp; Y.-S</forename><surname>Lee</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="171" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Defining a family of cognitive diagnosis models using log-linear models with latent variables</title>
		<author>
			<persName><forename type="first">R</forename><surname>Henson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Templin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Willse</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11336-008-9089-5</idno>
		<ptr target="https://doi.org/10.1007/s11336-008-9089-5" />
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="191" to="210" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bayesian model selection, model comparison, and model averaging</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Hollenbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Montgomery</surname></persName>
		</author>
		<idno type="DOI">10.4135/9781526486387</idno>
		<ptr target="https://doi.org/10.4135/9781526486387" />
	</analytic>
	<monogr>
		<title level="m">The SAGE handbook of research methods in political science and international relations</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Curini</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Franzese</surname></persName>
		</editor>
		<imprint>
			<publisher>SAGE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="937" to="960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Evaluation of model fit in cognitive diagnosis models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Huggins-Manley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1080/15305058.2015.1133627</idno>
		<ptr target="https://doi.org/10.1080/15305058.2015.1133627" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Testing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="119" to="141" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Measures of agreement to assess attribute-level classification accuracy and consistency for cognitive diagnostic assessments</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sinharay</surname></persName>
		</author>
		<idno type="DOI">10.1111/jedm.12196</idno>
		<ptr target="https://doi.org/10.1111/jedm.12196" />
	</analytic>
	<monogr>
		<title level="j">Journal of Educational Measurement</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="635" to="664" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">portableParallelSeeds: Allow replication of simulations on parallel and serial computers (R package version 0.97)</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="DOI">10.32614/CRAN.package.portableParallelSeeds</idno>
		<ptr target="https://doi.org/10.32614/CRAN.package" />
	</analytic>
	<monogr>
		<title level="m">The Comprehensive R Archive Network</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">Computer software</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<idno type="DOI">10.32614/CRAN.package.portableParallelSeeds</idno>
		<imprint/>
	</monogr>
	<note>portableParallelSeeds</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cognitive assessment models with few assumptions, and connections with nonparametric item response theory</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Junker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sijtsma</surname></persName>
		</author>
		<idno type="DOI">10.1177/01466210122032064</idno>
		<ptr target="https://doi.org/10.1177/01466210122032064" />
	</analytic>
	<monogr>
		<title level="j">Applied Psychological Measurement</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="258" to="272" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Performance of fit indices in choosing correct cognitive diagnostic models and Q-matrices</title>
		<author>
			<persName><forename type="first">P.-W</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1177/0146621616647954</idno>
		<ptr target="https://doi.org/10.1177/0146621616647954" />
	</analytic>
	<monogr>
		<title level="j">Applied Psychological Measurement</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="405" to="417" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An exploration of cognitive diagnosis in medical education: Constructing comprehensive feedback for enhanced student learning</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Willey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bangeranye</surname></persName>
		</author>
		<idno type="DOI">10.1007/s40670-024-02064-2</idno>
		<ptr target="https://doi.org/10.1007/s40670-024-02064-2" />
	</analytic>
	<monogr>
		<title level="j">Medical Science Educator. Advance online</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Using diagnostic classification models in psychological rating scales</title>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shi</surname></persName>
		</author>
		<idno type="DOI">10.20982/tqmp.16.5.p442</idno>
		<ptr target="https://doi.org/10.20982/tqmp.16.5" />
	</analytic>
	<monogr>
		<title level="j">The Quantitative Methods for Psychology</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="442" to="456" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An application of M2 statistic to evaluate the fit of cognitive diagnostic models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xin</surname></persName>
		</author>
		<idno type="DOI">10.3102/1076998615621293</idno>
		<ptr target="https://doi.org/10.3102/1076998615621293" />
	</analytic>
	<monogr>
		<title level="j">Journal of Educational and Behavioral Statistics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="26" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">GDINA: An R package for cognitive diagnosis modeling</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>De La Torre</surname></persName>
		</author>
		<idno type="DOI">10.18637/jss.v093.i14</idno>
		<ptr target="https://doi.org/10.18637/jss.v093.i14" />
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Limited-and full-information estimation and goodnessof-fit testing in 2 n contingency tables: A unified framework</title>
		<author>
			<persName><forename type="first">A</forename><surname>Maydeu-Olivares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Joe</surname></persName>
		</author>
		<idno type="DOI">10.1198/016214504000002069</idno>
		<ptr target="https://doi.org/10.1198/016214504000002069" />
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">471</biblScope>
			<biblScope unit="page" from="1009" to="1020" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Limited information goodness-of-fit testing in multidimensional contingency tables</title>
		<author>
			<persName><forename type="first">A</forename><surname>Maydeu-Olivares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Joe</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11336-005-1295-9</idno>
		<ptr target="https://doi.org/10.1007/s11336-005-1295-9" />
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="713" to="732" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Posterior predictive model checks for cognitive diagnostic models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-S</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1504/IJQRE.2015.071738</idno>
		<ptr target="https://doi.org/10.1504/IJQRE.2015.071738" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Quantitative Research in Education</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="244" to="264" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">R: A language and environment for statistical computing</title>
		<author>
			<persName><forename type="first">Team</forename><surname>Core</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Version</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<ptr target="https://www.R-project.org/" />
		<title level="m">R Foundation for Statistical Computing</title>
		<imprint/>
		<respStmt>
			<orgName>Computer software</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Diagnostic measurement: Theory, methods, and applications</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Templin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Henson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Guilford Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Toward a principled Bayesian workflow in cognitive science</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Schad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Betancourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vasishth</surname></persName>
		</author>
		<idno type="DOI">10.1037/met0000275</idno>
		<ptr target="https://doi.org/10.1037/met0000275" />
	</analytic>
	<monogr>
		<title level="j">Psychological Methods</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="103" to="126" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Estimating the dimension of a model</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Schwarz</surname></persName>
		</author>
		<idno type="DOI">10.1214/aos/1176344136</idno>
		<ptr target="https://doi.org/10.1214/aos/1176344136" />
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="461" to="464" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Comparison of relative fit indices for diagnostic model selection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bradshaw</surname></persName>
		</author>
		<idno type="DOI">10.1177/0146621617695521</idno>
		<ptr target="https://doi.org/10.1177/0146621617695521" />
	</analytic>
	<monogr>
		<title level="j">Applied Psychological Measurement</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="422" to="438" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Assessing fit of cognitive diagnostic models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sinharay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Almond</surname></persName>
		</author>
		<idno type="DOI">10.1177/0013164406292025</idno>
		<ptr target="https://doi.org/10.1177/0013164406292025" />
	</analytic>
	<monogr>
		<title level="j">Educational and Psychological Measurement</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="239" to="257" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Posterior predictive assessment of item response theory models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sinharay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Stern</surname></persName>
		</author>
		<idno type="DOI">10.1177/0146621605285517</idno>
		<ptr target="https://doi.org/10.1177/0146621605285517" />
	</analytic>
	<monogr>
		<title level="j">Applied Psychological Measurement</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="298" to="321" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Diagnostic tests (2)positive and negative predictive values</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.1258/phleb.2012.012J06</idno>
		<ptr target="https://doi.org/10.1258/phleb.2012.012J06" />
	</analytic>
	<monogr>
		<title level="j">Phlebology</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="305" to="306" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Rule space: An approach for dealing with misconceptions based on item response theory</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Tatsuoka</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1745-3984.1983.tb00212.x</idno>
		<ptr target="https://doi.org/10.1111/j.1745-3984.1983.tb00212.x" />
	</analytic>
	<monogr>
		<title level="j">Journal of Educational Measurement</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="345" to="354" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">blatent: Bayesian latent variable models (R package version 0.1.2)</title>
		<author>
			<persName><forename type="first">J</forename><surname>Templin</surname></persName>
		</author>
		<idno type="DOI">10.32614/CRAN.package.blatent</idno>
		<ptr target="https://doi.org/10.32614/CRAN.package" />
	</analytic>
	<monogr>
		<title level="m">The Comprehensive R Archive Network</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">Computer software</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<idno type="DOI">10.32614/CRAN.package.blatent</idno>
		<imprint/>
		<respStmt>
			<orgName>blatent</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Measurement of psychological disorders using cognitive diagnosis models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Templin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Henson</surname></persName>
		</author>
		<idno type="DOI">10.1037/1082-989X.11.3.287</idno>
		<ptr target="https://doi.org/10.1037/1082-989X.11.3.287" />
	</analytic>
	<monogr>
		<title level="j">Psychological Methods</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="287" to="305" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Obtaining diagnostic classification model estimates using mplus</title>
		<author>
			<persName><forename type="first">J</forename><surname>Templin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hoffman</surname></persName>
		</author>
		<idno type="DOI">10.1111/emip.12010</idno>
		<ptr target="https://doi.org/10.1111/emip.12010" />
	</analytic>
	<monogr>
		<title level="j">Educational Measurement: Issues and Practice</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="37" to="50" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Bayesian psychometrics for diagnostic assessments: A proof of concept</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Thompson</surname></persName>
		</author>
		<idno type="DOI">10.35542/osf.io/jzqs8</idno>
		<ptr target="https://doi.org/10.35542/osf.io/jzqs8" />
	</analytic>
	<monogr>
		<title level="j">Research Report</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
		<respStmt>
			<orgName>University of Kansas</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">measr: Bayesian psychometric measurement using Stan</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Thompson</surname></persName>
		</author>
		<idno type="DOI">10.21105/joss.05742</idno>
		<ptr target="https://doi.org/10.21105/joss.05742" />
	</analytic>
	<monogr>
		<title level="j">Journal of Open Source Software</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">91</biblScope>
			<biblScope unit="page">5742</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">measr: Bayesian psychometric measurement using &apos;stan&apos; (R package version 0.3.1)</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Thompson</surname></persName>
		</author>
		<idno type="DOI">10.32614/CRAN.package.measr</idno>
		<ptr target="https://doi.org/10.32614/CRAN.package.measr" />
	</analytic>
	<monogr>
		<title level="m">The Comprehensive R Archive Network</title>
		<imprint>
			<date type="published" when="2023">2023b</date>
		</imprint>
	</monogr>
	<note type="report_type">Computer software</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Improving instructional decision-making using diagnostic classification models</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Clark</surname></persName>
		</author>
		<idno type="DOI">10.1111/emip.12619</idno>
		<ptr target="https://doi.org/10.1111/emip.12619" />
	</analytic>
	<monogr>
		<title level="j">Educational Measurement: Issues and Practice</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>Advance online</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Using simulated retests to estimate the reliability of diagnostic assessment systems</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Hoover</surname></persName>
		</author>
		<idno type="DOI">10.1111/jedm.12359</idno>
		<ptr target="https://doi.org/10.1111/jedm.12359" />
	</analytic>
	<monogr>
		<title level="j">Journal of Educational Measurement</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Practical Bayesian model evaluation using leaveone-out cross-validation and WAIC</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vehtari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gabry</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11222-016-9696-4</idno>
		<ptr target="https://doi.org/10.1007/s11222-016-9696-4" />
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1413" to="1432" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">loo: Efficient leave-one-out cross-validation and WAIC for Bayesian models (R package version 2.6.0)</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vehtari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Magnusson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-C</forename><surname>Bürkner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Paananen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<idno type="DOI">10.32614/CRAN.package.loo</idno>
		<ptr target="https://doi.org/10.32614/CRAN.package.loo" />
	</analytic>
	<monogr>
		<title level="m">The Comprehensive R Archive Network</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">Computer software</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Introducing diagnostic classification modeling as an unsupervised method for screening probable eating disorders</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Barnhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Nagata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1177/10731911241247483</idno>
		<ptr target="https://doi.org/10.1177/10731911241247483" />
	</analytic>
	<monogr>
		<title level="j">Assessment. Advance online</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Statistical applications to cognitive diagnostic testing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev-statistics-033021-111803</idno>
		<ptr target="https://doi.org/10.1146/annurev-statistics-033021-111803" />
	</analytic>
	<monogr>
		<title level="j">Annual Review of Statistics and Its Application</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="651" to="675" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
