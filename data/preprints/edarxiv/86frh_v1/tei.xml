<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">REFLECTIONS ON SIX YEARS AT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Robert</forename><surname>Leek</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Mathematics</orgName>
								<orgName type="institution">University of Birmingham</orgName>
								<address>
									<addrLine>Edgb aston</addrLine>
									<postCode>B15 2TT</postCode>
									<settlement>Birmingham</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Jones</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Mathematics</orgName>
								<orgName type="institution">University of Birmingham</orgName>
								<address>
									<addrLine>Edgb aston</addrLine>
									<postCode>B15 2TT</postCode>
									<settlement>Birmingham</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">John</forename><surname>Meyer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Mathematics</orgName>
								<orgName type="institution">University of Birmingham</orgName>
								<address>
									<addrLine>Edgb aston</addrLine>
									<postCode>B15 2TT</postCode>
									<settlement>Birmingham</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">REFLECTIONS ON SIX YEARS AT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">05314D4E3B2DEF62AF4BE7D1C6937291</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-23T06:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this article, we reflect on our experience teaching on transnational Mathematics programmes at the Jinan-Birmingham Joint Institute, particularly the use of the Möbius computeraided assessment system throughout all modules and levels taught by the University of Birmingham. The particular context of our institute, including its transnational educational model used and the rationale for our heavy reliance on these assessments, is described in detail prior to discussing how we design and implement these assessments. The effect of the COVID-19 pandemic on academic integrity forced us to reconsider our assessment diet, and this paper concludes with a reflection on our teaching experience, focusing on the capabilities of computer-aided assessment systems and how they are used in practice by academics with a wide variety of experience.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In 2017 the first cohort of Jinan-Birmingham Joint Institute (J-BJI) students commenced their studies. Throughout the following years, we have led in delivering computer-aided assessment (CAA) at the J-BJI in various roles. Following the graduation of the 2017/18 cohort of J-BJI students in 2021, we believed it was important to share our experience of and reflections on delivering CAA via the Möbius platform at the J-BJI with the broader mathematics education community. We use CAA to refer to assessments with questions automatically marked by a prewritten grading code. This paper covers a 6-year period from Summer 2017, when we had no CAA content, to the start of the 2023/24 academic year, when we have overseen 188 Möbius assessments, spanning twelve lecture courses and all three stages of our degree programmes.</p><p>Support within the literature for wider dissemination of lecturers' perspectives on their engagement with CAA of mathematics can be found in a recent research agenda for e-assessment <ref type="bibr" target="#b24">(Kinnear, et al., 2024)</ref>, specifically Questions 32 and 33. About a decade ago <ref type="bibr" target="#b9">(Broughton, et al., 2013)</ref> and more recently <ref type="bibr" target="#b14">(Davies, et al., 2022;</ref><ref type="bibr">Davies, et al., 2024)</ref> have reported on lecturers' opinions of implementing CAA in a UK HE setting. As observed in the latter, we also agree that there is not a substantial literature on lecturers' opinions concerning the implementation of CAA in Mathematics programmes at scale. We also note that discussions of CAA implemented using Möbius are infrequent in the literature. This article intends to address both topics and contribute to this body of research.</p><p>Transnational education (TNE) is a significant form of higher education provision from UK universities, and as of the 2021/22 academic year there are over 500,000 students registered at TNE providers, with approximately 40% of these studying under collaborative provision (like at J-BJI), compared with approximately 2.86 million students registered at UK Higher Education Institutions <ref type="bibr">(HESA, 2023;</ref><ref type="bibr">Universities UK International, 2023)</ref>.</p><p>As TNE and CAA may be unfamiliar to many readers, we provide a detailed exposition of our teaching and assessment arrangements (sections 2-3) to provide the appropriate context for the rest of the paper. In later sections will discuss how we develop content in Möbius (section 4), the changes made to our assessment resulting from the COVID-19 pandemic (section 5), and finally we conclude with reflections on our experience (section 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">An overview of the J-BJI</head><p>The University of Birmingham (UoB) uses a fly-in/fly-out model <ref type="bibr">(QAA, 2024)</ref> for staff that teach mathematics at the J-BJI, whereby academic staff travel to Jinan University (JNU) in Guangzhou, China, to deliver their teaching. The 'flying faculty' are responsible for the delivery of core mathematics modules across four undergraduate dual-degree programmes (University of <ref type="bibr">Birmingham, n.d.)</ref>; these are B.Sc. Applied Mathematics with:</p><p>• Economics (Econ),</p><p>• Information Computing Science (ICS),</p><p>• Mathematics<ref type="foot" target="#foot_1">foot_1</ref> (MAM), and • Statistics (Stats).</p><p>Specialist J-BJI staff deliver English language education and JNU faculty deliver discipline-specific content for the four dual-degree programmes. The flying faculty have expertise in various mathematical subjects, primarily linked to subjects taught at the J-BJI. Notably, staff have vastly different experiences and familiarity with CAA, whether as students or instructors. Each academic year consists of two semesters, each containing sixteen teaching weeks, split into four blocks of four weeks. During each block, academics delivered ten course credits<ref type="foot" target="#foot_2">foot_2</ref> of material. Under ordinary proceedings, UoB academic staff travel to Guangzhou to provide teaching in person. However, due to travel restrictions caused by the COVID-19 pandemic, education has been delivered remotely <ref type="bibr" target="#b23">(Jones, et al., 2021)</ref> from February 2020 to June 2022, with in-person teaching resuming partially in Autumn 2022 and fully by Spring 2023. Further context for J-BJI can be found in <ref type="bibr" target="#b16">(Du Croz &amp; Morris, 2024)</ref>, particularly on our summer school and study abroad programmes.</p><p>Academic Year Year 1 Year 2 Year 3 Total Approx. No. of students No. of Möbius assessments Approx. No. of students No. of Möbius assessmen ts Approx. No. of students No. of Möbius assessmen ts Approx. No. of students No. of Möbius assessmen ts 2017/18 100 16 100 16 2018/19 200 16 100 16 300 32 2019/20 210 16 200 16 100 16 510 48 2020/21 260 15 210 14 200 15 670 44 2021/22 210 8 260 8 210 8 680 24 2022/23 270 8 210 8 250 8 730 24</p><p>TABLE 1. Number of Möbius assessments and students (based on assessment data) from the first 6 years of delivery at the J-BJI. Approximate numbers of students are to the nearest 10 for years 1-3, with the total in the last column being the sum of these approximations and not necessarily the true approximate total.</p><p>For the academic years listed in Table <ref type="table">1</ref>, continuous summative assessments for each 20credit module typically comprised of four assessments, and students were assessed every two weeks, national holidays permitting. <ref type="foot" target="#foot_3">3</ref> During the 2017/18-2020/21 academic years, all of these assessments were computer-aided, delivered using Möbius <ref type="bibr">(DigitalEd, n.d.)</ref>, formerly known as MapleTA. From the 2021/22 academic year, we converted half of these assessments to hand-written assessments, and we will be discussing our rationale for this change in section 6. A distinct requirement of our teaching arrangements is that a sufficiently sizeable proportion of each assessment must differ by at least 20% between the two streams of students: Econ &amp; Stats vs MAM &amp; ICS.</p><p>Initially in 2017/18, the continuous assessment arrangements for each module consisted of weekly, group-submitted solutions to problem sheets, and fortnightly, invigilated class tests lasting 45-minutes using Möbius. The former had no contribution to the final module grade, whilst the latter contributed 20%. There are sound pedagogical bases for students to work together, and work individually, to gain understanding of mathematics (D <ref type="bibr">'Souza &amp; Wood, 2003)</ref>. However, the practical constraints for providing these assessments were the marking time required from module leads for formative assessments, timeliness of feedback, and availability of suitable computer laboratories and invigilators <ref type="bibr">(Robinson, et al., 2012, pp. 115-116)</ref>. Until our first experience teaching at the J-BJI in the Spring of 2018, the total capacity of computer laboratories at the J-BJI was unknown to the authors.</p><p>These short class tests at the J-BJI typically contained up to 10 brief questions, which adhered to principles advocated in <ref type="bibr" target="#b41">(Smith, et al., 1996;</ref><ref type="bibr" target="#b19">Greenhow, 2015)</ref>. Given that most academic staff had little-to-no experience with CAA either as students or lecturers, this arrangement allowed staff to familiarise themselves with Möbius where requirements on randomisation and conceptual intricacy of questions were limited by the format of the assessment <ref type="bibr">(Robinson, et al., 2012, p. 114)</ref>; for example, invigilation meant that randomisation was less important, albeit still implemented via 3-5 variants of a randomised question, and questions testing recall were reasonable.</p><p>Following discussions in the flying faculty team in the Summer of 2018, we decided that our continuous assessments should be for learning <ref type="bibr" target="#b47">(Wiliam, 2011)</ref> rather than of learning. End-ofsemester examinations were deemed a suitable assessment of learning to determine if students had met learning outcomes and, consequently, progression requirements. Combined with the projected growth in student numbers, we decided to replace the class tests for future cohorts with noninvigilated assessments open for at least 24 hours. However, we maintained class tests for the 2017/18 cohort, as these students were already familiar with this format and student numbers were sufficiently small to manage. For newly appointed academic staff, this also provided a more manageable introduction to assessment authoring in Möbius.</p><p>From the 2018/19 cohort onwards, we granted more time for students to complete their continuous assessments, so we could:</p><p>• provide feedback to students on a broader variety of material than the 45-minute class tests permitted; • incorporate more types of questions (see the scheme of <ref type="bibr">(Pointon &amp; Sangwin, 2003, p. 675)</ref> for a suggested classification of question types); • give students more authentic assessments <ref type="bibr" target="#b46">(Wiggins, 2011)</ref>, as outside of an educational setting, one is rarely asked to answer questions with stringent time limits comparable to that of a class test; and • encourage students to not only answer questions but also consider how they can justify their correct answers.</p><p>Practicalities were also a driver for this change since weekly student timetables differed between the four dual degree programmes, and thus finding periods of time where students were all comparably free was only guaranteed over sufficiently short (&lt; 1 hour) or sufficiently large periods (&gt; 24 hours) of time. This did result in staff having to spend more time and effort designing and implementing their assessments, which we discuss this further in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Procurement and management of Möbius</head><p>The decision to use Möbius to deliver CAA was based on several factors: • existing institutional expertise in the use of MapleTA at UoB; • access to professional technical support from DigitalEd (formerly Maplesoft) to keep the CAA system running; • ability to deliver assessments based on a server physically hosted in China; and • the price-point.</p><p>We agree with the observations of other UK-based practitioners that for the advantages one gains from commercial solutions, one compromises their control and ability to develop the system. We find that without the following essential enhancements designed ad-hoc by UoB's Engineering and Physical Sciences (EPS) EdTech team and RL (re-grader; batch question testing; batch file downloader; wrapper for defining MapleTA variables in algorithm section of question authoring; see section 4.2 for details), we could not effectively use Möbius to author and test questions, assess students, or employ suitable quality assurance processes when grading assessments.</p><p>Physically locating the server in China is required to provide reliable access to Möbius for students, as well as security considerations for UoB. This creates manageable but frustrating syncing issues since academic staff typically develop Möbius content in the UK, on another instance of Möbius hosted within the EU that is used for Birmingham-based students. Due to differing timetables at UoB and J-BJI, updates to the Chinese Möbius server typically occur at different dates/times compared to the EU server, albeit sometimes during J-BJI teaching periods, which on occasion has caused disruption. Software updates are a general requirement of any CAA setup; disruption to staff and students can be avoided via clear communication with all stakeholders, however since 2021 DigitalEd automatically upgrades Möbius instances and no longer notifies of planned maintenance periods, as part of their move to cloud infrastructure. Moreover, Maple library locations for UoB pre-written grading code require manual updates whenever questions are transferred between both versions of Möbius.</p><p>When students' access to assessments/feedback in Möbius has been unintentionally limited, the primary causes have been:</p><p>1. the LTI link <ref type="bibr">(Blackboard, n.d.)</ref> between Möbius and the virtual learning environment Blackboard being broken, for example, due to required/ongoing maintenance of Blackboard, or inconsistent timestamps between servers; 2. human error, for example, due to academic staff setting incorrect assessment policies (DigitalEd, n.d.); 3. bugs within Möbius; and 4. students using unsupported browsers (e.g. Internet Explorer; Microsoft Edge; 360 Secure Browser; outdated versions of supported browsers) and issues with 3 rd party cookies and our LMS (for example, Safari blocks these by default).</p><p>Maintaining clear communication with JNU IT support and the UoB administrative team based at the J-BJI, who gather student feedback, has been key to mitigating item 1. To address item 2, a guide for setting assessment policies has been provided for staff, explicitly stating which options must be set and when (i.e. before/after the assessment). All authors confirm that no matter how much experience staff gain using Möbius, human error can still limit access to Möbius for J-BJI students. On item 3, we typically need to observe the bug before mitigating it. Bugs were reported to DigitalEd, and a practical workaround was created to avoid situations where the bugs arise. This often leaves bugs unfixed but mitigated for our purposes (for instance, only allowing specific staff to use the proctor tools to re-open assessments for individual users). Finally, item 4 is addressed by reminding students to use supported browsers <ref type="bibr">(DigitalEd, n.d.)</ref> which currently consists of Google Chrome and Safari (Firefox support was dropped in August 2022).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Developing content in Möbius</head><p>At the start of the 2017/18 academic year, teaching staff (including RL and JM) were trained in developing content for Möbius. This consisted of working through the Möbius School classes on the Möbius Community forum <ref type="bibr">(DigitalEd, n.d.)</ref> and attempting to create simple questions, including multiple-choice and Maple-graded response areas. As lecturers were busy preparing their teaching materials for the first cohort of students, as well as helping manage J-BJI, student interns were hired during the summer of 2017 and the winter of 2017/18 to code questions written by the lecturers. The interns were UoB students based in the UK and did not have contact with students at the J-BJI, and they have blogged about their experiences in <ref type="bibr">(Johnson, et al., n.d.)</ref>. This allowed lecturers to focus on the design of questions and feedback rather than their implementation, as well as providing valuable programming experience to interns, not just in Maple but also HTML, CSS, and JavaScript for the Möbius HTML response areas.</p><p>After the questions were written, interns tested them and provided there were no issues, these questions were passed back to lecturers to provide final checks. Ultimate responsibility for assessments rested with the relevant lecturers. Questions had to be occasionally revised by either interns or lecturers, sometimes due to imprecise specifications from lecturers. In later years, lecturers typically reworked these questions and wrote new questions using the interns' question generation and grading code as a template.</p><p>This working arrangement allowed for a separation of duties between design and implementation, and with slight variations, continued till the start of the 2020/21 academic year, under the supervision of the authors. We have not hired interns to develop questions for us since, as our team of lecturers were familiar with the design and implementation of assessments in Möbius, and we had sufficient assessment resources that could be reused with minor modifications (e.g. changing randomised parameters). Several colleagues could also provide administrative support for our assessments, adding resiliency to our operations.</p><p>Most applications of CAA are seemingly aimed at lower-level courses, as noted in <ref type="bibr">(Lowe &amp; Mestel, 2020, p. 68)</ref>, and use a core group of 'designers' to write and code the questions. In contrast, at J-BJI we use CAA throughout all levels of our programmes. With the former approach, it would be a formidable to write and design questions for all courses, in particular for advanced statistical and optimisation topics, which is why we have opted for staff to write questions themselves. This approach also gives staff additional agency in the delivery of their modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Question design and regrading</head><p>As part of their training, lecturers are provided with guidelines to aid in the design of their questions. The main points are summarised below:</p><p>• Lecturers should specify the layout of the question and response areas, as well as the expected format of answers (e.g. whether to include brackets and which type, if the answer is a list/vector). • Questions should ideally be randomised (e.g. drawing parameters from a specific range), ensuring that different instances are of comparable difficulty. The choice of suitable ranges should be made with due consideration when question difficulty is particularly sensitive to parameter choices (e.g. finding highest common factors using the Euclidean algorithm). • General solutions should be provided to each question. If the lecturer intends for students to apply algorithms, interns should be notified so that this algorithm can be implemented using Maple. This helps future-proof the question against changes to parameters.</p><p>• The mark scheme should specify when partial marks are to be awarded, and lecturers are encouraged to carry forward errors to avoid penalising students multiple times for the same mistakes. • Lecturers should avoid long, multi-part questions where possible, as it makes regrading harder (see below for details about our regrading process). • Question parts could be implemented as separate questions if the lecturer wishes.</p><p>Many of our lecturers first encounter CAA with the expectations of paper-based assessments, where students can give free-form answers, and mark schemes are revised for any edge cases that arise. This has also been observed in the literature <ref type="bibr">(Kinnear, et al., 2024, pp. research question 23, p. 13</ref>) and can cause issues, mainly when the form of a solution is essential. For example, consider the following question: give the second-order Taylor series expansion of exp(x) about the point x = 1 <ref type="bibr" target="#b28">(Meyer &amp; Leek, 2020)</ref>. Ideally, we would like to award partial marks for correct coefficients; however, if a student responds with '1 + (e/2)x^2', it is unclear how to interpret this answer. Did they calculate the expansion about the point x = 0 (in which case only the constant term is correct), or did they expand about the point x = 1 but miscalculate the constant term, and if so, did they intend to omit the linear term?</p><p>In a paper-based assessment, we would typically have their working on their answer and be able to infer the student's intent. Whilst we do request students upload their working for academic integrity purposes (see section 5 for more), this does not aid the automated marking. By designing these questions carefully and including intermediate steps, we aim to avoid such issues. Unfortunately, DigitalEd has yet to implement linked answer boxes <ref type="bibr">(Watkins, 2018)</ref> despite this being the most popular feature request on DigitalEd's Ideas Portal. If we want to create multi-part questions or questions that award marks dependent on multiple response areas, we either must incorporate the entire question with an HTML response area (which hinders our capability to regrade as loading hundreds of student responses for this question type takes an excessive amount of time, hence our advice to avoid this where possible) or ask students to provide all responses as a list. This creates unwanted tension between wanting to assess comprehensively, simplifying regrading, and reducing friction in the student experience by prompting for 'natural' input (for example, the 2D math response area allows students to input matrices, but if an HTML response area is required to link multiple response areas together, then such an input would need to be manually coded).</p><p>Much of the advice pertains to making the 'regrading' process more accessible and providing an improved student experience. Once an assessment has concluded, a regrader will check for any issues before releasing the results and feedback to students. This was initially someone other than the module lead and in later years this role involved moderation of our assessments (particularly important when continuous assessments contributed 50% towards module marks during the 2020/21 academic year, as discussed in section 5), before finally allowing module leads to regrade their own assessments as our Möbius questions became more stable and robust. Most problems are caused by incorrect syntax, however understanding Maple syntax is not part of our learning outcomes for our modules and hence should not be a factor in our marking. To mitigate these issues, we provide syntax hints (see figure <ref type="figure" target="#fig_1">2</ref>) and during their orientation week students undertake a 'student-readiness test' that teaches them how to use Möbius and input answers. Nevertheless, students will still occasionally make mistakes in future assessments. Unfortunately, as Möbius must output a mark for each response area (DigitalEd, n.d.) instead of a more detailed data format, if Maple encounters an error, then students will be awarded 0 marks. We feel very strongly that this situation should be avoided as much as possible, and we have developed several tools to improve the student and staff experience of Möbius.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Improvements to Möbius from UoB</head><p>To aid our error-detection, we have a standardised template for the grading code within a response area (called 'error-catching code') that assigns 1% in case an error is encountered in Maple so that these issues will be visible in the Gradebook. This method is not 100% fool-proof: whilst Maple expects asterisks when multiplying, it also treats numerals as constant functions, so '(x+1)2' raises an error, but '2(x+1)' does not and outputs '2'. This could be fixed by using custom parser to achieve comparable functionality with STACK, which can report back in real-time to students when they have syntax errors <ref type="bibr">(Sangwin, 2013, pp. 111-112)</ref>. Möbius provides an optional response preview, which is insufficient from our perspective, and a feature request to implement a syntax checker was turned down <ref type="bibr">(Watkins, 2018)</ref>. This ultimately results in more correction of marks due to syntax errors, resulting in more staff time spent on regrading and lower student confidence in the Möbius platform and/or their lecturers.</p><p>Another common issue with regrading is using Unicode characters that Maple/Möbius does not correctly interpret. As we teach Chinese students, they will often input characters using their Chinese keyboard layouts, such as fullwidth and local punctuation characters (Unicode, 2022; Unicode, 2022), as seen in the following string: '【１，２，３】'. The error-catching code identifies these issues for regrading, and was updated in 2022 to automatically substitute common characters and check for any remaining characters that are not printable ASCII characters (Unicode, 2022).</p><p>UoB's EPS EdTech team wrote a JavaScript bookmarklet (figure <ref type="figure" target="#fig_0">1</ref>) that collates student responses on the currently viewed question and modifies marks for all students that submitted a particular response. This improves the speed of regrading, but it can still take a significant amount of time, especially as the rendering of MathJax and HTML response areas in the Möbius gradebook is slow. The EdTech team also developed additional JavaScript bookmarklets to simplify the algorithm section of question authoring by using the Maple language and converting it automatically into the Möbius language, and RL has developed another bookmarklet to batch download PDFs submitted by students in the gradebook view (again, not available within Möbius).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Improvements to Möbius from DigitalEd</head><p>Since DigitalEd was spun-off from Maplesoft to handle Möbius product development, there have been a few notable improvements to the Möbius user experience, from our perspective:</p><p>• Question regrading (rerunning grading code) was introduced in version 2019.1. This allowed staff to alter their grading code after an assessment was released to correct any bugs in the code and/or adjust the code to reflect an updated mark scheme. • Policy Sets were introduced in version 2019.2. There have been several instances where staff have incorrectly set up their assessments, and we tried to use this feature by creating two policy sets ('Before and during test' and 'After regrading') that would handle all the settings needed for our assessments. However, when we tried to use this, we found we could not unset specific options and required an extensive workaround to get it working again. We have not used this feature since. • Activity Grading View (purple manual grade button) was introduced in version 2020.1. This has made it slightly easier to regrade HTML question types as it only loads a single student's response at a time. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">COVID-19, academic standards, and plagiarism</head><p>The UK government's then foreign secretary Dominic Raab's statement on 4 February 2020 advised against 'all but essential' travel to China (Foreign &amp; Commonwealth Office and Raab 2020). Shortly afterward on 7 February 2020, a 'closed management of communities' policy was implemented in Guangzhou (Foreign Affairs Office of the People's Government of Guangzhou Municipality, 2020). Hand-written, group-submitted assessments at J-BJI were cancelled for the remainder of the academic year and end-of-year examination were significantly modified for openbook, online assessment. This move from 'class tests' to 'take-home exams' <ref type="bibr" target="#b5">(Bengtsson, 2019)</ref> became necessary due to the logistical challenges of remote invigilation (see Jones, Meyer and Huang 2021 for more details on our remote teaching experience during this time), and inevitably increased the risk of plagiarism <ref type="bibr" target="#b25">(Lancaster &amp; Cotarlan, 2021;</ref><ref type="bibr" target="#b11">Comas-Forgas, et al., 2021;</ref><ref type="bibr">Institute of mathematics &amp; its applications, London Mathematical Society, Royal Statistical Society, 2022)</ref>.</p><p>In the 2019/20 academic year, this was less of a concern due to the limited contribution (20% per module) of these assessments towards overall module marks and the randomisation of parameters in most questions; however the Framework for Education Resilience introduced at UoB for the 2020/21 academic year increased this contribution of continuous assessment for each module to 50% (Armour &amp; Senior Education Team, 2020) so that students could progress/graduate even with disrupted end-of-semester exams, and incidentally provided further incentive for plagiarism. It was difficult to anticipate how, and to what extent, this would affect module marks and the student experience. It was not straightforward to create CAA questions that were sufficiently challenging relative to the exams of previous years, as expected by (QAA, 2020), although we improved over time and helpful, and practical ideas began to appear soon after <ref type="bibr" target="#b6">(Bickerton &amp; Sangwin, 2022)</ref>.</p><p>It was decided that increasing the frequency of summative assessments was not appropriate to accommodate the increase in continuous assessment weighting, but instead to increase the difficulty and scope of CAA assessments. Despite our efforts, most modules had very high average marks for these assessments, in many cases over 90%, which resulted in many students approaching their final exam having already passed that module, as well as little differentiation between students. Combined with limited feedback on their hand-written work, this likely led to misaligned expectations so students believed they would perform exceptionally well on exams, which were more challenging than usual due to their open-book nature and the School of Mathematics exam preparation guidance for that year. We list below several types of questions and examples used in CAA that were more successful in maintaining academic standards and providing a sufficient challenge to students:</p><p>1. Questions with increased complexity (e.g. requiring more intermediate steps to solve) that only award marks for the final answer (see figure <ref type="figure" target="#fig_1">2 below</ref>). 2. Questions requiring a proof or a counterexample to determine the answer but don't specify what the student should do. Multiple-choice can be a suitable format for these questions, as discussed in <ref type="bibr">(Lyakhova, 2023, pp. 12-13)</ref> and demonstrated in figure <ref type="figure">3</ref> below. 3. Questions with increased conceptual difficulty, including those that assess proof comprehension, such as the proof fallacy example in (Bickerton and Sangwin 2022, figure <ref type="figure">6</ref>). 4. Questions which defeat WolframAlpha<ref type="foot" target="#foot_4">foot_4</ref> (see figure <ref type="figure">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>below).</head><p>One should acknowledge that the difficulty of a question is not intrinsic to the question; the setting in which a question is answered matters. For example, inverting a given 3-by-3 matrix is more prone to error in an invigilated, closed-book setting without a calculator than in an open-book, noninvigilated setting without time pressure and access to online tools. FIG 3. A question form JM's first-year Vectors, Geometry, &amp; Linear Algebra course. To determine the correct answers, students should prove or construct a counterexample for each statement, connecting many different concepts in this course. FIG 4. A question from Yuzhao Wang's first-year Real Analysis and Calculus course. WolframAlpha</p><p>was unable to answer this question, even when specified as a discrete limit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Academic integrity measures</head><p>Plagiarism is a clear threat to take-home exams (Bengtsson 2019, section 4.2) such as our CAA setup; in particular, if questions lack randomisation, responses to plagiarism are not sufficiently severe, or the risk of getting caught is not strong enough. Inevitably, some students will commit plagiarism and not be caught, but this will be true for any system of detecting plagiarism. Various ideas were implemented to address the above concerns, as outlined below.</p><p>Randomisation can be difficult and time-consuming to implement. A suggested method of randomisation, typified by its simplicity in implementation, utilises single instances of parameterised values which modify the solution to a problem. This is considered desirable when plagiarism is a concern. We can easily detect a student providing a solution that is not correct for their question but valid for a different instance, as in figure <ref type="figure" target="#fig_2">5</ref> below, or even when multiple students submit the same peculiar response. Such questions should be designed so that it is unlikely students would accidentally submit the correct answer to a different question instance purely through calculation errors, to minimise the risk of genuine attempts being unfairly labelled as plagiarism. Ideally, students are unaware that any randomisation is taking place, although it is optimistic to think that students willing to collude will not notice randomisation. At the end of each assessment, we included a file upload which required students to submit their hand-written work as a PDF file, worth 5% to encourage students. We also notified students of our intent to consider handwritten submissions as evidence of their original work and we performed cursory checks to see if they contained sufficient working before awarding the 5% marks. Failing to provide an upload could be penalised more harshly (by scoring the student 0 for the entire assessment), but the intermittent server issues hinder this approach. These file uploads provided reliable evidence to manually check student's work and decide if they have arrived at suspect answers via genuine errors or collusion. If evidence of plagiarism was sufficient, students were referred to the J-BJI academic integrity officer, who would arrange a meeting with the student to discuss the evidence and determine whether plagiarism occurred.</p><p>There are potential problems with the file upload approach. Suppose a student is aware that randomisation is taking place in a question and can find another student who has the same instance of this question and submits the same response. In such cases, the file upload could contain no genuine attempt to show the student's work (either through omission, uploading the wrong document, or copying the working from the same student) and would go unnoticed by the regrader if the response is correct. These false negatives are concerning, and it is unclear to what extent they are taking place. The time available to a student to complete an assessment is also a relevant consideration in plagiarism mitigation (refer back to our discussion of logistical constraints in section 2), and question reuse leads to an increase in undetected plagiarism attempts.</p><p>There are ways to automatically detect certain types of plagiarism. RL programmed a Python script to extract pages and images from students who submitted handwritten work, hash the resultant files (using the MD5 algorithm) and checked for collisions (images/pages with the same hash). We performed this check at the end of each semester, resulting in a few hundred collisions for all modules from approximately 2500 submissions (a reasonable number to consider as these collisions were exact matches and most were easily dismissed as underlying background images or logos/watermarks) and several penalties were ultimately applied for collusion. RL also experimented with perceptual hashes <ref type="bibr" target="#b10">(Buchner, 2022)</ref>, which hashed the visual representation, as opposed to digital data, in such a way that visually similar images should have similar hashes. However, this resulted in too many false positives to be of practical use: one particular plagiarism case involved 2 students submitting essentially the same work, with one student having added extra written annotations on top of the other student's work; this was detected using the MD5 method as the annotations were stored as a separate embedded image (the original image was unmodified), but the Hamming distance for the perceptual hash between the original and annotated image was not small enough to make checking collisions up to that distance feasible.</p><p>Ultimately the laziest of plagiarists were typically those identified with this method as they either submitted the same file or their PDF files contained shared common parts with other students; many cases involved students sharing identical images in their handwritten work, which whilst not direct evidence of plagiarism, did prompt further investigation. The time invested in manual plagiarism detection could make one question the purpose of CAA since there is still a sizeable logistic advantage compared to marking handwritten work, even with our limited teaching assistant support. In those rare cases where plagiarism was confirmed, a typical outcome was to score the student 0 marks for the corresponding assessment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Reflection and conclusion</head><p>Whilst CAA can provide more efficient and effective assessment, in practice there are several obstructions to optimal performance; some are related to the nature of CAA generally or more specifically the Möbius platform we use, whilst others are due to the institutional context and operational constraints we operate within. Long complex proofs have a particular artistic license for how one might construct them, with most applications of CAA to proof-writing questions limited to particular problem types, often related to relatively elementary (in a tertiary-level mathematics context) mathematical material <ref type="bibr" target="#b37">(Sangwin, 2019;</ref><ref type="bibr" target="#b38">Sangwin &amp; Köcher, 2016;</ref><ref type="bibr" target="#b29">Moons &amp; Vandervieren, 2022)</ref>.</p><p>A semi-automated approach to marking that utilises a rubric marking system, such as in <ref type="bibr">(Graide, n.d.)</ref>, appears to combine the detailed feedback students expect from humans with the efficiencies gained from automated assessment, and our university is currently trialling this software. Proof assistants/interactive theorem provers such as Coq <ref type="bibr">(Coq Team, n.d.)</ref> and <ref type="bibr">(Lean Focused Research Organization, n.d.)</ref> are used to create formal mathematical proofs and have been used in mathematics undergraduate courses to help teach students proof comprehension and writing skills <ref type="bibr" target="#b8">(Blok &amp; Hoffman, n.d.;</ref><ref type="bibr" target="#b2">Avigad, 2019)</ref>. The use of such software for summative assessment in a module needs to be carefully considered as students would have to learn a new computer language/system as well as the ordinary mathematical content in that module <ref type="bibr" target="#b21">(Iannone &amp; Thoma, 2023)</ref>.</p><p>Even without using proof assistants, it is still possible to assess proof-adjacent skills within our CAA setup. A typical example is when students are given proof frameworks <ref type="bibr" target="#b40">(Selden, et al., 2018)</ref> with sections missing that the student needs to complete with justifications, which in a CAA will be provided by multiple-choice options that require the student to select only those conditions which are necessary. This can help develop student's proof comprehension skills as they do not have to create a whole proof, and in our experience have been effective at highlighting common misconceptions which can guide future study sessions. Integrating proof comprehension questions into the English language provision could help alleviate additional difficulties non-native speakers have in understanding mathematics <ref type="bibr" target="#b4">(Barton, et al., 2005)</ref>.</p><p>Rather than fight against the limitations CAA has in assessing all learning outcomes of our modules, we could instead embrace its advantages in providing practice exercises for students with rapid and efficient feedback by only using it formatively. Although such a move would admittedly require more resources initially to create assessments for each module, especially to create highquality feedback for each assessment <ref type="bibr" target="#b17">(Gill &amp; Greenhow, 2008)</ref>, they could easily be reused each year the module is delivered with little modification, assuming there are no issues in the implementation of the questions. Students seem to benefit from performing 'typical' examples multiple times until they gain confidence <ref type="bibr" target="#b0">(Anderson, et al., 2000)</ref> and questions which test basic understanding of definitions (such as asking for an example of a binary relation on a small set that is symmetric but neither reflexive nor transitive) seem particularly well-suited to CAA. Using CAA in this way would reduce the burden on academic staff in preparing questions with consistent difficulty across different instances, as well as eliminating plagiarism concerns. STACK can provide response-specific feedback (STACK, n.d.) and the authors believe this would be a very beneficial feature if added to Möbius. Combined with hand-written assessments that focus primarily on higher-order skills and detailed, individual feedback on students' submissions, this approach can combine the strengths of both forms of assessment, and is also advocated for in <ref type="bibr">(Lyakhova, 2023, pp. 4-5)</ref>.</p><p>Whilst adapting to the logistical issues caused by the COVID-19 pandemic, it became clear that students were in desperate need of feedback on their hand-written work so that they could adequately prepare for exams and gain insight into how their lecturers would assess them, such as their proofs in linear algebra or explanations of statistical analysis. The increased difficulty of exams during the 2020/21 academic year made this issue particularly acute as students were expected to write more clearly and coherently than previously, given the open-book nature of these exams. Hand-written assessments were finally reinstated late in the 2020/21 academic year, initially submitted in groups then individually from 2021/22, replacing half of our CAAs so that each form of continuous assessment now contributed 10% towards final module marks. The authors strongly believe that proofs and explanations should be a core part of our mathematics curriculum and the associated skills cannot currently be assessed directly/adequately with fully-automated CAA without students having to also learn to use interactive theorem provers; this is not to say that we view traditional closed-book exams as necessarily the gold-standard for all our modules, and within the school we are investigating an approach in the vein of standards-based grading <ref type="bibr" target="#b31">(Owens, 2015)</ref> whereby pre-exam assessments determine whether a student has passed a module, with a final exam determining their passing grade.</p><p>The authors are all intimately familiar with the Möbius platform and confident in their ability to create suitable assessment content, but we recognise that was not the case at first nor is it the case currently for several academics at J-BJI. The prominence of programming varies significantly in undergraduate and postgraduate UK mathematics programmes <ref type="bibr" target="#b39">(Sangwin &amp; O'Toole, 2017)</ref>, which may partially explain this variance. This is a significant concern given that all J-BJI academics authored CAAs, not just those who had interest/expertise. The perceived limitations of CAA, whether due to preconceptions of its capabilities or lack of experience, results in many colleagues relying upon multiple-choice questions or numerical response boxes almost exclusively. These types of questions have their place as part of our assessment diet but are unable to assess all of our learning outcomes <ref type="bibr">(Sangwin, 2013, pp. 2-3)</ref> for a given module, and when heavily reliedupon (such as during the pandemic) can lead to poor differentiation between students, especially when taken as open-book assessments.</p><p>At the other end of the spectrum, despite advising against this, some staff expected students to input complicated Maple expressions with insufficient guidance from the lecturer and/or Möbius platform, that resulted in many syntax errors which had to be corrected during regrading. This results in an uneven student experience across modules, however this would be the case for other aspects of their education (for example, there may be differences in the quality of learning materials and delivery). It is unclear whether the 20% difference between the MAM &amp; ICS / Econ &amp; Stats assessments resulted in staff considering the comparable difficulty of questions. STACK, another CAA system, allows instructors to select desired instances of a randomly generated question <ref type="bibr">(STACK, n.d.)</ref> to help ensure questions are of comparable difficulty, and again we would appreciate this feature in Möbius, as it would aid our staff in designing questions with greater consistency. Further research is needed on the implementation of CAA at scale, as well as academic's needs and training when learning to develop CAA content is required, as highlighted in questions 32 and 33 of <ref type="bibr" target="#b24">(Kinnear, et al., 2024)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FIG 1 .</head><label>1</label><figDesc>FIG 1. An example output from the regrader script. Different responses have been collated (seeCount column) so that marks can be changed simultaneously.</figDesc><graphic coords="8,56.73,56.70,481.85,269.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FIG 2 .</head><label>2</label><figDesc>FIG 2. A question from RL's first-year Algebra &amp; Combinatorics course, which asks students to convert a word into a normal form using the provided group relations. By choosing words which require the same number of applications of these relations, different versions of this question can becreated with comparable difficulty.</figDesc><graphic coords="9,95.90,373.10,403.17,211.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FIG 5 .</head><label>5</label><figDesc>FIG 5. A question from DJ's third-year Game Theory &amp; Multicriteria Decision Making course. Each student is presented with three values for μ, chosen at random from six possible values. For μ = 0.05, the student should return the answer -0.22. The regrader would raise a concern if a student responded with -0.22 for the value μ = 0.25, if μ = 0.05 was not given to that student.</figDesc><graphic coords="11,99.65,335.16,395.30,335.38" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>† Corresponding author. Email: r.leek@bham.ac.uk</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>'Mathematics' here refers to Pure Mathematics.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>These credits are worth the same as other degrees and modules provided by UoB but differ from the credit system in use at JNU.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>The number of assessments in the 2020/21 academic year are slightly lower, due to the COVID-19 pandemic.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4"><p>ChatGPT, one of the first publicly available, consumer-friendly AI chatbots based on a large language model, was only released in November 2022<ref type="bibr" target="#b30">(OpenAI, 2022)</ref> and as such AI chatbots were not considered a serious plagiarism threat at the time.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7.">Acknowledgements</head><p>We would like to thank <rs type="person">Jonathan Watkins</rs> and <rs type="person">Richard Mason</rs> for their support over the lifetime of the joint institute and our valuable discussions on this paper and computer-aided assessment generally. We would also like to <rs type="person">Yuzhao Wang</rs> for giving permission to Figure <ref type="figure">4</ref>.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Applications and Misapplications of Cognitive Psychology to Mathematics Education</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Reder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Texas Educational Review</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="29" to="49" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Framework for Educational Resilience 2020/21</title>
		<author>
			<persName><forename type="first">K</forename><surname>Armour</surname></persName>
			<affiliation>
				<orgName type="collaboration">Senior Education Team</orgName>
			</affiliation>
		</author>
		<ptr target="https://birminghamucu.files.wordpress.com/2020/05/framework-for-educational-resilience-18-may-d6-m3-and-all-staff.pdf" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning Logic and Proof with an Interactive Theorem Prover</title>
		<author>
			<persName><forename type="first">J</forename><surname>Avigad</surname></persName>
		</author>
		<editor>G.</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><surname>De Villiers</surname></persName>
		</author>
		<title level="m">Proof Technology in Mathematics Research and Teaching</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="277" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">EAL undergraduates learning mathematics</title>
		<author>
			<persName><forename type="first">B</forename><surname>Barton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Mathematical Education in Science and Technology</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="721" to="729" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Take-home exams in higher education: a systematic review</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bengtsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Education Sciences</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Practical online assessment of mathematical proof</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Bickerton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Sangwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Mathematical Education in Science and Technology</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2637" to="2660" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<ptr target="https://help.blackboard.com/Learn/Administrator/SaaS/Integrations/Learning_Tools_Interoperabilit" />
		<title level="m">Learning Tools Interoperability (LTI)</title>
		<imprint>
			<date type="published" when="2024-04-18">18 April 2024</date>
		</imprint>
		<respStmt>
			<orgName>Blackboard</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Blok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Spatchcoq Blog</surname></persName>
		</author>
		<ptr target="http://spatchcoq.co.uk/spatchcoq/" />
		<imprint>
			<date type="published" when="2023-08-13">13 August 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Lecturers&apos; perspectives on the use of a mathematics-based computer-aided assessment system</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Broughton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hernandez-Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Teaching Mathematics and its Applications</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="88" to="94" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">ImageHash</title>
		<author>
			<persName><forename type="first">J</forename><surname>Buchner</surname></persName>
		</author>
		<ptr target="https://pypi.org/project/ImageHash/" />
		<imprint>
			<date type="published" when="2022-08-13">2022. 13 August 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Exam cheating and academic integrity breaches during the COVID-19 pandemic: An analysis of internet search activity in Spain</title>
		<author>
			<persName><forename type="first">R</forename><surname>Comas-Forgas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lancaster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Calvo-Sastre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sureda-Negre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Heliyon</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The Coq Proof Assistant</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Coq Team</surname></persName>
		</author>
		<ptr target="https://coq.inria.fr/" />
		<imprint>
			<date type="published" when="2024-05">May 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A department-wide transition to a new mode of Computer-Aided Assessment using STACK</title>
		<author>
			<persName><forename type="first">B</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Crisan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Geraniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Smart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>Unpublished</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Smart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Geraniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Crisan</surname></persName>
		</author>
		<ptr target="https://www.digitaled.com/support/help/student/Content/GENERAL/System-Requirements.htm" />
		<title level="m">STACKification: automating assessments in tertiary mathematics. Bozen-Bolzano</title>
		<imprint>
			<publisher>Free University of Bozen-Bolzano and ERME</publisher>
			<date type="published" when="2022-08-13">2022. 13 August 2023. 18 April 2024. 13 August 2023. 19 August 2022. 13 August 2023</date>
		</imprint>
	</monogr>
	<note>Author a Maple-graded question Define assignment properties View Möbius system requirements</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Tertiary students&apos; views about group work in mathematics</title>
		<author>
			<persName><forename type="first">D'</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Australian Association for Research in Education</publisher>
			<pubPlace>Auckland</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Du Croz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Morris</surname></persName>
		</author>
		<ptr target="http://www.gzfao.gov.cn/ztlm/yqfk/content/post_137139.html" />
		<title level="m">Developing formal summer school and study abroad options within a dual degree (4+0) programme</title>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>QAA. Foreign Affairs Office of the People&apos;s Government of Guangzhou Municipality</publisher>
			<date type="published" when="2020-07">2024. 2020. July 2024</date>
		</imprint>
	</monogr>
	<note>广州 发布第三号防疫通告：全市小区实施封闭管理</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">How effective is feedback in Computer-Aided Assessments?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Greenhow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Learning, Media and Technology</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="207" to="220" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Organising Feedback</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Graide</surname></persName>
		</author>
		<ptr target="https://www.graide.co.uk/features/organising-feedback" />
		<imprint>
			<date type="published" when="2024-07-03">3 July 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Effective computer-aided assessment of mathematics; principles, practice and results</title>
		<author>
			<persName><forename type="first">M</forename><surname>Greenhow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Teaching Mathematics and Its Applications: International Journal of the IMA</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="117" to="137" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m">Higher Education Student Statistics</title>
		<meeting><address><addrLine>UK; Cheltenham</addrLine></address></meeting>
		<imprint>
			<publisher>Higher Education Statistics Agency</publisher>
			<date type="published" when="2021">2023. 2021/22</date>
		</imprint>
		<respStmt>
			<orgName>HESA</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Interactive theorem provers for university mathematics: an exploratory study of students&apos; perceptions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Iannone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thoma</surname></persName>
		</author>
		<ptr target="https://www.lms.ac.uk/news/assessment-statement-update" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Mathematical Education in Science and Technology</title>
		<imprint>
			<date type="published" when="2022-05">2023. 2022. May 2024</date>
			<publisher>Royal Statistical Society</publisher>
		</imprint>
		<respStmt>
			<orgName>Institute of mathematics &amp; its applications</orgName>
		</respStmt>
	</monogr>
	<note>Statement on Methods of Assessment in the Mathematical Sciences</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">T</forename><surname>Johnson</surname></persName>
		</author>
		<ptr target="https://mapletabham2017.wordpress.com/" />
		<imprint>
			<date type="published" when="2017-05-24">2017. 24 May 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reflections on remote teaching</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MSOR Connections</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="54" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A Collaboratively-Derived Research Agenda</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kinnear</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Research in Undergraduate Mathematics Education</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="201" to="231" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Contract cheating by STEM students through a file sharing website: a Covid-19 pandemic perspective</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lancaster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cotarlan</surname></persName>
		</author>
		<ptr target="https://leanprover.github.io/" />
	</analytic>
	<monogr>
		<title level="j">International Journal for Educational Integrity</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2021-08">2021. August 2023</date>
		</imprint>
		<respStmt>
			<orgName>Lean Focused Research Organization</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Using STACK to support student learning at masters level: a case study</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Mestel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Teaching Mathematics and its Applications</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="61" to="70" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">On the use of technology in university mathematics teaching and assessment in STEM degree schemes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lyakhova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>Joint Mathematical Council of the UK</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">E-assessment for the Jinan University -University of Birmingham joint institute: From content development to assignment re-grading</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Leek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>E-Assessment in Mathematical Sciences</publisher>
			<pubPlace>Newcastle</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Handwritten math exams with multiple assessors: researching the added value of semi-automated assessment with atomic feedback</title>
		<author>
			<persName><forename type="first">F</forename><surname>Moons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vandervieren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<pubPlace>Bozen-Bolzano</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Free University of Bozen-Bolzano and ERME</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Introducing ChatGPT</title>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<ptr target="https://openai.com/index/chatgpt/" />
		<imprint>
			<date type="published" when="2022-06">2022. June 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">A Beginner&apos;s Guide to Standards Based Grading</title>
		<author>
			<persName><forename type="first">K</forename><surname>Owens</surname></persName>
		</author>
		<ptr target="https://blogs.ams.org/matheducation/2015/11/20/a-beginners-guide-to-standards-based-grading/" />
		<imprint>
			<date type="published" when="2015-05">2015. May 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An analysis of undergraduate core material in the light of hand-held computer algebra systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pointon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Sangwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Mathematical Education in Science and Technology</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="671" to="686" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m">COVID-19: Initial Guidance for Higher Education Providers on Standards and Quality</title>
		<meeting><address><addrLine>Gloucester</addrLine></address></meeting>
		<imprint>
			<publisher>QAA</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
		<respStmt>
			<orgName>QAA</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m">Quality Evaluation and Enhancement of UK Transnational Higher Education: People&apos;s Republic of China</title>
		<meeting><address><addrLine>Gloucester</addrLine></address></meeting>
		<imprint>
			<publisher>QAA</publisher>
			<date type="published" when="2024">2024</date>
		</imprint>
		<respStmt>
			<orgName>QAA</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mathematics lecturers&apos; practice and perception of computer-aided assessment</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hernandez-Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Broughton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mapping university mathematics assessment practices</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Iannone</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Simpson</surname></persName>
		</editor>
		<meeting><address><addrLine>Norwich</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="105" to="118" />
		</imprint>
		<respStmt>
			<orgName>University of East Anglia</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Sangwin</surname></persName>
		</author>
		<title level="m">Computer Aided Assessment of Mathematics</title>
		<meeting><address><addrLine>Oxford</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Developing and evaluating an online linear algebra examination for university mathematics</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sangwin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
		<respStmt>
			<orgName>Utrecht, Freudenthal Group &amp; Freudenthal Institute, Utrecht University and ERME</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Automation of mathematics examinations</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Sangwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Köcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Education</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="215" to="227" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Computer programming in the UK undergraduate mathematics curriculum</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Sangwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>O'toole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Mathematical Education in Science and Technology</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1133" to="1152" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Proof Frameworks: A Way to Get Started</title>
		<author>
			<persName><forename type="first">A</forename><surname>Selden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Selden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Benkhalti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PRIMUS</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="45" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Constructing mathematical examinations to assess a range of knowledge and skills</title>
		<author>
			<persName><forename type="first">G</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Mathematical Education in Science and Technology</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="77" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Stack</surname></persName>
		</author>
		<ptr target="https://docs.stack-assessment.org/en/AbInitio/Authoring_quick_start_3/" />
		<title level="m">Authoring quick start 3: improving feedback</title>
		<imprint>
			<date type="published" when="2023-08-14">14 August 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Stack</surname></persName>
		</author>
		<ptr target="https://www.unicode.org/charts/PDF/UFF00.pdf" />
		<title level="m">Authoring quick start 4: randomisation</title>
		<meeting><address><addrLine>London; Universities UK</addrLine></address></meeting>
		<imprint>
			<publisher>Universities UK International</publisher>
			<date type="published" when="2022-08-13">August 2023. 2022. 13 August 2023. 2022. 13 August 2023. 2022. 13 August 2023. 2023</date>
		</imprint>
	</monogr>
	<note>C0 Controls and Basic Latin CJK Symbols and Punctuation Halfwidth and Fullwidth Forms The scale of UK HE TNE 2021-22</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Chained / Linked Response Areas</title>
		<author>
			<persName><forename type="first">J</forename><surname>Watkins</surname></persName>
		</author>
		<ptr target="https://web.archive.org/web/20210731061806/https:/ideas.digitaled.com/ideas/M-I-35" />
		<imprint>
			<date type="published" when="2018-07-31">2018. 31 July 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Watkins</surname></persName>
		</author>
		<ptr target="https://web.archive.org/web/20210507134859/https:/ideas.digitaled.com/ideas/M-I-32" />
		<title level="m">Syntax Checker for student input (1D Math/plain text</title>
		<imprint>
			<date type="published" when="2018-05">2018. May 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A True Test: Toward More Authentic and Equitable Assessment</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wiggins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phi Delta Kappan</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="81" to="93" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">What is assessment for learning?</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wiliam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Studies in educational evaluation</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="14" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
