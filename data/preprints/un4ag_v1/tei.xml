<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Binary binning: examining the mapping between continuous and binary review scales WORKING PAPER: 29 th Sep 2025</title>
				<funder>
					<orgName type="full">BSREC at the University of Warwick</orgName>
				</funder>
				<funder>
					<orgName type="full">HSSREC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Neel</forename><surname>Ocean</surname></persName>
							<email>neel.ocean@warwick.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Warwick</orgName>
								<address>
									<addrLine>Gibbet Hill Road</addrLine>
									<postCode>CV4 7AL</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Vasundhara</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Warwick</orgName>
								<address>
									<addrLine>Gibbet Hill Road</addrLine>
									<postCode>CV4 7AL</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rucha</forename><surname>Paricharak</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Warwick</orgName>
								<address>
									<addrLine>Gibbet Hill Road</addrLine>
									<postCode>CV4 7AL</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Binary binning: examining the mapping between continuous and binary review scales WORKING PAPER: 29 th Sep 2025</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D61E5DA18AA5DB76CE6B954974806138</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-10-21T13:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>consumer behaviour</term>
					<term>e-commerce</term>
					<term>product ratings</term>
					<term>binary-bias</term>
					<term>judgement and decisionmaking</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Products and services are usually rated on either a five-point scale or a binary scale (i.e. positive vs negative). Using a pilot study and two between-subjects experiments, this paper investigates how individuals evaluate products differently depending on the scale used, how they implicitly categorise ratings on a five-point scale into binary bins, and how they estimate five-point distributions from binary scales. Individuals perceive products as higher in quality when ratings are presented on a binary scale, assuming reviews have been assigned to positive or negative categories based on whether they are above or below the midpoint of a five-point scale. Individuals perceive products as being of equivalent quality across scales only when ratings of four and five are taken as positive, and the remainder as negative. However, when individuals are asked to generate a five-point ratings distribution from binary ratings, they do not account for this skewed perception of positivity unless the five-point scale is labelled so that the 'neutral' point of the scale is defined as four rather than three. These findings have theoretical implications for understanding how people implicitly categorise and bin ratings, as well as practical implications for industry and policymakers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Online ratings that communicate information about product quality are in widespread use across many different consumer domains. Products are typically evaluated on one of two main scales: (1) a binary scale, where individuals rate only whether something is 'good' or 'bad'; or (2) a multi-point scale, most commonly a five-point or five-star scale. However, it is not clear how product evaluations translate between the two scales. In particular, where do individuals draw the line between a 'good' rating and a 'bad' rating on a multi-point scale? Furthermore, how do consumers value a product that has the same ratings but summarised on different scales? Understanding this appears to be crucial if we want to understand how consumers make decisions from product rating information. The present research seeks to answer this question by running two main between-subjects experiments. First, we determine how individuals bin ratings from a five-point scale into a binary scale to arrive at product evaluations that are equivalent across scales. Second, we explore the five-point rating distributions individuals generate when shown product ratings on a binary scale to determine whether the implied underlying distribution suggests that people are aware of the tendency to bin ratings in an asymmetric way.</p><p>Previous literature has assessed how individuals evaluate products from reviews and ratings, as well as the impact of ratings on purchasing behaviour. In general, people have the tendency to moan or brag about products <ref type="bibr" target="#b16">(Hu et al., 2006)</ref>, though there may be subtle cultural differences in rating behaviour <ref type="bibr" target="#b19">(Koh et al., 2010)</ref>. As such, extreme ratings are frequently perceived as more useful, possibly because they convey a less confusing signal about quality <ref type="bibr" target="#b28">(Park &amp; Nicolau, 2015)</ref>, though the extent to which this is true depends on the type of product as well as distributional characteristics <ref type="bibr" target="#b25">(Mudambi &amp; Schuff, 2010;</ref><ref type="bibr" target="#b22">Lee et al., 2021)</ref>. Ratings are crucial in determining product choice and subsequent purchasing decisions. For example, <ref type="bibr" target="#b23">Luca (2016)</ref> found that a one-star increase in Yelp ratings led to a 5%-9% increase in restaurant revenue. In general, higher valence, higher volume, and lower variance of ratings are associated with more positive consumer responses and a greater likelihood of purchase, though these effects can be moderated by factors like product type and initial valence <ref type="bibr" target="#b4">(Chevalier &amp; Mayzlin, 2006;</ref><ref type="bibr" target="#b5">Chintagunta et al., 2010;</ref><ref type="bibr" target="#b35">Sun, 2012;</ref><ref type="bibr" target="#b21">Langan et al., 2017;</ref><ref type="bibr" target="#b9">Etumnu et al., 2020)</ref>. Consumers also appear to trust negative reviews more than positive reviews, unless the volume of positive reviews is particularly large <ref type="bibr" target="#b33">(Sparks &amp; Browning, 2011;</ref><ref type="bibr" target="#b12">Gavilan et al., 2018)</ref>, which is likely to be due to a negativity bias in how ratings are perceived <ref type="bibr" target="#b2">(Baumeister et al., 2001;</ref><ref type="bibr" target="#b30">Rozin &amp; Royzman, 2001;</ref><ref type="bibr" target="#b26">Ocean, 2024)</ref>. The significance of product ratings has continued to grow due to the increased prevalence of online shopping, especially since the Covid-19 pandemic <ref type="bibr" target="#b13">(Gu et al., 2021)</ref>.</p><p>Online purchasing often involves processing a large amount of information before making a final decision. It has been well established that people use heuristics to simplify the decision making process when cognitive processing becomes too effortful or is otherwise constrained (see <ref type="bibr" target="#b16">Hjeij &amp; Vilks, 2023</ref> for a review). One such heuristic is categorical thinking, which helps to reduce the complexity of decisions by reducing cognitive load and facilitating quicker decision-making (e.g. <ref type="bibr" target="#b14">Gutman, 1982;</ref><ref type="bibr" target="#b24">Mogilner et al., 2008)</ref>. For example, doctors often simplify complex symptoms and indications into a small number of broad groups (e.g. sick vs normal) so that treatment decisions are easier to make <ref type="bibr" target="#b8">(Elstein &amp; Schwarz, 2002)</ref>; and investors group firms into distinct 'styles' to make investment decisions despite the varied nature of corporate performance <ref type="bibr" target="#b1">(Barberis &amp; Shleifer, 2003)</ref>.</p><p>One specific form of categorisation that is particularly relevant to ratings is binary bias. People exhibit a tendency to simplify signals that come from a range of possible values into two groups that indicate two distinct evaluations <ref type="bibr">(Fisher et al., 2018;</ref><ref type="bibr" target="#b10">Fisher &amp; Keil, 2018)</ref>. When they studied five-point distributions, as are common in product ratings, <ref type="bibr">Fisher et al. (2018)</ref> found that people seem to take midpoint ratings (i.e. 3) as neutral, then bin ratings above this as 'good' (i.e. 4 and 5) and ratings below this as 'bad' (i.e. 1 and 2). Therefore, when faced with a multi-point rating scale, it appears that people essentially group ratings into two categories to simplify decision making.</p><p>Perhaps in implicit acknowledgement of this phenomenon, some online platforms have implemented binary rating systems that aim to simplify both the review process and the interpretation of reviews. For example, YouTube utilises a thumbs-up/thumbs-down system for video ratings, while Netflix formerly used a similar approach for content recommendations. One comparison of binary and five-star rating systems found that although the five-star scale leads to longer decision-making time, users appear to derive greater satisfaction and intention to use the scale <ref type="bibr" target="#b3">(Chen, 2017)</ref>, which may explain its relative ubiquity in online retail. However, not all previous research agrees with this. It has been argued that binary ratings are preferable to multi-point scales because they are more likely to eliminate decision-making biases, i.e. the natural System 1 tendency for people to categorise translates more realistically to binary ratings <ref type="bibr" target="#b15">(Harvey, 2016)</ref>.</p><p>The usage of both five-point and binary rating scales in practice leads to a natural question: how do evaluations of the same product with the same ratings differ when they are presented on different scales? Do people use a symmetric binning heuristic to interpret five-point ratings in the spirit of <ref type="bibr">Fisher et al. (2018</ref><ref type="bibr" target="#b6">)? Dehaene's (2011)</ref> research on numeric cognition also appears to support a midpoint split. He found that people can envision a five-point scale spatially, and that adults tend to perceive numerical differences relatively uniformly across a scale. Hence, treating the mathematical midpoint of the range as the inflection point between good and bad appears to be a natural dividing point on a mental number line. However, other work suggests that in the case of online ratings in particular, people do not view positive and negative signals in a uniform or symmetric way. Recent work by <ref type="bibr" target="#b26">Ocean (2024)</ref> suggests that an asymmetric weighting may be being applied to ratings, with people valuing any rating of three or higher as an equally positive signal, a rating of one as a negative signal, and a rating of two as somewhere in between. This suggests that, in contrast to <ref type="bibr">Fisher et al. (2018)</ref>, the 'neutral' score on a five-point scale may be seen as two rather than three. On the other hand, the fact that it takes stronger or a higher volume of 'good' signals to offset 'bad' ones <ref type="bibr" target="#b2">(Baumeister et al., 2001)</ref> may suggest that individuals only view very high ratings as positive signals.</p><p>This implies that the neutral category may even be seen as four. Some empirical evidence supports this idea. Participants who selected "yes" on a binary response scale for a question most frequently selected the fourth point rather than the highest fifth point when asked the same question on a fivepoint Likert scale, while participants who chose the "no" option responded more neutrally (second and third points) on the five-point scale <ref type="bibr" target="#b7">(Dolnicar &amp; Gr√ºn, 2013)</ref>. This result suggests that individuals may be more likely to interpret undecidedness or neutrality as a negative response in binary terms.</p><p>The idea that the perceived neutral point is above the scale midpoint is also implicit in a customer recommendation metric known as the Net Promoter Score <ref type="bibr" target="#b29">(Reichheld, 2011)</ref>, in which scores of seven or eight over a 10-point range are considered neutral.</p><p>Other models of judgement also suggest a neutral point that may lie away from the scale midpoint. <ref type="bibr" target="#b27">Parducci's (1965)</ref> range-frequency theory finds that judgments are based on both where a signal falls within the scale range, as well as the relative frequency of signals across the range (i.e. how the signals are distributed). This suggests that a person's interpretation of a rating as positive or negative depends on both its numerical value as well as on the observed distribution of ratings. Therefore, the point at which consumers split ratings into 'good' and 'bad' categories may not be fixed, but may instead shift depending on the context and overall distribution of ratings they encounter. While this means that ratings may not have a stable and non-contextual interpretation, individuals may categorise a priori based on the ratings distributions they have been exposed to and form a reference point from which to base judgements, in a similar way to decision by sampling <ref type="bibr" target="#b34">(Stewart et al., 2006)</ref>. Empirical evidence suggests most rating distributions are J-shaped, i.e. bimodal with a large peak at five-stars and a smaller peak at one-star <ref type="bibr" target="#b16">(Hu et al., 2006</ref><ref type="bibr" target="#b17">(Hu et al., , 2009))</ref>. Theoretically, this is due to the tendency for consumers to only post reviews when they are either extremely satisfied or extremely dissatisfied because leaving a review incurs a small cost <ref type="bibr" target="#b20">(Lafky, 2014)</ref>. If consumers are desensitised to high ratings from repeated exposure, this suggests that the perceived neutral point of a five-point scale lies above the midpoint rather than below it to compensate for the high volume of positive signals. In sum, the literature offers different predictions on where consumers draw the line between 'good' and 'bad' ratings. Therefore, the first goal of the present study is to understand which ratings are categorised as positive or negative.</p><p>However, some websites additionally attach verbal labels to each point of the rating scale. This is likely to prime individuals in how they categorise ratings. For example, TripAdvisor attaches the labels "Amazing", "Very Good", "Average", "Poor" and "Terrible" to ratings of 5, 4, 3, 2, and 1 respectively. Previous research has found that labelling scales affects judgements. When different labels are used for the same point rating, the distributions obtained from each scale are different <ref type="bibr" target="#b18">(Klockars &amp; Yamagishi, 1988)</ref>. Shifting an 11-point scale down from a midpoint of 5 to a midpoint of 0 reduced the proportion of responses to a subjective life evaluation that were in the lower half of the scale by 21 percentage points <ref type="bibr" target="#b32">(Schwarz et al., 1991)</ref>. Attaching neutral labelling to a point that is above the midpoint attenuates ratings away from the extremes, while adding emotive labels to the endpoints has the opposite effect <ref type="bibr" target="#b36">(Tsekouras, 2017)</ref>. <ref type="bibr">Fisher et al. (2018)</ref> showed participants two fivepoint rating distributions for cars and asked them which car they preferred. The distribution were shown with either no labelling, bivalent labelling ('very bad' to 'very good'), or univalent labelling ('fair' to 'extremely good'). They found that preferences under bivalent labelling appear to closely correspond with no labelling, but the absence of a neutral point in univalent labelling causes participants to tend towards the product that has the higher mean rating. When ratings scales are skewed by labelling (either with more negative than positive labels, or with more positive than negative labels), people attach more importance to the label than the numeric scale point value <ref type="bibr" target="#b37">(Wildt &amp; Mazis, 1978)</ref>. Furthermore, a scale with a predominance of negative labels has more impact than a scale with a predominance of positive labels because people are more reluctant to use the negative categories. Therefore, the ratings will be biased upwards <ref type="bibr" target="#b37">(Wildt &amp; Mazis, 1978)</ref>. Overall, the findings on scale labelling suggest that binning is dependent on conceptual frameworks and not statistical properties of the distribution, i.e. binary categorisation depends on where the perceived neutral point is <ref type="bibr">(Fisher et al., 2018)</ref>.</p><p>While it is clear that scale labelling affects rating behaviour, we still do not fully understand whether people can translate binary ratings back into a five-point distribution in a way that shows that they understand how other reviewers would bin ratings that were originally from a five-point scale. Given that prior research suggests that people may not create binary categories from a multipoint scale in an objectively neutral way (i.e. by splitting at the midpoint of the scale), could we assign a labelling scheme to the resulting multi-point distribution so that the generated rating distributions account for the asymmetric split point for binary binning? This is the second research question that we aim to answer in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pilot Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Materials &amp; Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>A total of 180 participants were recruited in Feb 2023 from Prolific Academic and paid ¬£0.75 each for task completion. Participants were English speakers over 18 years of age. No additional demographic data were collected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure</head><p>As an exploratory investigation into how product evaluations differ when ratings are provided in the form of a binary scale, relative to when they are provided on a five-point scale, we conducted a pilot study. Using a similar format to Ocean (2024), we provided respondents with a series of ten products: a chest of drawers; a fiction novel; wireless earbuds; a smartphone; a movie; a video game; a restaurant, a hotel, a digital camera; an air fryer. These represent an assortment of both search goods and experience goods. Participants were shown an image of the product, followed by ratings information. They were subsequently asked to evaluate product quality on a 0-100 scale and purchase intentions on a seven-point Likert scale ranging from "Extremely unlikely to purchase" to "Extremely likely to purchase".</p><p>In the control condition, we presented a typical five-star ratings distribution under each product. This included the mean rating, the total number of ratings, and the percentage of ratings of each score from one to five. The overall mean of the means for each product was 4.04 stars. In the treatment condition, we converted this ratings distribution into a binary "thumbs up" format by splitting the ratings at the midpoint of the scale so that 4-star and 5-star ratings were treated as a "thumbs-up" and 1-star and 2-star ratings were treated as a "thumbs-down". Ratings of 3-stars were divided equally between the two categories. The full experimental script can be found in the OSF repository for this study: <ref type="bibr">[redacted]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Discussion</head><p>Linearly mapping the mean for all 10 products to the 0-100 point quality scale in a na√Øve fashion (i.e. ùëûùë¢ùëéùëôùëñùë°ùë¶ = 25ùúá -25) suggests that the mean evaluation of product quality across the whole experiment should be approximately 76 if participants were simply mapping the mean rating directly to evaluate product quality. Mean product quality was 63.4 (n = 98, 95% confidence interval = [61.2, 65.6]) in the control (5-star) condition and 73.1 (n = 82, 95% confidence interval = <ref type="bibr">[71.4, 74.8]</ref>) in the treatment (thumbs-up) condition. A two-tailed two sample t-test of independent means yielded p &lt; 0.00005 and Cohen's d = 1.02. Similarly, for purchase likelihood on a 1 -7 scale, the means for each condition were 4.24 (95% <ref type="table">confidence interval = [4.07, 4.40])</ref> and<ref type="table">4.65 (95% confidence interval = [4.45,   4.85]</ref>) respectively, yielding a t-test p-value of p = 0.002 and effect size d = 0.472. The means and confidence intervals are plotted in Figure <ref type="figure" target="#fig_0">1</ref>. The standard errors are likely to be underestimates due to the fact that variances for evaluations of the same product are likely to be correlated even across individuals. However, an OLS regression controlling for product fixed effects and clustering standard errors by product also finds significant treatment effects for both perceived quality (ùõΩ / = 9.73, p = 0.0004) and purchase intentions (ùõΩ / = 0.411, p = 0.0058). We do not report regression results in detail for the pilot, however we do so later for our main studies.</p><p>Overall, the pilot results suggest that there is strong initial evidence to support the fact that products that are rated on an 'equivalent' binary scale are evaluated more highly than when they are rated on a 5-star scale. In other words, this suggests that it is likely that people do not treat the ratings scale linearly. The results imply that for participants to value the products equally in both cases, one would need to reduce the 'bin size' for what constitutes a "thumbs-up" because only the uppermost ratings on a numerical scale are treated as signals of high quality. However, we do not yet know where the 'cut point' lies such that products in the two rating systems are evaluated equivalently. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure</head><p>Study 1 was designed to test the threshold at which individuals categorise ratings from a continuous 5-star scale into binary 'good' and 'bad' bins to determine product quality. The flow of the study was the same as the pilot. Participants were sequentially presented with the following 10 products: Bluetooth speakers; chest of drawers; air fryer; electric kettle; book; badminton set; body lotion; shoes; alarm clock; laptop bag. In contrast to the pilot, we focused on retail products rather than including experiences such as hotels, which may be subject to a slightly different evaluation process. Each product screen contained a brief product name along with an image of the product, and finally the associated ratings information. Participants were asked to rate perceived product quality on a 0-100 scale as in the pilot study and in <ref type="bibr" target="#b26">Ocean (2024)</ref>. They were also asked to state their likelihood of purchasing the product based on the displayed information using a 7-point Likert scale.</p><p>The use of these two distinct measures allowed us to capture both people's perception of inherent product quality, as well as the personal preferences that drive purchase intentions. The order of products was randomly selected for each participant. Participants also provided demographic information (age and gender) before completing the main product evaluation task. The full experimental script can be found in the OSF repository for this study: <ref type="bibr">[redacted]</ref> We omitted information on price and branding so that evaluations would be based solely on ratings, and ensured that there was enough variability in average ratings and ratings distributions to cover a variety of contexts. The total volume of ratings was fixed at 200 across all products to avoid interactions between ratings volume and the relative weighting applied to a rating (people are likely to place higher weight on a given ratings distribution being an accurate signal of product quality when that distribution is being generated by a high volume of total ratings).</p><p>Study 1 extended the general design of the pilot by introducing different binary split-points.</p><p>Participants were randomly assigned to one of four conditions. In the control condition, participants were shown review information using the standard five-star format. The mean rating was shown, along with the total volume and the percentage distribution of ratings across the five points of the scale. In the mid-split condition, we replicated the treatment from the pilot study to convert the fivepoint scale ratings into a 'thumbs-up' rating by taking ratings of four and five and half of the threestar ratings as positive. In addition to the mid-split condition from the pilot, we added two further treatments where we changed the split point, i.e. the point at which we draw the line between 'positive' and 'negative' ratings. The low-split condition took all ratings of three and above as positive, while the high-split condition took only ratings of four and above as positive. These conditions explore the potential for individuals' subjective neutral point to lie either side of the midpoint of the scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We first computed the mean reported product quality and mean likelihood of purchase across all 10 products per individual. These means are plotted in Figures <ref type="figure" target="#fig_1">2a</ref> and<ref type="figure" target="#fig_1">2b</ref>, along with 95% confidence intervals. Overall mean perceived quality</p><p>As discussed in the analysis for the pilot study, a na√Øve t-test is likely to contain biased standard errors because of the experiment design involving multiple product evaluations per individual.</p><p>Therefore, to determine average treatment effects on perceived product quality and on purchase likelihood more formally, we estimated linear regression models. The base specification of the model is:</p><formula xml:id="formula_0">ùë¶ !" = ùõΩ # + ùõΩ $ ùë°ùëüùëíùëéùë°ùëöùëíùëõùë° !" + ùõΩ % ùëùùëüùëúùëëùë¢ùëêùë° !" + ùõΩ &amp; ùëã !" + ùúñ !"</formula><p>for i = 1, 2, ‚Ä¶, 554 (individuals) and j = 1, 2, ‚Ä¶, 10 (products), where ùë¶ !" represents the dependent variable (either perceived quality or purchase likelihood) and ùëã !" is a matrix of demographic variables (i.e. age and gender in the present case).</p><p>We used two different estimation methods to correct standard errors because of the nested structure of the data (i.e. ten products per individual). First, we used OLS with robust standard errors that were clustered by product (allowing for error terms to be correlated within the same product).</p><p>Second, we estimated a mixed effects model with a random intercept for each participant. This adds a separate error term ùë¢ ! to the model specification to account for individual specific effects.<ref type="foot" target="#foot_0">1</ref> Both estimation methods give identical parameter estimates because they are both linear models that are estimating the same specification. However, because the error terms are modelled differently, standard errors will differ across the approaches. The results using both approaches are shown in Table <ref type="table" target="#tab_0">1</ref>.</p><p>Table <ref type="table" target="#tab_0">1</ref> shows that regardless of the estimation method used, and regardless of whether we include additional demographic controls, we see a similar pattern for treatment effects. Perceived quality is deemed to be significantly greater when ratings are displayed in a binary fashion, compared with when they are displayed on a 5-star scale, but only when the ratings are split at or below the mid-point of the scale. When ratings are split at the mid-point, perceived quality is approximately 9 points higher. When ratings are split between 2-stars and 3-stars, perceived quality is approximately 16 points higher. However, there is no significant difference between quality using binary and 5-star scales in the high-split condition, i.e. when ratings are binned into positive only above 3-stars. This pattern is preserved even when we use purchase likelihood as the dependent variable instead of perceived quality. Notes: (1)-( <ref type="formula">4</ref>) estimate treatment effects with OLS with robust standard errors clustered by product. ( <ref type="formula">5</ref>)-( <ref type="formula">8</ref>) estimate treatment effects using a mixed (multilevel) model with a random intercept per individual. *** indicates p &lt; 0.01. The dependent variables are: Quality = perceived product quality on 0-100 scale; Likely = likelihood of purchasing product based on ratings information on 7-point Likert scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>Study 1 sought to investigate how consumers mentally categorize product ratings and determine the optimal mapping of continuous rating scales to binary categories. The results clearly and unambiguously suggest that products are evaluated equivalently regardless of whether the ratings are displayed in binary or 5-star formats only when the binary display uses a cut-point that is above the midpoint of the scale. This suggests that when consumers see five-point ratings distributions, it is likely that they are categorising 4-star and 5-star ratings as positive, and any rating below 4-stars as negative. This is consistent with <ref type="bibr" target="#b7">Dolnicar and Gr√ºn's (2013)</ref> finding that individuals regard Likert responses around the midpoint of a five-point scale as equivalent to negative responses on a binary response scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study 2</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Materials &amp; Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>We recruited 554 English speakers over 18 years of age via Prolific in Summer 2024 (275 male, 254 female, 5 non-binary, 3 preferred not to say, 1 non-response). Mean age was 30. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure</head><p>Study 2 was designed to answer two questions. First, we wanted to understand how consumers translate a binary ratings profile back into a five-point distribution to establish what kind of rating distribution individuals envisage as forming a particular binary categorisation. Second, we wanted to establish how changing the labelling on a five-point scale to change the perceived neutral point of the scale can moderate this mapping, so that we can find the scale interpretation that is closest to the high-split binning observed in Study 1.</p><p>As in Study 1, each participant was shown 10 different product screens. Each screen contained the product name and an image, along with rating information that showed the percentage of positive and negative reviews. Participants were told that the products were originally rated on a five-point scale but this had been converted to a binary format for presentation purposes. The task was for participants to try to reconstruct the original ratings distribution by allocating 100 ratings to each of the five points of the scale. They did this by moving five independent sliders that were constrained to a sum of 100. For convenience, the original distributions we converted were the same as the 10 distributions constructed for the control condition of Study 1, though we attached these distributions to 10 new products. The full list of products and experiment script can be found in the OSF repository for this study: <ref type="bibr">[redacted]</ref>. The distributions were converted using the neutral mid-split method from Study 1 (participants were not given any information on how the ratings were converted). The resulting positive to negative rating ratios for each of the 10 products using this method were: (1) 85:15, (2) 72:28, (3) 58:42, (4) 44:56, (5) 28:72, (6) 76:24, (7) 97:3, (8) 18:82, (9) 78:22, (10) 43:57. With a midpoint split, we might expect that people's reconstruction of the five-point scale would be skewed more towards higher ratings than the source distribution. This is because individuals may not consider that some ratings at the midpoint could spill over into either of the binary bins if there is a two-way equivalence between ratings of only four and five being interpreted as positive. If this is true, then the mean of the reconstructed distribution should therefore also be greater than the source distribution. On the other hand, if the mean of the reconstructed distribution is not significantly different from the mean of the source distribution, then it is possible that individuals exhibit a skew when categorising multiple points into binary bins, but do not account for this when going in the other direction, i.e. when expanding binary categories to a multi-point scale.</p><p>To test the second question, we randomly allocated participants to four groups. The control condition asked participants to expand the binary ratings to a typical unlabelled five-star rating scale.</p><p>We then designed three treatment conditions to determine whether verbal labelling of scales can moderate the reconstructed distribution mapping. The verbal-balanced condition replaced the five points with the following set of five labels (from best rating to worst rating): {amazing, very good, average, poor, terrible}. These labels were chosen as they correspond to those used on TripAdvisor.com, and emphasise that the neutrality of the midpoint. Specifically, the top two scale points are assigned positively-valenced labels, the middle scale point is assigned a neutral label, and the bottom two scale points are assigned negatively-valenced labels. This treatment was designed to be a baseline verbal conversion from the 5-star scale, and so we did not expect any difference in reconstructed distributions between this condition and the control condition. The verbal-negative treatment replaced the five points with the following set of five labels (from best rating to worst rating): {very good, This means that while providing a balanced scale is likely to lead to a reconstructed distribution that is more positively-valenced than the original (i.e. with a higher mean), providing a shifted scale may lead to a less biased reconstruction of the source distribution. In particular, by shifting the neutral point down as in the verbal-positive treatment, positive ratings may be more evenly distributed across the range, thereby lowering the reconstructed mean score and bringing it closer to the original mean rating than what we would expect from the other three treatment conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Figure <ref type="figure" target="#fig_4">3</ref> plots the average estimated distributions for each product under each treatment condition alongside the source distribution from which the binary rating was obtained. Overall, we see that participants tend to translate binary scores into unimodal distributions, regardless of the labelling used for the rating scale. This means that participants did not seem to recognise the tendency for people to either 'brag' or 'moan' and therefore did not produce a bimodal density function concentrated at the extremes. Although they were reasonably accurate in approximating the spread of opinions when the positive to negative ratio was relatively disparate, they were not so accurate in judging distributions when the ratio was similar (i.e. for products with a mixed opinion, such as Product 3). Figure <ref type="figure" target="#fig_4">3</ref> suggests that people are likely to interpret a relatively even ratio of positive to negative ratings as a mostly uniform spread across the range rather than as a bipolar distribution.</p><p>Participants also did not recognise the prevalence of the J-shape in ratings distributions but did tend to generate skewed distributions. Table <ref type="table" target="#tab_1">2</ref> shows the first three moments of the source and estimated distributions for each product as well as for the averaged distribution across all products. The verbal-positive and verbal-negative treatments skew the estimated distributions further from the skew of the averaged source distribution. We can also see this visually in Figure <ref type="figure" target="#fig_5">4</ref>, which plots the densities of the averaged distributions over all products for each treatment. While the estimated mean of the distribution is slightly higher in the control condition than the source distribution, the mean for the verbal-balanced condition is slightly lower overall. Therefore, we do not observe a consistently upwards biased mean when binary ratings are converted to a 5-point scale, apart from in the verbal-negative condition. In this treatment, only Product 7 had a lower estimated mean than the original source distribution, and this is likely because of noise at the extremes.  Note: con = control, v-b = verbal-balanced, v-n = verbal-neutral, v-p = verbal-positive. Ratios in parentheses represent the percentages of 'thumbs-up' relative to 'thumbs-down shown to participants for each product. To systematically analyse the differences between source and estimated distributions, we conducted Kolmogorov-Smirnov (K-S) tests. Table <ref type="table" target="#tab_0">A1</ref> presents the K-S test statistics for each product, under the null hypothesis that the source distribution is not significantly different from the generated distribution. Higher test statistics (lower p-values) indicate greater divergence from the original source distributions. The results suggest that on average, participants generated distributions that were closer to the source when asked to do so on the regular five-point scale, compared with any of the verbally labelled scales. Figures <ref type="figure" target="#fig_0">A1</ref> and<ref type="figure" target="#fig_1">A2</ref> plot the source and estimated cumulative distributions for each product and treatment, as well as for each treatment overall.</p><formula xml:id="formula_1">v-b v-n v-p source con v-b v-n v-p source con v-b v-n v</formula><p>Finally, we compared the proportion of positive ratings participants were shown relative to their estimated distributions to see how people implicitly categorise ratings into positive and negative, and subsequently translate this into a distribution. Table <ref type="table" target="#tab_3">3</ref> shows the proportion of 'thumbs-up' ratings  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>Study 2 finds that participants appear to be poor at recreating the source distribution when shown only a binary version of it, apart from in cases where it is unimodal and skewed towards higher ratings. Their tendency to generate smooth distributions suggests that individuals tend to underestimate the degree of noise / variation in other people's opinions. Attaching verbal labels to points of the scale instead of numerical values did little to change estimated distributions when the valence of the words was kept neutral. However, verbally skewing the labels appeared to worsen rather than improve fit. This suggests that recalibrating the perceived midpoint of the scale to account for the asymmetry in binary bias observed in Study 1 does not seem to improve correspondence between estimated and source distributions (although it may do in specific cases).</p><p>Second, if our finding from Study 1 were also to hold in reverse, then when asked to map binary ratings to a five-point distribution, we would expect individuals to assign positive reviews a rating of either four or five and negative reviews a rating of one, two, or three. The results from Study 2</p><p>showed that this happens only in the verbal-negative treatment, i.e. when the neutral point scale was deliberately shifted so that the fourth point on the scale was labelled as 'average'. This generates an interesting and slightly paradoxical conclusion. When people see a distribution of ratings on a fivepoint scale, they consider only ratings of four or five to be positive signals of quality. However, when people see ratings on a binary scale, then they do not appear to infer that binning would occur anywhere other than the midpoint unless we explicitly define a non-midpoint split point by labelling the scale so that the neutral point lies away from the midpoint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>General Discussion</head><p>The present studies set out to investigate the phenomenon that consumers appear to value products differently based on whether ratings are provided to them on a five-point scale or on a binary (i.e. positive and negative) scale. Overall, there are two main findings. First, there is a consistent tendency to value a product more highly when ratings are presented on a binary scale as opposed to a five-point scale when the mapping from five-point to binary is exactly at the midpoint (i.e. when ratings of one and two are considered negative, ratings of four and five are considered positive, and ratings of three are allocated evenly between positive and negative). Furthermore, we find that the binary mapping that results in equivalent product valuations is above the midpoint. In other words, when ratings of only 4 and 5 are considered as 'positive', products will be valued equivalently between the two scales. This implies that consumers view a rating at the midpoint of a scale as negative rather than neutral. This finding remained consistent after accounting for individual differences and demographic factors.</p><p>Second, our findings suggest that people do not anticipate or account for this 'above the midpoint' bias when presented with binary ratings to begin with. Instead, when consumers are provided with only binary ratings, they appear to implicitly assume that the underlying distribution considers a positive review to be anything at the midpoint of the scale or above. Only when we shift the valence of the five-point scale downwards by labelling a rating of four as 'average', therefore explicitly defining four as a neutral point, are individuals correctly able to adjust for this bias and generate a distribution that more closely represents how others actually categorise positive and negative ratings.</p><p>These findings contribute to the literature on binary bias (e.g. <ref type="bibr">Fisher et al., 2018)</ref> as well as to the wider understanding of categorical thinking in consumer behaviour that illustrates how simplification strategies might influence product evaluations and purchase intentions (e.g. <ref type="bibr" target="#b14">Gutman, 1982;</ref><ref type="bibr" target="#b24">Mogilner et al., 2008)</ref>. Prior work by <ref type="bibr" target="#b26">Ocean (2024)</ref> found evidence that the weights placed on ratings of three, four, and five on a five-point scale implied that they sent a broadly equivalent positive signal of product quality to consumers. However, the fact that individuals who see only binary reviews that are generated by splitting the five-point scale at the midpoint to generate 'positive' and 'negative' bins leads individuals to perceive higher product quality than when viewing the entire distribution suggests that this symmetric binning overestimates the number of positive signals of product quality that the full distribution is sending. This asymmetry may be a form of negativity bias <ref type="bibr" target="#b30">(Rozin &amp; Royzman, 2001)</ref> because it suggests that consumers require more stronglyvalenced positive signals to offset any indications of low product quality.</p><p>It appears that industry has already implicitly assumed this phenomenon to be true. For example, Reichheld's Net Promoter Score for company recommendations only considers scores of 9 or 10 on a 10-point scale to be a positive recommendation, score of 7 or 8 to be neutral, and any score below 7 to be a negative recommendation <ref type="bibr" target="#b29">(Reichheld, 2011)</ref>. Businesses and customers already seem to be attuned to the idea that truly positive ratings require surpassing a threshold higher than the numerical midpoint of a scale <ref type="bibr" target="#b19">(Koh et al., 2010)</ref>. <ref type="bibr" target="#b31">Schoenmueller et al. (2020)</ref> conducted a comprehensive examination of online product ratings and found evidence of "ratings bubbles" -the fact that average product ratings have consistently increased over time across several platforms, leading to a compression of ratings at the top of the scale. Whether or not consumers realise that the reason for this is likely to be because of a self-selection bias in reviewers <ref type="bibr" target="#b31">(Schoenmueller et al., 2020)</ref>, i.e. the tendency to either 'moan' or 'brag' about products (e.g. <ref type="bibr" target="#b16">Hu et al., 2006</ref>), it appears that they have come to expect high ratings as the norm.</p><p>There is an asymmetry implied by our results, which suggests different courses of action depending on the goal. Retailer websites that wish to summarise ratings information and also benefit from product sales will have an incentive to bin reviews into positive and negative by splitting exactly at the midpoint, because this will increase product valuation. This also highlights possible areas for regulatory involvement, as firms may use this fact to mislead consumers into making undesirable purchases. From a policy perspective, improving consumer welfare in online shopping may involve requiring firms to provide clear visual or verbal indicators to highlight the difference between ratings above and below the perceived inflection point, or perhaps mandating the provision of different rating summaries at different stages of the online shopping process <ref type="bibr" target="#b3">(Chen, 2017)</ref>. On the other hand, review aggregator websites that benefit from user engagement rather than product sales have more of an incentive to maintain unbiasedness. Therefore, they should choose to emphasise the inherent skew in the interpretation of the rating scale, either by binning above the midpoint if converting to binary, or labelling rating scales to emphasise a neutral point that is above the numerical midpoint of the scale. Finally, consumers may benefit just by being aware of this bias in others' ratings because they will be better able to infer the true perceived valuation of a product regardless of which rating scale is used.</p><p>The main limitation with the present studies is that there was no incentive compatibility or actual purchase behaviour inherent within the design. Therefore, although we think that our results provide valuable insight into consumer psychology and judgements, actual behaviour may differ in field settings based on context, especially when high value purchases are being made. Future research would benefit from website data on purchases or click-through rates to more directly analyse the impact of rating presentation on consumer activity. Our studies used a broad set of products categories that spanned commonly purchase product types on online shopping websites, but did not specifically seek to test differences in effect across different product categories. Future studies may seek to assess whether the effects we observed hold across a broader range of product types, particularly for experience goods versus search goods, as the influence of ratings may differ between these categories (e.g. <ref type="bibr" target="#b25">Mudambi &amp; Schuff, 2010)</ref>. Furthermore, our experimental design presented ratings in isolation. In real-world e-commerce settings, ratings are frequently complemented by written reviews. Future research could look into whether the interaction between ratings and written reviews changes how ratings are translated between binary and continuous scales. The studies did not investigate cultural background or familiarity with online purchasing. While our aim was to understand consumer behaviour more generally, we accept that there may be differences in perception of scales and rating behaviour across cultures <ref type="bibr" target="#b19">(Koh et al., 2010)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>Overall, this paper finds that when a five-point rating scale is translated into a binary scale by binning ratings into two groups (i.e. positive reviews and negative reviews) with a neutral point at the midpoint of the scale, individuals will perceive the product as having higher quality when they assess the product from the binary classification than when they assess the product from the fivepoint distribution. To translate the five-point scale to a binary scale without affecting product evaluations, one must classify only ratings of four and five as positive. Furthermore, people do not seem to be aware of this asymmetry if they are asked to estimate a five-point rating distribution when shown ratings on a binary scale. Instead, they generate distributions that imply they are taking the midpoint of the scale as the cut point between positive and negative ratings. Individuals will only be able to identify an asymmetric cut point if the five-point scale they are asked to generate assigns verbal labels to each point of the scale that suggest a rating of four, not three, is neutral.</p><p>These findings highlight an important behavioural bias in the interpretation of online reviews that potentially has significant implications for both consumer welfare as well as firm behaviour. The findings are of particular relevance to policymakers and regulators seeking to understand consumer biases to minimise consumer exploitation by firms. Future research should seek to verify these findings in secondary data from e-commerce websites that actually captures real-world decisions in the field, as well as understanding how well our result generalises across product categories, different types of consumers, as well as across cultures. Finally, psychological research should seek to determine whether this phenomenon reproduces when the number of points in the multi-point scale is increased. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Results from pilot study. Overall means and confidence intervals across all 10 products by treatment. Each panel refers to a different dependent variable: (a) perceived product quality; (b) purchase likelihood.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Results from Study 1. (a) Overall mean perceived product quality across all 10 products per participant. (b) Overall mean likelihood of purchase over all 10 products per participant. Bars represent 95% confidence intervals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>6 (min = 18, max = 70, ùúé = 10.98). As in Study 1, we aimed for a minimum group size of 105. Participants were randomly allocated to one of four conditions, and were distributed as follows: control = 135, verbal-balanced = 135, verbal-negative = 136, verbal-positive = 132. Participants were each paid ¬£1 for task completion. Mean completion time was 8.42 minutes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>average, poor, terrible, atrocious}. The verbal-negative treatment takes the verbal-balanced treatment and shifts the neutral label up to the fourth point of the scale. In effect, this corresponds more closely to how individuals appeared to categorise a five-point distribution into binary bins in Study 1. Finally the verbal-positive treatment replaced the five points with the following set of five labels (from best rating to worst rating): {incredible, amazing, very good, average, poor}. The verbal-positive treatment shifts the neutral label down to the second point of the scale.These treatments are somewhat analogous to the treatments in Study 1 where we manipulated the split point in the conversion of 5-star to binary ratings. The results from Study 1 suggest that people are binning ratings into positive and negative categories by splitting above the mid-point of the scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Participant estimated source distributions from binary ratings, by treatment (columns) and by product (rows)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The overall average of the reconstructed distributions by treatment condition, relative to the average source distribution</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>shown for each product, and the mean percentage of positive ratings derived from the estimated distributions. We use three different definitions of positive, based on the three different split points used in Study 1. The high-split takes only 4-star and 5-star ratings as positive, the mid-split also includes half of the 3-star ratings, and the low-split takes all 3-star ratings and above as positive. The results show that in the neutrally-valenced conditions (control, verbal-balanced) the estimated distributions follow the mid-split definition of a positive rating most closely. In the verbal-negative treatment, estimated distributions follow the high-split definition of a positive rating most closely. In the verbal-positive treatment, estimated distributions follow the low-split definition of a positive rating most closely. The average 'thumbs-up' to 'thumbs-down' ratio across all 10 products of 60:40 corresponds most closely to the average ratio observed in the distributions generated under the verbal-negative treatment of 62:38.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure</head><label></label><figDesc>Figure A2: Cumulative distribution functions per condition, averaged across all products</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 : Linear regressions estimating treatment effects in Study 1 OLS (with s.e. clustered by product) Mixed model (with random intercept per person)</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell>(1)</cell><cell>(2)</cell><cell>(3)</cell><cell>(4)</cell><cell>(5)</cell><cell>(6)</cell><cell>(7)</cell><cell>(8)</cell></row><row><cell>Dependent variable:</cell><cell cols="3">Quality Quality Likely</cell><cell>Likely</cell><cell>Quality</cell><cell>Quality</cell><cell>Likely</cell><cell>Likely</cell></row><row><cell>Mid-split</cell><cell cols="5">9.179*** 9.204*** 0.444*** 0.458*** 9.179***</cell><cell>9.204***</cell><cell>0.444***</cell><cell>0.458***</cell></row><row><cell></cell><cell>(1.294)</cell><cell>(1.286)</cell><cell>(0.117)</cell><cell>(0.119)</cell><cell>(1.173)</cell><cell>(1.178)</cell><cell>(0.101)</cell><cell>(0.100)</cell></row><row><cell>Low-split</cell><cell cols="5">15.97*** 15.97*** 0.861*** 0.868*** 15.97***</cell><cell>15.97***</cell><cell>0.861***</cell><cell>0.868***</cell></row><row><cell></cell><cell>(3.040)</cell><cell>(3.050)</cell><cell>(0.228)</cell><cell>(0.229)</cell><cell>(1.195)</cell><cell>(1.199)</cell><cell>(0.103)</cell><cell>(0.102)</cell></row><row><cell>High-split</cell><cell>-1.832</cell><cell>-1.805</cell><cell cols="2">-0.0131 -0.0045</cell><cell>-1.832</cell><cell>-1.805</cell><cell>-0.0131</cell><cell>-0.0045</cell></row><row><cell></cell><cell>(2.767)</cell><cell>(2.749)</cell><cell>(0.129)</cell><cell>(0.130)</cell><cell>(1.164)</cell><cell>(1.168)</cell><cell>(0.100)</cell><cell>(0.0994)</cell></row><row><cell>Includes fixed effects per product</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>Includes controls for age and gender</cell><cell>No</cell><cell>Yes</cell><cell>No</cell><cell>Yes</cell><cell>No</cell><cell>Yes</cell><cell>No</cell><cell>Yes</cell></row><row><cell>Constant</cell><cell cols="5">71.52*** 70.78*** 4.809*** 5.044*** 71.52***</cell><cell>70.78***</cell><cell>4.809***</cell><cell>5.044***</cell></row><row><cell></cell><cell>(0.794)</cell><cell cols="3">(1.037) (0.0741) (0.0931)</cell><cell>(0.979)</cell><cell>(1.589)</cell><cell>(0.0829)</cell><cell>(0.135)</cell></row><row><cell>Random intercept variance</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>79.48</cell><cell>79.17</cell><cell>0.600</cell><cell>0.583</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(5.957)</cell><cell>(5.938)</cell><cell>(0.0440)</cell><cell>(0.0430)</cell></row><row><cell>Observations</cell><cell>5540</cell><cell>5540</cell><cell>5540</cell><cell>5540</cell><cell>5540</cell><cell>5540</cell><cell>5540</cell><cell>5540</cell></row><row><cell>R-squared</cell><cell>0.657</cell><cell>0.657</cell><cell>0.515</cell><cell>0.52</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Number of groups</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>554</cell><cell>554</cell><cell>554</cell><cell>554</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 : Moments of estimated distributions relative to source for each treatment</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell>Mean</cell><cell>Standard Deviation</cell><cell>Skewness</cell></row><row><cell>Product source</cell><cell>con</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 : Mean proportion of 'positive' ratings in Study 2 estimated distributions based on the cut points from Study 1</head><label>3</label><figDesc></figDesc><table><row><cell>Product</cell><cell>Split point</cell><cell>control (% +ve)</cell><cell>verbal-balanced (% +ve)</cell><cell>verbal-negative (% +ve)</cell><cell>verbal-positive (% +ve)</cell></row><row><cell>1 (85% +ve)</cell><cell>High-split</cell><cell>77.24</cell><cell>73.67</cell><cell>84.93*</cell><cell>59.59</cell></row><row><cell></cell><cell>Mid-split</cell><cell>82.62*</cell><cell>80.87</cell><cell>88.76</cell><cell>70.41</cell></row><row><cell></cell><cell>Low-split</cell><cell>88.01</cell><cell>88.07*</cell><cell>92.58</cell><cell>81.23*</cell></row><row><cell>2 (72% +ve)</cell><cell>High-split</cell><cell>66.46</cell><cell>62.81</cell><cell>75.88*</cell><cell>44.32</cell></row><row><cell></cell><cell>Mid-split</cell><cell>73.84*</cell><cell>71.24*</cell><cell>81.61</cell><cell>56.17</cell></row><row><cell></cell><cell>Low-split</cell><cell>81.21</cell><cell>79.68</cell><cell>87.34</cell><cell>68.03*</cell></row><row><cell>3 (58% +ve)</cell><cell>High-split</cell><cell>45.98</cell><cell>39.84</cell><cell>59.89*</cell><cell>28.45</cell></row><row><cell></cell><cell>Mid-split</cell><cell>55.70*</cell><cell>52.56*</cell><cell>68.92</cell><cell>40.37</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We also tested a model that included random intercept and slope parameters, however a likelihood ratio test found that the slope term did not add additional explanatory power.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>Neel Ocean thanks <rs type="person">Kajim Hussein</rs> for pilot study data collection. Ethical approval for the pilot study was granted by <rs type="funder">BSREC at the University of Warwick</rs>. Ethical approval for the main studies was granted by <rs type="funder">HSSREC</rs> via the <rs type="institution">Psychology Department at the University of Warwick</rs>. Data for all studies is available at: [redacted]</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Note: The numbers are the percentage of &apos;positive&apos; ratings rounded to 2.d.p. For high-split, positive ratings are defined as either 4-star or 5-star. For mid-split, positive ratings are defined as 4-star, 5-star, and half of all 3-star ratings. For low-split, positive ratings are defined as 3-star, 4-star, or 5-star. Percentages in parentheses next to product numbers represent the percentages of &apos;thumbs-up</title>
		<idno>Low-split 66.37 65.98 78.30 56.01* n 135 135 136 132</idno>
		<imprint/>
	</monogr>
	<note>reviews as shown to participants in the task. * represents the split that most closely matches the ratio that participants were given. References</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<author>
			<persName><forename type="first">N</forename><surname>Barberis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shleifer</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0304-405X(03)00064-3</idno>
		<ptr target="https://doi.org/10.1016/S0304-405X(03)00064-3" />
	</analytic>
	<monogr>
		<title level="m">Style investing</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="161" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bad is stronger than good</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Baumeister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bratslavsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Finkenauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Vohs</surname></persName>
		</author>
		<idno type="DOI">10.1037/1089-2680.5.4.323</idno>
		<ptr target="https://doi.org/10.1037/1089-2680.5.4.323" />
	</analytic>
	<monogr>
		<title level="j">Review of General Psychology</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="323" to="370" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Five-star or thumbs-up? The influence of rating system types on users&apos; perceptions of information quality, cognitive effort, enjoyment and continuance intention</title>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1108/IntR-08-2016-0243</idno>
		<ptr target="https://doi.org/10.1108/IntR-08-2016-0243" />
	</analytic>
	<monogr>
		<title level="j">Internet Research</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="478" to="494" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The Effect of Word of Mouth on Sales: Online Book Reviews</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Chevalier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mayzlin</surname></persName>
		</author>
		<idno type="DOI">10.1509/jmkr.43.3.345</idno>
		<ptr target="https://doi.org/10.1509/jmkr.43.3.345" />
	</analytic>
	<monogr>
		<title level="j">Journal of Marketing Research</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="345" to="354" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The Effects of Online User Reviews on Movie Box Office Performance: Accounting for Sequential Rollout and Aggregation Across Local Markets</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Chintagunta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gopinath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venkataraman</surname></persName>
		</author>
		<idno type="DOI">10.1287/mksc.1100.0572</idno>
		<ptr target="https://doi.org/10.1287/mksc.1100.0572" />
	</analytic>
	<monogr>
		<title level="j">Marketing Science</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="944" to="957" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Dehaene</surname></persName>
		</author>
		<title level="m">The Number Sense: How the Mind Creates Mathematics, Revised and Updated Edition</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Translating&quot; between survey answer formats</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dolnicar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gr√ºn</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jbusres.2012.02.029</idno>
		<ptr target="https://doi.org/10.1016/j.jbusres.2012.02.029" />
	</analytic>
	<monogr>
		<title level="j">Journal of Business Research</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1298" to="1306" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Clinical problem solving and diagnostic decision making: Selective review of the cognitive literature</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Elstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schwarz</surname></persName>
		</author>
		<idno type="DOI">10.1136/bmj.324.7339.729</idno>
		<ptr target="https://doi.org/10.1136/bmj.324.7339.729" />
	</analytic>
	<monogr>
		<title level="j">BMJ</title>
		<imprint>
			<biblScope unit="volume">324</biblScope>
			<biblScope unit="issue">7339</biblScope>
			<biblScope unit="page" from="729" to="732" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Does the distribution of ratings affect online grocery sales? Evidence from Amazon</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Etumnu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">O</forename><surname>Widmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Lusk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Ortega</surname></persName>
		</author>
		<idno type="DOI">10.1002/agr.21653</idno>
		<ptr target="https://doi.org/10.1002/agr.21653" />
	</analytic>
	<monogr>
		<title level="j">Agribusiness</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="501" to="521" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The Binary Bias: A Systematic Distortion in the Integration of Information</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Keil</surname></persName>
		</author>
		<idno type="DOI">10.1177/0956797618792256</idno>
		<ptr target="https://doi.org/10.1177/0956797618792256" />
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1846" to="1858" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Seeing Stars: How the Binary Bias Distorts the Interpretation of Customer Ratings</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dhar</surname></persName>
		</author>
		<idno type="DOI">10.1093/jcr/ucy017</idno>
		<ptr target="https://doi.org/10.1093/jcr/ucy017" />
	</analytic>
	<monogr>
		<title level="j">Journal of Consumer Research</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The influence of online ratings and reviews on hotel booking consideration</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gavilan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Avello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Martinez-Navarro</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tourman.2017.10.018</idno>
		<ptr target="https://doi.org/10.1016/j.tourman.2017.10.018" />
	</analytic>
	<monogr>
		<title level="j">Tourism Management</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="53" to="61" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Impact of the COVID-19 Pandemic on Online Consumer Purchasing Behavior</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>≈ölusarczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hajizada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kovalyova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sakhbieva</surname></persName>
		</author>
		<idno type="DOI">10.3390/jtaer16060125</idno>
		<ptr target="https://doi.org/10.3390/jtaer16060125" />
	</analytic>
	<monogr>
		<title level="j">Journal of Theoretical and Applied Electronic Commerce Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2263" to="2281" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A Means-End Chain Model Based on Consumer Categorization Processes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gutman</surname></persName>
		</author>
		<idno type="DOI">10.1177/002224298204600207</idno>
		<ptr target="https://doi.org/10.1177/002224298204600207" />
	</analytic>
	<monogr>
		<title level="j">Journal of Marketing</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="60" to="72" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Binary Choice vs Ratings Scales: A behavioural science perspective</title>
		<author>
			<persName><forename type="first">C</forename><surname>Harvey</surname></persName>
		</author>
		<idno type="DOI">10.2501/IJMR-2016-041</idno>
		<ptr target="https://doi.org/10.2501/IJMR-2016-041" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Market Research</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="647" to="648" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Can online reviews reveal a product&apos;s true quality? Empirical findings and analytical modeling of Online word-of-mouth communication</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hjeij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vilks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Pavlou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1057/s41599-023-01542</idno>
		<ptr target="https://doi.org/10.1145/1134707.1134743" />
	</analytic>
	<monogr>
		<title level="m">A brief history of heuristics: How did research on heuristics evolve? Humanities and Social Sciences Communications</title>
		<imprint>
			<date type="published" when="2006">2023. 2006</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="324" to="330" />
		</imprint>
	</monogr>
	<note>Proceedings of the 7th ACM Conference on Electronic Commerce</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Overcoming the J-shaped distribution of product reviews</title>
		<author>
			<persName><forename type="first">N</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Pavlou</surname></persName>
		</author>
		<idno type="DOI">10.1145/1562764.1562800</idno>
		<ptr target="https://doi.org/10.1145/1562764.1562800" />
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="144" to="147" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The Influence of Labels and Positions in Rating Scales</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Klockars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yamagishi</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1745-3984.1988.tb00294.x</idno>
		<ptr target="https://doi.org/10.1111/j.1745-3984.1988.tb00294.x" />
	</analytic>
	<monogr>
		<title level="j">Journal of Educational Measurement</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="85" to="96" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Do online reviews reflect a product&apos;s true perceived quality? An investigation of online movie reviews across cultures</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Clemons</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.elerap.2010.04.001</idno>
		<ptr target="https://doi.org/10.1016/j.elerap.2010.04.001" />
	</analytic>
	<monogr>
		<title level="j">Electronic Commerce Research and Applications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="374" to="385" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Why do people rate? Theory and evidence on online ratings</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lafky</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.geb.2014.02.008</idno>
		<ptr target="https://doi.org/10.1016/j.geb.2014.02.008" />
	</analytic>
	<monogr>
		<title level="j">Games and Economic Behavior</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="554" to="570" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The effect of review valence and variance on product evaluations: An examination of intrinsic and extrinsic cues</title>
		<author>
			<persName><forename type="first">R</forename><surname>Langan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Besharat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Varki</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ijresmar.2016.10.004</idno>
		<ptr target="https://doi.org/10.1016/j.ijresmar.2016.10.004" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Research in Marketing</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="414" to="429" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Does the dispersion of online review ratings affect review helpfulness?</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Baek</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.chb.2020.106670</idno>
		<ptr target="https://doi.org/10.1016/j.chb.2020.106670" />
	</analytic>
	<monogr>
		<title level="j">Computers in Human Behavior</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page">106670</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reviews, Reputation, and Revenue: The Case of Yelp</title>
		<author>
			<persName><forename type="first">M</forename><surname>Luca</surname></persName>
		</author>
		<idno type="DOI">10.2139/ssrn.1928601</idno>
		<ptr target="https://doi.org/10.2139/ssrn.1928601" />
	</analytic>
	<monogr>
		<title level="m">Social Science Research Network</title>
		<title level="s">SSRN Scholarly Paper</title>
		<imprint>
			<date type="published" when="2016">2016. 1928601</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The Mere Categorization Effect: How the Presence of Categories Increases Choosers&apos; Perceptions of Assortment Variety and Outcome Satisfaction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mogilner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rudnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Iyengar</surname></persName>
		</author>
		<idno type="DOI">10.1086/588698</idno>
		<ptr target="https://doi.org/10.1086/588698" />
	</analytic>
	<monogr>
		<title level="j">Journal of Consumer Research</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="202" to="215" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Research Note: What Makes a Helpful Online Review? A Study of</title>
		<author>
			<persName><forename type="first">Schuff</forename><surname>Mudambi</surname></persName>
		</author>
		<idno type="DOI">10.2307/20721420</idno>
		<ptr target="https://doi.org/10.2307/20721420" />
	</analytic>
	<monogr>
		<title level="j">Customer Reviews on Amazon.com. MIS Quarterly</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="185" to="200" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Weighting ratings: Are people adjusting for bias in extreme reviews</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ocean</surname></persName>
		</author>
		<idno type="DOI">10.1037/xap0000497</idno>
		<ptr target="https://doi.org/10.1037/xap0000497" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Applied</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="391" to="409" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Category judgment: A range-frequency model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Parducci</surname></persName>
		</author>
		<idno type="DOI">10.1037/h0022602</idno>
		<ptr target="https://doi.org/10.1037/h0022602" />
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="407" to="418" />
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Asymmetric effects of online consumer reviews</title>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Nicolau</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.annals.2014.10.007</idno>
		<ptr target="https://doi.org/10.1016/j.annals.2014.10.007" />
	</analytic>
	<monogr>
		<title level="j">Annals of Tourism Research</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="67" to="83" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">The Ultimate Question 2.0 (Revised and Expanded Edition): How Net Promoter Companies Thrive in a Customer-Driven World</title>
		<author>
			<persName><forename type="first">F</forename><surname>Reichheld</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Harvard Business Review Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Negativity Bias, Negativity Dominance, and Contagion</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Royzman</surname></persName>
		</author>
		<idno type="DOI">10.1207/S15327957PSPR0504_2</idno>
		<ptr target="https://doi.org/10.1207/S15327957PSPR0504_2" />
	</analytic>
	<monogr>
		<title level="j">Personality and Social Psychology Review</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="296" to="320" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The Polarity of Online Reviews: Prevalence, Drivers and Implications</title>
		<author>
			<persName><forename type="first">V</forename><surname>Schoenmueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Stahl</surname></persName>
		</author>
		<idno type="DOI">10.1177/0022243720941832</idno>
		<ptr target="https://doi.org/10.1177/0022243720941832" />
	</analytic>
	<monogr>
		<title level="j">Journal of Marketing Research</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="853" to="877" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Rating Scales Numeric Values May Change the Meaning of Scale Labels</title>
		<author>
			<persName><forename type="first">N</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kn√§uper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Hippler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Noelle-Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Clark</surname></persName>
		</author>
		<idno type="DOI">10.1086/269282</idno>
		<ptr target="https://doi.org/10.1086/269282" />
	</analytic>
	<monogr>
		<title level="j">Public Opinion Quarterly</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="570" to="582" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The impact of online reviews on hotel booking intentions and perception of trust</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Sparks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Browning</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tourman.2010.12.011</idno>
		<ptr target="https://doi.org/10.1016/j.tourman.2010.12.011" />
	</analytic>
	<monogr>
		<title level="j">Tourism Management</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1310" to="1323" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Decision by sampling</title>
		<author>
			<persName><forename type="first">N</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D A</forename><surname>Brown</surname></persName>
		</author>
		<idno type="DOI">10.1016/J.COGPSYCH.2005.10.003</idno>
		<ptr target="https://doi.org/10.1016/J.COGPSYCH.2005.10.003" />
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">How Does the Variance of Product Ratings Matter?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1287/mnsc.1110.1458</idno>
		<ptr target="https://doi.org/10.1287/mnsc.1110.1458" />
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="696" to="707" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The Effect of Rating Scale Design on Extreme Response Tendency in Consumer Product Ratings</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tsekouras</surname></persName>
		</author>
		<idno type="DOI">10.1080/10864415.2016.1234290</idno>
		<ptr target="https://doi.org/10.1080/10864415.2016.1234290" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Electronic Commerce</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="270" to="296" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Determinants of Scale Response: Label versus Position</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Wildt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Mazis</surname></persName>
		</author>
		<idno type="DOI">10.1177/002224377801500209</idno>
		<ptr target="https://doi.org/10.1177/002224377801500209" />
	</analytic>
	<monogr>
		<title level="j">Journal of Marketing Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="261" to="267" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
