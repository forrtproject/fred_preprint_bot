<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Model-based eye-tracking: a new window to understand individual differences and psychiatric disorders</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qianying</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Division of the Humanities and Social Sciences</orgName>
								<orgName type="institution">California Institute of Technology</orgName>
								<address>
									<settlement>Pasadena</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Na</forename><forename type="middle">Yeon</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Division of the Humanities and Social Sciences</orgName>
								<orgName type="institution">California Institute of Technology</orgName>
								<address>
									<settlement>Pasadena</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Riverside, Riverside</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ralph</forename><surname>Adolphs</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Division of the Humanities and Social Sciences</orgName>
								<orgName type="institution">California Institute of Technology</orgName>
								<address>
									<settlement>Pasadena</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Model-based eye-tracking: a new window to understand individual differences and psychiatric disorders</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FB7BF5076FCEC598CF7FDBFF52E440F7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-10-21T13:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Our eyes are constantly moving, and where we look reveals what we attend to, influences the decisions we make, and what we remember. While traditional laboratorybased eye-tracking protocols have generated a large body of findings based on wellcontrolled stimuli, modern methods now offer the ability to collect gaze data at much larger scale and with more naturalistic stimuli. Powerful computational tools also enable new analyses of high-dimensional data, incorporating feature annotation of the stimuli and model-based evaluation of gaze. These advances in both data collection and analysis are providing new insights into individual differences in both health and disease. Here we discuss four key approaches to modeling eye movement data: saliency-based attention phenotyping, data-driven gaze pattern identification, supervised machine learning classification, and unsupervised clustering. We highlight their advantages in psychiatry research, as they inform better understanding of visual attention, provide more fine-grained characterization of individual differences, and make more powerful clinical predictions. Finally, we address key methodological considerations in applying the methods and take stock of future opportunities on the horizon.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>We live in a visually rich and dynamic environment. How we explore this world with our constantly moving eyes shapes-and is shaped by-our memory, decisions, and actions. This reciprocal link makes eye-tracking a powerful complement to traditional behavioral or neural measures. It has the potential to provide a rich window into our minds, in both health and disease, and the advances we will review here argue for prioritizing its application in psychiatry research. Under typical conditions, our eyes shift two to five times per second, resulting in over 100,000 rapid eye movements (saccades) every day <ref type="bibr">[1]</ref><ref type="bibr">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref> -an enormous repository of data if it could be efficiently sampled. While gaze patterns often converge across individuals, pronounced individual differences also exist, some linked to psychiatric disorders with genetic underpinnings <ref type="bibr" target="#b4">5</ref> . Over the past decade, eye-tracking has become a widely used tool in psychiatry research, advancing our understanding of clinically meaningful alterations in attention, memory, emotion, and decision-making <ref type="bibr" target="#b5">[6]</ref><ref type="bibr">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref> . The most recent advances in hardware and analysis tools now offer unprecedented opportunities to incorporate eye-tracking into clinical and research protocols.</p><p>Behind eye movements lies a remarkably intricate control system: 6 extraocular muscles -innervated by 3 cranial nerves -coordinate both voluntary and reflexive movements. Voluntary movements like saccades and smooth pursuit (cf. Supplementary Information S1) are driven by internal goals and motivations. These goals are processed in higher cortical regions, such as the frontal eye fields (FEF) and associated frontal and parietal areas, which in turn send motor commands to subcortical structures and, ultimately, brainstem nuclei, to initiate and coordinate movement <ref type="bibr">10</ref> (Figure <ref type="figure" target="#fig_0">1a</ref>). Importantly, where we look is influenced not only by external features of the visual stimuli (e.g., reward value, visual saliency, novelty), but also by internal states (e.g., emotion, arousal) and enduring traits (e.g., anxiety, schizotypy, autism). Given this rich array of factors, computational models that incorporate both stimulus-driven and individual-level factors can provide valuable insight into attention mechanisms and their variations across individuals.</p><p>Visual attention, the mechanism through which we selectively allocate cognitive resources to a restricted set of visual features or spatial locations, is foundational to eye movements and nearly all psychological processes that operate on visual content. It resolves the challenge of processing multiple objects or features within a scene <ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12</ref> , by prioritizing those that are salient or relevant to behavioral goals (Figure <ref type="figure" target="#fig_0">1b</ref>). Neurons involved in saccade planning respond both to the intended gaze locations as well as the resulting fixation targets, and also contribute to perceptual stability via mechanisms like saccadic suppression <ref type="bibr" target="#b12">13</ref> .</p><p>By capturing the spatial and temporal dynamics of gaze, eye-tracking provides a rich window into the cognitive processes that guide attention <ref type="bibr" target="#b13">14</ref> . Variations in gaze patterns have been documented across psychiatric conditions, such as reduced attention to faces and biological motion in autism spectrum disorder (ASD) <ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref> , biased attention towards threat in anxiety <ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19</ref> , and shorter fixation maintenance in children with Attention-Deficit/Hyperactivity Disorder (ADHD) <ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21</ref> . With its non-invasive nature, ease of application, minimal reliance on language, and objective measures, modern eyetracking holds exceptional promise for both research and clinical applications.</p><p>Despite this promise, a large portion of the eye-tracking studies to date have relied on relatively small samples, simple metrics (e.g. fixation durations, saccade lengths) and qualitative observations (e.g., gaze heatmaps, scanpaths, Supplementary Information S1) <ref type="bibr" target="#b21">22</ref> , and are often guided by hypotheses about pre-defined groups or case studies. These limit both the sensitivity to infer relevant cognitive processes, and the potential for data-driven discoveries. Over the past decade, however, advances in new computational tools have enabled more nuanced analyses of the intricate gaze -an opportunity ripe for more extensive application to psychiatric conditions. Moving beyond artificial and highly controlled settings, these methods offer better generalizability to the real world by capturing attention in naturalistic contexts -such as free exploration of natural scenes, gaze transitions in dynamic environments, and joint attention during live interactions <ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24</ref> . Importantly, they deepen our understanding of the mechanisms that guide visual attention, allow for more fine-grained characterization of individual differences that are clinically relevant, and enhance the predictive and clinical utility of eye-tracking.</p><p>In this review, we provide a broad overview of recent developments in computational eye-tracking -an umbrella term for analytic approaches that leverage mathematical models and machine learning techniques to extract structure, patterns and predictive signals from eye-movement data. We introduce four categories of methods that have been successfully applied in psychiatric research: (1) Visual saliency models, which link gaze behavior to stimulus attributes and delineate individual phenotypes of visual preference; (2) Data-driven gaze pattern identification, which defines meaningful, multivariate areas of interest (AOIs) on a visual stimulus through the spatio-temporal features of eye movements; (3) Supervised classification techniques that use eyetracking data to distinguish predefined clinical populations; and (4) Unsupervised pattern discovery methods that can identify subgroups or transdiagnostic categories based on shared gaze characteristics without prior labels. We discuss why they are helpful in advancing psychiatric research with examples. We also address key methodological considerations in how to choose and apply these methods that match different research purposes. We conclude by outlining current challenges and exploring future opportunities in the field. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Using visual saliency models to characterize attention profiles</head><p>Where we look is guided by two attention pathways: a top-down pathway driven by endogenous cognitive factors such as task and goal, and a bottom-up pathway driven by stimulus features <ref type="bibr" target="#b24">25</ref> . These typically come into play concurrently during natural vision. While top-down attention dominates goal-directed search (e.g., while looking for your lost car keys), much of our daily visual experience is spontaneous, with attention captured seemingly automatically in a bottom-up manner (e.g., looking at people's faces at a party). One well-established model posits that the visual system constructs a socalled saliency map, where regions that stand out -due to color, motion, or other features -automatically attract gaze <ref type="bibr" target="#b25">26</ref> . The most influential theory proposes that the features of an image are initially processed preattentively in parallel to generate the saliency map, which subsequently directs the focus of serial attention <ref type="bibr" target="#b26">27</ref> .</p><p>Early computational models of saliency, such as the Itti-Koch model in 1998 <ref type="bibr" target="#b27">28</ref> , decompose an image into low-level (or pixel-level) topographic feature maps, including color, orientation, and intensity. Spatial locations that stand out from their surroundings within each feature map are combined into a master "saliency map", that partly explains eye movements driven by bottom-up visual saliency <ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b27">28</ref> . Following this initial model, numerous hand-crafted models have progressively improved the ability to predict fixations by incorporating high-level features defined more by their semantics than their pixelwise attributes, such as people, faces, objects, or text <ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr">[31]</ref><ref type="bibr" target="#b31">[32]</ref> . Around 2015, deep neural networks (DNN) revolutionized saliency prediction by changing it to a learning problem: DNNs trained end-to-end on large sets of gaze data, were able to predict human gaze from images and could automatically incorporate higher-level features without manual specification. These DNN models achieved even higher accuracy. For example, one state-of-the-art model (DeepGaze IIE <ref type="bibr" target="#b32">33</ref> ) predicts human gaze on natural scene images that correlate at r = 0.82 with empirical human gaze maps, compared to r = 0.43 for the Itti-Koch model <ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35</ref> (see the MIT/Tübingen Saliency Benchmark <ref type="bibr" target="#b35">36</ref> ). Beyond the prediction of gaze locations on static images, more recent work has extended gaze predictions to videos, where saliency can be driven by acoustic features and dynamic visual information (e.g., actions) <ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38</ref> . Further extensions of these models predict scanpaths (i.e., both the direction and timing of saccades) on images by imposing other biologically-and cognitively-inspired constraints (e.g., center fixation bias, inhibition of return, evidence accumulation <ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr">[42]</ref><ref type="bibr" target="#b42">[43]</ref> ) on top of the visual saliency map.</p><p>Modern saliency models now offer predictively accurate and quantitative tools to assess individual attention patterns (Figure <ref type="figure" target="#fig_1">2a</ref>). Because most models are pre-trained on nonclinical samples <ref type="bibr" target="#b43">[44]</ref><ref type="bibr" target="#b44">[45]</ref><ref type="bibr" target="#b45">[46]</ref> , comparing model predictions to actual fixations can reveal which features are over-and under-prioritized by different clinical groups <ref type="bibr" target="#b46">[47]</ref><ref type="bibr" target="#b47">[48]</ref><ref type="bibr" target="#b48">[49]</ref> . For example, one study in schizophrenia (SZ) found that high-level saliency models had better predictions for control (compared to SZ) participants' fixations on natural scenes , yet the low-level saliency model had better predictions for the SZ (compared to control) group's gaze data, suggesting a bottom-up bias in SZ patients <ref type="bibr" target="#b46">47</ref> . In video watching protocols, similar dissociations were observed in ADHD: while high-level visual saliency predictions were less aligned with ADHD fixations compared to controls, low-level saliency predictions remained similar between the groups <ref type="bibr" target="#b48">49</ref> . This provided evidence for the hypothesis that atypical gaze patterns in ADHD are mainly caused by reduced engagement with semantically meaningful features rather than increased sensitivity to low-level salience.</p><p>A critical extension to the diagnosis prediction of saliency models is a further examination of the individual features that drive attention (Figure <ref type="figure" target="#fig_1">2b</ref>). A landmark study implemented a three-layer saliency model to examine visual attention in autistic adults during free-viewing of 700 complex natural scenes <ref type="bibr" target="#b14">15</ref> . The images were comprehensively annotated at the pixel-level (3 features: color, intensity, orientation), object-level (5 features: size, complexity, convexity, solidity, eccentricity), and semanticlevel (12 features: face, emotion, touched, gazed, motion, sound, smell, taste, touch, text, watchability, operability). Using a linear support vector machine (SVM) classifier, the authors evaluated the weights of each feature in predicting human fixations, and found that individuals with autism showed higher reliance on pixel-level features and reduced weighting of socially relevant semantic features <ref type="bibr" target="#b14">15</ref> . This model-based approach effectively extends a particular visual feature of interest to a broader set of features, providing a multi-dimensional profile of attention biases, thus allowing researchers to test multiple hypotheses in a single experiment. While earlier studies constructed feature maps from manual annotation <ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b49">50</ref> , more recent work leverages automated feature extraction through computer vision and large language models. Numerous open-source, validated, and user-friendly software packages have been in use for this purpose, offering automated body parts segmentation <ref type="bibr" target="#b50">51</ref> , facial landmark detection <ref type="bibr" target="#b51">52</ref> , and semantic meaning classification <ref type="bibr" target="#b15">16</ref> . The field is evolving rapidly to address several remaining challenges (e.g., object tracking <ref type="bibr">53</ref> , action understanding <ref type="bibr" target="#b53">54</ref> , emotion recognition <ref type="bibr" target="#b54">55</ref> ). These tools makes it possible, with relatively little effort, to study individual differences in more naturalistic environments (e.g., virtual reality, real life) that contain richer features (e.g., fine-grained segmentation of face and body, more types of semantic features) <ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b55">[56]</ref><ref type="bibr" target="#b56">[57]</ref><ref type="bibr" target="#b57">[58]</ref><ref type="bibr" target="#b58">[59]</ref> . Ultimately, the availability of automated annotation for large sets of features present in naturalistic stimuli could provide us with a much richer characterization of eye movements in psychiatric diseases, shedding light on more specific cognitive dysfunction.</p><p>Recent research examining the covariance structure among features that drive attention has begun to elucidate more specific latent factors <ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b59">[60]</ref><ref type="bibr" target="#b60">[61]</ref><ref type="bibr" target="#b61">[62]</ref> . For instance, individuals who tend to look at faces also show increased attention to motion, but less to bodies and text <ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b61">62</ref> . These studies may reveal common causes for seemingly different domains of behaviors, or different causes for seemingly similar behaviors: while both eye-to-mouth preference and face-to-object preference are considered to be measures of social attention and are linked to autism, a recent study reported weak correlation between them, suggesting that they may reflect dissociable underlying processes <ref type="bibr" target="#b60">61</ref> ; whereas another study showed eye-to-mouth preference co-varied with the upper-to-lower position preference when fixating on objects, suggesting a potential domain-general mechanism for processing the height of fixations. Characterizing associations of attention preference among a variety of features could also inform us about possible linkages in their genetic basis, which could ultimately suggest meaningful geneticallybased endophenotypes related to psychiatric symptoms. For each image (same as in Fig 1 ), both low-level saliency (based on features like color, intensity, and orientation) and high-level saliency (highlighting semantically meaningful elements such as people and objects) are computed. Model fit scores quantify how strongly each individual's gaze aligns with different saliency types, revealing differences in visual preferences. (b) Quantifying individual attention saliency weights. In one study, attention weights were derived for features across three levels: pixel-level (e.g., color, intensity), object-level (e.g., size, complexity), and semantic-level (e.g., faces, text, watchability). Weights for all the features comprise a computational phenotype for an individual's visual attention style <ref type="bibr" target="#b14">15</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data-driven gaze pattern identification</head><p>In a traditional protocol, researchers define a limited set of AOIs a priori, and then analyze metrics such as dwell time or fixation latency with respect to those AOIs. This approach has two key limitations: AOI definitions are often subjective, and informative gaze outside the AOIs may be overlooked <ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b63">64</ref> . For instance, when defining a "mouth" AOI on a face, one might restrict it to the lips, missing peri-oral regions that could carry relevant visual information. The many degrees of freedom for choosing and defining AOIs have been suggested to lead to inconsistent findings and decreased replicability across studies <ref type="bibr" target="#b64">65</ref> . On the other hand, visualizations like heatmaps or scanpaths can highlight unexpected areas of gaze that might otherwise be missed. Yet they do not provide new AOIs -a challenge addressed by new data driven approaches that can discover interpretable gaze patterns as well as new AOIs. These algorithms identify areas that are co-fixated at adjacent times, or gaze transition sequences that are commonly observed across individuals and stimuli. Those statistical dependencies can provide the location, shape and border of a multivariate gaze pattern -an aggregate of the fixations distributed over several discrete areas. The resulting multivariate gaze patterns can then be used to define new, statistically grounded AOIs, inform hypotheses about attention priorities across populations, and eventually guide future confirmatory studies.</p><p>Multivariate gaze patterns can be identified through dimensionality reduction and datadriven clustering techniques <ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b66">67</ref> . For instance, principal component analysis (PCA) has been used to summarize complex fixation distributions into a small set of orthogonal principal components (PCs) that capture major attentional variations across individuals <ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b65">66</ref> . In a study that performed PCA on gaze heatmaps during face viewing, five PCs were identified that explained 82% of the total variance of fixation distributions on faces across all the participants <ref type="bibr" target="#b62">63</ref> . These PCs reflected individual differences in the tendencies to fixate on the eyes relative to the mouth, and to fixate on the forehead relative to the chin <ref type="bibr" target="#b62">63</ref> . Clustering methods such as K-means have also been used to derive gaze-based AOIs. Instead of relying on predefined AOIs, one study identified clusters of high-density gaze points across participants, which served to partition the image into statistically defined gaze "hot spots" <ref type="bibr" target="#b63">64</ref> . Counting fixations within each datadriven AOI can then generate a lower-dimensional representation of gaze behavior for subsequent modeling of individual differences <ref type="bibr" target="#b63">64</ref> . This approach enables flexible application and generalizes to diverse contexts regardless of the stimulus.</p><p>Another prominent method for finding latent spatiotemporal patterns in eye movement data is the Eye Movement Hidden Markov Model (EMHMM) <ref type="bibr" target="#b67">[68]</ref><ref type="bibr" target="#b68">[69]</ref><ref type="bibr" target="#b69">[70]</ref><ref type="bibr" target="#b70">[71]</ref> . HMM is a statistical model that describes and predicts a sequence of events. It infers the probabilistic distributions of several underlying states that are not directly observable (hidden) based on observations that are dependent on the states. In the context of eye-tracking analysis, EMHMM summarizes one's gaze locations over time ('observations') as a collection of individual-specific AOIs ('states'), as well as the transition probabilities among the states. EMHMMs first fit individual gaze data with HMMs (resulting in several AOIs for the fixations and a transition matrix describing saccades) and then cluster the individual-level results into a small number of commonly observed gaze patterns shared among participants, which are referred to as the representative grouplevel HMMs <ref type="bibr" target="#b71">72</ref> . Individual HMMs can then be compared to the group representatives with a similarity score (per group HMM) as a continuous quantitative measure of eye movement style. Finally, depending on the research question, one can investigate the association between these HMM-derived similarity measures and other measures of individual differences, such as questionnaire-based scores (Figure <ref type="figure" target="#fig_2">3a</ref>). Note that EMHMMs assume eye movements to be Markovian processes (current fixation only depends on the previous one), which simplifies the dependencies between saccades (e.g., inhibition of return and facilitation of return). Nevertheless, they are richer models than most other AOI identification methods, and provide meaningful information about the temporal dynamics of visual exploration patterns, in addition to spatial gaze locations.</p><p>The EMHMM method has been applied across various tasks and populations <ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b72">73</ref> . During face recognition, younger adults tend to look at faces with an analytic pattern that features frequent transitions between the eyes, while older adults, particularly those with lower executive functioning, tend to adopt a holistic pattern that starts from the nose/mouth region (i.e., high prior probability) and stays around there (i.e., high probability of transitions to the same AOI) <ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b72">73</ref> . The EMHMM framework has also been applied to children with autism across three social tasks (Figure <ref type="figure" target="#fig_2">3b</ref>): static scene viewing, visual exploration, and activity monitoring <ref type="bibr" target="#b23">24</ref> . Across all tasks, autistic children exhibited more exploratory gaze patterns than controls, characterized by broader face AOIs without eye-region prioritization, and increased attention to non-social regions. These findings illustrate how EMHMMs can uncover latent attentional styles and link them to individual traits or clinical characteristics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supervised classification and prediction of clinical groups</head><p>The diagnosis of psychiatric disorders relies heavily on clinical observations and patient self-reports, which can be expensive, time-consuming, and subjective <ref type="bibr" target="#b73">[74]</ref><ref type="bibr" target="#b74">[75]</ref><ref type="bibr" target="#b75">[76]</ref> . Here, eyetracking can offer a rich array of objective and efficient measures that complement the assessment of mental health. Although specific eye-tracking metrics have been identified as particularly informative for some disorders (e.g., gaze at faces in the case of autism <ref type="bibr" target="#b76">[77]</ref><ref type="bibr" target="#b77">[78]</ref><ref type="bibr" target="#b78">[79]</ref><ref type="bibr" target="#b79">[80]</ref> ; abnormal smooth pursuit in the case of schizophrenia <ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b81">82</ref> ), the heterogeneity of psychiatric disorders generally precludes any eye movement measure in isolation from serving as a reliable clinical biomarker <ref type="bibr" target="#b82">83</ref> . Machine learning (ML) models address this challenge by weighing and combining multiple features that result in more accurate and robust prediction.</p><p>In a supervised ML pipeline, researchers extract features from eye-tracking data as input (e.g., fixations within specific AOIs) and combine these features (linearly or nonlinearly) to train a model that distinguishes between groups. To avoid overfitting, the model is typically trained on only a subset of the data, and then tested on a held-out portion; doing so many times produces a distribution of model accuracies. It is also important to further evaluate the model's generalizability (without any re-training) to entirely new datasets (for an overview of supervised machine learning, see <ref type="bibr" target="#b83">84</ref> ). A plethora of models have achieved good accuracy in predicting various psychiatric disorders (e.g., ASD, ADHD, schizophrenia, depression, and OCD) <ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b84">[85]</ref><ref type="bibr" target="#b85">[86]</ref><ref type="bibr" target="#b86">[87]</ref><ref type="bibr" target="#b87">[88]</ref><ref type="bibr" target="#b88">[89]</ref><ref type="bibr" target="#b89">[90]</ref><ref type="bibr" target="#b89">90,</ref><ref type="bibr" target="#b90">91</ref> . A recent meta-analysis of 24 studies (2016-2021) reported a pooled classification accuracy of 81%, specificity of 79%, and sensitivity of 84% when classifying ASD from typically developed individuals <ref type="bibr" target="#b91">92</ref> . Note, however, that many studies have not extensively tested generalizability to broader samples (e.g., other cultures, broader age ranges), and also have typically compared one clinical group to controls, rather than assessed specificity among multiple psychiatric conditions.</p><p>The choice of ML model and feature selection highly depend on sample size, computing resource, and desired interpretability (Figure <ref type="figure" target="#fig_3">4a</ref>). Classic ML algorithms such as regularized linear regression, support vector machine (SVM), random forest, and boosting classifiers are commonly used in combination with hand-crafted features to achieve high interpretability. These models can be trained using a personal computer with several hundred samples. A variety of eye-tracking features have been tested on such models, including (1) AOI-based fixations (e.g., time spent looking at faces, histograms of fixation frequencies among AOI partitions) <ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b84">85,</ref><ref type="bibr" target="#b86">87,</ref><ref type="bibr" target="#b92">93</ref> , (2) oculomotor features (e.g., fixation duration, saccade amplitude, saccade velocity, variation of fixation duration) <ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b86">87,</ref><ref type="bibr" target="#b92">93,</ref><ref type="bibr" target="#b93">94</ref> , (3) saliency-based attention bias estimations <ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b94">95,</ref><ref type="bibr" target="#b95">96</ref> , and (4) gaze transition patterns (e.g., transition probabilities assessed by HMM) <ref type="bibr" target="#b90">91,</ref><ref type="bibr" target="#b96">97</ref> . Comparing the contribution of the features (i.e., calculate discriminability scores <ref type="bibr" target="#b97">98,</ref><ref type="bibr" target="#b98">99</ref> , conduct feature selection <ref type="bibr" target="#b99">100,</ref><ref type="bibr" target="#b100">101</ref> , check coefficients, or calculate feature importance in prediction <ref type="bibr" target="#b101">102,</ref><ref type="bibr" target="#b102">103</ref> ) can suggest the most informative metric or test paradigm that differentiates the clinical group.</p><p>A potentially powerful alternative approach is to use deep neural networks (DNN), which can learn gaze patterns from raw or lightly processed eye-tracking data with minimal manual feature engineering. They do this by fitting highly complicated nonlinear functions to predict diagnostic categories from all available feature information. While they capture subtle gaze dynamics that may be lost in traditional feature extraction and yield better predictive performance, they are prone to overfitting, require larger datasets (often thousands of trials or subjects), and demand significant computational resources.</p><p>Among DNNs, convolutional neural networks (CNNs) are widely used for image-based inputs. CNNs are feed-forward networks that use convolutional layers to learn spatial features of the input images at different scales, progressing from low-level patterns near the input layers, to high-level semantic features in deeper layers. In eye-tracking research, CNNs have been applied to extract spatial gaze patterns across scales from gaze heatmaps or scanpath (in the form of 2D image input). These gaze patterns are then weighted in the final CNN layer to predict psychiatric conditions, which often outperform classic ML methods <ref type="bibr" target="#b90">91,</ref><ref type="bibr" target="#b103">[104]</ref><ref type="bibr" target="#b104">[105]</ref><ref type="bibr" target="#b105">[106]</ref><ref type="bibr" target="#b106">[107]</ref> . CNN can also identify novel image features that differentiate gaze patterns between target groups <ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b107">108</ref> . In a recent study, CNNs were trained on natural images to predict differences in fixation maps (of these images) between ASD and control groups <ref type="bibr" target="#b107">108</ref> . Learned representations of each image captured where (and on which image features) participants from each group tend to fixate more or less. The latent representations of each individual's fixated image pixels were used as input to an SVM classifier, which successfully classified ASD participants with an accuracy of 92% <ref type="bibr" target="#b107">108</ref> .</p><p>While images of gaze heatmaps or scanpaths capture spatial patterns, they collapse temporal information. To model temporal transitions of eye movements, recurrent neural networks (RNNs) can be helpful. RNNs (and their extensions such as LSTMs and Transformers) process sequential data, such as raw gaze coordinates or fixation sequences, by maintaining a memory of past inputs through recurrent connections. This allows RNNs to learn temporal dependencies in gaze trajectories, distinguishing, for example, between repetitive scanning and more exploratory gaze behavior. Compared to HMMs, which as we reviewed above also model temporal information, RNNs are better at modeling complex, nonlinear dependencies of gaze trajectories over longer time scales, yet their results are often less interpretable. In practice, RNNs are also often used for classifying participants into groups <ref type="bibr" target="#b87">88,</ref><ref type="bibr" target="#b89">90,</ref><ref type="bibr" target="#b108">109,</ref><ref type="bibr" target="#b109">110</ref> , and can be combined with CNNs to capture both the spatial and temporal features of eye movements <ref type="bibr" target="#b110">[111]</ref><ref type="bibr" target="#b111">[112]</ref><ref type="bibr" target="#b112">[113]</ref> . Beyond CNNs and RNNs, more efficient neural network architectures and novel optimization mechanisms are developed every day. Researchers who want to implement ML-based classifications should closely monitor relevant benchmarks and the state-of-the-art models in the field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unsupervised pattern discovery for eye movementbased subtyping</head><p>While supervised classification identifies differences between predefined categories, unsupervised clustering aims to uncover natural groupings within or across groups without prior labels. This holds out the promise of discovering new subtypes within a disorder, overlap between disorders, or even entirely new proposals for diagnostic categories as such. Clustering begins by quantifying pairwise similarity between gaze patterns. Similarity may be based on selected features (e.g., reduced face fixations or short fixation durations) or on the overall gaze trajectory (e.g., looking at similar regions over time). Once the pairwise similarities are calculated, clustering algorithms (e.g., hierarchical clustering, K-means clustering) can be applied to detect groups of individuals that share similar eye movement profiles (Figure <ref type="figure" target="#fig_3">4b</ref>; see <ref type="bibr" target="#b113">114</ref> for an overview of unsupervised machine learning). In psychiatric conditions where behavioral manifestations are heterogeneous, these data-driven patterns might eventually help refine categories or tailor personalized interventions to individual-specific attentional profiles <ref type="bibr" target="#b114">115</ref> .</p><p>Unsupervised clustering based on features positions individuals in a shared feature space and defines the similarity among individuals with common distance measures (e.g., Euclidean distance, cosine distance). Same as during supervised classification, these features may include traditional AOI-based fixations, oculomotor features, or model-based phenotypes (e.g., visual saliency estimations, HMM-based gaze transition patterns). Each dimension of the space typically corresponds to one well-defined gaze feature, enabling high interpretability <ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b115">116</ref> . For example, one study performed hierarchical clustering based on seven attention measures (e.g., overall attention to a scene, to various parts of a person, and to distraction), revealing three distinct gaze subgroups within autistic toddlers (age 2): one subgroup featured high eye vs. mouth looking ratio, and one subgroup featured low eye vs. mouth looking ratio <ref type="bibr" target="#b115">116</ref> . These three subtypes showed clinically relevant differences in skill acquisition rates during the subsequent year and predicted behavioral presentations at age 3. These findings support the use of unsupervised methods to parse heterogeneity of early syndrome expression, when standard diagnostic criteria may not yet be available. When many features are involved, dimensionality reduction such as PCA can help extract a more interpretable, low-dimensional latent space <ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b116">117</ref> . In a study that analyzed an initial total of 58 eye-tracking features, PCA revealed 3 dimensions that explained 59% of the total variance <ref type="bibr" target="#b116">117</ref> . These three feature dimensions, representing the fixation duration (PC1), gaze step direction (PC2), and gaze step length (PC3), suggested two clusters of participants: a static viewer cluster with less frequent and longer fixations, and a dynamic viewer cluster with the opposite pattern <ref type="bibr" target="#b116">117</ref> . As this example shows, combinations of computational methods condensed the rich feature set to yield compact and interpretable results.</p><p>The features can also be latent representations extracted from DNNs <ref type="bibr" target="#b117">118,</ref><ref type="bibr" target="#b118">119</ref> . Although features extracted from these more complicated algorithms are generally less interpretable, they potentially yield more robust and reliable results. For instance, Elbattah et al. fed scanpath images from autistic and control participants to an autoencoder (a DNN used for dimensionality reduction) and generated low-dimensional vectors that revealed two clusters through subsequent K-means clustering <ref type="bibr" target="#b117">118</ref> . In this study, other less sophisticated feature extraction methods, despite better interpretability, produced poorer separation of clusters. To enhance the interpretability of the latent representations generated with the autoencoder, the authors compared how the clusters were associated with several eye-tracking metrics, and found that elevated velocity was associated with a cluster mainly consisting of autistic individuals <ref type="bibr" target="#b117">118</ref> . Another more recent study designed a novel DNN architecture that successfully predicted scanpaths for individual participants (with and without ASD) through the learning of observer-specific features <ref type="bibr" target="#b118">119</ref> . Without providing the diagnostic labels to the model, the model-derived observer-specific features showed a natural separation of ASD and controls, suggesting the strong learning power of such models to discern unique gaze patterns in ASD <ref type="bibr" target="#b118">119</ref> .</p><p>Another unsupervised approach to consider is not based on features, but on the gaze trajectory itself. Feature-free gaze similarity typically relies on the inter-subject correlation (ISC) analysis, which measures the similarity of raw gaze data between individuals (see Box 1). ISC methods vary in focus <ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b119">[120]</ref><ref type="bibr" target="#b120">[121]</ref><ref type="bibr" target="#b121">[122]</ref><ref type="bibr" target="#b122">[123]</ref><ref type="bibr" target="#b123">[124]</ref><ref type="bibr" target="#b124">[125]</ref><ref type="bibr" target="#b125">[126]</ref><ref type="bibr" target="#b126">[127]</ref> . Some methods emphasize the onset and direction of gaze shifts, such as the average Pearson correlation of the x and y time series <ref type="bibr" target="#b120">121,</ref><ref type="bibr" target="#b124">125,</ref><ref type="bibr" target="#b126">127</ref> . Some methods emphasize the spatial proximities of the fixations, including correlations of fixation heatmaps <ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b121">122</ref> , Euclidean distances between the average gaze positions (per time window) <ref type="bibr" target="#b122">123</ref> , and proportions of fixation overlaps <ref type="bibr" target="#b125">126</ref> . Moreover, some scanpath comparison methods, such as the ScanMatch <ref type="bibr" target="#b127">128</ref> and MultiMatch 129 algorithm, can quantify both spatial and temporal similarity in a single metric. Unlike feature-based methods that assign coordinates in a feature space, ISC reveals relative similarities across individuals. These in turn can be further analyzed with approaches that are model-free and require no dimensionality reduction at all, such as representational similarity analysis <ref type="bibr" target="#b129">130</ref> (RSA, see Box 1). The relationships produced by ISC can also be visualized through various dimensionality reduction techniques (e.g., MDS <ref type="bibr" target="#b130">131</ref> , t-SNE <ref type="bibr" target="#b131">132</ref> , UMAP <ref type="bibr" target="#b132">133</ref> ) and may show clusters of participants that either confirm the hypothesized groups or suggest new subtypes.</p><p>One study used multi-dimensional scaling (MDS) to compare gaze to videos between typically developing children and children with ASD. Interestingly, instead of showing two clusters of participants (with vs. without ASD), the study found a similarity geometry that was characterized by a "core" (TD) vs. periphery" (ASD) structure, suggesting that TD children have higher gaze similarity to one another, while children with ASD deviate from TD in heterogeneous ways <ref type="bibr" target="#b133">134</ref> . This type of approach is completely hypothesis-free and thus unbiased from any feature selection. Importantly, it helps address the challenge of interindividual heterogeneity: within a clinical group, individuals may differ from controls in opposite directions (some showing higher, others lower values), so the group average can appear similar to controls even when meaningful individual differences exist <ref type="bibr" target="#b134">135</ref> . Such model-free similarity-based approaches have gained increasing attention in screening and diagnosis, since they can capture abnormalities by quantifying how an individual deviates from a normative sample in any direction <ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b135">136,</ref><ref type="bibr" target="#b136">137</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Box 1. Application of Inter-Subject Correlation Analysis in Eye-Tracking</head><p>The idea of inter-subject correlation (ISC) was initially proposed in fMRI analysis <ref type="bibr" target="#b137">138</ref> , to provide a way to quantify the similarity in brain responses across individuals even for complex stimuli (movies) for which no simple model was available. In eye-tracking, ISC measures the spatial and/or temporal synchronization of eye movements between pairs of samples. Beyond serving as a similarity metric for unsupervised clustering (see main text), ISC offers specific insights valuable to psychiatry research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gaze heterogeneity across and within individuals</head><p>The most direct application of ISC characterizes how gaze to the same stimuli correlates between individuals. A simple extension quantifies how an individual's gaze pattern deviates from the majority or mean of a group, reflecting that individual's unique cognitive style and motivation. The results not only predict behavioral outcomes (e.g., individual learning success <ref type="bibr" target="#b123">124</ref> ) but often have clinical relevance. For example, several studies have found more idiosyncratic gaze patterns in children and adults with ASD <ref type="bibr" target="#b138">139</ref> . The degree of this divergence correlates with ASD severity and other cognitive scores <ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b122">123,</ref><ref type="bibr" target="#b125">126</ref> , as has been reported also for brain-derived ISCs <ref type="bibr" target="#b139">140,</ref><ref type="bibr" target="#b140">141</ref> .</p><p>In addition to inter-subject correlation, one can also measure intra-subject correlation, which is a meaningful index of the consistency of gaze patterns within an individual (over repeated tests). Similar to the between-individual heterogeneity discussed above, studies have also found associations between clinical symptoms and intrasubject gaze consistencies <ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b122">123</ref> . For example, reduced intra-subject consistency (less reliability across time) has been observed in children with ASD and is associated with lower inter-subject correlation compared to typically developing peers <ref type="bibr" target="#b122">123</ref> ; again, parallel results have also been reported in fMRI <ref type="bibr" target="#b141">142</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Time-resolved ISC analysis</head><p>Both the locations onto which we fixate as well as the time of such fixations are informative. One approach to better understand individual differences in spatiotemporal visual exploration patterns uses a 'moving time window' method to compute time-resolved ISC <ref type="bibr" target="#b121">122,</ref><ref type="bibr" target="#b126">127</ref> . For example, a study that assessed ISC across monozygotic (MZ) and dizygotic (DZ) twins observed a time-varying ISC change: gaze locations across participants were more divergent within epochs of viewing complex natural scenes, and the level of divergence was higher in DZ than MZ pairs <ref type="bibr" target="#b121">122</ref> . Time-resolved ISC is also helpful in analyzing video viewing data -to examine what type of visual features drives gaze convergence/divergence. This not only helps the design of more engaging videos for marketing and movie production, but also facilitates the design of clinical materials that better distinguish certain patient groups <ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b119">120,</ref><ref type="bibr" target="#b122">123</ref> . For instance, a study compared the ISC between several types of movies, and found that eye movements on Hollywood movie trailers were more coherent than those on movies of natural scenes <ref type="bibr" target="#b119">120</ref> . Another study modeled timeresolved ISC between autistic and control individuals within a video as a function of the visual features <ref type="bibr" target="#b142">143</ref> . Autistic participants were found to show greatest divergence from controls at moments when multiple faces co-existed in the scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Connecting multiple modalities using RSA</head><p>Representational similarity analysis provides a way to compare information across different data modalities by focusing not on the value of a measure, but on the similarity structure among patterns of responses <ref type="bibr" target="#b129">130,</ref><ref type="bibr" target="#b143">[144]</ref><ref type="bibr" target="#b144">[145]</ref><ref type="bibr" target="#b145">[146]</ref> . It characterizes the geometry of how different stimuli, experimental conditions, or individuals relate to each other in representational space. This makes RSA a model-free approach that enables comparison across distinct data types, such as neural activity, eye-tracking, and behavioral responses. For example, gaze divergence across individuals (gaze ISC) has been found to predict neural divergence in the V1 and inferior temporal cortex (fMRI-based ISC) during movie watching <ref type="bibr" target="#b144">145</ref> ; and that differences in the way one describes a scene (description ISC) could also be explained by the differences in the way they look at the scene (gaze ISC) <ref type="bibr" target="#b145">146</ref> . These results suggest that idiosyncrasy of gaze patterns drives distinct neural profiles and shapes unique subjective perceptions in response to the same visual stimuli. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Advantages of using Computational Models in Eyetracking Research</head><p>The four categories of methods we introduced (visual saliency model, data-driven AOI identification, supervised classification, unsupervised clustering) encompass a huge variation of theoretical bases, statistical assumptions, model classes, and ultimatelyresearch goals. Nevertheless, they represent three key advantages that advance many types of psychiatry research: they inform psychological models of vision, provide richer and more sensitive measures for differentiating individuals, and make powerful clinical predictions.</p><p>First, the use of computational models strengthens our understanding of visual attention, and may suggest new hypotheses about attention mechanisms. Theorybased models, such as the visual saliency model, go beyond simple measures of fixation counts or dwell time by directly modeling multiple levels of features that guide gaze. ML models, such as CNNs, suggest image segments or visual features that contribute to distinguishing the fixation patterns between clinical groups. These approaches provide a more comprehensive characterization of spontaneous fixations in naturalistic settings and on complex stimuli, where multiple visual features co-exist and compete for attention (Supplementary Information S2). Unlike AOI-based analyses, many of these models are unbiased, without arbitrary AOI definitions, and readily applicable across clinical populations. Results from these models (e.g., saliency weight differences, image patches with high differentiability) can be mapped onto specific psychological processes, such as attention, control, or memory. As such, computational models can benefit the development of psychological theories of visual attention.</p><p>Second, more powerful models can help to characterize more fine-grained individual differences, some of which are not captured with traditional approaches. Several modern models are particularly helpful in extracting the temporal dynamics of gaze. Individuals who have the same dwell time on certain regions may have completely different gaze shift patterns (e.g., scan back and forth between two regions vs. look at each one for half the duration), which will be captured by EMHMM, RNN and ISC analyses. These richer metrics expand the dimensionality of analysis in biologically meaningful ways, offering saliency weights that quantify preferences to a wide array of features, and EMHMMs that generate fixation transition probabilities between AOIs. Such measures can serve as sensitive phenotypes for detecting clinically relevant individual variability that has been previously overlooked. Once such diagnostically meaningful constellations of visual features are discovered, they can inform the design of optimized stimuli that are tailored to differentiate clinical subgroups based on their unique attentional profiles. In the ideal case, one could iteratively use such an approach to generate eye-tracking stimuli that could be used in the clinic to screen for and to help diagnose psychiatric diseases. could be used in the clinic to screen for and to help diagnose psychiatric diseases.</p><p>Third, advanced machine learning algorithms enhance predictive and clinical utility. In addition to the success of supervised ML in classifying various clinical groups, some of the models show high interpretability and explain which eye movement features are most informative. Beyond classification, they have huge potential in tracking symptom progression and monitoring treatment efficacy. Unsupervised ML can even reveal transdiagnostic dimensions based on eye movement patterns, which benefits the design of individualized interventions within and across established diagnostic categories. Importantly, the predictive power of computational eye-tracking approaches can be further improved by integrating gaze with other modalities <ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b93">94</ref> , such as eye blinks, pupillometry, behavioral assessments, neuroimaging, physiological signals, or genotypes <ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b93">94,</ref><ref type="bibr" target="#b146">147,</ref><ref type="bibr" target="#b147">148</ref> . For instance, a mobile app developed for early detection of autism achieved strong accuracy with gaze features alone, and even higher accuracy when combined with facial expressions, head movements, and user interaction metrics -yet gaze remained one of the top predictive features <ref type="bibr" target="#b78">79</ref> . As multimodal fusion advances, including through large language models <ref type="bibr" target="#b146">147,</ref><ref type="bibr" target="#b147">148</ref> , eye-tracking will contribute significantly to precision psychiatry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Future Directions and Open Questions</head><p>Establishing standardized procedures A perennial challenge in eye-tracking research is the relatively low consistency of findings across studies, even when testing the same construct on the same clinical population <ref type="bibr" target="#b148">149,</ref><ref type="bibr" target="#b149">150</ref> . Part of the inconsistency stems from the lack of standardized procedures in eye-tracking experiments, spanning equipment (see discussions of advanced eye-tracking technologies in Supplementary Information S3), testing environment, quality control, data processing and documentation. Standardization becomes even more challenging in real-world protocols, where variability in lighting, stimulus features, and motion can further hinder comparability across studies (despite its benefit to ecological validity, see Supplementary Information S2). To mitigate this issue, recent guidelines argue for a set of parameters to report during data collection and data processing of eye-tracking research <ref type="bibr" target="#b150">151</ref> .</p><p>Although modern eye trackers offer high spatial and temporal resolution <ref type="bibr" target="#b151">152</ref> , the data remain susceptible to noise. Excessive blinks, head movement, and off-screen gazes can cause data loss or inaccurate gaze estimation <ref type="bibr" target="#b152">153</ref> . Because such behaviors may be associated with psychiatric symptoms, the resulting noise can confound analyses. Computational models might misinterpret noise as meaningful patterns and show spurious correlations with variables of interest. Therefore, careful quality checks and clear justification of inclusion and exclusion criteria are essential. Ideally, results are computed across a range of data quality, ensuring that no spurious findings emerge as data quality changes. When deriving individual phenotypes for group-level comparisons, one should assess model fits for each participant. The quality of the model fit can itself be informative about the participant's condition, yet poorly fitted models can introduce substantial noise if their parameters are used in further analyses. For novel mathematical models that have yet to be validated, additional steps -such as model simulations, parameter recovery, model comparisons, and posterior predictive checksare essential (see a practical guide of best practices in model validation <ref type="bibr" target="#b153">154</ref> ). These best practices build a solid foundation for integrating eye-tracking data across sites, studies, and modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Building larger and richer datasets</head><p>With standardized methods in place, the next key priority is to assemble larger and more representative eye-tracking datasets. Many existing studies in psychiatric research are limited by small sample sizes and lack normative data stratified by demographics, impeding statistical power, replicability, and clinical translation. When eye-tracking metrics are proposed as diagnostic biomarkers, the lack of demographicspecific normative references hinders their translation to clinical use. Establishing large normative datasets that capture the typical range of gaze characteristics across age, sex, and other demographic factors is therefore essential. Early efforts in this direction, such as mapping developmental trajectories of attention biases across sexes, have illustrated the potential of such resources in providing reliable reference points <ref type="bibr" target="#b154">155,</ref><ref type="bibr" target="#b155">156</ref> .</p><p>The need for larger datasets is further amplified by the use of computational models and machine learning. These approaches involve numerous parameters and require extensive data, both in terms of longer recordings for individualized modeling and larger samples for classification or clustering. Machine learning models trained on small and homogeneous datasets are prone to overfitting and may fail to generalize to new samples. Multi-site collaborations and open data sharing are making large-scale data curation increasingly feasible. To ensure generalizability, future studies should train models on combined datasets collected from diverse populations and sites, while maintaining open sharing practices that allow cumulative sample growth and crossstudy comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Toward better clinical translation</head><p>With standardized methods and large, demographically diverse datasets in place, the next step is to translate computational eye-tracking findings into clinically meaningful applications. Although computational phenotypes and machine learning models show promise for classifying psychiatric groups based on gaze patterns, their diagnostic and predictive validity remains uncertain.</p><p>For case-control group comparisons, results are fundamentally shaped by how groups are defined to begin with. For example, differences observed between healthy controls and patients who have only one psychiatric diagnosis without comorbidity may not generalize when comparing with patients who have other comorbid symptoms. Similarly, predictive algorithm trained on one demographic group may not generalize to others. Much of the existing work focuses on binary classifications against healthy controls. This leaves unclear the discriminant validity of findings when compared to other clinical groups and overlooks the complexity of psychiatric diagnosis, where overlapping symptoms are common across conditions. Future efforts should expand beyond control-versus-patient contrasts to include multiple patient groups with overlapping functional challenges (e.g., autism and social anxiety in social attention <ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b156">157,</ref><ref type="bibr" target="#b157">158</ref> , depression and anxiety in attention to emotional stimuli <ref type="bibr" target="#b149">150,</ref><ref type="bibr" target="#b158">159</ref> ). In fact, lack of discriminant validity for related psychiatric conditions might suggest a transdiagnostic nature of the psychological mechanism underlying a phenotype: disorders may share common processes, and interventions aimed at transdiagnostic dimensions may be effective for multiple diagnoses <ref type="bibr" target="#b159">160</ref> . Understanding whether a phenotype is shared across diagnoses or unique to a particular condition can validate or revise the current diagnosis classification, informing decisions about screening, diagnosis, and treatment that may be targeted at one or more clinical groups.</p><p>A second major obstacle for clinical translation is the lack of explainability of advanced computational models. Deep neural network models, in particular, are often referred to as 'black boxes', because how they work and what information they use when making predictions are undisclosed. Although their performance can surpass that of human experts, the opacity of judgements poses ethical challenges: if clinicians cannot understand the decision-making, they will not be able to communicate this to patients, thus affecting patients' ability to engage in informed consent <ref type="bibr" target="#b160">161</ref> . Furthermore, some high-performing models may accidentally achieve high accuracy by using confounding variables, which can eventually lead to misleading or even harmful outcomes <ref type="bibr" target="#b161">162</ref> . The pursuit of explainable AI therefore remains critical, and ongoing research has made efforts to address the concern by identifying features and layers of neural network that contribute to the prediction, offering a path toward safer and more trustworthy applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>Eye movement patterns are quantifiable, difficult to consciously control, and reflect neural processing of attention and perception. Many of the clinically relevant conclusions that have been drawn from neuroimaging studies are mirrored in eyetracking studies -for a fraction of the cost and substantially greater ease of application. Recent advances in machine learning and artificial intelligence, along with broader computational methods, have offered new ways of collecting and analyzing eye-tracking data to realize its full potential in psychiatry research.</p><p>Visual saliency models that predict gaze patterns based on low-level visual properties have been expanded to incorporate multiple levels of features, such as objects and semantic content. This has led to improved accuracy in predicting human gaze and greater sensitivity to individual differences in attention patterns. Data-driven, multivariate characterizations of gaze complement hypothesis-driven analyses by clustering parts of the stimuli that can be meaningfully grouped or by identifying distinct attention "styles" across individuals. Both supervised and unsupervised machine learning methods can utilize multiple eye-tracking metrics to reveal gaze patterns associated with a particular psychiatric condition, or can identify subtypes within or across clinical conditions.</p><p>Finally, these analytic approaches offer promising new directions for future researchsuch as combining these methods with scalable, camera-based eye-tracking through smartphones or webcams; applying novel experimental tools to probe gaze patterns during real-life interactions; and leveraging large-scale databases of clinical conditions with substantial sample sizes. Thus, computational approaches to eye-tracking research holds significant potential to reveal individual differences and to advance our understanding of cognition in both the general population and among psychiatric conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This study was funded by the Simons Foundation Autism Research Initiative grant (grant # 990500).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Author's Contributions</head><p>Q.W. and R.A. conceived the study. Q.W. and N.K. conducted the literature review and drafted the initial manuscript. R.A. provided substantial feedback and edits to the original draft. All authors reviewed and revised the manuscript and approved the final version. tracking <ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref> have emphasized higher ecological validity -the extent to which findings 94 generalize to the natural environments. 95   The need for ecological validity depends on the research goal. If the aim is clinical -to 96 find a biomarker for screening and diagnosis, then paradigms that effectively elicit group 97 differences may be more valuable than those maximizing naturalism. For example, 98 smooth pursuit measures in schizophrenia rely on simple moving targets and 99 outperform more naturalistic tasks in distinguishing patients from controls <ref type="bibr" target="#b29">30,</ref><ref type="bibr">31</ref> . In fact, 100 stimuli that are supposed to be more naturalistic could perform worse if the resulting 101 metric serves as a biomarker: one study showed that fixation duration differences 102 between SZ and controls are much less in the naturalistic environment compared to 103 laboratory tests <ref type="bibr" target="#b31">32</ref> . Similarly, movies, compared to static images from them, evoke 104 stronger synchrony of eye thereby blurring individual variability <ref type="bibr">22,33 . 105</ref> In contrast, when the goal is to understand underlying mechanisms or etiology, 106 ecological validity becomes crucial. This echoes Brunswik's notion of "representative 107 design," emphasizing that experimental stimuli and tasks should mirror the real-world 108 contexts in which psychological processes occur <ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35</ref> . While some of the findings from 109 static image viewing may well generalize to the real world (e.g., see an example of such 110 success in ref <ref type="bibr" target="#b35">36</ref> ) for some populations, others may not -especially when studying social </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S1. Conventional eye movement types and their analysis</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Studying eye movements when viewing visual stimuli. (a) Multiple brain structures are involved to initiate and control eye movements. (b) Eye movements, including fixations and saccades, can be visualized in the form of heatmaps or scanpaths.</figDesc><graphic coords="4,72.00,101.55,468.00,359.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Using visual saliency models to understand individual differences in attention. (a) Individual gaze patterns coming from two predefined types of participants (Type 1, Type 2) are analyzed by comparing them to visual saliency maps of the stimuli. For each image (same as in Fig 1), both low-level saliency (based on features like color, intensity, and orientation) and high-level saliency (highlighting semantically meaningful elements such as people and objects) are computed. Model fit scores quantify how strongly each individual's gaze aligns with different saliency types, revealing differences in visual preferences. (b) Quantifying individual attention saliency weights. In one study, attention weights were derived for features across three levels: pixel-level (e.g., color, intensity), object-level (e.g., size, complexity), and semantic-level (e.g., faces, text, watchability). Weights for all the features comprise a computational phenotype for an individual's visual attention style 15 .</figDesc><graphic coords="8,121.50,72.00,368.75,357.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Data-driven AOI identification through Eye Movement Hidden Markov Models (EMHMM). (a) EMHMM procedures. Step 1: generate individual HMMs, including estimation of personalized AOIs (each color represents an AOI) and transition probabilities within and between them. Step 2: derive representative group HMMs (AOIs and their transitions) based on individual HMMs, using the variational hierarchical expectation maximization algorithm (VHEM). In the face stimuli example, two representative patterns can be nose-focused and eyes-focused patterns. The similarity between individual HMMs and representative HMMs is calculated as the log-likelihood of the representative pattern being generated by the individual. Step 3: associate similarity scores with other measures of interest. The similarity to eyes-focused pattern (relative to nose-focused pattern) becomes a behavioral phenotype, which can be compared across predefined types of participants, or correlated with measurement of interest. (b) Beyond face stimuli, EMHMM can be applied to a variety of images, such as static social scenes and visual exploration stimuli. Adapted from 24,70 .</figDesc><graphic coords="11,72.00,72.00,450.35,393.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Machine learning in eye-tracking data analysis. (a) Supervised classification. Classic machine learning models can predict group labels with processed eye-tracking metrics. Convolutional neural networks are good at processing 2D images of gaze such as heatmap or scanpath and extracting lower-dimensional latent features for classification. Recurrent neural networks handle the temporal dependencies of time series data. Each dot represents a sample (one individual, or one individual's trial), and different shapes and colors represent different predefined classes. (b) Unsupervised clustering. When clustering is based on processed ET metrics, the (dis-)similarity can be defined as the distance (e.g., Euclidean distance) between samples in a feature space. When clustering based on the entire ET data, the similarity can be defined as the inter-subject correlation (ISC) and can be visualized in a low dimensional space. Clustering can be conducted given the similarities using different techniques, such as Kmeans clustering and hierarchical clustering. Each gray dot represents a sample that may come from different populations (different shapes), colors represent different datadriven clusters.</figDesc><graphic coords="19,72.00,72.00,468.00,387.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>24 For 29 Microsaccade:</head><label>2429</label><figDesc>are ballistic rapid eye movements from one point (fixation) to another. During saccades,22   visual perception is suppressed, and thus our perception of the world is smoothly 23 continuous. Fixations are typically brief (around 200 ms) pauses between saccades 1 . data analysis, saccades and fixations are identified from the timeseries of gaze 25 positions that is measured by an eye tracker, based on the velocity and acceleration of 26 the eye movement. Commonly used metrics include fixation duration, first fixation 27 latency (often from an intertrial fixation cross), saccade amplitude (distance), and 28 saccade velocity. Microsaccades are small, involuntary movements that occur 30 spontaneously during fixation. These movements typically occur 1-3 times per second, 31 and lasts 6-30 ms 2 . Microsaccades have been found to be closely associated with 32 covert attention shifts 3 . Similar to saccades, microsaccades are most often defined by 33 thresholds of the velocity, amplitude and duration 4 . 34 Smooth pursuit: Smooth pursuit occurs when the eyes track a moving object and keep 35 the object stabilized on the fovea. Unless highly trained, one cannot make a smooth 36 pursuit without a moving target 5 . In fact, like saccades, it is observed during the rapid 37 eye movement (REM) epochs that accompany dreaming, and has been used as 38 evidence that dreamers can experience conscious visual imagery of moving objects 6 . 39 Smooth pursuit is a clinical marker of schizophrenia. When schizophrenia patients are 40 asked to track a moving target with their eyes, they tend to show lagged eye trajectories 41 following the target, and catch-up saccades immediately afterwards 7 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>111 attention.</head><label>111</label><figDesc>Real-time social interactions involve eye contact, joint attention, and signaling 112 of intentions or desires, and gaze patterns observed in static face viewing may not 113 generalize to dynamic, interactive situations (e.g., talking faces or mutual gaze)<ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38</ref> .114 Indeed, gaze-to-face behaviors can differ markedly between interactive and non-115 interactive contexts<ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39</ref> . The highly context-dependent nature of social attention could 116 lead to conflicting results observed between lab and naturalistic environments. For 117 instance, a study found that individuals with higher social anxiety scores show reduced 118 fixations at persons closer in distance, yet the effect only existed in the real life condition 119 but not the in-lab video-watching condition 40 . 120 Another situation when ecological validity matters is the study of goal-directed attention 121 allocation in everyday life activities, such as driving, navigation, and shopping 41-43 . In 122 these behaviors, attention is guided primarily by top-down control rather than bottom-up 123 visual saliency, and eye movements reflect how individuals flexibly allocate attention to 124 gather information, coordinate actions, reduce uncertainty, and maximize reward 44-46 . 125 Therefore, proximity to real life situations is crucial for capturing the cognitive 126 constraints that shape such behaviors, including time pressure, cognitive load, and 127 motor coordination. These task-oriented gaze patterns have also been modeled under 128 various computational frameworks 47-50 , and are promising tools for examining 129 dysfunctions in executive functioning, reward processing, and action planning in the 130 clinical population 41,51 . S3. Camera-based eye-tracking 132 New technologies use camera input from smartphones, tablets, or glasses offering 133 scalable alternatives to traditional infrared-based desktop systems 25,38,52-57 . These 134 approaches apply computer vision models to detect face and the eye regions, and 135 estimate gaze locations through pre-trained algorithms. Compared to the standard 136 desktop-based, infrared eye-trackers (e.g., Tobii, EyeLink), which achieve high spatial 137 accuracy (less than 1° error) and sampling rates up to 1200 Hz, current camera-based 138 methods generally have limited spatial and temporal (15-50 Hz) resolution. However, 139 gaze estimation has been improving with deep learning-based models, which to date 140 have achieved spatial accuracies as low as 2.4° and precision of 0.47° visual angle 52 . 141 Notably, these methods have been in psychology research: a recent paper 142 demonstrated sufficient accuracy of a smartphone-based eye-tracking tool for analyzing 143 gaze on specific features (e.g., human face, body area) of YouTube videos and 144 replicated well-established findings on atypical social attention in autism 25 . 145 Wearable eye-tracking glasses further expand these possibilities by enabling the study 146 of real-world social interactions, such as making and sustaining eye contact. A recent 147 model, the Pupil Labs Neon, does not require any calibration, delivers a spatial 148 resolution of 1.3° visual angle at 200Hz, and collects audio stream and head movement 149 data simultaneously 58,59 . As models continue to improve, camera-based eye-tracking 150 holds great promise for broadening access to large-scale, high-quality gaze data 151 collection across diverse populations.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Saccade &amp; Fixation: Saccades, the most common behavior when we view a scene,</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>Academy of Sciences116, 11687-11692 (2019).</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Competing Interests</head><p>The authors declare no competing interests. images and revealed robust individual differences in gaze patterns <ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref> . However, 92 recent paradigms using movies, immersive virtual reality, or real-world first-person eye-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Idiosyncratic characteristics of saccadic eye movements when viewing different visual environments</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Coppola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2947" to="2953" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Human gaze control during real-world scene perception</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="498" to="504" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An oculomotor continuum from exploration to fixation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Otero-Millan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Macknik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Langston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Martinez-Conde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="6175" to="6180" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Visual search for arbitrary objects in real scenes</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wolfe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rosenholtz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">I</forename><surname>Kuzmova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Sherman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Atten Percept Psychophys</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="1650" to="1671" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Infant viewing of social scenes is under genetic control and is atypical in autism</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Constantino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">547</biblScope>
			<biblScope unit="page" from="340" to="344" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Visual Attention and Eye Movements</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Hoffman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Psychology Press</publisher>
		</imprint>
	</monogr>
	<note>in Attention</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Understanding cognitive and affective mechanisms in social psychology through eye-tracking</title>
		<author>
			<persName><forename type="first">R.-M</forename><surname>Rahal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fiedler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Social Psychology</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page">103842</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Attention and choice: A review on eye movements in decision making</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Orquin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mueller Loose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Psychologica</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="page" from="190" to="206" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Worth a glance: using eye movements to investigate the cognitive neuroscience of memory</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Hannula</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front Hum Neurosci</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">166</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Functional neuroanatomy of the human eye movement network: a review and atlas</title>
		<author>
			<persName><forename type="first">B</forename><surname>Coiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain Struct Funct</title>
		<imprint>
			<biblScope unit="volume">224</biblScope>
			<biblScope unit="page" from="2603" to="2617" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural mechanisms of selective visual attention</title>
		<author>
			<persName><forename type="first">R</forename><surname>Desimone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Duncan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu Rev Neurosci</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="193" to="222" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A biased competition theory for the developmental cognitive neuroscience of visuo-spatial attention</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kastner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Opinion in Psychology</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="219" to="228" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Visual Perception and Eye Movements</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Greenlee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kimmig</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-20085-5_5</idno>
	</analytic>
	<monogr>
		<title level="m">Eye Movement Research: An Introduction to its Scientific Foundations and Applications</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Klein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Ettinger</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="165" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Computational modelling of visual attention</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Rev Neurosci</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="194" to="203" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Atypical Visual Saliency in Autism Spectrum Disorder Quantified through Model-Based Eye Tracking</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="604" to="616" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reduced social attention in autism is magnified by perceptual load in naturalistic environments</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Haskins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autism Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="2310" to="2323" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A constellation of eye-tracking measures reveals social attention differences in ASD and the broad autism phenotype</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nayar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Winston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Losh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Molecular Autism</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Anxiety and attention to threatening pictures</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yiend</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mathews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Quarterly Journal of Experimental Psychology Section A</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="665" to="681" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Anxiety and the Allocation of Attention to Threat</title>
		<author>
			<persName><forename type="first">C</forename><surname>Macleod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mathews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Quarterly Journal of Experimental Psychology Section A</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="653" to="670" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Comparison of change detection performance and visual search patterns among children with/without ADHD: Evidence from eye movements</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Türkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Ercan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Perçinel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Res Dev Disabil</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">50</biblScope>
			<biblScope unit="page" from="205" to="215" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The Effect of Task on Attentional Performance in Children With ADHD</title>
		<author>
			<persName><forename type="first">S</forename><surname>Caldani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Integr. Neurosci</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Temporal Methods for Eye Movement Analysis</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Bischof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kingstone</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-20085-5_10</idno>
	</analytic>
	<monogr>
		<title level="m">Eye Movement Research</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="407" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Look into my eyes: Investigating joint attention using interactive eyetracking and fMRI in a developmental sample</title>
		<author>
			<persName><forename type="first">E</forename><surname>Oberwelland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="page" from="248" to="260" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Griffin</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.bpsc.2024.08.017</idno>
		<idno>doi:10.1016/j.bpsc.2024.08.017</idno>
		<ptr target="https://doi.org/10.1016/j.bpsc.2024.08.017" />
	</analytic>
	<monogr>
		<title level="m">Spatiotemporal Eye Movement Dynamics Reveal Altered Face Prioritization in Early Visual Processing Among Autistic Children</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">How is visual salience computed in the brain? Insights from behaviour, neurobiology and modelling</title>
		<author>
			<persName><forename type="first">R</forename><surname>Veale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">M</forename><surname>Hafed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phil. Trans. R. Soc. B</title>
		<imprint>
			<biblScope unit="volume">372</biblScope>
			<biblScope unit="page">20160113</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Shifts in Selective Visual Attention: Towards the Underlying Neural Circuitry</title>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-94-009-3833-5_5</idno>
	</analytic>
	<monogr>
		<title level="m">Matters of Intelligence: Conceptual Structures in Cognitive Neuroscience</title>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Vaina</surname></persName>
		</editor>
		<meeting><address><addrLine>Netherlands, Dordrecht</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="115" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A feature-integration theory of attention</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Treisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gelade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="97" to="136" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to predict where humans look</title>
		<author>
			<persName><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2009.5459462</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="2106" to="2113" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Graph-Based Visual Saliency</title>
		<author>
			<persName><forename type="first">J</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Saliency Detection: A Spectral Residual Approach</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2007.383267</idno>
	</analytic>
	<monogr>
		<title level="m">2007 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">SUN: A Bayesian framework for saliency using natural statistics</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">DeepGaze IIE: Calibrated Prediction in and Out-of-Domain for State-of-the-Art Saliency Modeling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Linardos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kümmerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12919" to="12928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Reducing the Semantic Gap in Saliency Prediction by Adapting Deep Neural Networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Boix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><surname>Salicon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">262</biblScope>
			<biblScope unit="page">270</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">DeepFix: A Fully Convolutional Neural Network for Predicting Human Eye Fixations</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S S</forename><surname>Kruthiventi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ayush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="4446" to="4456" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">MIT/Tübingen Saliency Benchmark</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kümmerer</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">ViNet: Pushing the limits of Visual Modality for Audio-Visual Saliency Prediction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2012.06170</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2012.06170" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint at</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">A Deep Audio-Visual Embedding for Dynamic Saliency Prediction</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Tavakoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName><surname>Dave</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1905.10693</idno>
		<ptr target="https://doi.org/10.48550/arXiv.1905.10693" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint at</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">DeepGaze III: Modeling free-viewing human scanpaths with deep learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kümmerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S A</forename><surname>Wallis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Objects guide human gaze behavior in dynamic real-world scenes</title>
		<author>
			<persName><forename type="first">N</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rolfs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Hellwich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Obermayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">1011512</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A robotics-inspired scanpath model reveals the importance of uncertainty and semantic object cues for gaze guidance in dynamic scenes</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mengers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Obermayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rolfs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Gravitational Laws of Focus of Attention</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zanca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Melacci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Pattern Anal Mach Intell</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="2983" to="2995" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">State-of-the-Art in Human Scanpath Prediction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kümmerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2102.12239</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2102.12239" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint at</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">A Benchmark of Computational Models of Saliency to Predict Human Fixations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<ptr target="https://dspace.mit.edu/handle/1721.1/68590" />
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><surname>Cat</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1505.03581</idno>
		<ptr target="https://doi.org/10.48550/arXiv.1505.03581" />
		<title level="m">A Large Scale Fixation Dataset for Boosting Saliency Research</title>
		<imprint>
			<date type="published" when="2000">2000. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint at</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Saliency models perform best for women&apos;s and young adults&apos; fixations</title>
		<author>
			<persName><forename type="first">C</forename><surname>Strauch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun Psychol</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">The Gaze of Schizophrenia Patients Captured by Bottom-up Saliency</title>
		<author>
			<persName><forename type="first">P</forename><surname>Adámek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Sensory salience processing moderates attenuated gazes on faces in autism spectrum disorder: a case-control study</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Molecular Autism</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Saliency Models Reveal Reduced Top-Down Attention in Attention-Deficit/Hyperactivity Disorder: A Naturalistic Eye-Tracking Study</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dziemian</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jaacop.2024.03.001</idno>
	</analytic>
	<monogr>
		<title level="j">JAACAP Open S</title>
		<imprint>
			<biblScope unit="volume">2949732924000280</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Individual differences in visual salience vary along semantic dimensions</title>
		<author>
			<persName><forename type="first">B</forename><surname>De Haas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Iakovidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Schwarzkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Gegenfurtner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="11687" to="11692" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Atypical gaze patterns in autistic adults are heterogeneous across but reliable within individuals</title>
		<author>
			<persName><forename type="first">U</forename><surname>Keles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Molecular Autism</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">39</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A novel machine learning analysis of eye-tracking data reveals suboptimal visual information extraction from facial stimuli in individuals with autism</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Król</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Król</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuropsychologia</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="397" to="406" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Facing challenges: A survey of object tracking</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Signal Processing</title>
		<imprint>
			<biblScope unit="volume">161</biblScope>
			<biblScope unit="page">105082</biblScope>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Computer vision for primate behavior analysis in the wild</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vogg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Methods</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1154" to="1166" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Facial Expression Analysis: Unveiling the Emotions Through Computer Vision</title>
		<author>
			<persName><surname>Uneza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saini</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICRITO61523.2024.10522418</idno>
	</analytic>
	<monogr>
		<title level="m">2024 11th International Conference on Reliability, Infocom Technologies and Optimization</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Active vision in immersive, 360° real-world environments</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Haskins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mentch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Botch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci Rep</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">14304</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Report: Differences in Naturalistic Attention to Real-World Scenes in Adolescents with 16p.11.2 Deletion</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Haskins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mentch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Van Wicklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">B</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><surname>Brief</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Autism Dev Disord</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="1078" to="1087" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Machine learning accurately classifies age of toddlers based on eye tracking</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Dalrymple</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Elison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci Rep</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">6255</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Meaning-based guidance of attention in scenes as revealed by meaning maps</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Hayes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Hum Behav</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="743" to="747" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Individual differences in human gaze behavior generalize from faces to objects</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Broda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>De Haas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page">2322149121</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">The breakdown of social looking</title>
		<author>
			<persName><forename type="first">T</forename><surname>Falck-Ytter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroscience &amp; Biobehavioral Reviews</title>
		<imprint>
			<biblScope unit="volume">161</biblScope>
			<biblScope unit="page">105689</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Individual differences in looking at persons in scenes</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Broda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>De Haas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Using principal component analysis to characterize eye movement fixation patterns during face viewing</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wegner-Clemens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rennig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Magnotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Beauchamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Identifying children with autism spectrum disorder based on their face processing abnormality: A machine learning framework</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autism Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="888" to="898" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A manifesto for reproducible science</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Munafò</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Hum Behav</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Data-driven analysis of gaze patterns in face perception: Methodological and clinical contributions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Masulli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cortex</title>
		<imprint>
			<biblScope unit="volume">147</biblScope>
			<biblScope unit="page" from="9" to="23" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">NMF-Based Analysis of Mobile Eye-Tracking Data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Klötzl</surname></persName>
		</author>
		<idno type="DOI">10.1145/3649902.3653518</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page" from="1" to="9" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Eye movement analysis with hidden Markov models (EMHMM) with co-clustering</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behav Res</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="2473" to="2486" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Understanding the role of eye movement consistency in face recognition and autism through integrating deep neural networks and hidden Markov models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K S</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">npj Sci. Learn</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Understanding Human Cognition Through Computational Modeling</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Hsiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Topics in Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="349" to="376" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Understanding eye movements in face recognition using hidden Markov models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Hsiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Clustering Hidden Markov Models with Variational HEM</title>
		<author>
			<persName><forename type="first">E</forename><surname>Coviello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R G</forename><surname>Lanckriet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="697" to="747" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Eye-movement patterns in face recognition are associated with cognitive decline in older adults</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Hsiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychon Bull Rev</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="2200" to="2207" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Individual differences in computational psychiatry: A review of current challenges</title>
		<author>
			<persName><forename type="first">P</forename><surname>Karvelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">O</forename><surname>Diaconescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroscience &amp; Biobehavioral Reviews</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="page">105137</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">The promise of a model-based psychiatry: building computational models of mental ill health</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">U</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Skvortsova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Koutsouleris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Lancet Digital Health</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="816" to="e828" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Data-Driven Diagnostics and the Potential of Mobile Artificial Intelligence for Digital Therapeutic Phenotyping in Computational Psychiatry</title>
		<author>
			<persName><forename type="first">P</forename><surname>Washington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Psychiatry: Cognitive Neuroscience and Neuroimaging</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="759" to="769" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Eye-Tracking-Based Measurement of Social Visual Engagement Compared With Expert Clinical Diagnosis of Autism</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA</title>
		<imprint>
			<biblScope unit="volume">330</biblScope>
			<biblScope unit="page" from="854" to="865" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Development and Replication of Objective Measurements of Social Visual Engagement to Aid in Early Diagnosis and Assessment of Autism</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA Network Open</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">2330145</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Early detection of autism using digital behavioral phenotyping</title>
		<author>
			<persName><forename type="first">S</forename><surname>Perochon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Med</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2489" to="2497" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Large-scale examination of early-age sex differences in neurotypical toddlers and those with autism spectrum disorder or other developmental conditions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nazari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Hum Behav</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1697" to="1709" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Smooth pursuit in schizophrenia: A meta-analytic review of research since 1993</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>O'driscoll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Callahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain and Cognition</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="359" to="370" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Towards Clinically Relevant Oculomotor Biomarkers in Early Schizophrenia</title>
		<author>
			<persName><forename type="first">F</forename><surname>Athanasopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O.-V</forename><surname>Saprikis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Margeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Smyrnis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Behav. Neurosci</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Application of eye trackers for understanding mental disorders: Cases for schizophrenia and autism spectrum</title>
		<author>
			<persName><forename type="first">E</forename><surname>Shishido</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuropsychopharmacology Reports</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="72" to="77" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Best Practices in Supervised Machine Learning: A Tutorial for Psychologists</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pargent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schoedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stachl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Methods and Practices in Psychological Science</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">25152459231162559</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Eye gaze as a biomarker in the recognition of autism spectrum disorder using virtual reality and machine learning: A proof of concept for diagnosis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Alcañiz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autism Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="131" to="145" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Simple Viewing Tests Can Detect Eye Movement Abnormalities That Distinguish Schizophrenia Cases from Controls with Exceptional Accuracy</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Benson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Psychiatry</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="716" to="724" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Use of eye tracking to improve the identification of attentiondeficit/hyperactivity disorder in children</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci Rep</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14469</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Classifying ASD children with LSTM based on raw videos</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">390</biblScope>
			<biblScope unit="page" from="226" to="238" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Diagnosing and tracking depression based on eye movement in response to virtual reality</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front Psychiatry</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">1280935</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Development of an eye-tracking system based on a deep learning model to assess executive function in patients with mental illnesses</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci Rep</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">18186</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">An Analysis of Eye-Tracking Features and Modelling Methods for Free-Viewed Standard Stimulus: Application for Schizophrenia Detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kacur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Polec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Smolejova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Heretik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Biomedical and Health Informatics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="3055" to="3065" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Machine learning based on eye-tracking data to identify Autism Spectrum Disorder: A systematic review and meta-analysis</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Biomedical Informatics</title>
		<imprint>
			<biblScope unit="volume">137</biblScope>
			<biblScope unit="page">104254</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Attention and impulsivity assessment using virtual reality games</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mendez-Encinas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sujar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bayona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Delgado-Gomez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci Rep</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">13689</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Virtual reality-assisted prediction of adult ADHD based on eye tracking, EEG, actigraphy and behavioral indices: a machine learning analysis of independent training and test samples</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transl Psychiatry</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">508</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Classification of Autism Spectrum Disorder Severity Using Eye Tracking Data Based on Visual Attention Model</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Revers</surname></persName>
		</author>
		<idno type="DOI">10.1109/CBMS52027.2021.00062</idno>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE 34th International Symposium on Computer-Based Medical Systems (CBMS)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">High-throughput classification of clinical populations from natural viewing eye movements</title>
		<author>
			<persName><forename type="first">P.-H</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Neurol</title>
		<imprint>
			<biblScope unit="volume">260</biblScope>
			<biblScope unit="page" from="275" to="284" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Scanpath modeling and classification with hidden Markov models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Coutrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behav Res</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="362" to="379" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">The Use of Multiple Measurements in Taxonomic Problems</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Eugenics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="179" to="188" />
			<date type="published" when="1936">1936</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">A Mathematical Theory of Communication</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell System Technical Journal</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="379" to="423" />
			<date type="published" when="1948">1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Gene Selection for Cancer Classification using Support Vector Machines</title>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Barnhill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="389" to="422" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy</title>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1226" to="1238" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">A unified approach to interpreting model predictions</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-I</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4768" to="4777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Why Should I Trust You?&apos;: Explaining the Predictions of Any Classifier</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="DOI">10.1145/2939672.2939778</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining 1135-1144</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining 1135-1144<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Investigation of Eye-Tracking Scan Path as a Biomarker for Autism Screening Using Machine Learning Algorithms</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Kanhirakadavath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S M</forename><surname>Chandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Diagnostics (Basel)</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">518</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Using Machine Learning to Diagnose Autism Based on Eye Tracking Technology</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Jaradat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wedyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Alomari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Barhoush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Diagnostics (Basel)</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">66</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Detection of ADHD Based on Eye Movements During Natural Viewing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<editor>
			<persName><forename type="first">M.-R</forename><surname>Amini</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Nature Switzerland</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">13718</biblScope>
			<biblScope unit="page" from="403" to="418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">EMS: A Large-Scale Eye Movement Dataset, Benchmark, and New Model for Schizophrenia Recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2024.3441928</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Learning Visual Attention to Identify People with Autism Spectrum Disorder</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.354</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting><address><addrLine>Venice</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3287" to="3296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Applying Eye Tracking with Deep Learning Techniques for Early-Stage Detection of Autism Spectrum Disorders</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">A T</forename><surname>Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">168</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">An Analysis of Eye-Tracking Features and Modelling Methods for Free-Viewed Standard Stimulus: Application for Schizophrenia Detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kacur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Polec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Smolejova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Heretik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J Biomed Health Inform</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="3055" to="3065" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">CNN-LSTM Based ASD Classification Model using Observer ScanPaths</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-L</forename><surname>Shyu</surname></persName>
		</author>
		<author>
			<persName><surname>Sp-Asdnet</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICMEW.2019.00124</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia &amp; Expo Workshops (ICMEW)</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="641" to="646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Enhanced multilevel autism classification for children using eye-tracking and hybrid CNN-RNN deep learning models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cheekaty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Muneeswari</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00521-024-10633-0</idno>
		<idno>doi:10.1007/s00521-024-10633-0</idno>
		<ptr target="https://doi.org/10.1007/s00521-024-10633-0" />
	</analytic>
	<monogr>
		<title level="j">Neural Comput &amp; Applic</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Online eye-movement classification with temporal convolutional networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Elmadjian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gonzales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Morimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behav Res</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="3602" to="3620" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">An overview of clustering methods with guidelines for application in mental health research</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">X</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychiatry Research</title>
		<imprint>
			<biblScope unit="volume">327</biblScope>
			<biblScope unit="page">115265</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Interactive eye tracking for gaze strategy modification</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1145/2771839.2771888</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Conference on Interaction Design and Children</title>
		<meeting>the 14th International Conference on Interaction Design and Children<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">247</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Gaze Response to Dyadic Bids at 2 Years Related to Outcomes at 3 Years in Autism Spectrum Disorders: A Subtyping Analysis</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Macari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chawarska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Autism Dev Disord</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="431" to="442" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Visual exploration dynamics are low-dimensional and driven by intrinsic factors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zangrossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Celli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zorzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Corbetta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun Biol</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">1100</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Learning Clusters in Autism Spectrum Disorder: Image-Based Clustering of Eye-Tracking Scanpaths with Deep Autoencoder</title>
		<author>
			<persName><forename type="first">M</forename><surname>Elbattah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Carette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dequen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Guérin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cilia</surname></persName>
		</author>
		<idno type="DOI">10.1109/EMBC.2019.8856904</idno>
	</analytic>
	<monogr>
		<title level="m">2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1417" to="1420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Beyond Average: Individualized Visual Scanpath Prediction</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR52733.2024.02402</idno>
	</analytic>
	<monogr>
		<title level="m">2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="25420" to="25431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Variability of eye movements when viewing dynamic natural scenes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Martinetz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Gegenfurtner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Barth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="28" to="28" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Human-Monkey Gaze Correlations Reveal Convergent and Divergent Patterns of Movie Viewing</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Shepherd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Steckenfinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Hasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Ghazanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Biology</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="649" to="656" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Genetic Influence on Eye Movements to Complex Scenes at Short Timescales</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Biology</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3554" to="3560" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Children with autism observe social interactions in an idiosyncratic manner</title>
		<author>
			<persName><forename type="first">I</forename><surname>Avni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autism Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="935" to="946" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Synchronized eye movements predict test scores in online video education</title>
		<author>
			<persName><forename type="first">J</forename><surname>Madsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">U</forename><surname>Júlio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Gucik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Steinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Parra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<date type="published" when="2021">2016980118. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Onscreen presence of instructors in video lectures affects learners&apos; neural synchrony and visual attention during multimedia learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Nastase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page">2309054121</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Atypical and variable attention patterns reveal reduced contextual priors in children with autism spectrum disorder</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autism Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1572" to="1585" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Autistic differences in the temporal dynamics of social attention</title>
		<author>
			<persName><forename type="first">N</forename><surname>Hedger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chakrabarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autism</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1615" to="1626" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">ScanMatch: A novel method for comparing fixation sequences</title>
		<author>
			<persName><forename type="first">F</forename><surname>Cristino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mathôt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Theeuwes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">D</forename><surname>Gilchrist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="692" to="700" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">It depends on how you look at it: Scanpath comparison in multiple dimensions with MultiMatch, a vector-based approach</title>
		<author>
			<persName><forename type="first">R</forename><surname>Dewhurst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behav Res</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="1079" to="1100" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Representational similarity analysisconnecting the branches of systems neuroscience</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kriegeskorte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Bandettini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Syst. Neurosci</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Multidimensional scaling: I. Theory and method</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Torgerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="401" to="419" />
			<date type="published" when="1952">1952</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Visualizing Data using t-SNE</title>
		<author>
			<persName><forename type="first">L</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Van Der &amp; Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<monogr>
		<title level="m" type="main">UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Melville</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1802.03426</idno>
		<ptr target="https://doi.org/10.48550/arXiv.1802.03426" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint at</note>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Atypical gaze patterns in children and adults with autism spectrum disorders dissociated from developmental changes in gaze behaviour</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nakano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. R. Soc. B</title>
		<imprint>
			<biblScope unit="volume">277</biblScope>
			<biblScope unit="page" from="2935" to="2943" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Embracing variability in the search for biological mechanisms of psychiatric illness</title>
		<author>
			<persName><forename type="first">A</forename><surname>Segal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="85" to="99" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Mapping the Heterogeneous Phenotype of Schizophrenia and Bipolar Disorder Using Normative Models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wolfers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA Psychiatry</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="1146" to="1155" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Normative modelling of brain morphometry across the lifespan with CentileBrain: algorithm benchmarking and model optimisation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Lancet Digital Health</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="211" to="e221" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Intersubject synchronization of cortical activity during natural vision</title>
		<author>
			<persName><forename type="first">U</forename><surname>Hasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fuhrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Malach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">303</biblScope>
			<biblScope unit="page" from="1634" to="1640" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Viewing Gaze Behavior in Infants and Adults</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Franchak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Heeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Hasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Adolph</surname></persName>
		</author>
		<author>
			<persName><surname>Free</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Infancy</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="262" to="287" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Idiosyncratic Brain Activation Patterns Are Associated with Poor Social Comprehension in Autism</title>
		<author>
			<persName><forename type="first">L</forename><surname>Byrge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Tyszka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Adolphs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurosci</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="5837" to="5850" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Shared and idiosyncratic cortical activation patterns in autism revealed under continuous real-life viewing conditions</title>
		<author>
			<persName><forename type="first">U</forename><surname>Hasson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autism Res</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="220" to="231" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Unreliable evoked responses in autism</title>
		<author>
			<persName><forename type="first">I</forename><surname>Dinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="981" to="991" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Modeling Eye Gaze to Videos Using Dynamic Trajectory Variability Analysis</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Adolphs</surname></persName>
		</author>
		<ptr target="https://www.biologicalpsychiatryjournal.com/article/S0006-3223(23)00470-5/abstract" />
	</analytic>
	<monogr>
		<title level="j">INSAR</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Linking patterns of infant eye movements to a neural network model of the ventral stream using representational similarity analysis</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Kiat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Developmental Science</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">13155</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Individual gaze shapes diverging neural representations</title>
		<author>
			<persName><forename type="first">P</forename><surname>Borovska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>De Haas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page">2405602121</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Individual gaze predicts individual scene descriptions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kollenda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-S</forename><surname>Reher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>De Haas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci Rep</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">9443</biblScope>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<monogr>
		<title level="m" type="main">Multimodal Fusion with LLMs for Engagement Prediction in Natural Conversation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2409.09135</idno>
		<ptr target="https://doi.org/10.48550/ARXIV.2409.09135" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint at</note>
</biblStruct>

<biblStruct xml:id="b147">
	<monogr>
		<title level="m" type="main">Hypergraph Multi-modal Large Language Model: Exploiting EEG and Eyetracking Modalities to Evaluate Heterogeneous Responses for Video Understanding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3664647.3680810</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<publisher>ACM</publisher>
			<biblScope unit="volume">7316</biblScope>
			<pubPlace>Melbourne VIC Australia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">A systematic review and meta-analysis of eye-tracking studies in children with autism spectrum disorders</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Papagiannopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Chitty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Hermens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">B</forename><surname>Hickie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lagopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Neuroscience</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="610" to="632" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Attentional biases to emotional information in clinical depression: A systematic and meta-analytic review of eye tracking findings</title>
		<author>
			<persName><forename type="first">T</forename><surname>Suslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hußlack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bodenschatz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Affective Disorders</title>
		<imprint>
			<biblScope unit="volume">274</biblScope>
			<biblScope unit="page" from="632" to="642" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Minimal reporting guideline for research involving eye tracking (2023 edition)</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Dunn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behav Res Methods</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="4351" to="4357" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Which Eye Tracker Is Right for Your Research? Performance Evaluation of Several Cost Variant Eye Trackers</title>
		<author>
			<persName><forename type="first">G</forename><surname>Funke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Factors and Ergonomics Society Annual Meeting</title>
		<meeting>the Human Factors and Ergonomics Society Annual Meeting</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="1240" to="1244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Robustness and Precision: How Data Quality May Influence Key Dependent Variables in Infant Eye-Tracker Analyses</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Wass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Forssman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leppänen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Infancy</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="427" to="460" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<monogr>
		<title level="m" type="main">Ten simple rules for the computational modeling of behavioral data</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Collins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">49547</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Protracted development of gaze behaviour</title>
		<author>
			<persName><forename type="first">M</forename><surname>Linka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Karimpur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>De Haas</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41562-025-02191-9</idno>
	</analytic>
	<monogr>
		<title level="j">Nat Hum Behav</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Gaze data of 4243 participants shows link between leftward and superior attention biases and age</title>
		<author>
			<persName><forename type="first">C</forename><surname>Strauch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Hoogerbrugge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Brink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Exp Brain Res</title>
		<imprint>
			<biblScope unit="volume">242</biblScope>
			<biblScope unit="page" from="1327" to="1337" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Autistic Traits and Symptoms of Social Anxiety are Differentially Related to Attention to Others</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Kleberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eyes in Social Anxiety Disorder. J Autism Dev Disord</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="3814" to="3821" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Social Anxiety and Attention away from Emotional Faces</title>
		<author>
			<persName><forename type="first">W</forename><surname>Mansell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anke</forename><forename type="middle">&amp;</forename><surname>Ehlers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-P</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition and Emotion</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="673" to="690" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Eye tracking of attention in the affective disorders: A metaanalytic review and synthesis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Armstrong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">O</forename><surname>Olatunji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clinical Psychology Review</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="704" to="723" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Transdiagnostic psychiatry: a systematic review</title>
		<author>
			<persName><forename type="first">P</forename><surname>Fusar-Poli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">World Psychiatry</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="192" to="207" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Medical artificial intelligence and the black box problem: a view based on the ethical principle of &quot;do no harm</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M J</forename><surname>Shuttleworth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intelligent Medicine</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="52" to="57" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Explainability and artificial intelligence in medicine</title>
		<author>
			<persName><forename type="first">S</forename><surname>Reddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Lancet Digital Health</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="214" to="e215" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">Temporal Methods for Eye Movement 155 Analysis</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Bischof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kingstone</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-156030-20085-5_10.157</idno>
	</analytic>
	<monogr>
		<title level="m">Eye Movement Research</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="407" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">Microsaccades reflect attention shifts: a mini review of 20 years of 158 microsaccade research</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Psychol</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">An integrated model of fixational eye 160 movements and microsaccades</title>
		<author>
			<persName><forename type="first">R</forename><surname>Engbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mergenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pikovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>161 E765-E770</note>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">What makes a microsaccade? A review of 163 70 years of research prompts a new detection method</title>
		<author>
			<persName><forename type="first">A.-K</forename><surname>Hauperich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Smithson</surname></persName>
		</author>
		<idno type="DOI">10.16910/jemr.12.6.13</idno>
	</analytic>
	<monogr>
		<title level="j">J Eye Mov Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<monogr>
		<title level="m" type="main">Types of Eye Movements and Their Functions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Purves</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Sinauer Associates</publisher>
		</imprint>
	</monogr>
	<note>in Neuroscience. 2nd 166 edition</note>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">Smooth tracking of visual targets distinguishes 168 lucid REM sleep dreaming and waking perception from imagination</title>
		<author>
			<persName><forename type="first">S</forename><surname>Laberge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Baird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Zimbardo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Commun</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">170</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">Eye movement characteristics in 171 schizophrenia: A recent update with clinical implications</title>
		<author>
			<persName><forename type="first">K</forename><surname>Morita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Miura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hashimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuropsychopharmacol Rep</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="2" to="9" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">Test-retest reliability of eye tracking measures in a computerized 174 Trail Making Test</title>
		<author>
			<persName><forename type="first">L</forename><surname>Recker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Poth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Vis</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">Individual 176 differences in visual salience vary along semantic dimensions</title>
		<author>
			<persName><forename type="first">B</forename><surname>De Haas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Iakovidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Schwarzkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Gegenfurtner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National</title>
		<meeting>the National</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">Individual differences in human eye movements: An oculomotor 179 signature?</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bargary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Res</title>
		<imprint>
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="page" from="157" to="169" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<monogr>
		<title level="m" type="main">Reliability of smooth pursuit, fixation and saccadic eye movements</title>
		<author>
			<persName><forename type="first">U</forename><surname>Ettinger</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="180" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Psychophysiology</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="620" to="628" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">Robustness and Precision: How Data Quality 183 May Influence Key Dependent Variables in Infant Eye-Tracker Analyses</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Wass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Forssman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leppänen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bâce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bulling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Impact of Gaze Uncertainty on 186 AOIs in Information Visualisations. in 2022 Symposium on Eye Tracking Research and 187 Applications</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2014">2014. 2022</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<monogr>
		<title/>
		<idno type="DOI">10.1145/3517031.3531166</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">Atypical Visual Saliency in Autism Spectrum Disorder Quantified through 190 Model-Based Eye Tracking</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="604" to="616" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<monogr>
		<title level="m" type="main">The Gaze of Schizophrenia Patients Captured by Bottom-up Saliency</title>
		<author>
			<persName><forename type="first">P</forename><surname>Adámek</surname></persName>
		</author>
		<imprint>
			<date>191 15</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">Simple Viewing Tests Can Detect Eye Movement Abnormalities That 198 Distinguish Schizophrenia Cases from Controls with Exceptional Accuracy</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Griffin</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.bpsc.2024.08.017</idno>
		<idno>doi:10.1016/j.bpsc.2024.08.017. 197 17</idno>
		<ptr target="https://doi.org/10.1016/j.bpsc.2024.08.017" />
	</analytic>
	<monogr>
		<title level="m">Spatiotemporal Eye Movement Dynamics Reveal Altered Face 194 Prioritization in Early Visual Processing Among Autistic Children. Biological Psychiatry: 195 Cognitive Neuroscience and Neuroimaging</title>
		<imprint>
			<date type="published" when="2012">2024. 2012</date>
			<biblScope unit="volume">196</biblScope>
			<biblScope unit="page" from="716" to="724" />
		</imprint>
	</monogr>
	<note>Psychiatry</note>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">Genetic Influence on Eye Movements to Complex Scenes at Short 201 Timescales</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Biology</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3554" to="3560" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">The Autism Biomarkers Consortium for Clinical Trials: evaluation of a battery 203 of candidate eye-tracking biomarkers for use in autism clinical trials</title>
		<author>
			<persName><forename type="first">F</forename><surname>Shic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Molecular Autism</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">39</biblScope>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note>Molecular Autism</note>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">Gaze Response to Dyadic at 2 Years Related to Outcomes at 3 Years in Autism Spectrum Disorders: A Subtyping 209 Analysis</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Macari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chawarska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Autism Dev Disord</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="431" to="442" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">Variability of eye movements when 211 viewing dynamic natural scenes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Martinetz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Gegenfurtner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Barth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="28" to="28" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">Development and Replication of Objective Measurements of Social Visual 213 Engagement to Aid in Early Diagnosis and Assessment of Autism</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA Network Open</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">2330145</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<analytic>
		<title level="a" type="main">Infant viewing of social scenes is under genetic control and is 216 atypical in autism</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Constantino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">547</biblScope>
			<biblScope unit="page" from="340" to="344" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<analytic>
		<title level="a" type="main">Smartphone-based gaze estimation for in-home autism research</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Research</title>
		<imprint>
			<biblScope unit="volume">218</biblScope>
			<biblScope unit="page" from="1140" to="1148" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>Autism</note>
</biblStruct>

<biblStruct xml:id="b186">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Haskins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mentch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Botch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<title level="m">Active vision in immersive</title>
		<imprint>
			<biblScope unit="volume">360</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main">220 real-world environments</title>
	</analytic>
	<monogr>
		<title level="j">Sci Rep</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">14304</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b188">
	<analytic>
		<title level="a" type="main">Diagnosing and tracking depression based on eye movement in response to 222 virtual reality</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front Psychiatry</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">1280935</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<analytic>
		<title level="a" type="main">Attention and impulsivity 224 assessment using virtual reality games</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mendez-Encinas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sujar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bayona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Delgado-Gomez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci Rep</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">13689</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b190">
	<analytic>
		<title level="a" type="main">Virtual reality-assisted prediction of adult ADHD based on eye tracking, 226 EEG, actigraphy and behavioral indices: a machine learning analysis of independent 227 training and test samples</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transl Psychiatry</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">508</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b191">
	<analytic>
		<title level="a" type="main">Towards Clinically 231 Relevant Oculomotor Biomarkers in Early Schizophrenia</title>
		<author>
			<persName><forename type="first">F</forename><surname>Athanasopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O.-V</forename><surname>Saprikis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Margeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Smyrnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dowiasch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Behav. Neurosci</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="233" to="265" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Eye movements of patients with schizophrenia in a natural environment</note>
</biblStruct>

<biblStruct xml:id="b192">
	<analytic>
		<title level="a" type="main">What Would Jaws Do? The 236 Tyranny of Film and the Relationship between Gaze and Higher-Level Narrative Film 237 Comprehension</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Loschky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur Arch Psychiatry Clin Neurosci</title>
		<imprint>
			<biblScope unit="volume">266</biblScope>
			<biblScope unit="page">142474</biblScope>
			<date type="published" when="2015">2016. 2015</date>
		</imprint>
	</monogr>
	<note>PLOS ONE</note>
</biblStruct>

<biblStruct xml:id="b193">
	<monogr>
		<title level="m">Brunswik, E. Representative design and probabilistic theory in a functional psychology</title>
		<imprint>
			<biblScope unit="page" from="238" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b194">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="193" to="217" />
			<date type="published" when="1955">1955</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b195">
	<analytic>
		<title level="a" type="main">The &apos;Real-World Approach&apos; 241 and Its Problems: A Critique of the Term Ecological Validity</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Holleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">T C</forename><surname>Hooge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kemner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Hessels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Psychol</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b196">
	<analytic>
		<title level="a" type="main">Individual differences in face-looking 243 behavior generalize from the lab to the world</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Zaun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kanwisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b197">
	<analytic>
		<title level="a" type="main">How does gaze to faces support face-to-face interaction? A review and 245 perspective</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Hessels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychon Bull Rev</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="856" to="881" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b198">
	<analytic>
		<title level="a" type="main">Eye tracking in human interaction: Possibilities and limitations</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Valtakari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behav 247 Res</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="1592" to="1608" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b199">
	<analytic>
		<title level="a" type="main">The effect of social anxiety on 249 social attention in naturalistic situations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Teigeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kümpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gamer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Anxiety, Stress, &amp; Coping</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="326" to="342" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b200">
	<analytic>
		<title level="a" type="main">Social anxiety modulates visual exploration in real life 251 -but not in the laboratory</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rubo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huestegge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gamer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Br J Psychol</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page" from="233" to="245" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b201">
	<analytic>
		<title level="a" type="main">Contribution of Eye-Tracking to Study Cognitive Impairments Among 253 Clinical Populations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ueda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Psychol</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b202">
	<analytic>
		<title level="a" type="main">Eye movements in natural behavior</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hayhoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">255</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="188" to="194" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b203">
	<analytic>
		<title level="a" type="main">Modeling Task Control of Eye Movements</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hayhoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Biology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="257" to="R622" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b204">
	<analytic>
		<title level="a" type="main">Visual saliency does not 259 account for eye movements during visual search in real-world scenes</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Brockmole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Castelhano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eye movements: A 260 window on mind and brain</title>
		<meeting><address><addrLine>Amsterdam, Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">537</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b205">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/B978-008044980-7/50027-6</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b206">
	<analytic>
		<title level="a" type="main">Task and context determine where you 263 look</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Rothkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Hayhoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b207">
	<analytic>
		<title level="a" type="main">Gaze bias both reflects and influences 265 preference</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shimojo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Simion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shimojo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Scheier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Neurosci</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1317" to="1322" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b208">
	<analytic>
		<title level="a" type="main">Visual fixations and the computation and comparison of 267 value in simple choice</title>
		<author>
			<persName><forename type="first">I</forename><surname>Krajbich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Armel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rangel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Neurosci</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1292" to="1298" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b209">
	<analytic>
		<title level="a" type="main">Gaze bias 269 differences capture individual choice behaviour</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Molter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Krajbich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Heekeren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N C</forename><surname>Mohr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Hum Behav</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="625" to="635" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b210">
	<analytic>
		<title level="a" type="main">Attentional Cues in Real Scenes, 271 Saccadic Targeting, and Bayesian Priors</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Eckstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Drescher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Shimozaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol Sci</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="973" to="980" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b211">
	<analytic>
		<title level="a" type="main">275 51. Spering, M. Eye Movements as a Window into Decision-Making. Annual Review of Vision 276</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Lakshminarasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Arfaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Angelaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="427" to="448" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note>Eye movements reveal 273 spatiotemporal dynamics of visually-informed planning in navigation</note>
</biblStruct>

<biblStruct xml:id="b212">
	<analytic>
		<title level="a" type="main">Deep learning models for webcam eye tracking in 278 online experiments</title>
		<author>
			<persName><forename type="first">S</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Fink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Lange</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behav Res</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="3487" to="3503" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b213">
	<analytic>
		<title level="a" type="main">OWLET: An automated, open-source 280 method for infant gaze tracking using smartphone and webcam recordings</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Werchan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Thomason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Brito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behav Res</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="3149" to="3163" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b214">
	<analytic>
		<title level="a" type="main">iCatcher+: Robust and Automated Annotation of Infants&apos; and Young Children&apos;s 283 Gaze Behavior From Videos Collected in Laboratory, Field, and Online Studies</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Erel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="827" to="836" />
			<date type="published" when="2021">2023. 2021</date>
		</imprint>
	</monogr>
	<note>JAMA Pediatrics</note>
</biblStruct>

<biblStruct xml:id="b215">
	<analytic>
		<title level="a" type="main">Accelerating eye movement research via accurate and affordable 288 smartphone eye tracking</title>
		<author>
			<persName><forename type="first">N</forename><surname>Valliappan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Commun</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">4553</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b216">
	<analytic>
		<title level="a" type="main">Using Head-Mounted Eye Tracking to Examine Infant 293 Face Looking During Naturalistic Freeplay</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2104.12668291</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2104.12668291" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the</title>
		<meeting>the Annual Meeting of the</meeting>
		<imprint>
			<date type="published" when="2024">2024. 2025</date>
			<biblScope unit="volume">292</biblScope>
			<biblScope unit="page">58</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Preprint at</note>
	<note>Appearance-based Gaze Estimation With Deep 290 Learning: A Review and Benchmark</note>
</biblStruct>

<biblStruct xml:id="b217">
	<monogr>
		<title level="m" type="main">Eye contact avoidance in crowds: A large wearable eye-tracking study</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Hessels</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b218">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Atten Percept Psychophys</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="2623" to="2640" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
