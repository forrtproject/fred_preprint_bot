<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Kinetic Audio-Visualizers in Immersive Music Experiences for Hearing-Impaired Listeners</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2025-10-11">October 11, 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Bennhoff</forename><surname>Gannon</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Campolindo High School</orgName>
								<address>
									<settlement>Moraga</settlement>
									<region>California</region>
									<country>United States of America</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Gallo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Campolindo High School</orgName>
								<address>
									<settlement>Moraga</settlement>
									<region>California</region>
									<country>United States of America</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Kinetic Audio-Visualizers in Immersive Music Experiences for Hearing-Impaired Listeners</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-10-11">October 11, 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">187AF167644B8518F103CC8DC05730A9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-10-19T16:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advancements in GLSL-based compute shaders that perform parallelized particle simulations and spectral mapping have contributed to the rise of audio-visualizers for mainstream use. This is primarily due to their non-code programming customizability and multimodal input accessibility. Stem-separation features possible in softwares such as Touchdesigner, Ableton Live, Max/MSP, and more pose the question of whether these audio-visualizers could bridge the physiological gap for DHH individuals in sonic environments. Using three different tests: lyric detection, post-listening visual correspondence, and heart rate synchronization, this study empirically serves to find practical evidence, when combined with biological, neurological, and cognitive research in music, on the extent to which audio-visualizers bridge this gap. It was determined through these three tests that yes, participants engaged with the dynamic variety of songs at an increased rate when provided access to the Touchdesigner particle displacement audio-visualizer, attributing perceptual and attentional changes to the multiple distinguishable aspects within the visualizer. I dive into how these findings might be applicable to various music environments that want to increase engagement to DHH individuals, or how these individuals might go about listening to music themselves.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>While much research has identified the factors that effectively engage listeners, and other research has identified the audio-visual design techniques required to actualize this engagement, there is minimal accessible information on how specific audio-visual design elements can replicate these physiological responses for hearing-impaired audiences. I will state the different factors that will contribute to the source audio-visualizer that will be used for the tests in this study. A study by <ref type="bibr" target="#b0">Bora, Krishna, and Phukan (2017)</ref> examined how the BPM, or beat per minute of a song can influence the heart rate of listeners. The autonomic nervous system controls how the body responds to stimuli like music. A process called sympathetic activation raises heart rate during fast paced music, while parasympathetic activation lowers it during slow paced music. The participants in this study listened to two different types of music, one around 50-60 beats per minute and another at 120 to 130 beats per minute for 10 minutes each. The results showed a significant increase in both the systolic and diastolic pressures when listing to the fast music, and an inverse relationship when listening to slower music.</p><p>The same relationships were recorded for the heart rates of the participants as well (100% of participants experienced heart rates closer to the BPM of the song). Gauhati Medical College also provides insight on how BPM-tailored music can help relieve symptoms of depression and anxiety.</p><p>While the purpose of my study is to understand how audio-visualizers engage listeners that are hard of hearing, it's still important to consider the role music plays in human anatomy and psychology. A similar study by <ref type="bibr" target="#b1">Suguna &amp; Deepika (2017)</ref> proved that even healthy adults, in a sedentary settings (not in a concert setting like many studies in this field) are prone to blood pressure and heart rate increases upon listening to fast-paced music, and inverse effects for slow paced-music . This means that engagement is purely autonomic, something that is relevant to my study, as if music alone is able to enact psychological arousal, then audio-visualizers, designed upon the same BPM, dynamics changes, and accent structures, could similarly engage listeners who might not be able to hear the music in an environment as well as others.</p><p>To maximize this sonic-to-visual conversions that would in-theory enact similar autonomic responses, the design of the audio-visualizer used in this experiment would require a representation for every dynamic accent in the source music. A study by <ref type="bibr" target="#b2">Gu et al. (2012)</ref> illustrated how details as minute as vocal pitch glides can influence listener responses. This is why the audio-visualizer used in my study must represent not just tempo, noise spikes+dips, but also the timbre shifts of the sound itself. A study by <ref type="bibr" target="#b3">(Spence, 2011)</ref> correlates to the cross-modal correspondence theory, which suggests that the brain naturally maps auditory features like pitch and timbre to visual dimensions such as size, brightness, and motion. By visualizing the subconscious links in real time, associated autonomic pathways will be activated, something that could be monumental for hearing-impaired individuals in music environments.Using audio-visual programming software Touchdesigner by Derivative, there are simple nodes to control audio-reactiveness to kicks, snares, high hats, and other main aspects of a song. But analyzing timbre itself requires more complex feedback chains. An audio-visualizer project that embodies timbre-analyzation is the Touchdesigner project by <ref type="bibr" target="#b4">Daigle &amp; Patie, (2025)</ref>. They utilize multidimensional audio-visual interaction, moving beyond the default loudness-based audiovisual interactions. They used melbands, loudness, pitch, spectral shape, and MFCCs to allow the visuals to respond dynamically to various aspects of the source music. Much of the modern timbre analysis over the past few years has relied on processed models, but <ref type="bibr" target="#b4">Daigle &amp; Patie, (2025)</ref> were able to integrate real-time visualizations that use particle spectrums to engage the audience. In Touchdesigner, I will implement these models to an extent to maximize engagement, but I also must consider what aspects of sound the hearing-impaired are least perceptive to. The visual aspects of an audio-visualizer created by researchers <ref type="bibr" target="#b4">Ciriaco, et al. (2020)</ref> used specific visual elements to represent different instruments, (e.g., bass, melody, percussion), claiming that distinguishable sound source allows hearing-impaired listeners to be become more engaged. Similarly, mood and intensity are key factors to provide a more intuitive understanding of the sound. The researchers used a method they called "layer independence", where they separated different instruments into different parts of the visualizer, as opposed to a visualizer that reacts to a song as a whole. This is something I'll implement into the audio-visualizer I use for my research that I believe will allow users to specifically excel in the Visual Recognition Test portion of the experiment.</p><p>Different physiological reactions occur depending on what source the music is being emitted from. For example, more immersive speaker systems might be more likely to produce heart rate synchronization, increased blood pressure, or other signs of sensorimotor engagement. This is why I will be conducting audio-visualizer tests from three different mediums: an Iphone speaker, a pair of AKG K240 headphones, and a JBL Flip 5 Speaker. This ensures that the indicative results of the tests will be applicable to multiple listening contexts across a wide variety of media environments. Visual association tests will also take place to extract data on what types of songs correlate to greater stimulation. <ref type="bibr" target="#b5">Jakubowski et al. (2017)</ref>, investigated involuntary music imagery in this context. Their test results revealed that the songs that become "stuck in people's heads" are composed of easily identifiable components such as rhythm and melody, directly indicating higher listening engagement. So upon watching visualizers with audio, then without, then attempting to identify which visualizer applies to which song, the participants will report data that will indicate which songs, and their corresponding visualizer, are more sensorily engaging and therefore easier for the hearing-impaired to enjoy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>For the Lyric Detection Test, three songs were chosen for Iphone, headphones, and speaker portions for the test each. Every song contained a chosen "keyword" that the participants were asked to identify after listening to the song with, and without the audio-visualizer. Each song is played at 40 db SPL, a point similar to a quiet indoor environment or a soft whisper in terms of loudness, low enough to make lyrics just faintly intelligible to some listeners without full clarity. In each group of 3, the first song (songs, 1, 4, and 7) consisted of less dynamic changes, a simple timbre profile, and intertwining vocals. The second song in each group (songs, 2, 5, and 8) reflected average scores in each of these categories. And the third song in each group (songs 3, 6, and 9) reflected dynamic loudness and timbre changes, and a clear lead vocal. The method used to find songs that fit into each of these three categories was through testing them across a few different Touchdesigner audio-visualizers.</p><p>Measurements of overall amplitude were tested through FL Studio's 2D "Wave Candy" wave-from visualizer (see below) that displayed loudness envelopes over time, or in frozen moments. To measure timbre variance, the songs were uploaded to the 3D Touchdesigner audio-visualizer used in this experiment(see below), a particle displacement simulator, and were inspected for particle clumping the indicator of timbre difference (as designed through math CHOP nodes in Touchdesinger).</p><p>(Image of 3D Particle Displacement Audio-Visualizer Used in Tests + For Pre-Screen Timbre Variance Detection -designed in Touchdesigner with starter nodes from X) (Fl Studio Wave Candy) Songs 1, 4, and 7 all scored within -18 to -14 dB in overall amplitude and 1200 -1800 Hz range in spectral centroid variance. Songs 2, 5, and 8 scored within -14 to -10 dB in overall amplitude and 1800 -2600 Hz in spectral centroid variance. Songs 3, 6, and 9 scored within -10 to -6 dB in overall amplitude and 2600 -4000 Hz in spectral centroid variance. Another metric considered when finding songs for each of the categories were the amount of instruments included, and the relationship between percussive and harmonic content. Each of the songs selected for each group ranked in their appropriate third when compared to the others. These categories were chosen to confirm the study by Kirchberger, M. ( <ref type="formula">2016</ref>) that explored the dynamic range of an audio corpus and discussed how variations in dynamic range across genres can influence the perception of musical emotions, which is relevant to understanding engagement in audio-visual contexts. In each of the categories themselves, we will also be tested if participants were able to detect keywords more easily when listening to music with the visualizer. Similar to the previously stated study <ref type="bibr" target="#b5">Jakubowski et al. (2017)</ref>, a study by <ref type="bibr" target="#b7">Gu (2024)</ref> explains how lyric detection is a leading factor towards indicated audience engagement with a song. This provides credible reasoning on why an increased success rate in identifying the keyword in the word means greater engagement with the song and audio-visualizer, the very engagement that the directly benefits the hearing-impaired as previously proven by <ref type="bibr" target="#b7">Gu (2024</ref><ref type="bibr" target="#b4">), Ciriaco, et al. (2020)</ref>, <ref type="bibr" target="#b5">Jakubowski et al. (2017)</ref>. Participants who successfully identified the keyword without the audio-visualizer moved onto the next songs(as indicated by the dash in the "With Audio-Visualizer" tables), while those who were unsuccessful tried again with the audio-visualizer. See charts below for recorded data.</p><p>The Visual Recognition Test was conducted after every three Lyrics Detection tests.</p><p>Participants watched the visualizer of one of the three songs, without audio, and without knowing which song it corresponded to. After watching the visualizer once the entire way through, they were tested on this recognition to see if they knew which song the visualizer belonged to. It's stated how the research by <ref type="bibr" target="#b4">Ciriaco et al. (2020)</ref> demonstrates that separating instruments into distinct visual layers, or using "layer independence," allows hearing-impaired listeners to more easily identify and engage with specific elements of a song. So this test will examine if the visualization of these components will engage listeners more, allowing us to better understand the type of audio-visual interactions needed to enhance sonic experiences.</p><p>The Heart Synchronization Test consists of measuring users heart rates after two songs, one with a fast BPM(150), one with a slow BPM(100). Participants will listen to each song twice, once without the audio-visualizer and once with. Participants must wait until their heart rate has reached its baseline again before each listen to prevent and carryover effects. It's stated from the study by <ref type="bibr" target="#b0">Bora, Krishna, and Phukan (2017)</ref> that heart rate change is dependent from song tempo. Analyzing whether these effects are exaggerated when combined with an audio-visualiser will help determine how visual components can enhance autonomic engagement and overall listener arousal. *It must be stated that Participant X is a participant diagnosed with moderate sensorineural hearing loss and has cochlear implants. They are neither at an advantage or disadvantaged (pre-screened sound localization, phoneme recognition, and pitch discrimination tests), but will be able to give valuable insight on the effectiveness of the audio-visualizers. In the Lyric Detection Test, across eight of nine songs, participants that were not able to detect the keyword without the audio-visualizer were able to on a second listen with it. Participants were more likely to notice the keyword in songs 3, 6, and 9, those with the greatest dynamic range and clearest lead vocal, and less likely to notice the keyword in songs 1,4 and ,7, those with the least dynamic range and intertwining vocals. Participants 5 reported that "it was easier to decipher lyrics while watching the audio-visualizer because [they] could see the flowing of the words separately from the rest of the song. No difference in successful detection was noticed across the three different listening methods used (phone, headphones, and speakers) in the round with no audio visualizer. But for the round with the audio-visualizer, the 3 songs played on the JBL Flip 5 Speaker garnered a significantly higher lyric-detection success rate. Participant X, the participant with cochlear implants, was able to report the correct keyword on three different songs, two of which being on the second, audio-visualizer round. They reported that they "couldn't see the vocals independently in the visualizer but were able to feel the rhythm better". Participant X also stated that "the three songs played on the speaker felt more immersive even though they were equally as loud as the other songs".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lyric Detection Test</head><p>For the Visual Recognition Test, participants struggled to identify which visualizer corresponded to songs 1-3, those songs with the least dynamic range, with only a 44% accuracy rate.</p><p>For songs 4-6, and 7-9 though, they matched the visualizer to the song with 68% accuracy. When asked what factors of the songs were most memorable and identifiable in the visualizers, participants overwhelmingly agreed on the timbre (as represented by particle clumping). Participants also noted that songs with less ambiguous percussion patterns were the ones that they were able to identify on a visual scale easier.</p><p>In the Heart Rate Synchronization Test, the heart rate of participants drew closer to the BPM of the song, even before the implementation of the audio-visualizer, at an 88% mark. On a second listen, after a recalibration waiting period, with the audio-visualizer, that number rose to 94%.</p><p>Participant 1 noted that "seeing the quick pulsing of the kicks in [Song 10] made me feel more energized than without the visualizer".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>The 88% lyric detection increase resulting from the audio-visualizer gives insight on how multisensory integration reduces cognitive load. Cognitive load is defined by <ref type="bibr">Sweller (1998)</ref> as the total amount of mental effort being used in working memory. In this case, the findings can be correlated to the research by <ref type="bibr" target="#b9">Shams &amp; Seitz (2008)</ref> that showed how combining auditory and visual information can enhance perceptual accuracy and speed. The audiovisual in my experiment, similar to their theory, made the "flow of the words" separable to listeners, something that adheres to the "layer independence" theory previously stated. A study by <ref type="bibr" target="#b10">Calvert et al. (1999)</ref> used fMRI to show that auditory and visual speech information activates overlapping brain areas in the superior temporal sulcus, suggesting that synchronized visuals literally boost auditory processing. The particle displacement audio-visualizer used in my experiment functions similarly to the real-time visual captions examined in this study, but instead of words, it gives timbre and rhythm cues that increase temporal lobe processing. Participant X's responses can provide valuable insight on how immersive music experiences in the future could specifically benefit the hearing-impaired. Their responses reveal how vibrotactile and rhythmic visual cues can be a reliable substitute for acoustic clarity. Their ability to detect lyrics on the second round of listing, (with the audio-visualizer) suggests that said visualizers do enact these cognitive reactions that <ref type="bibr">Calvert et al. explored. Gfeller et al. (2000)</ref> found that cochlear implant users often rely on rhythm more than melody for musical enjoyment. As so, Participants X "felt the rhythm better" with the aid of the visualizer, suggesting that visualizers could be an immersive alternative to captions in environments like concerts, festivals, or other large venues where captions might be less effective. <ref type="bibr">Peterson et al. (2015)</ref> studied audiovisual training in cochlear implant users and found that visual cues (e.g., mouth movements) improve speech perception significantly over audio-only conditions. In concert settings where an audience member might not have a sufficient view of the performer, an audio-visualizer that encapsulates the same physical variances of a mouth could become a new assistive technology direction, with the plus that is aesthetics.</p><p>The greater lyric-detection success in songs with high dynamic range (songs 3, 6, 9) reflects emotional salience. This connects to why people remember or engage with certain songs more. The previously mentioned study by <ref type="bibr" target="#b6">Kirchberger (2016)</ref> showed that dynamic range is strongly associated with perceived emotional intensity. Similarly, <ref type="bibr" target="#b5">Jakubowski et al. (2017)</ref> found that songs with clear hooks and memorable melodic structure are more likely to induce involuntary musical imagery ("earworms") which aligns to the higher visual recognition accuracy for those same songs. Interpreting these findings within the contexts of these two studies develops a conclusion that dynamic variation makes both auditory and visual patterns more distinctive, increasing recognition and engagement through heightened arousal and memory encoding.</p><p>The JBL Flip 5's better lyric detection with the visualizer is interesting -even at equal dB, participants perceived it as "more immersive." This leads me to believe that spatial audio increases presence and physiological arousal compared to stereo playback. Even though the JBL Flip 5 speaker isn't high level spatial audio (just a single speaker), room reflections might have contributed to increased coherence with the visualizer. This means that the medium of playback matters in multimodal experiments across different mediums of sound. Audio installations in environments that want to support those hard of hearing might consider this.</p><p>The bump from 88% to 94% synchronization after adding the visualizer is a clear physiological marker that visual cues amplify arousal. <ref type="bibr" target="#b13">Nozaradan et al. (2011)</ref> suggesting that external cues (like a visualizer) could amplify this synchronization via multiple sensory pathways, including proprioceptive systems and motor responses. Visualizers over time have been used primarily for aesthetics, but these bodily clues stress the vital role they could play in engagement for those not at-par for standard music loudness. The study by <ref type="bibr" target="#b11">Gfeller et al. (2000)</ref> shows that the hearing-impaired, especially cochlear implants users, rely heavily on other senses than just sound, (vision being one of the most important) when intaking sonic stimuli. Similarly, A study by <ref type="bibr" target="#b14">Yi, H. B. (2025)</ref>examined the everyday music activities of Deaf and hard-of-hearing (DHH) individuals. The research identified six dimensions of music engagement, including emotional connection, social interaction, and personal meaning. This, combined with the findings of my research, emphasizes the importance of considering the diverse needs and preferences of DHH individuals in the design of music-related technologies and experiences.</p><p>Moreso, when it comes to visualers in society as of 2025, LED-based audio-visualizers are very common, but usually for spectacle. An estimated 10-15% of concert goers are hard of hearing, so this research might suggest that they could genuinely boost audience engagement and comprehension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>Through timbre-variance based increased detection to lyrics, temporal engaging visual cues that enhanced association between audio and visuals post-listen, and positive + negative heart rate synchronization, TouchDesigner particle displacement audio-visualizer successfully captured the multisensory engagement and perceptual accessibility proven by previously stated studies to be beneficial for those hard of hearing in various sonic environments.</p><p>The results found can serve as strong advocates for the development of inclusive music technologies that leverage audiovisual mapping of timbre, rhythm, and dynamic accents. Future research that would be beneficial towards a great understanding on how to aid hearing-impaired music enjoyers might want to explore how to apply live "layer independence" and timbre grouping techniques in live settings.</p><p>Larger participant pools in similar studies to mine would be able to understand how these findings correlate to those on a broader scale, and in diverse music genres with other speaker mediums.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="4,73.50,163.54,468.00,255.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="4,73.50,433.47,468.00,255.00" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>Thanks to <rs type="person">Nick Marriott</rs> for helpful source models from Youtube that contributed to the final audio-visualizer used in this study. I learned much of what I know on TouchDesigner programming from his tutorials by the name "PJ Creations".</p><p>My biggest gratitude for the nine individuals who participated in these tests. Your contribution and insightful feedback is truly meaningful towards helping not only me, but the music technology world as a whole understand how to help DHH individuals engage with the universal binding love we share that is music.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The effects of tempo of music on heart rate, blood pressure and respiratory rate -A study in Gauhati Medical College</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Phukan</surname></persName>
		</author>
		<ptr target="https://www.ijpp.com/IJPP%20archives/2017_61_4/445-448.pdf" />
	</analytic>
	<monogr>
		<title level="j">Indian Journal of Physiology and Pharmacology</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="445" to="448" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The effects of music on pulse rate and blood pressure in healthy young adults</title>
		<author>
			<persName><forename type="first">S</forename><surname>Suguna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Deepika</surname></persName>
		</author>
		<idno type="DOI">10.18203/2320-6012.ijrms20175438</idno>
		<ptr target="https://doi.org/10.18203/2320-6012.ijrms20175438" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Research in Medical Sciences</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5202" to="5205" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Preattentive auditory processing of pitch glide</title>
		<author>
			<persName><forename type="first">F</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="https://www.isca-archive.org/tal_2012/gu12b_tal.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Symposium on Tonal Aspects of Languages</title>
		<meeting>the Third International Symposium on Tonal Aspects of Languages</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Crossmodal correspondences: A tutorial review</title>
		<author>
			<persName><forename type="first">C</forename><surname>Spence</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13414-010-0073-7</idno>
		<ptr target="https://doi.org/10.3758/s13414-010-0073-7" />
	</analytic>
	<monogr>
		<title level="j">Perception, &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="971" to="995" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>Attention</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Real-time timbral analysis for musical and visual augmentation. Timbre and Orchestration Resource</title>
		<author>
			<persName><forename type="first">M</forename><surname>Daigle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Patie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Ciriaco</surname></persName>
		</author>
		<author>
			<persName><surname>Jr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Santos</surname></persName>
		</author>
		<idno type="DOI">10.1145/3334480.3383046</idno>
		<ptr target="https://doi.org/10.1145/3334480.3383046" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2020 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2025, February 26. 2020</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
	<note>ViTune: A visualizer tool to allow the deaf and hard of hearing to experience music</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dissecting an earworm: Melodic features and song popularity predict involuntary musical imagery</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jakubowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Müllensiefen</surname></persName>
		</author>
		<idno type="DOI">10.1037/aca0000090</idno>
		<ptr target="https://doi.org/10.1037/aca0000090" />
	</analytic>
	<monogr>
		<title level="j">Psychology of Aesthetics, Creativity, and the Arts</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="122" to="135" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dynamic Range Across Music Genres and the Perception of Musical Emotions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kirchberger</surname></persName>
		</author>
		<idno type="DOI">10.1177/2331216516630549</idno>
		<ptr target="https://doi.org/10.1177/2331216516630549" />
	</analytic>
	<monogr>
		<title level="j">SAGE Open</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">2331216516630549</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Enhancing social media engagement using AI-modified background music: Examining the roles of event relevance, lyric resonance, AI-singer origins, audience interpretation, emotional resonance, and social media engagement</title>
		<author>
			<persName><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2024.1267516</idno>
		<ptr target="https://doi.org/10.3389/fpsyg.2024.1267516" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">1267516</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cognitive load during problem solving: Effects on learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sweller</surname></persName>
		</author>
		<idno type="DOI">10.1207/s15516709cog1202_4</idno>
		<ptr target="https://doi.org/10.1207/s15516709cog1202_4" />
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="257" to="285" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Benefits of multisensory learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Seitz</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2008.07.006</idno>
		<ptr target="https://doi.org/10.1016/j.tics.2008.07.006" />
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="411" to="417" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Activation of auditory cortex during silent lipreading</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Calvert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Brammer</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.276.5312.593</idno>
		<ptr target="https://doi.org/10.1126/science.276.5312.593" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">276</biblScope>
			<biblScope unit="issue">5312</biblScope>
			<biblScope unit="page" from="593" to="596" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Effects of training on timbre recognition and appraisal by postlingually deafened cochlear implant recipients</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gfeller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Witt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Woodworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Mehr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Knutson</surname></persName>
		</author>
		<idno type="DOI">10.1055/s-0042-1748336</idno>
		<ptr target="https://doi.org/10.1055/s-0042-1748336" />
	</analytic>
	<monogr>
		<title level="j">Journal of the American Academy of Audiology</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="314" to="328" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cochlear implants and spoken language processing abilities: Review and assessment of the literature</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Pisoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Miyamoto</surname></persName>
		</author>
		<idno type="DOI">10.3233/RNN-2010-0535</idno>
		<ptr target="https://doi.org/10.3233/RNN-2010-0535" />
	</analytic>
	<monogr>
		<title level="j">Restorative Neurology and Neuroscience</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="237" to="250" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Tagging the neuronal entrainment to beat and meter</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nozaradan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Peretz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Missal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mouraux</surname></persName>
		</author>
		<idno type="DOI">10.1523/JNEUROSCI.0411-11.2011</idno>
		<ptr target="https://doi.org/10.1523/JNEUROSCI.0411-11.2011" />
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">28</biblScope>
			<biblScope unit="page" from="10234" to="10240" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Understanding Deaf and Hard-of-hearing individuals&apos; music engagement: Six dimensions and design implications</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Yi</surname></persName>
		</author>
		<idno type="DOI">10.1145/3715336.3735757</idno>
		<ptr target="https://doi.org/10.1145/3715336.3735757" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2025 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2025">2025</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
