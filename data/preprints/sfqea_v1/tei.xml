<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Distinct paths to false memory revealed in hundreds of narrative recalls</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Phoebe</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">New York University</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Omri</forename><surname>Raccah</surname></persName>
							<email>omri.raccah@yale.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vy</forename><forename type="middle">A</forename><surname>Vo</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Intel Labs</orgName>
								<orgName type="institution" key="instit2">Intel Corporation</orgName>
								<address>
									<settlement>Hillsboro</settlement>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Max Planck Institute for Software Systems</orgName>
								<address>
									<addrLine>Saarbr ücken</addrLine>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Poeppel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">New York University</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Center for Language, Music, and Emotion</orgName>
								<orgName type="institution">NYU (CLaME)</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Todd</forename><forename type="middle">M</forename><surname>Gureckis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">New York University</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Distinct paths to false memory revealed in hundreds of narrative recalls</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C52432141E3853FAC090ABFB835E67AE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-10-21T13:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>† denotes equal contribution ‡ Now at Thomson Reuters Labs. This work was not performed as part of this position.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Far from being veridical records of our lives, our memories are distorted reflections of the truth. In his seminal study of narrative recall, <ref type="bibr">Bartlett 1</ref> showed that people distorted story details over repeated retellings-"canoes" became "boats," "five men" became "some warriors." They also generated fictitious details consistent with prior events or knowledge, such as replacing "something black" with "blood" or fabricating that the man died "at sunset." These errors illustrate that memory is an active process of reconstructing encoded information, rather than a passive repository of details.</p><p>Studying distortions and errors in memory, especially those that occur under naturalistic conditions, is exceedingly difficult.</p><p>One reason is that memory errors can be rare and highly variable across individuals. As a result, small-sample laboratory studies might fail to observe substantial variance in memory errors, and lack the ability to detect the situational patterns that cause them. While certain forms of false memory are reliably elicited in laboratory settings <ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3</ref> , extending these findings to more naturalistic settings presents additional challenges. Naturalistic memory studies typically employ small samples (20-30     participants) to enable manual scoring of the recall by human raters, and lack statistical power to detect infrequent memory distortions. In fact, it can be difficult even to reliably define what constitutes an memory error in everyday life because memory of experiences are rarely retrieved exactly as they occurred. Therefore, addressing false memories under naturalistic conditions demands large-scale datasets and novel methodological approaches to annotate and score the data.</p><p>In the present study we leverage two new methodological innovations to address our primary goal of understanding memory errors in naturalistic recall. First, we analyze a recently published large-scale dataset consisting of hundreds of spoken narrative recollections <ref type="bibr" target="#b3">4</ref> . This dataset contains professionally coded transcripts of hundreds of individuals retelling a narrative story from memory. Analyzing such a large, unstructured data set would be expensive and time-consuming. However, our second innovation is that we leverage state-of-the-art large language models (LLMs), which are trained using in-context learning to detect and classify memory errors. We validate this approach showing that human-AI agreement is at least a high as inter-human agreement.</p><p>The current work builds upon a century of psychological research that has used controlled experimental paradigms to explore how these memory distortions occur, and what kinds of errors can be observed. Forgetting encompasses errors of omission, while other errors commonly described as false memories are errors of commission <ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6</ref> . Commission errors have been further split into subtypes <ref type="bibr" target="#b6">7</ref> , mainly by considering how they may arise from different steps of the reconstructive process of memory recall. For example, binding the contents of memory to the wrong context results in misattribution errors <ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9</ref> . This includes errors that attribute imagined content (e.g., fabricated childhood events) to the real past, a phenomenon robustly studied by a set of paradigms iterating on the notion of 'imagination inflation' or 'misinformation' <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref> . Another type of error might involve a memory that has been weakly encoded, or encoded along with a strong overarching structure-these memories may be recalled with newly introduced details that are consistent with the person's beliefs or the overarching structure of the memory, but are unfaithful to the original inputs <ref type="bibr" target="#b13">14</ref> . The classic Deese-Roediger-McDermott (DRM) paradigm demonstrates this principle: participants study lists of semantically related words (e.g., bed, rest, dream) and subsequently show robust false recognition for non-presented but related lures (e.g., sleep), often at rates comparable to actually presented items <ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16</ref> .</p><p>These effects extend beyond word lists to semantically rich images (e.g., farm scenes) and occur across both recognition and free-recall paradigms <ref type="bibr" target="#b1">2</ref> . While these controlled paradigms have provided crucial empirical tractability, it is unclear how and when they might generalize to real world situations <ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref> .</p><p>Highly controlled paradigms oversimplify the complex, interacting semantic factors that drive memory reconstruction in everyday contexts. While recent studies have begun to address how accurate recall occurs in naturalistic settings <ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref> , they have not addressed inaccurate, or distorted, memory. Methodological barriers have impeded this progress: controlled paradigms to study false memory rely on some experimental manipulation to encourage, or induce, a higher rate of memory distortions to provide enough statistical power to characterize the effect. In list learning studies, the experimental manipulation is the choice of structured stimulus sets, such as words that all belong to the same category. In misinformation studies, the manipulation is the introduction of inaccurate information about an already-encoded event. By contrast, naturalistic memory studies simply present information and prompt the subjects to recall the information in a realistic way, such as by freely recalling the events of a movie <ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23</ref> . To move beyond these limitations, we apply recent advances in natural language processing to analyze naturalistic memory recalls. These tools can enable automated text stimulus annotation and other tasks in developing and analyzing psychological experiments <ref type="bibr" target="#b23">24</ref> . For example, some work has employed large language models (LLMs) to generate narrative stimuli <ref type="bibr" target="#b24">25</ref> , predict human reading times <ref type="bibr" target="#b25">26</ref> , automate segmentation of narratives <ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28</ref> and score recalls <ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b24">25</ref> .</p><p>In this work, we investigate spontaneously occurring false memories in the naturalistic recollection of spoken narratives. We leverage our recently published dataset containing recollections from hundreds of participants across four spoken narratives <ref type="bibr" target="#b3">4</ref> .</p><p>The dataset has already been validated for the study of memory, as it has been used to replicate key findings in the broader memory literature <ref type="bibr" target="#b28">29</ref> . Our work makes two main contributions with these data. First, we developed an automatic LLM-based pipeline to detect and label two types of memory distortions, factual errors and confabulations, in naturalistic story recollections.</p><p>Across two experiments, we validate this approach through showing that these annotations are comparable to human raters. of natural language processing (NLP) to help define these metrics. This work addresses a fundamental gap in our understanding of memory by examining false memories as they naturally occur during narrative recollection-an understudied yet ubiquitous aspect of human behavior. Our approach provides novel methodological tools for investigating naturally occurring memory distortions and reveals complex semantic dimensions that capture individual tendencies to misremember beyond traditional item-list paradigms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Narrative free recall paradigm</head><p>To investigate the spontaneous occurrence of memory distortions under naturalistic conditions, we sought a paradigm that (1)     closely mimics real-world remembering-the retelling of narrative content and (2) provides enough data to reliably observe memory errors. Our recently published "Naturalistic Free Recall" Dataset 4 (NFRD) meets these demands: hundreds of online participants listened to a spoken narrative and immediately recalled it in as much detail as possible. To the best of our knowledge, this dataset represents the largest free recall dataset for naturalistic materials (see also <ref type="bibr" target="#b24">25</ref> ), making it particularly well-suited for the identification of naturally occurring false memories.</p><p>The stimuli comprised four distinct spoken narratives in English. Three of the narratives were sourced from The Moth Radio Hour podcast series, specifically Pieman, Eyespy, and Oregontrail. The fourth narrative was the first chapter from an audiobook, henceforth referred to as Baseball. On average, each audio clip lasted 11 minutes and 35 seconds (see Methods and Materials section). Each participant was presented with two of the four narrative stimuli included in the NFRD, and performed a spoken recall for at least 4 minutes following each story (Figure <ref type="figure" target="#fig_0">1A</ref>). The dataset includes professionally curated, high-fidelity, transcripts of each narrative and all verbal recollections. We conducted analyses across the entire dataset, collapsing across the four narratives to increase statistical power (N = 229; 145 female; Mean age = 25.03, SD age = 11.15). An detailed description of the dataset can be found in our published paper <ref type="bibr" target="#b3">4</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Annotating memory distortions with LLMs</head><p>In recent years, natural language processing (NLP) and LLMs have become powerful tools in studying narrative memory <ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30</ref> .</p><p>Specifically, generative LLMs have enabled automated segmentation of narratives <ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28</ref> and scoring of recall quality <ref type="bibr" target="#b24">25</ref> . We developed an automated pipeline using GPT-4o <ref type="bibr" target="#b30">31</ref> to identify and categorize false memories in narrative recollections-a task that requires the model to distinguish between accurate recall and false memories based on comparison with source material.</p><p>We relied on in-context learning to leverage the ability of modern LLMs to perform complex classification tasks when provided with detailed instructions and examples-avoiding the need for task-specific training or fine-tuning via backpropagation. This required the development and tuning of specific prompts to optimize LLM performance on this task (see Methods and Materials).</p><p>We first segmented original narratives into events <ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b20">21</ref> , representing semantically coherent portions of the story. We also segmented the recalls into sentences to allow the LLM to annotate false memory details at a finer grain. This relied on a two-stage classification pipeline. First, an event-matching task identified which event of the original narrative each recall sentence referenced, or marked it as unmatched if no clear correspondence existed (1.56% of all sentences; see Methods &amp; Materials). Second, a memory classification task evaluated each sentence for three types of content: factual errors (details contradicting the story), confabulations (fabricated information absent from the story), and inferences (plausible details derived from world knowledge). Both tasks contained the complete narrative text and detailed task instructions as context (Table <ref type="table" target="#tab_4">S2</ref>, S3</p><p>and S4), allowing the LLM to make informed comparisons between participants' recollections and the source material.</p><p>Sentences containing factual errors or confabulations were classified as false memories, while inferences were treated as an instance of true recollection. We included inferences for two reasons. First, prior research shows that inferences often reflect successful reconstructive retrieval <ref type="bibr" target="#b31">32</ref> , where new information is combined with past experiences. By including an inference category, we aim to separate adaptive reconstruction from memory errors <ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34</ref> . Second, this approach follows a common strategy in machine learning: introducing an "unknown" or a "catch" category to handle ambiguous cases <ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36</ref> . In practice we found that it improved the classification of factual errors and confabulations. Participants listened to two spoken stories and immediately recalled them verbally for at least four minutes; we then performed automatic speech-to-text transcription with professional review of the audio files. (B) Recall annotation approach. We employed in-context learning with GPT-4o to perform two consecutive tasks: (1) matching recall sentences to corresponding story events, and (2) classifying types of memory. We defined three distinct memory types based on prior theoretical frameworks <ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38</ref> : factual errors (details that contradict the original narrative), confabulations (fabricated information not present in the story), and inferences (plausible details derived from world knowledge or reasonable interpretation). Full prompts for both tasks are attached in Table <ref type="table" target="#tab_4">S2</ref>, S3 and S4. (C) Example classification outputs. An example story event from Pieman (left) is shown alongside three sample recollections from different participants (center) that were matched to this event. The LLM was instructed to return the specific portions of the recollections classified as factual errors (blue), confabulations (red), or true-memory inferences (light green).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human validation</head><p>Next we sought to validate the performance of our annotation pipeline. Validating LLM-based classifications of memory phenomena presents unique challenges, as participants' recollections lack an objective ground-truth and their relation to the narratives rely on their personal interpretations. These challenges have sparked important discussions around how to validate the performance of LLMs when identifying and labeling complex psychological phenomena. Demszky et al. <ref type="bibr" target="#b38">39</ref> argue that validation of LLMs against human behaviors should account for inherent human variability, rather than assume a single ground truth. In line with this perspective, recent work evaluates LLMs by comparing human-AI agreement to inter-rater agreement, effectively treating the LLM as a potential proxy for average human judgments <ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27</ref> . This approach reframes human-AI validation as a question of alignment with collective human interpretation, rather than with a single ground truth label. Importantly, recent work has applied this approach to distinct aspects of narrative memory, including autobiographical details <ref type="bibr" target="#b39">40</ref> , event boundary detection <ref type="bibr" target="#b26">27</ref> and empathy judgments <ref type="bibr" target="#b40">41</ref> .</p><p>To apply this approach to study of spontaneously occurring false memories, we conducted two validation experiments comparing human-human agreement (inter-rater reliability) with human-AI agreement. In the following experiments, two cohorts of participants evaluated the scoring of recall sentences after listening to the original narratives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Validation Experiment 1: Open-ended classification</head><p>Twenty-seven participants evaluated 60 recall sentences (30 per story) in an open-ended format. Each sentence was presented with the corresponding (matched) story context and the AI's binary classification (accurate vs. inaccurate), with incorrect segments underlined in false memory trials. Participants judged whether they agreed with the classification and reported their confidence levels. This validation did not distinguish between factual errors and confabulations (see Experiment 2). To avoid bias, we only told participants that they were evaluating another rater, and did not specify that it was an LLM.</p><p>We calculated both inter-subject agreement (the proportion of trials on which pairs of human raters made identical judgments) and subject-AI agreement (the proportion of trials on which each human rater's judgment matched the AI's classification).</p><p>We used a nonparametric bootstrapping procedure to perform statistical comparisons and error estimates (see section Human validation in Methods for details). We found that subject-AI agreement significantly exceeded inter-subject agreement for both accurate trials (subject-AI: 0.71 [95% CI: 0.66, 0.76] vs. inter-subject: 0.62 [95% CI: 0.57, 0.66], p &lt; 0.001) and false memory trials (subject-AI: 0.66 [95% CI: 0.60, 0.72] vs. inter-subject: 0.60 [95% CI: 0.57, 0.64], p = 0.002, Figure <ref type="figure" target="#fig_1">2A</ref>). This suggests that the AI's ratings more closely approximated the average human judgment than any individual participant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Validation Experiment 2: Comprehension-based validation</head><p>The findings from the first experiment show that AI ratings were more consistent with the average human judgment than human ratings were with one another. However, the lower inter-human agreement, particularly for false memories, suggested that the validation task could be improved. This variability in ratings could be due to inconsistent criteria in how individuals judge true versus false memory. For example, raters might deploy different boundaries between reasonable inferences and inaccuracies based on their prior notions of false memory. We therefore designed a second validation task that was framed as a story comprehension task. Inaccurate recalls were reformulated into two-alternative forced-choice questions. Instead of evaluating subtle distinctions, participants selected the option that best matched their understanding of the story. For example, a factual error "two uncles and two aunts" became: "How many uncles and aunts were there?" with options "two and two" (recalled version) versus "zero and two" (original story). Questions were mostly phrased using wh-constructions (e.g., what, where, when, etc.), and the options were drawn directly from the recalled and original versions of the detail.</p><p>For this experiment, we recruited 70 participants, each of whom listened to two stories and completed 25 comprehension trials per story. Compared to the open-ended task, this task substantially improved inter-human agreement (from 0.60 to 0.78 for false memories). Importantly, there was no significant difference between inter-subject and subject-AI agreement in both factual error (subject-AI: 0.78 [95% CI: 0.72, 0.84] vs. inter-subject: 0.81 [95% CI: 0.77, 0.85], p = 0.257) and confabulation trials (subject-AI: 0.67 [95% CI: 0.54, 0.80] vs. inter-subject: 0.75 [95% CI: 0.68, 0.82], p = 0.188, Figure <ref type="figure" target="#fig_1">2B</ref>), suggesting that the AI's ratings remained consistent with average human comprehension, even when evaluated under more constrained and objective task demands.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Characteristics of spontaneously occurring false memory</head><p>Our validation experiments show that the LLM-based annotation method is an effective way to detect and label memory distortions in our paradigm. Having validated the annotation approach, we next characterized the prevalence and distribution of each false memory type across the entire dataset. We considered a story event as recalled by a participant if at least one recall sentences was matched to that event. An event was labeled as containing a false memory if any matched sentences contained a factual error, confabulation, or both. Importantly, an event could be marked as containing a false detail even if some of its details were accurately remembered-this departs from traditional false memory paradigms in list-learning tasks, in which items are treated as either wholly true or false.</p><p>First, we computed the percentage of recalled events which contained a memory distortion for each participant (Figure <ref type="figure" target="#fig_1">2C</ref>). Participants produced significantly more factual errors than confabulations across all narratives (factual errors: 0.24±0.01 (SEM); confabulation: 0.09±0.01 (SEM); t = 19.8, p &lt; 0.001). For each event, we next calculated the proportion of participants who produced false memories among all participants who recalled that event. We refer to this as the event-level false memory rate. The pairwise correlations for event-level factual error, confabulation and inference rates are shown in Figure <ref type="figure" target="#fig_1">S2</ref>. Notably, event-level false memory rates varied substantially across the four narratives (Figure <ref type="figure" target="#fig_1">2D</ref>), suggesting that story-specific features influence the likelihood of memory distortions. Across all stories, we find that factual errors occurred more frequently than confabulations (factual errors: 0.23±0.01 (SEM); confabulation: 0.08±0.004 (SEM); false memory: 0.28±0.10 (SEM); t = 13.7, p &lt; 0.001; Figure <ref type="figure" target="#fig_1">2E</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic factors driving spontaneous false memories</head><p>Next we analyzed how semantic features of story events influence the likelihood of false memory. An extensive literature has demonstrated that semantic relationships between studied items are among the strongest predictors of false memory rates <ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b41">42</ref> .</p><p>In classic paradigms, participants who study semantically coherent word list or view thematically related images exhibit elevated false recognition for category-consistent but non-presented items. This pattern reflects the operation of gist-based memory processes, in which semantic knowledge structures in long-term memory generate plausible but inaccurate details during retrieval. To investigate how semantic context shapes memory distortions in naturalistic recall, we leveraged recent NLP advances <ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44</ref> -specifically, text embedding models 3A-B and autoregressive language models 3C -to quantify semantic features of narratives and test their relationship to false memory types. In our analyses, we used a recent text embedding model (MPNet <ref type="bibr" target="#b44">45</ref> ) that scores highly on a benchmark dataset of human similarity judgments (STS-B <ref type="bibr" target="#b45">46</ref> ) and the GPT-2 language model <ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48</ref> . The correlations for the three semantic predictors are shown in Figure <ref type="figure" target="#fig_1">S2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic centrality</head><p>Semantic centrality quantifies the similarity of a given event to all other events in a narrative (Figure <ref type="figure" target="#fig_2">3A</ref>). This measure provides a proxy for the importance of any given event in a narrative in conveying the meaning of the narrative as a whole. While recent work using narratives has shown that semantic centrality is a reliable predictor of memory accuracy <ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b19">20</ref> , it is unclear how it may influence false memory. Similar to prior work, we constructed undirected graphs representing relationships between narrative events based on their cosine similarity in embedding space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Similarity to narrative corpus</head><p>Prior knowledge structures drive many false memory effects in laboratory paradigms, suggesting that events resembling common world knowledge are more susceptible to memory distortions <ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b49">[50]</ref><ref type="bibr" target="#b50">[51]</ref><ref type="bibr" target="#b51">[52]</ref> . We therefore computed how closely each narrative event matched prototypical narrative content by measuring semantic similarity between individual events and representations derived from a large-scale corpus of diverse stories, NarrativeXL <ref type="bibr" target="#b48">49</ref> (Figure <ref type="figure" target="#fig_2">3B</ref>). This measure is intended to capture the degree to which any given event aligns with stereotypical narrative patterns -for example, common story arcs, character archetypes, or plot conventions that appear frequently across many narratives. Events with high corpus similarity may trigger schema-based false memories because they strongly activate familiar narrative templates, leading to intrusions of expected but non-presented details during recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contextual surprisal</head><p>Prediction errors, or surprise, reflect the difference between one's prediction and a real-world output. Surprisal has long been shown to have an influence on memory performance, including for narrative memory <ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54</ref> . Of particular relevance, Sinclair et al. <ref type="bibr" target="#b54">55</ref> show that unexpected interruptions in videos led to a greater rate of naturally occurring false memories by encouraging the integration of prior knowledge with recently learned information. Therefore, we sought to understand how surprisal may drive naturally occurring false memories in our natural recollections (Figure <ref type="figure" target="#fig_2">3C</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relating semantic factors to false memory types</head><p>We built three separate linear mixed-effects models to predict recall, factual error, or confabulation rate, respectively (Figure <ref type="figure" target="#fig_3">4</ref>; Table <ref type="table" target="#tab_0">1</ref>). Participant and story identity were modeled as random intercepts. For fixed effects, we included each of the semantic factors, as well as serial position of the event given its wide-spread influence in list-learning paradigms and possible extension to narrative stimuli <ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b55">56</ref> . Finally, we also included event length as a fixed effect, as a memory error is more likely to occur in longer events given the current approach. This analysis revealed three key findings (Table <ref type="table" target="#tab_0">1</ref>). First, consistent with prior studies <ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b19">20</ref> , events with higher semantic centrality were more likely to be recalled (β = 0.10, p &lt; 0.001). There was no significant effect of recall rate with respect to contextual surprisal (β = -0.03, p = 0.145) or similarity to narrative corpus (β = -0.03, p = 0.189). Second, we found a strikingly clear double-dissociation between semantic factors and the two types of false memory. Events that closely resembled the narrative corpus contributed to more factual errors but not confabulations(β = 0.14, p = 0.001), while surprising events  (A) Semantic centrality quantifies how well-connected each event is to the overall narrative structure. Event text is embedded in high-dimensional semantic space, and centrality is calculated as the average cosine similarity between each event and all other events within the same story <ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b19">20</ref> . (B) Similarity to narrative corpus measures how closely each event matches semantic patterns in a selected text corpus, such as a corpus of narratives. This is measured as the average similarity of a given event embedding to all events in the corpus <ref type="bibr" target="#b48">49</ref> . (C) Contextual surprisal captures how unexpected each event is given the preceding narrative context. Surprisal is computed as the negative log probability of each word conditioned on prior context, averaged across all words within the event.</p><p>were associated with more confabulations (β = 0.16, p = 0.002) but not factual errors. Semantic centrality did not contribute significantly to either false memory metrics. Third, we found serial position to negatively contribute to factual error rate (β = -0.16, p &lt; 0.001), but not recall rate (β = -0.01, p = 0.477) or confabulations (β = 0.08, p = 056). That is, events later in the story had fewer memory distortions. The full results from the mixed effects models are shown shown in Table <ref type="table" target="#tab_0">1</ref>.</p><p>To test the robustness of these results, we re-computed semantic factors using an alternative embedding model (Universal Sentence Encoder <ref type="bibr" target="#b56">57</ref> ) and an alternative language model for computing surprise (Pythia <ref type="bibr" target="#b57">58</ref> ). The choice of these models is based on existing benchmarks or prior behavioral modeling studies (see LLM-based Annotation Pipeline in Methods section). We replicated the disassociation between factual error (USE and Pythia: centrality: β = -0.13, p &lt; 0.001; similarity to narrative corpus: β = 0.15, p &lt; 0.001) and confabulation factors (contextual surprisal: β = 0.12, p = 0.028), as well as the effect of centrality on recall rate (β = 0.9, p &lt; 0.001). See Table <ref type="table">S5</ref> for linear mixed effects models' results using USE and GPT2-Small, and Table <ref type="table">S6</ref> for those using USE and Pythia. Events with higher semantic centrality are more likely to be recalled, consistent with the importance of this measure for memory retention <ref type="bibr" target="#b19">20</ref> (β = 0.10, p &lt; 0.001). (B) Factual errors are more likely to occur when events have greater similarity to narrative corpus (β = 0.14, p = 0.001). (C) Confabulations were more frequent for events with higher contextual surprisal (β = 0.16, p = 0.002), suggesting a link between unpredictability and memory fabrication. Bars show standardized regression coefficients (β ); error bars indicate 95% confidence intervals. All models included semantic centrality, similarity to narrative corpus, surprisal, serial position, and length as fixed effects, with participant and story identity modeled as random effects (see Table <ref type="table" target="#tab_0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>This work establishes a computational approach for detecting memory distortions in naturalistic narrative recall, revealing that distinct semantic factors drive separable classes of false memories. We demonstrate that contextual surprise predicts confabulations-entirely fabricated details-while similarity to prototypical narrative patterns predicts factual errors-distortions of narrative content. This dissociation provides the first empirical evidence that factual errors and confabulations reflect distinct cognitive mechanisms operating during memory reconstruction, supporting theoretical proposals that have lacked direct empirical validation in naturalistic contexts <ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b59">60</ref> . Critically, this separation emerges in naturalistic recall, where participants generate rich, unconstrained responses -a distinction that would be impossible to detect using traditional list-learning or recognition paradigms that treat false memories as a unitary phenomenon. Our LLM-based approach enables scalable analysis of these complex memory processes without requiring extensive manual annotation, opening new avenues for studying how semantic context shapes memory errors in real-world settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lessons learned from validating model performance</head><p>Validating LLM performance in classifying psychological phenomena presents significant methodological challenges. We validated model classifications of memory distortions through direct comparison with human ratings on a subset of participants' recollections. In our open-ended experiment, we found that human inter-rater reliability was notably low (60% for false memory trials), despite participants receiving detailed instructions and access to the complete story text and relevant narrative context. Importantly, human-AI agreement was significantly higher (66% for false memory trials), suggesting that the AI's classifications more closely approximated the average human judgment than individual human raters agreed with each other, which was observed in other psychological tasks comparing AI task behaviors and human raters <ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b60">61</ref> .</p><p>This low inter-rater agreement reveals a fundamental challenge in validating computational approaches to psychology: many constructs lack objective ground truth <ref type="bibr" target="#b38">39</ref> . Individual differences in interpreting what constitutes "false" memory in naturalistic recollections may reflect genuine variability in how humans process and evaluate memory distortions. This challenge extends beyond false memory research to the validation of many psychological constructs in naturalistic settings. Studies of event segmentation <ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b61">[62]</ref><ref type="bibr" target="#b62">[63]</ref><ref type="bibr" target="#b63">[64]</ref> , causal structure <ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b65">66</ref> , and narrative coherence <ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b20">21</ref> have similarly relied on aggregated human judgment to establish statistical ground truth when objective criteria are unavailable or inadequately defined.</p><p>To test whether task ambiguity contributed to low inter-rater reliability, we conducted a second validation experiment that reframed memory evaluation as a multiple-choice comprehension task with clearly defined alternatives. This design eliminated the need for participants to make subjective judgments about memory accuracy by instead asking them to select which version of a detail-the original story content or the participant's recalled version-better matched their understanding of the narrative.</p><p>This approach substantially improved inter-rater reliability (78% for factual errors, 75% for confabulations) while maintaining comparable human-AI agreement, confirming that our computational approach remained aligned with human comprehension even under more constrained evaluation conditions.</p><p>These validation experiments demonstrate that our LLM-based approach provides reliable detection and classification of memory distortions in naturalistic recall. More broadly, our findings suggest that future computational studies of psychological phenomena may benefit from validation designs that minimize interpretive ambiguity, either through objective task framing or by explicitly acknowledging and modeling human variability as an inherent feature of the construct under investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic metrics drive distinct memory distortions</head><p>A central contribution of this work is demonstrating that different semantic factors drive distinct types of memory distortion.</p><p>Our findings reveal a clear dissociation: events resembling prototypical narrative content generate factual errors-distortions of actual story details-while semantically surprising events trigger confabulations-entirely fabricated information. This dissociation provides direct empirical evidence for theoretical proposals that false memories reflect multiple underlying mechanisms operating during memory reconstruction <ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b66">67</ref> .</p><p>The role of prior knowledge in generating factual errors aligns with established findings from controlled paradigms. In DRM tasks, semantic associations between studied and non-studied items produce robust false recognition <ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b67">68</ref> , while misinformation studies demonstrate how existing beliefs shape memory distortions <ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12</ref> . Our results extend these findings to naturalistic settings, showing that narrative events matching familiar schemas are particularly vulnerable to factual errors. This suggests that when story content strongly activates existing knowledge structures, memory reconstruction may inappropriately integrate expected but non-presented details.</p><p>Our findings suggest that the link between contextual surprise and confabulations may reflect a distinct cognitive process.</p><p>Surprising events violate predictions based on preceding narrative context, and may trigger compensatory mechanisms that generate novel content to maintain narrative coherence. This finding converges with research on flashbulb memories, where unexpected real-world events show heightened distortion rates <ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b69">70</ref> , as well as recent work demonstrating that unexpected narrative breaks increase false memory occurrence <ref type="bibr" target="#b54">55</ref> . The fabrication of entirely new details following surprising events may 10/27</p><p>represent an adaptive response to prediction error, in which the memory system attempts to resolve contextual inconsistencies during encoding.</p><p>Our findings provide new evidence of a functional separation between processes that drive false memory. While theoretical frameworks have proposed frameworks with distinct types of false memories, empirical studies typically use single paradigms to study single types <ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b70">[71]</ref><ref type="bibr" target="#b71">[72]</ref><ref type="bibr" target="#b72">[73]</ref> . Others have noted that the empirical validation of distinct false memory types has remained elusive due to methodological constraints <ref type="bibr" target="#b73">74</ref> . Traditional paradigms typically classify recall as simply correct or incorrect, precluding fine-grained analysis of error types <ref type="bibr" target="#b16">17</ref> . Our naturalistic approach reveals qualitative differences in memory distortion that emerge only when participants generate rich, unconstrained responses reflecting the full complexity of reconstructive memory processes.</p><p>The ecological validity of narrative recall proves essential for detecting this dissociation. Unlike list-learning or recognition tasks that constrain possible responses, free recall of complex narratives allows multiple types of memory errors to co-occur and compete, creating the behavioral richness necessary to distinguish between factual errors and confabulations. This methodological advance opens new avenues for investigating how different cognitive mechanisms contribute to memory distortion in real-world contexts, where multiple semantic factors interact dynamically during retrieval.</p><p>Our findings offer a key contribution by linking specific semantic features to distinct types of false memory. We replicate prior work showing that surprisal and prior knowledge influence memory errors, but extend it by demonstrating a clear dissociation: factual errors are predicted by similarity to narrative corpus, while confabulations arise from surprisal. This pattern aligns with prior studies showing that memory of surprising real-world events, particularly flashbulb memories, has a high likelihood of distortion <ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b69">70</ref> , and that unexpected breaks in narrative continuity increase false memory rates <ref type="bibr" target="#b54">55</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>In the current study, we demonstrate that narrative memory distortions can be reliably identified and meaningfully analyzed using computational approaches at scale. Across studies and participants, false memories occurred in up to 27.6% of recalled events, with factual errors (22.6%) more frequent than confabulations (7.7%). Building on established uses of generative language models in memory research, we made key innovations with the use of in-context learning examples and a novel taxonomy for classifying memory errors.</p><p>The study has several limitations which should be addressed. First, we determined the two types of false memories apriori based on existing literature, but may have overlooked other error types, especially given the complexity of naturalistic recall. Future work should explore the interactions of more known memory error types, and how individuals differ in drawing boundaries between them. Second, while we provide evidence for a double-dissociation and hypothesize about why these processes are distinct, further research will be needed to test these hypotheses in naturalistic recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Real-world implications</head><p>False memory research carries important real-world consequences, particularly in domains such as eyewitness testimony where errors can shape legal outcomes <ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b75">76</ref> . In this work, we refrain from assigning harm or deception to the term "false memory"; rather, we use it broadly to describe distortions that can arise in recall that can encompass a range of types <ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b77">78</ref> .</p><p>The prevalence rates we report should therefore be interpreted with caution: they reflect specific experimental choices and should not be taken as general benchmarks of false memory occurrence. Similarly, our automated detection method is not intended for judging the truth of individual statements. Its value lies in revealing variability across individuals and events to study the systematic conditions leading to memory distortions. The classification methods used in this study serve as research tools for understanding memory phenomena, not as instruments for evaluating truth or accuracy in real-world situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Free recall experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>We used the full cohort from our recently published dataset (the "Naturalistic Free Recall" dataset; NFRD  The same four spoken narratives used in the NFRD were leveraged in this study. The set comprised three personal stories from</p><p>The Moth Radio Hour-"Pieman", "Eyespy" and "Oregontrail"-alongside the first chapter of Lester Chadwick's "Baseball Joe in the Big League" (hereafter "Baseball"), obtained from LibriVox (www.librivox.org). Each audio file was paired with a verbatim transcript: the "Pieman" text was drawn from a previously published neuroimaging dataset <ref type="bibr" target="#b52">53</ref> , and the "Baseball" transcript was sourced from Project Gutenberg (www.gutenberg.org); for "Eyespy" and "Oregontrail," 4 generated and verified the narrative transcripts. The NFRD also segmented the narrative transcript text into events, using a previously validated method <ref type="bibr" target="#b20">21</ref> , with some optimization in parameter selection <ref type="bibr" target="#b3">4</ref> . On average, each audio clip lasted 11 minutes and 35 seconds. A complete overview of the stimuli and their sources is provided in Table <ref type="table" target="#tab_3">S1</ref>.</p><p>Participant verbal recollections were first transcribed using the Speech-to-Text API offered by Google Cloud (cloud.google.com/speechto-text), then proofread and corrected by a professional transcription service to ensure high-fidelity transcriptions (Figure <ref type="figure" target="#fig_0">1A</ref>; <ref type="bibr" target="#b3">4</ref> for complete details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LLM-based Annotation Pipeline</head><p>We developed a two-stage automated classification pipeline (Figure <ref type="figure" target="#fig_0">1B</ref>) using GPT-4o (version "2024-05-13") <ref type="bibr" target="#b30">31</ref> to identify and categorize false memories in text recollections. Each stage was implemented as a prompt-driven interaction using LangChain (www.langchain.com) with Python 3.9. The entire text of the original story was provided in the prompt (see below, Tables <ref type="table" target="#tab_4">S2</ref>, <ref type="table">S3</ref> and S4). To perform the scoring, each recall sentence was given in a new conversation message. To stay below context window limits, we maintained a conversation history with the five most recent recall sentences and AI annotations. Model temperature was set to 0.3 for event alignment and 0 for memory classification to balance between task reproducibility and flexibility, a practice based on prior literature <ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b79">80</ref> .</p><p>Event matching task In the first stage, the LLM was prompted to align each recall sentence with a corresponding narrative event (Figure <ref type="figure" target="#fig_0">1B</ref>). The system prompt consisted of two key components (Table <ref type="table">S4</ref>): ( <ref type="formula">1</ref>) instructions directing the model to match each recall sentence to one of the numbered story events, and (2) the full list of story events, each labeled with an ordered index.</p><p>Recall transcripts were segmented into individual sentences. Sentences shorter than 8 characters were concatenated to the preceding sentence to ensure sufficient content for event matching. For each transcript, a new GPT-4o <ref type="bibr" target="#b30">31</ref> instance was initialized with the system prompt, and recall sentences were passed in sequentially. The model's index outputs (e.g., "event 5" or "5")</p><p>were recorded for subsequent analysis. Outputs that are either index "0" or texts without (e.g., "I'm sorry, but I need more context to identify the specific event. ") indices were coded as no match for the recall sentence.</p><p>In contrast to prior work <ref type="bibr" target="#b3">4</ref> , our method uses sentence-level granularity to better align with generative LLMs, which process grammatically complete content more effectively. Event-level recall, defined as the proportion of participants with at least one sentence matched to an event, strongly correlated with prior recall measures using event-level granularity in participants' recalls ( 4 ; r = 0.46, p &lt; 0.001).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Memory classification task</head><p>In the second stage, the LLM was prompted to classify each recall segment into different types of memory distortions.</p><p>This step used in-context learning, leveraging the model's ability to perform complex tasks based on task-specific examples without updating model parameters <ref type="bibr" target="#b80">81</ref> . In-context learning is particularly suitable here because it allows the model to flexibly adapt to nuanced psychological categories <ref type="bibr" target="#b81">82</ref> . To ensure the model interprets the task as psychological research and responds appropriately, the role of the human speaker in the conversation chain was renamed from "user" to "psychologist".</p><p>The system prompt included three components (see Table <ref type="table" target="#tab_4">S2</ref> and S3 for full content): (1) detailed classification instructions,</p><p>(2) the complete original story for reference, and (3) eight in-context examples illustrating classification labels.</p><p>Our classification framework contained three mutually exclusive categories for memory distortions (Figure <ref type="figure" target="#fig_0">1C</ref>): factual errors (details that contradict the original story), confabulations (fabricated details not present in the story), and inferences (plausible but unmentioned details derived from prior knowledge or reasonable interpretations). These types of memory distortions, including inferences <ref type="bibr" target="#b33">34</ref> , were inspired by theoretical accounts <ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b59">60</ref> and empirical studies on memory distortions <ref type="bibr" target="#b73">74</ref> .</p><p>The in-context examples included two representative cases of each memory type, i.e., factual error, confabulation, and inference, as well as two examples that did not fall into any of these categories. This balanced set was designed to guide model classification decisions and reduce ambiguity for edge cases (see Table <ref type="table">S3</ref>). After promoting the model, recall segments were given to the model in sequential order. For each recall segment, the model returned labeled text outputs for each of the three categories (see Table <ref type="table">S3</ref> for in-context learning examples). The LLM was instructed to either return the verbatim portion corresponding to the memory distortion, or to return "None" to indicate that the memory distortion did not appear in that recall segment (Figure <ref type="figure" target="#fig_0">1C</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>We recruited 30 participants through the NYU's SONA subject pool to evaluate the alignment between GPT-4o's false memory classifications and human judgments. To disambiguate between participants in the validation task and the original recall task, we henceforth refer to validation task participants as raters. Raters were randomly assigned to one of two story pair conditions:</p><p>13 were assigned to Pieman and Eyespy, and 17 to Oregontrail and Baseball. One rater in the Pieman/Eyespy group was excluded for selecting "neutral" on the item "I understood the task instructions," leaving a final sample of 29 raters (12 for Pieman/Eyespy and 17 for Oregontrail/Baseball; mean age = 18.61, SD age = 1.06; 17 female). All raters were native-english speakers and completed the task in person.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Materials</head><p>To validate the AI-generated false memory labels, we selected a balanced and representative sample of 30 recall sentences per story (120 sentences total). This was done by randomly selecting six events from each narrative. As such, this approach was intended to avoid potential selection bias and allowed us to test the model's performance across a range of stories and events.</p><p>For each event, we randomly sampled five recall sentences that had been matched to that event by our annotation pipeline. This ensured that multiple participants' recollections for the same narrative were evaluated. To balance the evaluation and reduce bias in rater judgments, we selected an equal number of recall sentences labeled by the AI as accurate and as containing a memory distortion (15 per each story; 78.1% are factual errors and 21.9% are confabulations across all stories). The resulting 30 trials per story provided a manageable yet statistically meaningful sample. Each rater reviewed two stories (60 trials total).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task</head><p>We programmed the task using JsPsych <ref type="bibr" target="#b82">83</ref> and hosted it on Google Firebase (https://firebase.google.com/). At the beginning of the in-person experiment, the experimenter introduced the purpose of the study, namely to provide a separate evaluation of false memory for recalled details of a story, given an existing rater's judgment. Notably, human raters were not told that they were evaluating an AI rater. The experimenter then walked each rater through sample trials to ensure they understood the instructions and task format. Raters then proceeded to the main task on lab computers. The task was divided into two blocks, one for each of their assigned stories.</p><p>In each block, raters listened to the full audio recording of the assigned story. This ensured that they were familiar with the story content and could make informed judgments during the validation trials. The raters were also provided with a physical print-out of the story transcripts for reference during the task. Following the listening phase, they were presented with a series of validation questions, each corresponding to a single recall sentence that had been previously matched to a specific story event.</p><p>Each trial displayed the matched story context on the left side of the screen and the corresponding participant recall on the right side. The story text on the left consisted of the focal event along with the immediately preceding and following events for context. This consistent presentation was intended to help raters locate the relevant section of the narrative while encouraging them to draw upon their broader understanding of the story. The trials were presented in mini-blocks: a single story event was</p><p>given and raters viewed and validated all five recalls for that event during the mini-block. Raters were instructed to try to base their judgments on the entire story, not just the story context given on the left side of the screen.</p><p>At the top of the screen, raters viewed the AI model's binary classification for the recall sentence: either "The rater thinks this recollection is accurate" for correctly classified sentences, or "The rater thinks this recollection contains inaccuracies" for those containing false memories. For trials involving false memories, the recall text that was returned by the model was underlined to guide the rater evaluations.</p><p>The task required the raters to agree or disagree with the AI's classification (two-alternative forced choice), followed by a 5-point Likert confidence rating ranging from "not at all" to "completely" (labels: "not at all", "slightly", "somewhat", "fairly", "completely").</p><p>After completing the experimental blocks, raters answered a short post-task questionnaire that assessed their engagement and experience. This questionnaire included statements such as "I understood the task instructions," "I was engaged in the experiment," and "I found the experiment difficult," all rated on a 5-point Likert scale. These responses were used to assess data quality and to confirm raters' understanding and task compliance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis</head><p>To evaluate how reliably humans aligned with the automated pipeline, we computed two agreement measures: inter-subject agreement and subject-AI agreement. Agreement refers to the extent to which two raters (human-human or human-AI), made the same binary judgment about a recall trial (i.e., whether they both judged the recall as accurate or inaccurate). We computed agreement as a simple proportion (e.g., matched judgments on 55/60 trials). By comparing inter-subject agreement and subject-AI agreement in a permutation test, we test whether AI provides a reasonable proxy for human judgment <ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41</ref> .</p><p>For inter-subject agreement, within each story, we calculated the proportion of identical responses for every possible pair of raters across all trials <ref type="bibr" target="#b27">28</ref> . These pairwise agreement scores were averaged to yield a single inter-subject agreement score per story. For subject-AI agreement, we computed the proportion of trials on which each rater's response matched the AI's original classification and averaged those proportions across raters within each story.</p><p>To estimate uncertainty and compare the agreement metrics statistically, we required a structured format that captured rater-level judgments across trials. To this end, we constructed two binary matrices per story: one representing subject-AI agreement and another representing inter-subject agreement. The first matrix (the dimension is the number of questions by the number of raters) indicated whether each rater agreed with the AI on each trial (1 for agreement, 0 for disagreement). The second matrix (the dimensions is the number of questions by the number of all possible rater pairs) indicated whether each pair of raters agreed with each other on each trial. These matrices provided a structured format for quantifying trial-level agreement patterns and laid the groundwork for further statistical analysis.</p><p>We used nonparametric bootstrapping to estimate confidence intervals and test for statistical differences between agreement types. For confidence intervals, we resampled the trial rows of each matrix with replacement 2,000 times per story, recalculating average agreement for each resample to generate 95% confidence intervals. To aggregate results across stories, we computed pooled standard errors by taking the square root of the sum of squared standard errors divided by the number of narratives.</p><p>To test whether subject-AI agreement exceeded inter-subject agreement, we employed a two-stage bootstrap procedure to account for both within-story and between-story variability. First, we resampled trial rows 2,000 times per story and calculated the difference between subject-AI and inter-subject agreement for each resample, yielding a bootstrap distribution of agreement differences for each story. Second, we performed 2,000 iterations where we randomly sampled (with replacement) one difference value from each story's distribution and averaged them, generating a grand bootstrap distribution of agreement differences across stories. From the bootstrap distribution of averaged effects we obtained 95% confidence intervals as the 2.5th and 97.5th percentiles, and calculated two-sided p-values by doubling the fraction of bootstrap means that lie on the opposite side of zero from the observed mean.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model validation Experiment 2 (comprehension task)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>For the second Validation Experiment, we recruited 126 raters using NYU's SONA system. Raters were given course credit for their participation. We applied two exclusion criteria to ensure data quality. First, same as Validation Experiment 1, we removed 39 raters based on their self-reported engagement and level of understanding of the task (lower than or equal to 3 out of a 5-point Likert scale). Second, we excluded 17 raters who scored below 90% on embedded engagement check questions.</p><p>These checks were designed to identify individuals who had not adequately understood the core story content. The final sample included 70 raters (mean age = 20.13, SD age = 1.14; 41 female). Raters were randomly assigned to receive two of the four stories.</p><p>The number of unique raters completing each story was as follows: Pieman (47), Eyespy (39), Oregon Trail (29), and Baseball (25).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Materials</head><p>This experiment adapted the materials from Validation Experiment 1 by adopting a more constrained comprehension-based format. In addition to the trials used in Validation Experiment, we selected four other events (i.e., 20 recall sentences), yielding 50 recall sentences per story. We focused exclusively on half of those that the AI had classified as containing false memories (either factual errors or confabulations), as these were the primary targets for validation.</p><p>The sentences were reformulated into a two-alternative forced-choice comprehension question using either wh-(who, what, where, etc.) or yes-no formats. Questions focused on the specific detail that the AI had identified as inaccurate. We aimed to preserve the original sentence structure while isolating the distorted element. The AI-identified erroneous phrase became one answer option (the "recalled" option), while the corresponding correct detail from the original story served as the alternative (the "verbatim" option). For example, a recall sentence containing the incorrect detail "he asked the principal..." was converted to: "Who did Jim question about whether the raise is against the school's policy?" with options "the dean" (verbatim) and "the principal" (recalled). We also included yes/no questions when the verbatim detail was unclear or unmentioned. For example, the recall sentence "He went to the dean's office" was converted to "Did Jim go to Dean McGowan's office?".</p><p>To assess raters' overall task engagement, we also included five engagement check questions per story (catch trials). These questions were used to identify raters who failed to grasp the core narrative content. We randomly sampled five events from the existing question set and added one extra question asking about basic factual details of the event. The two choices were a verbatim answer and an incorrect answer that we designed to be very easily identified. For example, one engagement question in the story Pieman was "Did the person who pied the dean run away?" with the options yes and no.</p><p>Each rater was randomly assigned two out of the four stories, receiving all 25 comprehension questions and five engagement checks per story. Psychologist: "and so the guy gets really excited, and he goes and starts writing the story up. It's his first-ever story. He's really excited, and he writes a story up but better." answer: 1) None. 2) "It's his first-ever story." 3) None. Psychologist: "which is what Pie Man would say after an attack, or like for justice. An attack for justice, not in a negative connotation." answer: 1) None. 2) None. 3) "An attack for justice" Psychologist: "The guy ends up going to those library steps at that time and sees the student body president, who's female. I forgot her name. She is, as the guy describes, well-bred and is not working class like most of the foreign student body was, and is wealthy" answer: 1) None. 2) None. 3) "is not working class like most of the foreign student body was, and is wealthy"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Continued on next page</head><p>Psychologist: "Then he goes to a bar one night and meets a girl named Amanda I believe, and he has a crush on her or he says like they were flirting, but then he said he was actually the only one flirting with her." answer: 1) "Amanda" 2) None. 3) None. Psychologist: "He gives us context to that, saying that she's the student body president and that she's the reason that Fordham University has a no beer or no drinking policy now, and that she is well-read and seems to be the type of student that the dean wants more of at Fordham University." answer: 1) None. 2) None. 3) None. Psychologist: "So they meet up at the steps of the library and dean runs off or something in a cape at least thats what he depicted it in his fabricated story again, and he was pied again, I think." answer: 1) None. 2) "So they meet up at the steps of the library" 3) None. Psychologist: "By running the story, he said hes going to do some embellishments on it and the embellishment he added was making him more of a superhero by giving him a catchphrase like, I am not an animal, but he said it in Latin. This guy was wearing a cape." answer: 1) None. 2) None. 3) None.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Eyespy</head><p>Psychologist: "Ferry people started getting more and more close and like easy to see and it said there on became just like little jumping dots. They were I'm assuming screaming at them for to come back." answer: 1) None. 2) "screaming at them for to come back." 3) None. Psychologist: "there was a specific event where they would go on vacation on I think she said Nantucket Island I forgot where it was" answer: 1) "Nantucket Island". 2) None. 3) None. Psychologist: "they were telling the story to those in the family. As a punishment to the kids, the kids were not allowed to play in the water. They had to stand on the beach and the kids were like, 'oh like can we just go in the water?' And they were like 'fine you can go'." answer: 1) None. 2) None. 3) "kids were like, 'oh like can we just go in the water?' And they were like 'fine you can go.'" Psychologist: "their whole her whole family was quite obsessed with the Kennedys, to the point that when JFK died she and her sister were in the car on a road trip of some sort." answer: 1) "when JFK died". 2) None. 3) None. Psychologist: "Then, shes talking about their summers where they would go to the beach with their aunts and cousins and uncles, and at the beach, the aunts and uncles loved to watch not the beach but the Kennedy house because the beach house that they would stay at was directly across from the Kennedys beach house." answer: 1) None. 2) None. 3) None. Psychologist: "I didn't know the last video guys name, but her name was Mikayla and she knocked out her uncles -I think her uncle most likely, his eyeball. And they then the eyeball. They then got punished, whatever." answer: 1) None. 2) "They then got punished" 3) None. Psychologist: "Then, the girl and her brother or cousins went out into the water, and they were messing around too much, and they got on a raft and the riptide pulled them under." answer: 1) None. 2) None. 3) "and they were messing around too much" Psychologist: "The aunts get mad and they were like, Okay, you guys are not allowed to touch the water in the beach, just the sand." answer: 1) None. 2) None. 3) None.</p><p>Oregontrail Psychologist: "She was a teacher and then she became a comedian. Her kids apparently really liked the Oregon Trail and they liked the thought of dying." answer: 1) None. 2) "and then she became a comedian." 3) None. Psychologist: "It ended up being a baby wolf named, Benny-the-something, Benny-the-Chinny or something like that." answer: 1) None. 2) None. 3) "Benny-the-Chinny or something like that" Psychologist: "She compared -she did some parallels between Oregon Trail and the Dragons, what is it called, Dungeon and Dragons game. Every day they were talking about different things. The kids loved how dangerous." answer: 1) None. 2) None. 3) None. Psychologist: "When the kid rolled that she was going to die, another kid remembered, Hey, Im a doctor! So the teacher came over and was like, Its great! Come up here and help this child.So, they rolled and I think they rolled the wrong number." answer: 1) "they rolled the wrong number" 2) None. 3) "so the teacher came over and was like, Its great!" Psychologist: "I believe they ended up with a three and theyve obtained a wolf. Lets see." answer: 1) None. 2) "they ended up with a three" 3) None. Psychologist: "The teacher responded by saying that the wagon was, like I said, dedicated or reserved for carrying either a wounded, the sick, or supplies and that children never rode the cart." answer: 1) None. 2) None. 3) None. Psychologist: "They had like 790 miles to go to get to Oregon City, and she said they made it incredibly fast." answer: 1) "790 miles" 2) None. 3) None.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Continued on next page</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Task design and false memory annotation pipeline. (A) Task design of the narrative free recall paradigm<ref type="bibr" target="#b3">4</ref> . Participants listened to two spoken stories and immediately recalled them verbally for at least four minutes; we then performed automatic speech-to-text transcription with professional review of the audio files. (B) Recall annotation approach. We employed in-context learning with GPT-4o to perform two consecutive tasks: (1) matching recall sentences to corresponding story events, and (2) classifying types of memory. We defined three distinct memory types based on prior theoretical frameworks<ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38</ref> : factual errors (details that contradict the original narrative), confabulations (fabricated information not present in the story), and inferences (plausible details derived from world knowledge or reasonable interpretation). Full prompts for both tasks are attached in TableS2, S3 and S4. (C) Example classification outputs. An example story event from Pieman (left) is shown alongside three sample recollections from different participants (center) that were matched to this event. The LLM was instructed to return the specific portions of the recollections classified as factual errors (blue), confabulations (red), or true-memory inferences (light green).</figDesc><graphic coords="4,56.69,164.32,498.63,321.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Results from human validation experiments and summary of GPT-4o classification of false memories. (A) Subject-AI agreement exceeded inter-subject agreement in Validation Experiment 1. Twenty-nine raters evaluated 60 recall sentences (30 per story), each shown with story context and the AI's binary classification (accurate vs. inaccurate), with incorrect segments underlined in false memory cases. Raters made a binary AI agreement judgment and rated their confidence. Subject-AI agreement (light gray) was significantly higher than inter-subject agreement (dark gray) for both accurate, or correct trials (p &lt; 0.001) and false memory trials (p = 0.002)-suggesting the AI's judgments better reflected the group consensus. (*** p &lt; 0.001; ** p &lt; 0.01). (B) AI ratings aligned with group-level comprehension in Validation Experiment 2.To reduce ambiguity, we developed a structured comprehension task where seventy raters completed 50 questions (25 per story). Inaccurate recall segments were converted into two-alternative forced-choice questions (falsely recalled detail vs. true original detail). This format improved inter-subject agreement (0.60 to 0.78 across memory types). No significant differences emerged between subject-AI and inter-subject agreement for either factual errors (p = 0.257) or confabulations (p = 0.188). For A-B, error bars represent bootstrapped confidence intervals. (C) Average percentage of recalled events containing memory distortions per participant (each dot in swarm plot corresponds to a single participant). Participants exhibited a higher proportion of factual errors (0.24 ± 0.01 SEM) compared to confabulations (0.09 ± 0.01 SEM; t = 19.8, p &lt; 0.001). (D) Proportion of participants who falsely recalled each event, averaged across events within each story. Bar colors indicate the proportion attributed to factual error (blue), confabulation (red), or both (purple). Variability across narratives suggest that their content influences the rate of false recall. (E) Histogram of the proportion of events with different false memory rates, based on values from (D). Overlaid histograms separately show the distribution for factual errors (blue) and confabulations (red). For a given event, confabulations were less frequent than factual errors (left skewed distribution), consistent with the view of the data from (C).</figDesc><graphic coords="7,56.69,116.55,498.64,309.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Computational measures of semantic context in narrative events. (A) Semantic centrality quantifies how well-connected each event is to the overall narrative structure. Event text is embedded in high-dimensional semantic space, and centrality is calculated as the average cosine similarity between each event and all other events within the same story<ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b19">20</ref> . (B) Similarity to narrative corpus measures how closely each event matches semantic patterns in a selected text corpus, such as a corpus of narratives. This is measured as the average similarity of a given event embedding to all events in the corpus<ref type="bibr" target="#b48">49</ref> . (C) Contextual surprisal captures how unexpected each event is given the preceding narrative context. Surprisal is computed as the negative log probability of each word conditioned on prior context, averaged across all words within the event.</figDesc><graphic coords="8,106.55,63.78,398.90,232.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Semantic factors differentially predict memory accuracy and distortion types. Linear mixed-effects models were used to assess how semantic properties of story events predict recall likelihood and different forms of memory distortions. (A) Events with higher semantic centrality are more likely to be recalled, consistent with the importance of this measure for memory retention<ref type="bibr" target="#b19">20</ref> (β = 0.10, p &lt; 0.001). (B) Factual errors are more likely to occur when events have greater similarity to narrative corpus (β = 0.14, p = 0.001). (C) Confabulations were more frequent for events with higher contextual surprisal (β = 0.16, p = 0.002), suggesting a link between unpredictability and memory fabrication. Bars show standardized regression coefficients (β ); error bars indicate 95% confidence intervals. All models included semantic centrality, similarity to narrative corpus, surprisal, serial position, and length as fixed effects, with participant and story identity modeled as random effects (see Table1).</figDesc><graphic coords="9,56.69,68.03,498.62,170.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="23,56.69,109.56,498.63,179.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="23,56.69,446.84,498.63,177.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>). Linear</figDesc><table><row><cell></cell><cell>β coefficient</cell><cell>CI (Wald)</cell><cell>p value</cell></row><row><cell cols="3">Recall: 229 participants (16,345 trials)</cell><cell></cell></row><row><cell>Intercept</cell><cell>0.09</cell><cell>-0.33, 0.51</cell><cell>0.674</cell></row><row><cell>Centrality</cell><cell>0.10</cell><cell>0.04, 0.16</cell><cell>&lt;0.001</cell></row><row><cell>Contextual surprisal</cell><cell>-0.03</cell><cell>-0.08, 0.01</cell><cell>0.145</cell></row><row><cell>Sim. to prior corpus</cell><cell>-0.03</cell><cell>-0.08, 0.02</cell><cell>0.189</cell></row><row><cell>Event length</cell><cell>0.32</cell><cell>0.28, 0.36</cell><cell>&lt;0.001</cell></row><row><cell>Serial position</cell><cell>-0.01</cell><cell>-0.05, 0.02</cell><cell>0.477</cell></row><row><cell cols="3">Factual error: 229 participants (8,295 trials)</cell><cell></cell></row><row><cell>Intercept</cell><cell>-1.21</cell><cell cols="2">-1.45, -0.97 &lt;0.001</cell></row><row><cell>Centrality</cell><cell>-0.02</cell><cell>-0.12, 0.07</cell><cell>0.635</cell></row><row><cell>Contextual surprisal</cell><cell>0.04</cell><cell>-0.03, 0.11</cell><cell>0.273</cell></row><row><cell>Sim. to prior corpus</cell><cell>0.14</cell><cell>0.06, 0.22</cell><cell>0.001</cell></row><row><cell>Event length</cell><cell>0.10</cell><cell>0.05, 0.16</cell><cell>&lt;0.001</cell></row><row><cell>Serial position</cell><cell>-0.16</cell><cell cols="2">-0.21, -0.10 &lt;0.001</cell></row><row><cell cols="3">Confabulation: 229 participants (8,295 trials)</cell><cell></cell></row><row><cell>Intercept</cell><cell>-2.63</cell><cell cols="2">-2.85, -2.41 &lt;0.001</cell></row><row><cell>Centrality</cell><cell>0.02</cell><cell>-0.12, 0.17</cell><cell>0.752</cell></row><row><cell>Contextual surprisal</cell><cell>0.16</cell><cell>0.06, 0.27</cell><cell>0.002</cell></row><row><cell>Sim. to prior corpus</cell><cell>0.03</cell><cell>-0.10, 0.16</cell><cell>0.627</cell></row><row><cell>Event length</cell><cell>0.13</cell><cell>0.05, 0.22</cell><cell>0.002</cell></row><row><cell>Serial position</cell><cell>0.08</cell><cell>0.00, 0.17</cell><cell>0.056</cell></row></table><note><p>mixed-effects model results for predicting recall, factual error, and confabulation rates. Standardized regression coefficients (β ), 95% Wald confidence intervals (CIs), and p-values are reported for each fixed effect in the three models. The significance threshold was Bonferroni-corrected for multiple comparisons (α = 0.0167). p-values for significant predictors are shown in bold. All models included participant and story identity as random intercepts.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc><ref type="bibr" target="#b3">4</ref> </figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>). This includes 229 native English speaking participants (145 female; Mean age = 25.03, SD age = 11.15), who took part in the online study (see</figDesc><table /><note><p><p><p><p>Table</p>S1</p>for detailed demographic information). This cohort was selected after excluding participants based on self-reported engagement (lower than or equal to 2 out of a 5-point Likert scale). Data collection was split between New York University's SONA Systems platform (n = 167; Mean age = 19.67, SD age = 1.33), where undergraduates earned course credit, and Prolific (www.prolific.com; n = 62; mean age = 39.77, SD age = 12.90), whose members were compensated at a rate of $10 per hour.</p>All participants provided informed consent prior to experimental testing. This study was approved by New York University's Committee on Activities Involving Human Subjects (IRB-FY2016-1357).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table S1 .</head><label>S1</label><figDesc>Overview of the narrative stimuli and participant demographics.InstructionYou are going to hear a story told by [name of the narrator] in its entirety, and then see a subject's recall of the story sentence by sentence. Using common sense, you will rate whether each recall sentence contains two mutually exclusive types of false memories. Please note paraphrasing or transcription typos are not false memory. False memory that is corrected by the subject themself does not count, either.1) Factual error: Where in the statement, if any, contains factual error that contradicts the story? Factual errors include wrong subject, object, location, timing, characters' speech or action.</figDesc><table><row><cell></cell><cell>Stimulus source</cell><cell cols="2">Speaker gender Length in words</cell><cell>Length in audio (seconds)</cell><cell cols="2">Participant demographics N age range gender</cell></row><row><cell>pieman</cell><cell cols="2">The Moth Radio Hour Male</cell><cell>948</cell><cell>489</cell><cell>116 17-29</cell><cell>88 female</cell></row><row><cell>eyespy</cell><cell cols="2">The Moth Radio Hour Female</cell><cell>2318</cell><cell>779</cell><cell>116 17-29</cell><cell>88 female</cell></row><row><cell cols="3">oregontrail The Moth Radio Hour Female</cell><cell>2389</cell><cell>743</cell><cell>113 18-75</cell><cell>57 female</cell></row><row><cell>baseball</cell><cell>LibriVox Audio</cell><cell>Male</cell><cell>2088</cell><cell>768</cell><cell>113 18-75</cell><cell>57 female</cell></row><row><cell></cell><cell></cell><cell>System prompt content</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">In addition, you will also include ratings for:</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="5">3) Inference: Where in the statement, if any, has inferences derived from common</cell></row><row><cell></cell><cell></cell><cell cols="3">world knowledge or characters' mental state?</cell><cell></cell><cell></cell></row><row><cell cols="2">Story text</cell><cell>[Full story transcript]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Examples</cell><cell cols="2">Eight examples in the following format:</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Psychologist:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="5">Answer: 1) [details considered factual errors or "None"] 2) [details considered</cell></row><row><cell></cell><cell></cell><cell cols="4">confabulations or "None"] 3) [details considered inferences or "None"]</cell><cell></cell></row></table><note><p>2) Made-up information: Where in the statement, if any, contains baseless information absent in the story? This includes new characters or actions that are intuitively false and not inferences or factual errors.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table S2 .</head><label>S2</label><figDesc>System prompt template for the memory type classification model. The system prompt consists of three sections: instruction, the full story transcript, and in-context learning examples. The exact content varies based on the story.</figDesc><table><row><cell></cell><cell>In-context learning examples</cell></row><row><cell>Pieman</cell><cell>Psychologist: "He recalls the place, but I don't quite remember it. Rhode Island in Massachusetts? I</cell></row><row><cell></cell><cell>forgot." answer: 1) "Rhode Island in Massachusetts?" 2) None. 3) None.</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task</head><p>Raters completed the experiment remotely using their personal devices. Following the design of Validation Experiment 1, raters listened to two complete story recordings and then completed the associated comprehension trials for each story.</p><p>Each trial presented relevant story context consisting of the focal event and its immediately preceding and following events.</p><p>Raters were instructed to try to base their responses on their complete understanding of the story, not just on the local context provided. Furthermore, raters were instructed to select one of the two options before advancing to each subsequent trial. Upon completing all trials, raters completed the same engagement questionnaire from Experiment 1 to assess task understanding and effort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis</head><p>We evaluated agreement for each question containing a memory error. Raters were considered to agree with the AI if they selected the verbatim option, which aligned with the detail from the original story. Selecting the recalled (i.e., false memory) option was treated as disagreement with the AI's judgment.</p><p>We then computed inter-subject and subject-AI agreement separately for questions derived from factual errors and confabulations, respectively. To test for significant differences, we applied the same two-stage nonparametric bootstrapping procedure used in Validation Experiment 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluating the distribution of false memories</head><p>To investigate how false memories manifest across participants and story events, we combined outputs from both the event matching and memory classification tasks. For each story event at the participant-level, we determined: (1) whether the event was recalled-defined as at least one recall sentence being matched to that event by the event alignment task-and (2) whether any of those matched recall sentences contained a factual error, confabulation, or inference, as identified by the classification task.</p><p>If a participant recalled a story event and any of the corresponding recall sentences included a false memory (i.e., either a factual error or a confabulation), we considered that participant to have recalled the event with a distortion. This approach ensured that multiple recall attempts for the same event were consolidated at the event level, and that the presence of any memory distortion was sufficient to count the event as distorted for that participant. Prior to all subsequent analyses, we excluded story events that were recalled by fewer than 20 participants. This threshold was chosen to ensure sufficient sample size for estimating event-level false memory rates.</p><p>Using this method, we computed two sets of measures. At the participant level, we calculated the proportion of recalled events that contained each type of distortion. At the event level, we calculated the proportion of participants who recalled each event with an error, normalized by the total number of participants who recalled that event at all. This allowed us to quantify the rate of false memories for any given event across participants.</p><p>To compare the relative rates of each error type, we conducted paired t-tests on both participant-level and event-level data, comparing the rates of factual errors and confabulations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Extraction of semantic features</head><p>To characterize the semantic properties for individual events, we extracted three event-level features: semantic centrality, similarity to narrative corpus, and contextual surprisal.</p><p>Our primary analyses relied on sentence embeddings and perplexity (as a measure of model-based surprisal) values derived from pretrained embedding models and autoregressive language models, respectively. For embeddings, we used the all-mpnet-base-v2 model <ref type="bibr" target="#b44">45</ref> via the Sentence Transformers Python library <ref type="bibr" target="#b83">84</ref> , chosen for its strong performance on the STS-B benchmark of human similarity judgments <ref type="bibr" target="#b45">46</ref> . For surprisal estimation, we employed GPT-2 Small (124M parameters) <ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48</ref> , the latest open-source GPT model demonstrated to predict human reading and memory behaviors <ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b84">[85]</ref><ref type="bibr" target="#b85">[86]</ref><ref type="bibr" target="#b86">[87]</ref><ref type="bibr" target="#b87">[88]</ref> .</p><p>To assess the robustness of our findings, we also tested an alternative embedding model -Universal Sentence Encoder (USE) <ref type="bibr" target="#b56">57</ref> and one alternative autoregressive language model of similar size: Pythia (160M) <ref type="bibr" target="#b57">58</ref> . USE was selected for its broad applicability in narrative memory studies <ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b88">[89]</ref><ref type="bibr" target="#b89">[90]</ref><ref type="bibr" target="#b90">[91]</ref> . We chose Pythia because it has a similar number of parameters to GPT-2 small, and has also been shown to predict human reading times <ref type="bibr" target="#b91">92</ref> . Results of the linear mixed effects models from these alternatives are reported in the supplementary materials. All models were downloaded from Hugging Face (huggingface.co).</p><p>To compute semantic centrality, we calculated the pairwise cosine similarity between the embedding of each event and all other events within the same story (in line with previous work <ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b19">20</ref> ). We then averaged these similarity scores for each event, yielding a measure of how semantically connected an event was to the rest of the narrative-an index of its conceptual centrality within the story's overall structure.</p><p>To estimate similarity to prior narrative corpus, we compared each story event to a large external corpus of fiction using the NarrativeXL dataset <ref type="bibr" target="#b48">49</ref> , which includes full-text versions of 1,500 fiction books from Project Gutenberg. We segmented into 55-word windows to match the structure of our narrative events, resulting in approximately 2.3 million segments comprising over 126 million words. We embedded each segment using the same sentence embedding model and computed the average cosine similarity between each story event and all NarrativeXL segments. This score quantifies how closely each event resembles prototypical narrative content-that is, common story patterns found across a large fiction corpus.</p><p>As a measure of contextual surprisal, we estimated how suprising each event was given its preceding story context. Using GPT-2 Small, we concatenated each event with all preceding words in the story and computed token-level cross-entropy loss.</p><p>A binary mask was applied to isolate the tokens from the current event, and the average negative log-likelihood (NLL) over those tokens was used as the surprisal score. Higher values indicate greater deviation from expected narrative progression.</p><p>The analysis was implemented using PyTorch <ref type="bibr" target="#b92">93</ref> , and Hugging Face's Transformers library <ref type="bibr" target="#b93">94</ref> . Results from an alternative comparable model (Pythia <ref type="bibr" target="#b57">58</ref> ) are provided in the supplementary materials for comparison (Table <ref type="table">S5</ref> and<ref type="table">S6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relating semantic measures to memory behavior</head><p>To investigate how semantic features influence both memory accuracy and error rates, we constructed three linear mixed-effects models predicting participants' behavioral responses. The models targeted three distinct outcomes: (1) whether an event was recalled, (2) whether it was recalled with a factual error, (3) whether it was recalled with a confabulation. Note that the first model included unrecalled events, whereas the second and third models only contained recalled events. These models allowed us to test which event-level features predicted successful recall and specific types of memory distortion. All models were fit using the lme4 package (version 1.1-28) in R (version 4.1.2) <ref type="bibr" target="#b94">95</ref> .</p><p>We selected a consistent set of fixed effects across models. These included: semantic centrality, similarity to prior narrative corpus, contextual surprisal, serial position (order of the event within the story), and event length (in words). All predictors were z-score normalized prior to model fitting. Participant and story ID were included as random intercepts to account for individual differences and story-level effects.</p><p>To control for multiple comparisons across the three memory error models, we applied Bonferroni correction, adjusting the significance threshold by a factor of three (α = 0.0167). This conservative approach reduces the likelihood of Type I errors when testing related hypotheses across the recall, factual error, and confabulation models. The significance values were generated as part of the summary output based on asymptotic Wald tests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Code and data availability</head><p>All of our analysis code are available at https://github.com/phoebsc/Distinct-paths-to-false-memory.</p><p>The published narrative recall dataset <ref type="bibr" target="#b3">4</ref> analyzed in the study can be downloaded at https://osf.io/h2pkv/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>Psychologist: "They would play different games where they would roll dice and after each time they roll the dice, it would be a different scenario and they had to make different choices." answer: 1) None. 2) None. 3) "after each time they roll the dice, it would be a different scenario and they had to make different choices"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseball</head><p>Psychologist: "Joe Varley was standing and leaning, looking pensively out a window as he writes a letter he just got in the post." answer: 1) "Joe Varley". 2) "looking pensively out a window as he writes a letter" 3) None. Psychologist: "I think I remember him talking about earning more money if he were to be a part of the Pittston's team earning more money and fame." answer: 1) "the Pittston's team" 2) None. 3) None. Psychologist: "He's going to find out he's a pitcher, and as he's teasing his sister and swinging around his arm like he's pitching, their mother walks into the room, she's about to water the plants, he knocks the water pot out of her hands, gets her involved." answer: 1) None. 2) None. 3) None. Psychologist: "So, the sister doesn't really want to go because of that, but he convinces her to go with him anyway to go meet them." answer: 1) "but he convinces her to go with him anyway to go meet them." 2) None. 3) None.</p><p>Psychologist: "Oh yes, it was also revealed to be a snowy day and I believe the sister, Clara, mentioned something about the family or the father having debt, presumably a deceased father, something about a business." answer: 1) None. 2) "the father having debt, presumably a deceased father" 3) None. Psychologist: "He just said, 'I guess I'll get paid more,' and then I think he finally told her that he was going to get paid more and she got excited." answer: 1) None. 2) None. 3) None. Psychologist: "His mother says that that she doesn't quite like that idea or something along those lines, basically, saying that she refused the topic or the legitimacy of it." answer: 1) None. 2) None. 3) "saying that she refused the topic or the legitimacy of it." Psychologist: "He comes back in X, Y, Z hour, and he goes like, 'Hey,' and he goes, 'Joe, the train is not going to get in this time around.' He was like, 'Oh, what do you mean? Has it been derailed?'" answer: 1) None. 2) None. 3) "Has it been derailed?"</p><p>Table <ref type="table">S3</ref>. In-context learning examples for each story. These examples are concatenated to the instruction and story transcript in Table <ref type="table">S2</ref>, as part of the system prompt in the memory type classification model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prompt content</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instruction</head><p>You are a helpful agent. Here is a story splitted into a list of events in order. For each user message, your job is to identify which event the user is talking about. Please respond with the event index.</p><p>Segmented story text 1. I began my illustrious career in journalism in the Bronx where I toiled as a hardboiled reporter for the Ram, the student newspaper at Fordham University. 2. And one day I'm walking toward the campus center and out comes the elusive Dean McGowan, architect of a policy to replace Fordham's traditionally working-to middle-class students with wealthier, more prestigious ones. . . . Table <ref type="table">S4</ref>. System prompt template for the event matching model. The system prompt consists of two sections: instruction and the segmented story transcript. The story transcript varies based on the story; we include the first two events from *Pieman* for reference.   </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A study in experimental and social psychology</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Bartlett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1932">1932</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Creating false memories: Remembering words not presented in lists</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Roediger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Mcdermott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. experimental psychology: Learn. Mem. Cogn</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">803</biblScope>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>Loftus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Pickrell</surname></persName>
		</author>
		<title level="m">The formation of false memories</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Naturalistic Free Recall&quot; dataset: Four stories, hundreds of participants, and high-fidelity transcriptions</title>
		<author>
			<persName><forename type="first">O</forename><surname>Raccah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Gureckis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Poeppel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName><surname>The</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41597-024-04082-6</idno>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Brainerd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">F</forename><surname>Reyna</surname></persName>
		</author>
		<title level="m">The science of false memory</title>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Influences of intentional and unintentional forgetting on false memories</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Kimball</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Bjork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Exp. Psychol. Gen</title>
		<imprint>
			<biblScope unit="volume">131</biblScope>
			<biblScope unit="page">116</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The seven sins of memory: How the mind forgets and remembers</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Schacter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>HMH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Misattribution, false recognition and the sins of memory</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Schacter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Dodson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philos. Transactions Royal Soc. London. Ser. B: Biol. Sci</title>
		<imprint>
			<biblScope unit="volume">356</biblScope>
			<biblScope unit="page" from="1385" to="1393" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Jacoby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dywan</surname></persName>
		</author>
		<author>
			<persName><surname>Memory</surname></persName>
		</author>
		<author>
			<persName><surname>Attributions</surname></persName>
		</author>
		<title level="m">Varieties of memory and consciousness</title>
		<imprint>
			<publisher>Psychology Press</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="391" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Memory distortion: An adaptive perspective</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Schacter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Guerin</surname></persName>
		</author>
		<author>
			<persName><surname>St</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Jacques</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2011.08.004</idno>
	</analytic>
	<monogr>
		<title level="j">Trends cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="467" to="474" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagination inflation: Imagining a childhood event inflates confidence that it occurred</title>
		<author>
			<persName><forename type="first">M</forename><surname>Garry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>Loftus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Sherman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychon. bulletin &amp; review</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="208" to="214" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Misinformation and memory: the creation of new memories</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>Loftus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Hoffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. experimental psychology: Gen</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="page">100</biblScope>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagination inflation for action events: Repeated imaginings lead to illusory recollections</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Goff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Roediger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mem. &amp; Cogn</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="20" to="33" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A new intuitionism: Meaning, memory, and development in Fuzzy-Trace Theory</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">F</forename><surname>Reyna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Judgm. &amp; Decis. Mak</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="332" to="359" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On the prediction of occurrence of particular verbal intrusions in immediate recall</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. experimental psychology</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="1959">1959</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">False memories and fantastic beliefs: 15 years of the drm illusion</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Gallo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mem. &amp; cognition</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="833" to="848" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">What research paradigms have cognitive psychologists used to study &quot;false memory</title>
		<author>
			<persName><forename type="first">K</forename><surname>Pezdek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Conscious. cognition</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="2" to="17" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>and what are the implications of these choices?</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">False claims about false memory research</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Wade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Conscious. cognition</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="18" to="28" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The deese-roediger-mcdermott (drm) task: A simple cognitive paradigm to investigate false memories in the laboratory</title>
		<author>
			<persName><forename type="first">E</forename><surname>Pardilla-Delgado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Payne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. visualized experiments: JoVE</title>
		<imprint>
			<biblScope unit="volume">54793</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Predicting memory from the network structure of naturalistic events</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">4235</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Geometric models reveal behavioural and neural signatures of transforming experiences into memories</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Heusser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Fitzpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41562-021-01051-6</idno>
	</analytic>
	<monogr>
		<title level="j">Nat. Hum. Behav</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="905" to="919" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rapid memory reactivation at movie event boundaries promotes episodic encoding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Baldassano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fuentemilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurosci</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="8538" to="8548" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A functional neuroimaging dataset acquired during naturalistic movie watching and narrated recall of a series of short cinematic films</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Hasson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Brief</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page">108788</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">How developments in natural language processing help us in understanding human behaviour</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41562-024-01938-0</idno>
	</analytic>
	<monogr>
		<title level="j">Nat. Hum. Behav</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1877" to="1889" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Large-scale study of human memory for meaningful narratives</title>
		<author>
			<persName><forename type="first">A</forename><surname>Georgiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Can</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Katkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tsodyks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Learn. &amp; Mem</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">54043</biblScope>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The neural architecture of language: Integrative modeling converges on predictive processing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schrimpf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl. Acad. Sci</title>
		<meeting>Natl. Acad. Sci</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="page">2105646118</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Large language models can segment narrative events similarly to humans</title>
		<author>
			<persName><forename type="first">S</forename><surname>Michelmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Toneva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behav. Res. Methods</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Event segmentation applications in large language model enabled automated recall assessments</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Panela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Barnett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Barense</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Herrmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2502.13349</idno>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Kahana</surname></persName>
		</author>
		<title level="m">Foundations of human memory</title>
		<imprint>
			<publisher>OUP USA</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Studying memory narratives with natural language processing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Fenerci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Addis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bellana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sheldon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends Cogn. Sci</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Large language model</title>
		<ptr target="https://openai.com/research/gpt-4" />
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
		<respStmt>
			<orgName>OpenAI</orgName>
		</respStmt>
	</monogr>
	<note>Gpt</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adaptive constructive processes and the future of memory</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Schacter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. Psychol</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page">603</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The ghosts of past and future</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Schacter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Addis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">445</biblScope>
			<biblScope unit="page" from="27" to="27" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Flexible retrieval: When true inferences produce false memories</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Schacter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Exp. Psychol. Learn. Mem. Cogn</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page">335</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Open-category classification by adversarial sample generation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08722</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Managing the unknown in machine learning: Definitions, related areas, recent advances, and prospects</title>
		<author>
			<persName><forename type="first">M</forename><surname>Barcina-Blanco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Lobo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Garcia-Bringas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Del Ser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">128073</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">How fuzzy-trace theory predicts true and false memories for words, sentences, and narratives</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">F</forename><surname>Reyna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Corbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Weldon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Brainerd</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jarmac.2015.12.003</idno>
	</analytic>
	<monogr>
		<title level="j">J. Appl. Res. Mem. Cogn</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fuzzy-trace theory: An interim synthesis</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">F</forename><surname>Reyna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Brainerd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Learn. individual Differ</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="75" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Using large language models in psychology</title>
		<author>
			<persName><forename type="first">D</forename><surname>Demszky</surname></persName>
		</author>
		<idno type="DOI">10.1038/s44159-023-00241-5</idno>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Psychol</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="688" to="701" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Modeling memories, predicting prospections: Automated scoring of autobiographical detail narration using large language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Klus</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Heart-felt narratives: Tracing empathy and narrative style in personal stories with llms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Breazeal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sap</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.17633</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Categorical and associative relations increase false memory relative to purely associative relations</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Coane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Mcbride</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-L</forename><surname>Termonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Cutting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mem. &amp; cognition</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="37" to="49" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Understanding autobiographical memory content using computational text analysis</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stastna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Fernandes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Memory</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1267" to="1287" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Differences in the content and coherence of autobiographical memories between younger and older adults: Insights from text analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sheldon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol. aging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page">59</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Masked and permuted pre-training for language understanding</title>
		<author>
			<persName><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Mpnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="16857" to="16867" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Semeval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
		<meeting>the 11th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">SemEval-2017. 2017</date>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Gpt-2 small</title>
		<ptr target="https://huggingface.co/openai-community/gpt2" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
		<respStmt>
			<orgName>OpenAI</orgName>
		</respStmt>
	</monogr>
	<note>Pretrained 117M parameter version of GPT-2</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Language Models are Unsupervised Multitask Learners</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Narrativexl: a large-scale dataset for long-term memory models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Moskvichev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-V</forename><surname>Mai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.13877</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">False recall in the deese-roediger-mcdermott paradigm: The roles of gist and associative strength</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Cann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mcrae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Q. J. Exp. Psychol</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="1515" to="1542" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Semantic similarity between old and new items produces false alarms in recognition memory</title>
		<author>
			<persName><forename type="first">M</forename><surname>Montefinese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Zannino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ambrosini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol. research</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="785" to="794" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Manipulations of list type in the drm paradigm: A review of how structural and conceptual similarity affect false memory</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Coane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Psychol</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">668550</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Moment-by-moment tracking of naturalistic learning and its underlying hippocampo-cortical interactions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Michelmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">5394</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Memory and surprisal in human sentence comprehension</title>
		<author>
			<persName><forename type="first">R</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sentence processing</title>
		<imprint>
			<publisher>Psychology Press</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="78" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Prediction Errors Disrupt Hippocampal Representations and Update Episodic Memories</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Sinclair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Manalili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">K</forename><surname>Brunec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Adcock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Barense</surname></persName>
		</author>
		<idno type="DOI">10.1101/2020.09.29.319418</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>bioRxiv 2020.09.29.319418</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Contextual variability and serial position effects in free recall</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Kahana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Exp. Psychol. Learn. Mem. Cogn</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">923</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Universal sentence encoder</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.11175</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Pythia: A suite for analyzing large language models across training and scaling</title>
		<author>
			<persName><forename type="first">S</forename><surname>Biderman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.01373</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Fuzzy-trace theory and false memory</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Brainerd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">F</forename><surname>Reyna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Curr. Dir. Psychol. Sci</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="164" to="169" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The seven sins of memory: insights from psychology and cognitive neuroscience</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Schacter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. psychologist</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page">182</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Annotation alignment: Comparing llm and human annotations of conversational safety</title>
		<author>
			<persName><forename type="first">R</forename><surname>Movva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pierson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.06369</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Event structure in perception and conception</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Zacks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tversky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol. bulletin</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Discovering event structure in continuous narrative perception and memory</title>
		<author>
			<persName><forename type="first">C</forename><surname>Baldassano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="709" to="721" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Representation of real-world event schemas during narrative perception</title>
		<author>
			<persName><forename type="first">C</forename><surname>Baldassano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Hasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Norman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurosci</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="9689" to="9699" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">The causal structure and computational value of narratives</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Bornstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends Cogn. Sci</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Moca: Measuring human-language model alignment on causal and moral judgment tasks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="78360" to="78393" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Fuzzy-trace theory and children&apos;s false memories</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Brainerd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">F</forename><surname>Reyna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. experimental child psychology</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="81" to="129" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Both associative activation and thematic extraction count, but thematic false memories are more easily rejected</title>
		<author>
			<persName><forename type="first">P</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Garcia-Marques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Albuquerque</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Memory</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1024" to="1040" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Confidence, not consistency, characterizes flashbulb memories</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Talarico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol. science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="455" to="461" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Flashbulb memories</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hirst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Phelps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Curr. Dir. Psychol. Sci</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="36" to="41" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Factors that determine false recall: A multiple regression analysis</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Roediger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Gallo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychon. bulletin &amp; review</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="385" to="407" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Assessing activation of true and false memory traces: A study using the drm paradigm</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sergi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Senese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pisani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Nigro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol. Belg</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="171" to="179" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">The seven sins of memory: An update</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Schacter</surname></persName>
		</author>
		<idno type="DOI">10.1080/09658211.2021.1873391</idno>
	</analytic>
	<monogr>
		<title level="j">Memory</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="37" to="42" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Semantic and episodic processes differently predict false memories in the drm task</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rinaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mazzoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vecchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Reports</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">256</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Reconstructing memory: The incredible eyewitness</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>Loftus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Jurimetrics J</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="188" to="193" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Planting misinformation in the human mind: A 30-year investigation of the malleability of memory</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>Loftus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Learn. &amp; memory</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="361" to="366" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">What&apos;s in a name for memory errors? implications and ethical issues arising from the use of the term&quot; false memory&quot; for errors in memory for details</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Deprince</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Allard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Freyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ethics &amp; Behav</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="201" to="233" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">What science tells us about false and repressed memories</title>
		<author>
			<persName><forename type="first">H</forename><surname>Otgaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Howe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Patihis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Memory</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="16" to="21" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><surname>Truthfulqa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.07958</idno>
		<title level="m">Measuring how models mimic human falsehoods</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Holistic evaluation of language models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.09110</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">A survey on in-context learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.00234</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">A survey on in-context learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Dong</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2024.emnlp-main.64</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Al-Onaizan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Bansal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y.-N</forename><surname>Chen</surname></persName>
		</editor>
		<meeting>the 2024 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Miami, Florida, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="1107" to="1128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">jspsych: A javascript library for creating behavioral experiments in a web browser</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>De Leeuw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behav. research methods</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName><surname>Sentence-Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10084</idno>
		<title level="m">Sentence embeddings using siamese bert-networks</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">On the predictive power of neural language models for human real-time comprehension behavior</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Wilcox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gauthier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Levy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.01912</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Memory in humans and deep language models: Linking hypotheses for model augmentation. Paper at Memory in Real and Artificial Intelligence Workshop</title>
		<author>
			<persName><forename type="first">O</forename><surname>Raccah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Willke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Poeppel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Vo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Bayesian surprise predicts human event segmentation in story listening</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cogn. science</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page">13343</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Word frequency and predictability dissociate in naturalistic reading</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Open Mind</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="177" to="201" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">The hippocampus constructs narrative memories across distant events</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">I</forename><surname>Cohn-Sheehy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Curr. Biol</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="4935" to="4945" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Narratives bridge the divide between distant events in episodic memory</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">I</forename><surname>Cohn-Sheehy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mem. &amp; Cogn</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="478" to="494" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Machine-learning as a validated tool to characterize individual differences in free recall of naturalistic events</title>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Houser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Murty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychon. Bull. &amp; Rev</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="308" to="316" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Frequency explains the inverse correlation of large language models&apos; size, training data amount, and surprisal&apos;s fit to reading times</title>
		<author>
			<persName><forename type="first">B.-D</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Schuler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.02255</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03771</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">lme4: Linear mixed-effects models using eigen and s4. R package version</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bates</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
